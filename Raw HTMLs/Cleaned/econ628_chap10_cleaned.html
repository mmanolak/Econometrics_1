<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 10: Introduction – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap11.html" rel="next"/>
<link href="../chapters/chap09.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 10: Introduction</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level2" id="sampling-theory">
<h2 class="anchored" data-anchor-id="sampling-theory">10.1 SAMPLING THEORY</h2>
<p>In this section, we discuss the basics of sampling theory. Suppose we have a population random variable denoted by <span class="math inline">\(X\)</span>. We wish to learn something about this population based on a sample of size <span class="math inline">\(n\)</span>, <span class="math inline">\(\{X_1, \dots, X_n\}\)</span>, drawn from the population.</p>
<ul>
<li>The leading case is <strong>random sampling</strong> where each draw is done “with replacement” (e.g., from an “infinite” population) and in the same way. In this case, each <span class="math inline">\(X_j\)</span> is <strong>independent and identically distributed</strong> with the same distribution as the population.</li>
<li>In some applications, we may weaken the assumption that the population is the same for each observation, allowing for <strong>heterogeneous individuals</strong>. In time series, we might allow the sampled variables to possess <strong>dependence</strong>, e.g., <span class="math inline">\(X_j\)</span> and <span class="math inline">\(X_k\)</span> may be correlated.</li>
<li>In survey sampling, we often consider <strong>stratified sampling</strong>, where we divide a population into groups (e.g., men and women) and then construct a random sample from each group. This is useful when one of the subgroups is quite rare, in which case ordinary random sampling is likely to draw a sample that contains no members of that group. Stratified sampling should be combined with a reweighting procedure if one seeks to understand the original population.</li>
<li>A big issue with survey sampling is <strong>nonresponse</strong>, where some units do not respond. This may lead to <strong>sample selection bias</strong>, where the nonresponders are somehow different from the responders in a way that reflects the objective of the study.</li>
<li>If the population is finite, i.e., consists of <span class="math inline">\(\{x_1, \dots, x_N\}\)</span>, we may sample <strong>without replacement</strong>. We draw <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> sequentially without returning the chosen values to the population, where <span class="math inline">\(n \le N\)</span>. This causes heterogeneity and dependence since if <span class="math inline">\(X_1 = x_1\)</span>, then the distribution of <span class="math inline">\(X_2\)</span> is affected. In particular, its support changes since it cannot take the value <span class="math inline">\(x_1\)</span>. This setting is difficult to analyze, and in many cases, one may assume that <span class="math inline">\(N\)</span> is very large, so as to ignore the issue.</li>
</ul>
<p>A distinction is made between <strong>observational data</strong> and <strong>experimental data</strong>. In the latter case, one has some measure of control of the experimental settings that determined the data, whereas in the former case no control is possible.</p>
</section>
<section class="level2" id="sample-statistics">
<h2 class="anchored" data-anchor-id="sample-statistics">10.2 SAMPLE STATISTICS</h2>
<p><strong>Data description</strong> is a common first step in statistical analysis. Graphical tools such as box plots, histograms, and empirical distribution functions are commonly used. For discrete data, a common way of presentation is the <strong>Contingency Table</strong>.</p>
<section class="level3" id="example-contingency-table">
<h3 class="anchored" data-anchor-id="example-contingency-table">Example: Contingency Table</h3>
<p>Consider the following <span class="math inline">\(2 \times 2\)</span> contingency table relating smoking status to cancer status:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: right;">Cancer</th>
<th style="text-align: right;">Not Cancer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Smokers</td>
<td style="text-align: right;">643</td>
<td style="text-align: right;">812</td>
</tr>
<tr class="even">
<td style="text-align: left;">Non-smokers</td>
<td style="text-align: right;">117</td>
<td style="text-align: right;">911</td>
</tr>
</tbody>
</table>
<p>A <strong>statistic</strong> is any quantity we can write as a measurable function of the sample, i.e., <span class="math inline">\(T(X_1, \dots, X_n)\)</span> for some function <span class="math inline">\(T: \mathbb{R}^n \to \mathbb{R}\)</span>. Important statistics include:</p>
<ul>
<li><strong>Sample mean</strong>: <span class="math inline">\(\bar{X} = \dfrac{1}{n} \sum_{i=1}^{n} X_i\)</span></li>
<li><strong>Sample variance</strong>: <span class="math inline">\(s^2 = \dfrac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2\)</span></li>
<li><strong>Unbiased sample variance</strong>: <span class="math inline">\(s_{*}^2 = \dfrac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})^2\)</span></li>
<li><strong>Sample covariance</strong>: <span class="math inline">\(s_{XY} = \dfrac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})\)</span></li>
<li><strong>Empirical distribution function</strong>: <span class="math inline">\(F_n(x) = \dfrac{1}{n} \sum_{i=1}^{n} \mathbb{1}(X_i \le x)\)</span></li>
</ul>
<p>The <strong>Kernel Density Estimate</strong> is an estimator of the density <span class="math inline">\(f(x)\)</span> of a random variable. It is defined as:</p>
<p><span class="math inline">\(\hat{f}(x) = \dfrac{1}{nh} \sum_{i=1}^{n} K \left( \dfrac{x - X_i}{h} \right)\)</span></p>
<p>where <span class="math inline">\(K(\cdot)\)</span> is a density function (e.g., standard normal) and <span class="math inline">\(h\)</span> is a bandwidth. This gives a smooth curve compared with the lumpy histogram. The smoothness of the curve is controlled by <span class="math inline">\(h\)</span>: large <span class="math inline">\(h\)</span> will make a smooth curve and small <span class="math inline">\(h\)</span> will make a very wiggly curve.</p>
</section>
<section class="level3" id="example-sp500-daily-returns">
<h3 class="anchored" data-anchor-id="example-sp500-daily-returns">Example: SP500 Daily Returns</h3>
<p>The text provides an example using the empirical cumulative distribution function (CDF) and kernel density estimate of daily returns on the S&amp;P500 index.</p>
<ul>
<li>The empirical CDF is compared to the standard normal CDF (Figure 10.1).</li>
<li>The kernel density estimate is compared to the standard normal density (Figure 10.2).</li>
<li>The probability of observing a large negative return (<span class="math inline">\(X_t \le -25\)</span>) is calculated under different distributional assumptions:
<ul>
<li>If stock returns were normally distributed: <span class="math inline">\(\Pr(X_t \le -25) = 3 \times 10^{-138}\)</span></li>
<li>If stock returns were logistic distributed: <span class="math inline">\(\Pr(X_t \le -25) = 1 \times 10^{-11}\)</span></li>
<li>If stock returns were Cauchy distributed: <span class="math inline">\(\Pr(X_t \le -25) = 0.13\)</span></li>
</ul></li>
</ul>
</section>
<section class="level3" id="example-descriptive-statistics-of-sp500-returns-by-frequency">
<h3 class="anchored" data-anchor-id="example-descriptive-statistics-of-sp500-returns-by-frequency">Example: Descriptive Statistics of SP500 Returns by Frequency</h3>
<p>The text presents descriptive statistics for the returns on the S&amp;P500 index for the period 1955-2002 for three different data frequencies (daily, weekly, monthly).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;">Statistic</th>
<th style="text-align: right;">Daily</th>
<th style="text-align: right;">Weekly</th>
<th style="text-align: right;">Monthly</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">Mean (×100)</td>
<td style="text-align: right;">0.029</td>
<td style="text-align: right;">0.141</td>
<td style="text-align: right;">0.606</td>
</tr>
<tr class="even">
<td style="text-align: left;">St. Deviation (×100)</td>
<td style="text-align: right;">0.038</td>
<td style="text-align: right;">0.200</td>
<td style="text-align: right;">0.903</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Skewness</td>
<td style="text-align: right;">-1.546</td>
<td style="text-align: right;">-0.375</td>
<td style="text-align: right;">-0.589</td>
</tr>
<tr class="even">
<td style="text-align: left;">Excess Kurtosis</td>
<td style="text-align: right;">43.334</td>
<td style="text-align: right;">6.521</td>
<td style="text-align: right;">5.588</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Minimum</td>
<td style="text-align: right;">-25.422</td>
<td style="text-align: right;">-6.577</td>
<td style="text-align: right;">-5.984</td>
</tr>
<tr class="even">
<td style="text-align: left;">Maximum</td>
<td style="text-align: right;">9.623</td>
<td style="text-align: right;">6.534</td>
<td style="text-align: right;">3.450</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Sample Size</td>
<td style="text-align: right;">11893</td>
<td style="text-align: right;">2475</td>
<td style="text-align: right;">568</td>
</tr>
<tr class="even">
<td style="text-align: left;">Number zeros</td>
<td style="text-align: right;">81</td>
<td style="text-align: right;">15</td>
<td style="text-align: right;">3</td>
</tr>
</tbody>
</table>
</section>
<section class="level3" id="properties-of-descriptive-statistics">
<h3 class="anchored" data-anchor-id="properties-of-descriptive-statistics">10.2.1 Properties of Descriptive Statistics</h3>
<p>The descriptive statistics can be analyzed in terms of their means, variances, and large sample distributions.</p>
</section>
<section class="level3" id="theorem-10.1-properties-of-descriptive-statistics">
<h3 class="anchored" data-anchor-id="theorem-10.1-properties-of-descriptive-statistics">Theorem 10.1 Properties of Descriptive Statistics</h3>
<p>Let <span class="math inline">\(X_1, \dots, X_n\)</span> be a random sample from a population with mean <span class="math inline">\(\mu\)</span>, variance <span class="math inline">\(\sigma^2\)</span>, and kurtosis <span class="math inline">\(\kappa_4\)</span>. Then, <span class="math inline">\(E\bar{X} = \mu\)</span> and <span class="math inline">\(\text{var}(\bar{X}) = \sigma^2/n\)</span>. Furthermore, as <span class="math inline">\(n \to \infty\)</span>:</p>
<ul>
<li><span class="math inline">\(\bar{X} \overset{p}{\to} \mu\)</span></li>
<li><span class="math inline">\(s^2 \overset{p}{\to} \sigma^2\)</span></li>
<li><span class="math inline">\(\sqrt{n}(\bar{X} - \mu) \overset{d}{\to} N(0, \sigma^2)\)</span></li>
<li><span class="math inline">\(\sqrt{n}(s^2 - \sigma^2) \overset{d}{\to} N(0, (\kappa_4 + 2)\sigma^4)\)</span></li>
</ul>
<p><strong>Proof:</strong></p>
<p>First, we have</p>
<p><span class="math inline">\(E\bar{X} = E \left( \dfrac{1}{n} \sum_{i=1}^{n} X_i \right) = \dfrac{1}{n} \sum_{i=1}^{n} E(X_i) = \dfrac{1}{n} (n\mu) = \mu\)</span></p>
<p>Furthermore,</p>
<p><span class="math inline">\(\begin{aligned}
\text{var}(\bar{X}) &amp;= \text{var} \left( \dfrac{1}{n} \sum_{i=1}^{n} X_i \right) \\
&amp;= \dfrac{1}{n^2} \text{var} \left( \sum_{i=1}^{n} X_i \right) \\
&amp;= \dfrac{1}{n^2} \sum_{i=1}^{n} \text{var}(X_i) \quad \text{(by independence)} \\
&amp;= \dfrac{1}{n^2} (n\sigma^2) \quad \text{(by identical distribution)} \\
&amp;= \dfrac{\sigma^2}{n}
\end{aligned}\)</span></p>
<p>For the sample variance,</p>
<p><span class="math inline">\(s^2 = \dfrac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2 = \dfrac{1}{n} \sum_{i=1}^{n} (X_i - \mu + \mu - \bar{X})^2\)</span></p>
<p><span class="math inline">\(\begin{aligned}
s^2 &amp;= \dfrac{1}{n} \sum_{i=1}^{n} (X_i - \mu)^2 + \dfrac{1}{n} \sum_{i=1}^{n} (\mu - \bar{X})^2 + \dfrac{2}{n} \sum_{i=1}^{n} (X_i - \mu)(\mu - \bar{X}) \\
&amp;= \dfrac{1}{n} \sum_{i=1}^{n} (X_i - \mu)^2 - (\mu - \bar{X})^2
\end{aligned}\)</span></p>
<p>We can calculate the exact mean and variance of <span class="math inline">\(s^2\)</span>, but we defer this to later and use a large sample argument. By the Slutsky theorem, <span class="math inline">\(s^2 \overset{p}{\to} \sigma^2\)</span> since the second term converges to zero in probability, and the first term converges to <span class="math inline">\(\sigma^2\)</span> because <span class="math inline">\(E[(X_i - \mu)^2] = \sigma^2\)</span>.</p>
<p>Furthermore,</p>
<p><span class="math inline">\(\sqrt{n}(s^2 - \sigma^2) = \dfrac{1}{\sqrt{n}} \sum_{i=1}^{n} ((X_i - \mu)^2 - \sigma^2) + R_n\)</span></p>
<p>where <span class="math inline">\(R_n = -\sqrt{n}(\mu - \bar{X})^2 \overset{p}{\to} 0\)</span>. By the Lindeberg-Levy Central Limit Theorem (CLT), with <span class="math inline">\(Z_i = (X_i - \mu)^2 - \sigma^2\)</span>,</p>
<p><span class="math inline">\(\sqrt{n}(s^2 - \sigma^2) \overset{d}{\to} N(0, \text{var}(Z_i))\)</span></p>
<p>We have</p>
<p><span class="math inline">\(\begin{aligned}
\text{var}(Z_i) &amp;= E[((X_i - \mu)^2 - \sigma^2)^2] \\
&amp;= E[(X_i - \mu)^4] - \sigma^4 \\
&amp;= E[(X_i - \mu)^4] - \sigma^4 \\
&amp;= \sigma^4 \left( \dfrac{E[(X_i - \mu)^4]}{\sigma^4} - 1 \right) \\
&amp;= \sigma^4 (\kappa_4 + 2 - 1) \\
&amp;= (\kappa_4 + 2)\sigma^4
\end{aligned}\)</span></p>
<p>This equals <span class="math inline">\(3\sigma^4 - \sigma^4 = 2\sigma^4\)</span> when <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span>.</p>
<p>Finally, we note that</p>
<p><span class="math inline">\(\text{cov}(\bar{X}, s^2) = \dfrac{1}{n} E[(X_i - \mu)^3]\)</span></p>
<p>If <span class="math inline">\(E[(X - \mu)^3] = 0\)</span>, then <span class="math inline">\(\text{cov}(\bar{X}, s^2) = 0\)</span>.</p>
</section>
<section class="level3" id="exact-properties-specific-to-the-normal-distribution">
<h3 class="anchored" data-anchor-id="exact-properties-specific-to-the-normal-distribution">10.2.2 Exact Properties Specific to the Normal Distribution</h3>
<p>Here, we consider some exact distributional results involving the sample mean and variance from a normal distribution.</p>
</section>
<section class="level3" id="theorem-10.2">
<h3 class="anchored" data-anchor-id="theorem-10.2">Theorem 10.2</h3>
<p>Suppose that <span class="math inline">\(X_i \sim N(\mu, \sigma^2)\)</span>. Then we have the following results:</p>
<ol type="1">
<li><span class="math inline">\(\bar{X} \sim N(\mu, \sigma^2/n)\)</span></li>
<li><span class="math inline">\(\bar{X}\)</span> and <span class="math inline">\(s_*^2\)</span> (or <span class="math inline">\(s^2\)</span>) are independent</li>
<li><span class="math inline">\(\dfrac{(n-1)s_*^2}{\sigma^2}\)</span> has a chi-squared distribution with <span class="math inline">\((n-1)\)</span> degrees of freedom</li>
<li><span class="math inline">\(\dfrac{\bar{X} - \mu}{s_*/\sqrt{n}}\)</span> has the <span class="math inline">\(t(n-1)\)</span> distribution</li>
<li>Suppose that <span class="math inline">\(X_i \sim N(\mu_X, \sigma_X^2)\)</span> and <span class="math inline">\(Y_i \sim N(\mu_Y, \sigma_Y^2)\)</span> with <span class="math inline">\(X_i, Y_i\)</span> independent of each other. Then <span class="math inline">\(\dfrac{s_{*X}^2 / \sigma_X^2}{s_{*Y}^2 / \sigma_Y^2} \sim F(n, m)\)</span>.</li>
</ol>
<p><strong>Proof:</strong></p>
<p><strong>Proof of (1)</strong> is by characteristic function (c.f.) or moment generating function (m.g.f.) argument. We have</p>
<p><span class="math inline">\(\phi_{\bar{X}}(t) = \phi_X(t/n)^n\)</span></p>
<p>For a normally distributed random variable, the characteristic function and moment generating function are respectively:</p>
<p><span class="math inline">\(\phi_X(t) = e^{i\mu t - \frac{1}{2}\sigma^2 t^2}; \quad \text{mgf}_X(t) = e^{\mu t + \frac{1}{2}\sigma^2 t^2}\)</span></p>
<p>Therefore,</p>
<p><span class="math inline">\(\phi_{\bar{X}}(t) = \left( e^{i\mu (t/n) - \frac{1}{2}\sigma^2 (t/n)^2} \right)^n = e^{i\mu t - \frac{1}{2}(\sigma^2/n) t^2}\)</span></p>
<p>which is the c.f. of a normal random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2/n\)</span>.</p>
<p><strong>Proof of (2).</strong> Let <span class="math inline">\(n=2\)</span>, <span class="math inline">\(X_1 \sim N(0, 1)\)</span>, <span class="math inline">\(X_2 \sim N(0, 1)\)</span>, and</p>
<p><span class="math inline">\(\bar{X} = \dfrac{X_1 + X_2}{2}\)</span></p>
<p>Then</p>
<p><span class="math inline">\(s_*^2 = \left( X_1 - \dfrac{X_1 + X_2}{2} \right)^2 + \left( X_2 - \dfrac{X_1 + X_2}{2} \right)^2 = \left( \dfrac{X_1 - X_2}{2} \right)^2 + \left( \dfrac{X_2 - X_1}{2} \right)^2 = \dfrac{(X_1 - X_2)^2}{2}\)</span></p>
<p>Claim that <span class="math inline">\(Z_1 = X_1 + X_2\)</span> and <span class="math inline">\(Z_2 = X_1 - X_2\)</span> are independent. The joint m.g.f. is</p>
<p><span class="math inline">\(\begin{aligned}
E[e^{t_1 Z_1 + t_2 Z_2}] &amp;= E[e^{t_1(X_1 + X_2) + t_2(X_1 - X_2)}] \\
&amp;= E[e^{(t_1 + t_2)X_1 + (t_1 - t_2)X_2}] \\
&amp;= E[e^{(t_1 + t_2)X_1}] E[e^{(t_1 - t_2)X_2}] \\
&amp;= e^{\frac{1}{2}(t_1 + t_2)^2} e^{\frac{1}{2}(t_1 - t_2)^2} \\
&amp;= e^{t_1^2 + t_2^2} \\
&amp;= E[e^{t_1 Z_1}] E[e^{t_2 Z_2}]
\end{aligned}\)</span></p>
<p>But <span class="math inline">\(E[e^{t_1 Z_1}] = e^{t_1^2}\)</span> and <span class="math inline">\(E[e^{t_2 Z_2}] = e^{t_2^2}\)</span>. A simpler way of seeing this is to note that <span class="math inline">\(Z_1\)</span> and <span class="math inline">\(Z_2\)</span> are normally distributed since they are both a linear combination of the same normals, and are mutually uncorrelated, since</p>
<p><span class="math inline">\(E[Z_1 Z_2] = E[(X_1 - X_2)(X_1 + X_2)] = E[X_1^2 - X_2^2] = 0\)</span></p>
<p>For normal distributions, uncorrelatedness implies independence.</p>
<p>Finally, suppose for simplicity that <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span>. Then we claim that</p>
<p><span class="math inline">\(\sum_{i=1}^{n} (X_i - \bar{X})^2 = \sum_{i=1}^{n} X_i^2 - n\bar{X}^2\)</span></p>
<p>is chi-squared with <span class="math inline">\(n\)</span> degrees of freedom. We have</p>
<p><span class="math inline">\(\sqrt{n}\bar{X} \sim N(0, 1) \implies n\bar{X}^2 \sim \chi^2(1)\)</span></p>
<p>while by independence of sample mean and variance</p>
<p><span class="math inline">\(E[e^{t \sum_{i=1}^{n} X_i^2}] = E[e^{t \sum_{i=1}^{n} (X_i - \bar{X})^2}] E[e^{t n \bar{X}^2}]\)</span></p>
<p>But</p>
<p><span class="math inline">\(E[e^{t \sum_{i=1}^{n} X_i^2}] = \left( E[e^{tX_i^2}] \right)^n = \left( \dfrac{1}{1-2t} \right)^{n/2}\)</span></p>
<p>and</p>
<p><span class="math inline">\(E[e^{tn\bar{X}^2}] = \dfrac{1}{(1-2t)^{1/2}}\)</span></p>
<p>Therefore,</p>
<p><span class="math inline">\(E[e^{t \sum_{i=1}^{n} (X_i - \bar{X})^2}] = \left( \dfrac{1}{1-2t} \right)^{(n-1)/2}\)</span></p>
<p>which is the m.g.f. of a <span class="math inline">\(\chi^2(n-1)\)</span> random variable.</p>
<p>The proofs of (4) and (5) are omitted, but they follow from the definition of <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> random variables and the properties already established.</p>
</section>
</section>
<section class="level2" id="statistical-principles">
<h2 class="anchored" data-anchor-id="statistical-principles">10.3 STATISTICAL PRINCIPLES</h2>
<p>There are two general paradigms in statistical inference: <strong>Frequentist</strong> and <strong>Bayesian</strong>.</p>
<p>In the <strong>Frequentist approach</strong>, we have a model <span class="math inline">\(\mathcal{P}\)</span>, which is a collection of distributions or probability measures <span class="math inline">\(P\)</span>. We obtain a sample <span class="math inline">\(X^n = \{X_1, \dots, X_n\}\)</span> from some <span class="math inline">\(P\)</span>, and we estimate various quantities <span class="math inline">\(\theta(P)\)</span>, called <strong>parameters</strong>, by some function of the sample. The properties of <span class="math inline">\(\hat{\theta}\)</span> are calculated treating this quantity as a random variable which arose as one realization of the sampling process. In the classical parametric approach, <span class="math inline">\(\{\mathcal{P}_{\theta}, \theta \in \Theta\}\)</span>, where <span class="math inline">\(\Theta \subset \mathbb{R}^k\)</span> is the parameter space and for each <span class="math inline">\(\theta\)</span>, <span class="math inline">\(P_{\theta}\)</span> is a probability measure [or equivalently density function when we are talking about continuous variables]. The objective is to learn about <span class="math inline">\(\theta\)</span> from the data. In this case, the entire distribution of the data is specified apart from the unknown parameters, i.e., correct specification assumes that <span class="math inline">\(P \in \{P_{\theta}, \theta \in \Theta\}\)</span>.</p>
<section class="level3" id="example-10.1">
<h3 class="anchored" data-anchor-id="example-10.1">Example 10.1</h3>
<p>Suppose that <span class="math inline">\(X_i\)</span> are i.i.d. with unknown distribution <span class="math inline">\(P\)</span>. Let <span class="math inline">\(\theta = E(X)\)</span> be the parameter of interest. In this case, very little is specified about <span class="math inline">\(P\)</span>.</p>
</section>
<section class="level3" id="example-10.2">
<h3 class="anchored" data-anchor-id="example-10.2">Example 10.2</h3>
<p>Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\theta = (\mu, \sigma^2) \in \mathbb{R}^2\)</span> are the parameters of interest. Here, the entire distribution is specified apart from two unknown quantities.</p>
<p>The <strong>likelihood function</strong> plays an important role in parametric statistics. It is defined as</p>
<p><span class="math inline">\(L(\theta|X^n) = f(X_1, \dots, X_n|\theta)\)</span></p>
<p>i.e., it is the density of the data under the parameter value <span class="math inline">\(\theta\)</span>, but treated as a function of <span class="math inline">\(\theta\)</span> for given data. When <span class="math inline">\(X_1, \dots, X_n\)</span> are i.i.d., we may write the likelihood and log-likelihood as</p>
<p><span class="math inline">\(L(\theta|X^n) = \prod_{i=1}^{n} f(X_i|\theta)\)</span></p>
<p><span class="math inline">\(l(\theta|X^n) = \log L(\theta|X^n) = \sum_{i=1}^{n} \log f(X_i|\theta) = \sum_{i=1}^{n} l_i(\theta|X_i)\)</span></p>
<p>where <span class="math inline">\(l_i(\theta|X_i) = \log f(X_i|\theta)\)</span>. The likelihood function is used to generate estimators and test statistics. The <strong>Maximum Likelihood Estimator</strong> <span class="math inline">\(\hat{\theta}_{MLE}\)</span> maximizes <span class="math inline">\(L(\theta|X^n)\)</span> with respect to <span class="math inline">\(\theta\)</span> or equivalently maximizes <span class="math inline">\(l(\theta|X^n)\)</span> with respect to <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="example-10.3-bernoulli-population">
<h3 class="anchored" data-anchor-id="example-10.3-bernoulli-population">Example 10.3 Bernoulli Population</h3>
<p>Consider the Bernoulli population</p>
<p><span class="math inline">\(X = \begin{cases} 1 &amp; \text{with probability } p \\ 0 &amp; \text{with probability } 1-p \end{cases}\)</span></p>
<p>where <span class="math inline">\(p \in [0, 1]\)</span> is an unknown parameter. We observe a random sample <span class="math inline">\(X_1, \dots, X_n\)</span> with <span class="math inline">\(k\)</span> ones. The likelihood function is</p>
<p><span class="math inline">\(L(p|X^n) = \prod_{i=1}^{n} p^{X_i} (1-p)^{1-X_i} = \binom{n}{k} p^k (1-p)^{n-k}\)</span></p>
</section>
<section class="level3" id="example-10.4-normal-population">
<h3 class="anchored" data-anchor-id="example-10.4-normal-population">Example 10.4 Normal Population</h3>
<p>Suppose that <span class="math inline">\(X_i\)</span> are i.i.d. normal with parameters <span class="math inline">\(\mu, \sigma^2\)</span>. The likelihood function is</p>
<p><span class="math inline">\(L(\mu, \sigma^2|X^n) = \prod_{i=1}^{n} \dfrac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\dfrac{(X_i - \mu)^2}{2\sigma^2} \right)\)</span></p>
<p><strong>Identification</strong> is a key question in frequentist methods.</p>
</section>
<section class="level3" id="definition-10.1">
<h3 class="anchored" data-anchor-id="definition-10.1">Definition 10.1</h3>
<p>Two parameter points <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta'\)</span> in a parameter set <span class="math inline">\(\Theta\)</span> are said to be <strong>observationally equivalent</strong> if for all <span class="math inline">\(X^n\)</span></p>
<p><span class="math inline">\(L(\theta|X^n) = L(\theta'|X^n)\)</span></p>
</section>
<section class="level3" id="definition-10.2">
<h3 class="anchored" data-anchor-id="definition-10.2">Definition 10.2</h3>
<p>A parameter point <span class="math inline">\(\theta\)</span> in <span class="math inline">\(\Theta\)</span> is said to be <strong>identifiable</strong> if there is no other <span class="math inline">\(\theta' \in \Theta\)</span> that is observationally equivalent to it. If there does not exist a single <span class="math inline">\(X^n\)</span> for which the above holds for some <span class="math inline">\(\theta' \ne \theta\)</span>, then the parameter is <strong>unidentified</strong>.</p>
</section>
<section class="level3" id="example-10.5">
<h3 class="anchored" data-anchor-id="example-10.5">Example 10.5</h3>
<p>Suppose that <span class="math inline">\(X_i\)</span> are i.i.d. normal with mean <span class="math inline">\(\mu_1 + \mu_2\)</span>, where <span class="math inline">\(\mu_1\)</span> and <span class="math inline">\(\mu_2\)</span> are unknown, and variance one. Then for any <span class="math inline">\(\mu_1', \mu_2'\)</span> with <span class="math inline">\(\mu_1' + \mu_2' = \mu_1 + \mu_2\)</span></p>
<p><span class="math inline">\(l(\mu_1, \mu_2|X^n) = -\dfrac{n}{2} \log 2\pi - \dfrac{1}{2} \sum_{i=1}^{n} (X_i - \mu_1 - \mu_2)^2 = l(\mu_1', \mu_2'|X^n)\)</span></p>
<p>One might conclude that the model is badly specified, or that there are too many unknown parameters.</p>
</section>
<section class="level3" id="example-10.6">
<h3 class="anchored" data-anchor-id="example-10.6">Example 10.6</h3>
<p>Suppose that <span class="math inline">\(X_i\)</span> are i.i.d. normal with mean <span class="math inline">\(\sin(\theta)\)</span>, and variance one. Then for any <span class="math inline">\(\theta'\)</span> with <span class="math inline">\(\theta' = \theta + 2\pi\)</span></p>
<p><span class="math inline">\(l(\theta|X^n) = -\dfrac{n}{2} \log 2\pi - \dfrac{1}{2} \sum_{i=1}^{n} (X_i - \sin(\theta))^2 = l(\theta'|X^n)\)</span></p>
<p>Although identification fails in both the previous examples, in the second case the failure is less problematic because observationally equivalent points are separated by a distance <span class="math inline">\(2\pi\)</span>. This is called a failure of <strong>global identification</strong> but not <strong>local identification</strong>. It can be resolved by defining a parameter space that is of width less than <span class="math inline">\(2\pi\)</span>, whereas no such fix is available in the former example.</p>
<p>In the <strong>nonparametric approach</strong>, <span class="math inline">\(\mathcal{P}\)</span> is an even larger family of probability measures. For example, <span class="math inline">\(\mathcal{P}\)</span> could consist of all twice differentiable density functions. In this case, the objective is to estimate the true density <span class="math inline">\(f \in \mathcal{P}\)</span> based on the data. The likelihood function is less central in this approach.</p>
</section>
<section class="level3" id="sufficiency">
<h3 class="anchored" data-anchor-id="sufficiency">10.3.1.1 Sufficiency</h3>
<p>Let <span class="math inline">\(T(\text{data})\)</span> be some statistic computed from the data. If the conditional distribution of the sample given the value of <span class="math inline">\(T(\text{data})\)</span> does not depend on <span class="math inline">\(\theta\)</span>, then we say that <span class="math inline">\(T\)</span> is a <strong>sufficient statistic</strong> for <span class="math inline">\(\theta\)</span>. That is, given <span class="math inline">\(T\)</span>, there is nothing left in the data that can tell us more about <span class="math inline">\(\theta\)</span>. The <strong>sufficiency principle</strong> is that any inference about <span class="math inline">\(\theta\)</span> should only depend on <span class="math inline">\(T\)</span> not on the sample space <span class="math inline">\(S\)</span> as well. Obviously, the data itself has this property so we are looking for a reduction of this to a small number of statistics.</p>
</section>
<section class="level3" id="theorem-10.3-factorization">
<h3 class="anchored" data-anchor-id="theorem-10.3-factorization">Theorem 10.3 (Factorization)</h3>
<p><span class="math inline">\(T\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>, i.e., <span class="math inline">\(f(X^n|T)\)</span> does not depend on <span class="math inline">\(\theta\)</span>, if and only if</p>
<p><span class="math inline">\(\dfrac{f(X^n|\theta)}{f(T|\theta)}\)</span></p>
<p>does not depend on <span class="math inline">\(\theta\)</span>. Here, <span class="math inline">\(f\)</span> is generic notation for density function.</p>
<p><strong>Proof:</strong></p>
<p>The proof of this theorem involves the change of variables arguments that we learned already.</p>
</section>
<section class="level3" id="example-10.7">
<h3 class="anchored" data-anchor-id="example-10.7">Example 10.7</h3>
<p>Suppose that <span class="math inline">\(X_1, \dots, X_n \sim N(\mu, 1)\)</span>. Then</p>
<p><span class="math inline">\(\begin{aligned}
f(X^n|\mu) &amp;= \prod_{i=1}^{n} \dfrac{1}{\sqrt{2\pi}} \exp \left( -\dfrac{(X_i - \mu)^2}{2} \right) \\
&amp;= \dfrac{1}{(2\pi)^{n/2}} \exp \left( -\dfrac{1}{2} \sum_{i=1}^{n} (X_i - \mu)^2 \right) \\
&amp;= \dfrac{1}{(2\pi)^{n/2}} \exp \left( -\dfrac{1}{2} \left[ \sum_{i=1}^{n} (X_i - \bar{X})^2 + n(\bar{X} - \mu)^2 \right] \right)
\end{aligned}\)</span></p>
<p>Note that <span class="math inline">\(\bar{X} \sim N(\mu, 1/n)\)</span> has density</p>
<p><span class="math inline">\(f(\bar{X}|\mu) = \sqrt{\dfrac{n}{2\pi}} \exp \left( -\dfrac{n}{2} (\bar{X} - \mu)^2 \right)\)</span></p>
<p>Therefore</p>
<p><span class="math inline">\(\dfrac{f(X^n|\mu)}{f(\bar{X}|\mu)} = \dfrac{1}{(2\pi)^{(n-1)/2}} \exp \left( -\dfrac{1}{2} \sum_{i=1}^{n} (X_i - \bar{X})^2 \right)\)</span></p>
<p>which does not depend on <span class="math inline">\(\theta\)</span>.</p>
<p>More generally if <span class="math inline">\((X, Y)\)</span> are bivariate normal, the minimal sufficient statistics are: <span class="math inline">\(\bar{X}, \bar{Y}, s_X^2, s_Y^2, s_{XY}\)</span>. This means that regarding bivariate normal data, we may just confine our attention to functions of these five sample quantities without any loss of generality.</p>
</section>
<section class="level3" id="ancillarity">
<h3 class="anchored" data-anchor-id="ancillarity">10.3.1.2 Ancillarity</h3>
<p>Suppose you have two imperfect, e.g., time measuring devices, one based in New York and one in LA. The one in LA is much more accurate. However, for political reasons, which one is used is determined by the flip of a coin. The process is illustrated below:</p>
<pre><code>          LA  N(μ, 0.01)
        /
.5H --
        \
.5T --
        /
          NY  N(μ, 0.1)</code></pre>
<p>The coin is flipped with outcome the random variable <span class="math inline">\(Z\)</span>. It turns out that it comes out heads and LA does its thing, call it <span class="math inline">\(X\)</span>. The question is: how do we evaluate the uncertainty about <span class="math inline">\(X\)</span>? A blind approach would be to say that</p>
<p><span class="math inline">\(\text{var}(X) = 0.5 \times 0.01 + 0.5 \times 0.1 = 0.055\)</span></p>
<p>But surely, we would agree that we should use <span class="math inline">\(\text{var}(X|Z = \text{LA}) = 0.01\)</span>. Furthermore, if we are interested in learning about <span class="math inline">\(\mu\)</span>, we might as well work with the conditional distribution of <span class="math inline">\(X|Z\)</span> rather than the joint distribution of <span class="math inline">\(X, Z\)</span>. This is an example of conditioning on an <strong>ancillary statistic</strong> – the coin tossing contains no information about <span class="math inline">\(\mu\)</span>.</p>
<p>A more substantive example is provided by regression: suppose that <span class="math inline">\((y_i, x_i)\)</span> are i.i.d. with <span class="math inline">\(E[y_i|x_i] = \beta x_i\)</span>. If the marginal distribution of <span class="math inline">\(x_i\)</span> does not depend on <span class="math inline">\(\beta\)</span>, then we can say that <span class="math inline">\(x_1, \dots, x_n\)</span> are ancillary and that any inference statement in this case should be made in the conditional distribution <span class="math inline">\(f(\cdot|x_1, \dots, x_n)\)</span>, i.e., we should really think of the covariates as being fixed in repeated samples. This concept has been considerably elaborated in time series, where it goes under the name of <strong>exogeneity</strong>.</p>
</section>
<section class="level3" id="bayesian-methods">
<h3 class="anchored" data-anchor-id="bayesian-methods">10.3.2 Bayesian Methods</h3>
<p><strong>Bayesians</strong> have a comprehensive approach to inference that is different from the Frequentist approach. In the Bayesian approach, parameters are also random variables. There is a <strong>prior density function</strong>, denoted by <span class="math inline">\(\pi(\theta)\)</span>, which reflects our knowledge about <span class="math inline">\(\theta\)</span> before seeing the sample. The objective is to update the prior using the sample data as represented by the likelihood and obtain the <strong>posterior</strong> by Bayes’ Theorem. We use the notation <span class="math inline">\(\pi(\theta|X^n)\)</span> to denote the posterior density of the parameter given the data. This is obtained by the formula</p>
<p><span class="math inline">\(\pi(\theta|X^n) \propto L(\theta|X^n) \times \pi(\theta) = f(X^n|\theta) \times \pi(\theta)\)</span></p>
<p>where <span class="math inline">\(\propto\)</span> denotes “proportional to”. Here, we are using both the notation we used in the frequentist session for the likelihood <span class="math inline">\(L(\theta|X^n)\)</span> and to make it clearer how this is just a rewrite of Bayes theorem we also use <span class="math inline">\(f(X^n|\theta)\)</span> for the conditional density of the data given <span class="math inline">\(\theta\)</span>, so that <span class="math inline">\(f(X^n|\theta) = L(\theta|X^n)\)</span>. Here, the proportional sign means that the right hand side does not necessarily integrate to one and one should normalize the prior so obtained to satisfy this requirement, that is, divide by the marginal density, which we may denote by <span class="math inline">\(f(X^n) = \int f(X^n|\theta) \pi(\theta) d\theta\)</span>. To summarize, what you know about <span class="math inline">\(\theta\)</span> after the data arrive is what you knew before (the prior) and what the data told you (likelihood). Once one has the posterior density the plan is to report various features of it like its mean, its median or its mode as ‘the estimator of <span class="math inline">\(\theta\)</span>’. The mode of the posterior is the natural analogue of the MLE since it maximizes the posterior.</p>
</section>
<section class="level3" id="example-10.8">
<h3 class="anchored" data-anchor-id="example-10.8">Example 10.8</h3>
<p>Suppose that <span class="math inline">\(X\)</span> is the outcome of a coin toss, whose bias is unknown, i.e., <span class="math inline">\(X=1\)</span> with probability <span class="math inline">\(p \in (0, 1)\)</span> and <span class="math inline">\(X=0\)</span> with probability <span class="math inline">\(1-p\)</span>. Suppose that one tosses the coin <span class="math inline">\(n\)</span> times and find <span class="math inline">\(k\)</span> heads and <span class="math inline">\(n-k\)</span> tails. The likelihood of the sample (distribution of the data) is</p>
<p><span class="math inline">\(f(X^n|p) = \binom{n}{k} p^k (1-p)^{n-k}\)</span></p>
<p>Suppose that the prior distribution on <span class="math inline">\(p\)</span>, <span class="math inline">\(\pi(p)\)</span>, is uniform over <span class="math inline">\([0, 1]\)</span>, that is, it places equal probability on different values of <span class="math inline">\(p\)</span> within this interval. In this case, the posterior density of <span class="math inline">\(p\)</span> is</p>
<p><span class="math inline">\(\pi(p|X^n) = \dfrac{f(X^n|p)\pi(p)}{\int f(X^n|p)\pi(p) dp} = \dfrac{p^k (1-p)^{n-k}}{\int_0^1 p^k (1-p)^{n-k} dp} = \dfrac{(n+1)!}{k!(n-k)!} p^k (1-p)^{n-k}\)</span></p>
<p>In fact, this is the distribution of a <span class="math inline">\(Beta(k+1, n-k+1)\)</span> random variable. The posterior mean is <span class="math inline">\((k+1)/(n+2)\)</span>.</p>
</section>
<section class="level3" id="example-10.9">
<h3 class="anchored" data-anchor-id="example-10.9">Example 10.9</h3>
<p>Suppose that we observe a single data point <span class="math inline">\(X \sim N(\mu_X, \sigma_X^2)\)</span>, where <span class="math inline">\(\sigma_X^2\)</span> is assumed to be known and not of interest. The prior for the parameter <span class="math inline">\(\mu_X\)</span> is the density of <span class="math inline">\(\mu_X \sim N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are known and fixed. Then the posterior density of <span class="math inline">\(\mu_X\)</span> is given by</p>
<p><span class="math inline">\(\mu_X|X \sim N(m, v)\)</span></p>
<p>where</p>
<p><span class="math inline">\(m = \dfrac{\sigma_X^2}{\sigma^2 + \sigma_X^2} \mu + \dfrac{\sigma^2}{\sigma^2 + \sigma_X^2} X\)</span></p>
<p><span class="math inline">\(v = \dfrac{\sigma^2 \sigma_X^2}{\sigma^2 + \sigma_X^2} = \left( \dfrac{1}{\sigma^2} + \dfrac{1}{\sigma_X^2} \right)^{-1}\)</span></p>
<p>The posterior mean is a weighted average of the prior mean and the data mean, while the posterior variance is the harmonic average of the variance of the prior and the data.</p>
</section>
<section class="level3" id="example-10.10">
<h3 class="anchored" data-anchor-id="example-10.10">Example 10.10</h3>
<p>Suppose now we observe <span class="math inline">\(n\)</span> observations <span class="math inline">\(X^n\)</span>. In this case we have <span class="math inline">\(\bar{X} \sim N(\mu_X, \sigma_X^2/n)\)</span>. We can show that the posterior density becomes</p>
<p><span class="math inline">\(\mu_X|X^n \sim N(m_n, v_n)\)</span></p>
<p>with <span class="math inline">\(m_n, v_n\)</span> as defined above except that <span class="math inline">\(\sigma_X^2 \to \sigma_X^2/n\)</span>. That is, the posterior mean is</p>
<p><span class="math inline">\(m_n = \dfrac{\sigma_X^2/n}{\sigma^2 + \sigma_X^2/n} \mu + \dfrac{\sigma^2}{\sigma^2 + \sigma_X^2/n} \bar{X}\)</span></p>
<p><span class="math inline">\(v_n = \left( \dfrac{1}{\sigma^2} + \dfrac{n}{\sigma_X^2} \right)^{-1}\)</span></p>
<p>When <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(m_n \overset{p}{\to} \mu_X\)</span>. Bayesians also have a law of small numbers, that is, when <span class="math inline">\(n \to 0\)</span>, we have <span class="math inline">\(m_n \overset{p}{\to} \mu\)</span>.</p>
<p>In this case, it is as if we had two independent measurements of <span class="math inline">\(\mu_X\)</span>, one from the distribution <span class="math inline">\(N(\mu_X, \sigma_X^2/n)\)</span> and the other from the distribution <span class="math inline">\(N(\mu, \sigma^2)\)</span> and we combined the two measurements in an equal fashion.</p>
<p>The prior in the above example is <strong>informative</strong>, and it puts most weight close to the point <span class="math inline">\(\mu\)</span>, with a degree of certainty expressed by <span class="math inline">\(\sigma^2\)</span>. When <span class="math inline">\(\sigma^2\)</span> is very small, the prior information is very precise. An alternative class of prior distributions are called <strong>ignorance priors</strong>, in which effectively <span class="math inline">\(\sigma^2 \to \infty\)</span>. Formally, we take the prior measure (not probability measure) to be Lebesgue measure on <span class="math inline">\(\mathbb{R}\)</span>, that is <span class="math inline">\(\mu\)</span> is essentially equally likely to be in any interval of the real line. This is called an <strong>improper prior</strong>, because it is not a probability measure. Nevertheless, Bayes theorem works in this case, since the normalization enforces that the posterior is a proper density (integrates to one). In that case, the posterior density of <span class="math inline">\(\mu_X|X\)</span> is <span class="math inline">\(N(X, \sigma_X^2)\)</span> consistent with the above arguments, and the posterior density of <span class="math inline">\(\mu_X|X^n\)</span> is <span class="math inline">\(N(\bar{X}, \sigma_X^2/n)\)</span>. Do we need a prior for the prior? In the above example how come <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> are known?</p>
<p>In general, calculating posterior densities analytically can be too difficult. However, it is easy to do it by simulation methods such as <strong>Markov Chain Monte Carlo (MCMC)</strong>.</p>
<p>Some Bayesians talk a lot about coherence and rationality. Others talk about flexibility and applicability. They often say words to the effect that: “Let me through, I am a Bayesian, I have the solution to the problem that you Frequentists have messed up”. Efron (1986) and Cox (2005) compare the Frequentist and Bayesian approaches. The Bayesian approach generically imposes much more structure, such as the prior distribution, which may not be warranted. However, it can deliver reasonable answers in some complex cases where the frequentist approach fails. Both approaches have merit and ultimately it may be a matter of taste, as to which approach you prefer. There is even a term that signifies possession of both tastes simultaneously, <strong>Empirical Bayes</strong>, where the prior is replaced by preexisting estimated distributions.</p>
</section>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch10exercise1">
<h3 class="anchored" data-anchor-id="sec-ch10exercise1">Exercise 1</h3>
<p><a href="#sec-ch10solution1">Solution 1</a></p>
<p>Suppose we are conducting a survey to estimate the proportion of the population that supports a particular policy. We divide the population into two strata: urban and rural residents. We then draw a random sample from each stratum. Explain why this is an example of <strong>stratified sampling</strong> and discuss one advantage of using this approach over simple random sampling.</p>
</section>
<section class="level3" id="sec-ch10exercise2">
<h3 class="anchored" data-anchor-id="sec-ch10exercise2">Exercise 2</h3>
<p><a href="#sec-ch10solution2">Solution 2</a></p>
<p>A researcher is studying the relationship between income and education level. They collect data on income and education level from a sample of individuals. They find that the sample covariance between income and education is positive. What does this indicate about the relationship between income and education in the sample?</p>
</section>
<section class="level3" id="sec-ch10exercise3">
<h3 class="anchored" data-anchor-id="sec-ch10exercise3">Exercise 3</h3>
<p><a href="#sec-ch10solution3">Solution 3</a></p>
<p>Suppose we have a sample of size <span class="math inline">\(n\)</span> drawn from a population with unknown mean <span class="math inline">\(\mu\)</span> and known variance <span class="math inline">\(\sigma^2\)</span>. Derive the expected value of the sample mean <span class="math inline">\(\bar{X}\)</span>.</p>
</section>
<section class="level3" id="sec-ch10exercise4">
<h3 class="anchored" data-anchor-id="sec-ch10exercise4">Exercise 4</h3>
<p><a href="#sec-ch10solution4">Solution 4</a></p>
<p>Suppose we have a sample of size <span class="math inline">\(n\)</span> drawn from a population with unknown mean <span class="math inline">\(\mu\)</span> and known variance <span class="math inline">\(\sigma^2\)</span>. Derive the variance of the sample mean <span class="math inline">\(\bar{X}\)</span>.</p>
</section>
<section class="level3" id="sec-ch10exercise5">
<h3 class="anchored" data-anchor-id="sec-ch10exercise5">Exercise 5</h3>
<p><a href="#sec-ch10solution5">Solution 5</a></p>
<p>Explain the concept of <strong>sample selection bias</strong> in the context of survey sampling. Give an example of a situation where sample selection bias might occur.</p>
</section>
<section class="level3" id="sec-ch10exercise6">
<h3 class="anchored" data-anchor-id="sec-ch10exercise6">Exercise 6</h3>
<p><a href="#sec-ch10solution6">Solution 6</a></p>
<p>What is the difference between <strong>observational data</strong> and <strong>experimental data</strong>? Provide an example of each type of data.</p>
</section>
<section class="level3" id="sec-ch10exercise7">
<h3 class="anchored" data-anchor-id="sec-ch10exercise7">Exercise 7</h3>
<p><a href="#sec-ch10solution7">Solution 7</a></p>
<p>Suppose we have a sample of size <span class="math inline">\(n\)</span> drawn from a population with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. What does the <strong>Central Limit Theorem</strong> say about the distribution of the sample mean <span class="math inline">\(\bar{X}\)</span> as <span class="math inline">\(n\)</span> becomes large?</p>
</section>
<section class="level3" id="sec-ch10exercise8">
<h3 class="anchored" data-anchor-id="sec-ch10exercise8">Exercise 8</h3>
<p><a href="#sec-ch10solution8">Solution 8</a></p>
<p>What is a <strong>kernel density estimate</strong> and how does it differ from a histogram? What is the role of the bandwidth parameter <span class="math inline">\(h\)</span> in kernel density estimation?</p>
</section>
<section class="level3" id="sec-ch10exercise9">
<h3 class="anchored" data-anchor-id="sec-ch10exercise9">Exercise 9</h3>
<p><a href="#sec-ch10solution9">Solution 9</a></p>
<p>Suppose <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. What is the distribution of the statistic <span class="math inline">\(\dfrac{(n-1)s_*^2}{\sigma^2}\)</span>, where <span class="math inline">\(s_*^2\)</span> is the unbiased sample variance?</p>
</section>
<section class="level3" id="sec-ch10exercise10">
<h3 class="anchored" data-anchor-id="sec-ch10exercise10">Exercise 10</h3>
<p><a href="#sec-ch10solution10">Solution 10</a></p>
<p>Suppose <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. What is the distribution of the statistic <span class="math inline">\(\dfrac{\bar{X} - \mu}{s_*/\sqrt{n}}\)</span>, where <span class="math inline">\(s_*\)</span> is the unbiased sample standard deviation?</p>
</section>
<section class="level3" id="sec-ch10exercise11">
<h3 class="anchored" data-anchor-id="sec-ch10exercise11">Exercise 11</h3>
<p><a href="#sec-ch10solution11">Solution 11</a></p>
<p>Explain the concept of <strong>identification</strong> in the context of parametric statistical models. What does it mean for a parameter to be <strong>unidentified</strong>?</p>
</section>
<section class="level3" id="sec-ch10exercise12">
<h3 class="anchored" data-anchor-id="sec-ch10exercise12">Exercise 12</h3>
<p><a href="#sec-ch10solution12">Solution 12</a></p>
<p>What is the <strong>likelihood function</strong> in the context of parametric statistical models? How is it used to generate estimators?</p>
</section>
<section class="level3" id="sec-ch10exercise13">
<h3 class="anchored" data-anchor-id="sec-ch10exercise13">Exercise 13</h3>
<p><a href="#sec-ch10solution13">Solution 13</a></p>
<p>What is the <strong>maximum likelihood estimator (MLE)</strong>? How is it related to the likelihood function?</p>
</section>
<section class="level3" id="sec-ch10exercise14">
<h3 class="anchored" data-anchor-id="sec-ch10exercise14">Exercise 14</h3>
<p><a href="#sec-ch10solution14">Solution 14</a></p>
<p>Suppose we are interested in estimating the parameter <span class="math inline">\(\theta\)</span> of a Bernoulli distribution. We observe a sample <span class="math inline">\(X_1, \dots, X_n\)</span> with <span class="math inline">\(k\)</span> successes (ones) and <span class="math inline">\(n-k\)</span> failures (zeros). Derive the likelihood function for <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch10exercise15">
<h3 class="anchored" data-anchor-id="sec-ch10exercise15">Exercise 15</h3>
<p><a href="#sec-ch10solution15">Solution 15</a></p>
<p>What is a <strong>sufficient statistic</strong>? Explain the <strong>sufficiency principle</strong>.</p>
</section>
<section class="level3" id="sec-ch10exercise16">
<h3 class="anchored" data-anchor-id="sec-ch10exercise16">Exercise 16</h3>
<p><a href="#sec-ch10solution16">Solution 16</a></p>
<p>Explain the concept of <strong>ancillarity</strong> in statistical inference. Provide an example of an ancillary statistic.</p>
</section>
<section class="level3" id="sec-ch10exercise17">
<h3 class="anchored" data-anchor-id="sec-ch10exercise17">Exercise 17</h3>
<p><a href="#sec-ch10solution17">Solution 17</a></p>
<p>What is the difference between the <strong>Frequentist</strong> and <strong>Bayesian</strong> approaches to statistical inference?</p>
</section>
<section class="level3" id="sec-ch10exercise18">
<h3 class="anchored" data-anchor-id="sec-ch10exercise18">Exercise 18</h3>
<p><a href="#sec-ch10solution18">Solution 18</a></p>
<p>What is a <strong>prior distribution</strong> in the context of Bayesian inference? How is it used to obtain the <strong>posterior distribution</strong>?</p>
</section>
<section class="level3" id="sec-ch10exercise19">
<h3 class="anchored" data-anchor-id="sec-ch10exercise19">Exercise 19</h3>
<p><a href="#sec-ch10solution19">Solution 19</a></p>
<p>Suppose we have a prior distribution for a parameter <span class="math inline">\(\theta\)</span> that is uniform over the interval <span class="math inline">\([0, 1]\)</span>. We then observe a sample <span class="math inline">\(X_1, \dots, X_n\)</span> from a Bernoulli distribution with parameter <span class="math inline">\(\theta\)</span>. How can we use <strong>Bayes’ Theorem</strong> to obtain the posterior distribution of <span class="math inline">\(\theta\)</span>?</p>
</section>
<section class="level3" id="sec-ch10exercise20">
<h3 class="anchored" data-anchor-id="sec-ch10exercise20">Exercise 20</h3>
<p><a href="#sec-ch10solution20">Solution 20</a></p>
<p>What is an <strong>improper prior</strong> in Bayesian inference? Provide an example of an improper prior.</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch10solution1">
<h3 class="anchored" data-anchor-id="sec-ch10solution1">Solution 1</h3>
<p><a href="#sec-ch10exercise1">Exercise 1</a></p>
<p>This is an example of <strong>stratified sampling</strong> because the population is divided into two groups (strata), urban and rural residents, and a random sample is drawn from each stratum.</p>
<p>One advantage of using stratified sampling over simple random sampling is that it can ensure that each subgroup of the population is adequately represented in the sample. In this case, if the proportion of the population that supports the policy differs significantly between urban and rural residents, simple random sampling might, by chance, overrepresent or underrepresent one of these groups. Stratified sampling guarantees that both urban and rural residents are represented in the sample in proportion to their size in the population, leading to a more accurate estimate of the overall population proportion that supports the policy. This relates to the concept of stratified sampling discussed in Section 10.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10solution2">
<h3 class="anchored" data-anchor-id="sec-ch10solution2">Solution 2</h3>
<p><a href="#sec-ch10exercise2">Exercise 2</a></p>
<p>A positive sample covariance between income and education in the sample indicates that there is a positive linear relationship between these two variables within the sample. In other words, individuals with higher education levels tend to have higher incomes, and individuals with lower education levels tend to have lower incomes. This is consistent with the sample covariance formula discussed in Section 10.2 of the text:</p>
<p><span class="math inline">\(s_{XY} = \dfrac{1}{n-1} \sum_{i=1}^{n} (X_i - \bar{X})(Y_i - \bar{Y})\)</span></p>
<p>If <span class="math inline">\(s_{XY} &gt; 0\)</span>, it means that the terms <span class="math inline">\((X_i - \bar{X})\)</span> and <span class="math inline">\((Y_i - \bar{Y})\)</span> tend to have the same sign, implying a positive association.</p>
</section>
<section class="level3" id="sec-ch10solution3">
<h3 class="anchored" data-anchor-id="sec-ch10solution3">Solution 3</h3>
<p><a href="#sec-ch10exercise3">Exercise 3</a></p>
<p>The expected value of the sample mean <span class="math inline">\(\bar{X}\)</span> is derived as follows:</p>
<p><span class="math inline">\(\begin{aligned}
E[\bar{X}] &amp;= E \left[ \dfrac{1}{n} \sum_{i=1}^{n} X_i \right] \\
&amp;= \dfrac{1}{n} E \left[ \sum_{i=1}^{n} X_i \right] \quad \text{(by linearity of expectation)} \\
&amp;= \dfrac{1}{n} \sum_{i=1}^{n} E[X_i] \quad \text{(by linearity of expectation)} \\
&amp;= \dfrac{1}{n} \sum_{i=1}^{n} \mu \quad \text{(since } E[X_i] = \mu \text{ for all } i) \\
&amp;= \dfrac{1}{n} (n\mu) \\
&amp;= \mu
\end{aligned}\)</span></p>
<p>Thus, the expected value of the sample mean is equal to the population mean <span class="math inline">\(\mu\)</span>. This is a fundamental property of the sample mean and is discussed in Section 10.2.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10solution4">
<h3 class="anchored" data-anchor-id="sec-ch10solution4">Solution 4</h3>
<p><a href="#sec-ch10exercise4">Exercise 4</a></p>
<p>The variance of the sample mean <span class="math inline">\(\bar{X}\)</span> is derived as follows:</p>
<p><span class="math inline">\(\begin{aligned}
\text{Var}(\bar{X}) &amp;= \text{Var} \left( \dfrac{1}{n} \sum_{i=1}^{n} X_i \right) \\
&amp;= \dfrac{1}{n^2} \text{Var} \left( \sum_{i=1}^{n} X_i \right) \quad \text{(since } \text{Var}(aX) = a^2 \text{Var}(X) \text{)} \\
&amp;= \dfrac{1}{n^2} \sum_{i=1}^{n} \text{Var}(X_i) \quad \text{(since the } X_i \text{ are independent)} \\
&amp;= \dfrac{1}{n^2} \sum_{i=1}^{n} \sigma^2 \quad \text{(since } \text{Var}(X_i) = \sigma^2 \text{ for all } i) \\
&amp;= \dfrac{1}{n^2} (n\sigma^2) \\
&amp;= \dfrac{\sigma^2}{n}
\end{aligned}\)</span></p>
<p>Thus, the variance of the sample mean is equal to the population variance <span class="math inline">\(\sigma^2\)</span> divided by the sample size <span class="math inline">\(n\)</span>. This result is presented in Section 10.2.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10solution5">
<h3 class="anchored" data-anchor-id="sec-ch10solution5">Solution 5</h3>
<p><a href="#sec-ch10exercise5">Exercise 5</a></p>
<p><strong>Sample selection bias</strong> occurs in survey sampling when the individuals who choose to respond to the survey are systematically different from those who choose not to respond, and this difference is related to the survey’s objective.</p>
<p>An example of sample selection bias might occur in a survey about customer satisfaction with a particular product. Suppose the survey is conducted online, and customers who are more tech-savvy are more likely to respond. If tech-savvy customers also tend to have different opinions about the product than less tech-savvy customers, the survey results will be biased because they will overrepresent the opinions of tech-savvy customers. This concept is discussed in Section 10.1 of the text, where it is mentioned that nonresponse can lead to sample selection bias if nonresponders differ from responders in a way that reflects the objective of the study.</p>
</section>
<section class="level3" id="sec-ch10solution6">
<h3 class="anchored" data-anchor-id="sec-ch10solution6">Solution 6</h3>
<p><a href="#sec-ch10exercise6">Exercise 6</a></p>
<p><strong>Observational data</strong> is data collected without any control over the factors that influence the variables of interest. In contrast, <strong>experimental data</strong> is data collected in a controlled setting where the researcher manipulates one or more factors to determine their effect on the variables of interest.</p>
<p>An example of observational data is data on the relationship between smoking and lung cancer collected by observing a group of people over time and recording their smoking habits and whether they develop lung cancer. An example of experimental data is data collected in a clinical trial where participants are randomly assigned to receive either a new drug or a placebo, and the effect of the drug on a particular health outcome is measured. This distinction is presented at the end of Section 10.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10solution7">
<h3 class="anchored" data-anchor-id="sec-ch10solution7">Solution 7</h3>
<p><a href="#sec-ch10exercise7">Exercise 7</a></p>
<p>The <strong>Central Limit Theorem (CLT)</strong> states that as the sample size <span class="math inline">\(n\)</span> becomes large, the distribution of the sample mean <span class="math inline">\(\bar{X}\)</span> approaches a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2/n\)</span>, regardless of the shape of the population distribution. This is expressed in the text in Theorem 10.1 as <span class="math inline">\(\sqrt{n}(\bar{X} - \mu) \overset{d}{\to} N(0, \sigma^2)\)</span>. In simpler terms, for large samples, the sample mean will be approximately normally distributed, even if the individual data points are not. The CLT is a powerful result because it allows us to make inferences about the population mean using the sample mean, even when we don’t know the shape of the population distribution.</p>
</section>
<section class="level3" id="sec-ch10solution8">
<h3 class="anchored" data-anchor-id="sec-ch10solution8">Solution 8</h3>
<p><a href="#sec-ch10exercise8">Exercise 8</a></p>
<p>A <strong>kernel density estimate</strong> is a non-parametric way to estimate the probability density function of a random variable. It is similar to a histogram but provides a smoother estimate of the density. Unlike a histogram, which represents the density by a series of bars, a kernel density estimate uses a smooth function (the kernel) centered at each data point. The individual kernel functions are then summed to produce the overall density estimate. The formula is provided in Section 10.2 of the text:</p>
<p><span class="math inline">\(\hat{f}(x) = \dfrac{1}{nh} \sum_{i=1}^{n} K \left( \dfrac{x - X_i}{h} \right)\)</span></p>
<p>The bandwidth parameter <span class="math inline">\(h\)</span> controls the smoothness of the density estimate. A larger bandwidth results in a smoother estimate, while a smaller bandwidth results in a more wiggly estimate that more closely follows the individual data points. The choice of <span class="math inline">\(h\)</span> is discussed in Section 10.2 of the text, where it’s stated that “large <span class="math inline">\(h\)</span> will make a smooth curve and small <span class="math inline">\(h\)</span> will make a very wiggly curve.”</p>
</section>
<section class="level3" id="sec-ch10solution9">
<h3 class="anchored" data-anchor-id="sec-ch10solution9">Solution 9</h3>
<p><a href="#sec-ch10exercise9">Exercise 9</a></p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the statistic <span class="math inline">\(\dfrac{(n-1)s_*^2}{\sigma^2}\)</span> has a chi-squared distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. This is stated in Theorem 10.2, part (3) of the text. This result is important for making inferences about the population variance <span class="math inline">\(\sigma^2\)</span> when the population is normally distributed.</p>
</section>
<section class="level3" id="sec-ch10solution10">
<h3 class="anchored" data-anchor-id="sec-ch10solution10">Solution 10</h3>
<p><a href="#sec-ch10exercise10">Exercise 10</a></p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the statistic <span class="math inline">\(\dfrac{\bar{X} - \mu}{s_*/\sqrt{n}}\)</span> has a t-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. This is stated in Theorem 10.2, part (4) of the text. This result is crucial for constructing confidence intervals and performing hypothesis tests about the population mean <span class="math inline">\(\mu\)</span> when the population is normally distributed and the population variance is unknown.</p>
</section>
<section class="level3" id="sec-ch10solution11">
<h3 class="anchored" data-anchor-id="sec-ch10solution11">Solution 11</h3>
<p><a href="#sec-ch10exercise11">Exercise 11</a></p>
<p>In the context of parametric statistical models, <strong>identification</strong> refers to the ability to uniquely determine the values of the model’s parameters from the distribution of the observed data. A parameter is <strong>unidentified</strong> if different values of the parameter give rise to the same distribution of the observed data, making it impossible to distinguish between these parameter values based on the data alone. This is discussed in Section 10.3 of the text, with Definitions 10.1 and 10.2 defining observational equivalence and identifiability, respectively.</p>
</section>
<section class="level3" id="sec-ch10solution12">
<h3 class="anchored" data-anchor-id="sec-ch10solution12">Solution 12</h3>
<p><a href="#sec-ch10exercise12">Exercise 12</a></p>
<p>The <strong>likelihood function</strong> in the context of parametric statistical models is a function that expresses the probability (or probability density) of observing the given data as a function of the unknown parameters of the model. It is defined as the joint probability density function of the data, evaluated at the observed data, and treated as a function of the parameters. It is introduced in Section 10.3 of the text with the formula:</p>
<p><span class="math inline">\(L(\theta|X^n) = f(X_1, \dots, X_n|\theta)\)</span></p>
<p>The likelihood function is used to generate estimators by finding the parameter values that maximize the likelihood of observing the data.</p>
</section>
<section class="level3" id="sec-ch10solution13">
<h3 class="anchored" data-anchor-id="sec-ch10solution13">Solution 13</h3>
<p><a href="#sec-ch10exercise13">Exercise 13</a></p>
<p>The <strong>maximum likelihood estimator (MLE)</strong> is the value of the parameter (or set of parameters) that maximizes the likelihood function. In other words, it is the parameter value that makes the observed data most probable under the assumed statistical model. It is defined in Section 10.3 of the text as the value of <span class="math inline">\(\theta\)</span> that maximizes <span class="math inline">\(L(\theta|X^n)\)</span>.</p>
</section>
<section class="level3" id="sec-ch10solution14">
<h3 class="anchored" data-anchor-id="sec-ch10solution14">Solution 14</h3>
<p><a href="#sec-ch10exercise14">Exercise 14</a></p>
<p>If we observe a sample <span class="math inline">\(X_1, \dots, X_n\)</span> from a Bernoulli distribution with parameter <span class="math inline">\(\theta\)</span>, where there are <span class="math inline">\(k\)</span> successes (ones) and <span class="math inline">\(n-k\)</span> failures (zeros), the likelihood function for <span class="math inline">\(\theta\)</span> is given by:</p>
<p><span class="math inline">\(L(\theta|X_1, \dots, X_n) = \prod_{i=1}^{n} \theta^{X_i} (1-\theta)^{1-X_i} = \theta^k (1-\theta)^{n-k}\)</span></p>
<p>This is because each <span class="math inline">\(X_i\)</span> is either 1 with probability <span class="math inline">\(\theta\)</span> or 0 with probability <span class="math inline">\(1-\theta\)</span>. The likelihood is the product of these probabilities for each observation in the sample. The likelihood function for a Bernoulli distribution is also presented in Example 10.3 of the text.</p>
</section>
<section class="level3" id="sec-ch10solution15">
<h3 class="anchored" data-anchor-id="sec-ch10solution15">Solution 15</h3>
<p><a href="#sec-ch10exercise15">Exercise 15</a></p>
<p>A <strong>sufficient statistic</strong> for a parameter <span class="math inline">\(\theta\)</span> is a statistic that captures all the information about <span class="math inline">\(\theta\)</span> that is contained in the sample data. In other words, once the sufficient statistic is known, no other function of the sample data can provide any additional information about <span class="math inline">\(\theta\)</span>. The <strong>sufficiency principle</strong> states that if <span class="math inline">\(T(X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>, then any inference about <span class="math inline">\(\theta\)</span> should depend on the sample <span class="math inline">\(X\)</span> only through the value of <span class="math inline">\(T(X)\)</span>. This concept is discussed in Section 10.3.1.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10solution16">
<h3 class="anchored" data-anchor-id="sec-ch10solution16">Solution 16</h3>
<p><a href="#sec-ch10exercise16">Exercise 16</a></p>
<p><strong>Ancillarity</strong> is a concept in statistical inference that refers to a statistic whose distribution does not depend on the parameter of interest. An <strong>ancillary statistic</strong> provides no information about the parameter of interest on its own. It is a statistic that is not informative about the parameters. It’s a characteristic of the experimental design, rather than a characteristic of the parameters themselves. This concept is discussed in Section 10.3.1.2 of the text with an example involving two measuring devices. In that example, the outcome of the coin flip (<span class="math inline">\(Z\)</span>) is an ancillary statistic because its distribution does not depend on the parameter of interest, <span class="math inline">\(\mu\)</span>.</p>
</section>
<section class="level3" id="sec-ch10solution17">
<h3 class="anchored" data-anchor-id="sec-ch10solution17">Solution 17</h3>
<p><a href="#sec-ch10exercise17">Exercise 17</a></p>
<p>The <strong>Frequentist</strong> and <strong>Bayesian</strong> approaches are two different paradigms for statistical inference. The Frequentist approach interprets probability as the long-run frequency of events and bases inference on the sampling distribution of statistics. The Bayesian approach, on the other hand, interprets probability as a degree of belief and bases inference on the posterior distribution of the parameters, which is obtained by combining prior beliefs with the information from the data. The distinction between these two approaches is described in Section 10.3 of the text.</p>
</section>
<section class="level3" id="sec-ch10solution18">
<h3 class="anchored" data-anchor-id="sec-ch10solution18">Solution 18</h3>
<p><a href="#sec-ch10exercise18">Exercise 18</a></p>
<p>In the context of Bayesian inference, a <strong>prior distribution</strong> is a probability distribution that represents our initial beliefs about the possible values of a parameter before observing any data. It is used to obtain the <strong>posterior distribution</strong> by combining it with the likelihood function, which represents the information about the parameter contained in the data. This is done using <strong>Bayes’ Theorem</strong>, as shown in the formula in Section 10.3.2 of the text:</p>
<p><span class="math inline">\(\pi(\theta|X^n) \propto L(\theta|X^n) \times \pi(\theta)\)</span></p>
<p>Here, <span class="math inline">\(\pi(\theta)\)</span> is the prior distribution, <span class="math inline">\(L(\theta|X^n)\)</span> is the likelihood function, and <span class="math inline">\(\pi(\theta|X^n)\)</span> is the posterior distribution.</p>
</section>
<section class="level3" id="sec-ch10solution19">
<h3 class="anchored" data-anchor-id="sec-ch10solution19">Solution 19</h3>
<p><a href="#sec-ch10exercise19">Exercise 19</a></p>
<p>If we have a prior distribution for a parameter <span class="math inline">\(\theta\)</span> that is uniform over the interval <span class="math inline">\([0, 1]\)</span>, and we observe a sample <span class="math inline">\(X_1, \dots, X_n\)</span> from a Bernoulli distribution with parameter <span class="math inline">\(\theta\)</span>, we can use <strong>Bayes’ Theorem</strong> to obtain the posterior distribution of <span class="math inline">\(\theta\)</span> as follows:</p>
<p><span class="math inline">\(\pi(\theta|X^n) = \dfrac{f(X^n|\theta)\pi(\theta)}{\int f(X^n|\theta)\pi(\theta) d\theta}\)</span></p>
<p>where <span class="math inline">\(\pi(\theta)\)</span> is the prior distribution (uniform in this case), <span class="math inline">\(f(X^n|\theta)\)</span> is the likelihood function, and <span class="math inline">\(\pi(\theta|X^n)\)</span> is the posterior distribution. Since the prior is uniform, <span class="math inline">\(\pi(\theta) = 1\)</span> for <span class="math inline">\(\theta \in [0, 1]\)</span>. The likelihood function for a Bernoulli sample with <span class="math inline">\(k\)</span> successes and <span class="math inline">\(n-k\)</span> failures is <span class="math inline">\(f(X^n|\theta) = \theta^k (1-\theta)^{n-k}\)</span>. Therefore, the posterior distribution is proportional to:</p>
<p><span class="math inline">\(\pi(\theta|X^n) \propto \theta^k (1-\theta)^{n-k}\)</span></p>
<p>This is the kernel of a Beta distribution. The posterior distribution is obtained by normalizing this expression so that it integrates to 1. This example is presented in Example 10.8 of the text.</p>
</section>
<section class="level3" id="sec-ch10solution20">
<h3 class="anchored" data-anchor-id="sec-ch10solution20">Solution 20</h3>
<p><a href="#sec-ch10exercise20">Exercise 20</a></p>
<p>An <strong>improper prior</strong> in Bayesian inference is a prior distribution that does not integrate to 1 and therefore is not a proper probability distribution. Improper priors are often used when there is no prior information about the parameter, and they can sometimes lead to valid posterior distributions. An example of an improper prior is a uniform distribution over the entire real line, which is often used as a prior for a location parameter when there is no prior information about its value. This is mentioned in the text in the discussion following Example 10.10, where it is stated that we can “take the prior measure (not probability measure) to be Lebesgue measure on <span class="math inline">\(\mathbb{R}\)</span>,” which corresponds to a uniform distribution over the entire real line.</p>
</section>
</section>
<section class="level2" id="r-scripts">
<h2 class="anchored" data-anchor-id="r-scripts">R Scripts</h2>
<section class="level3" id="r-script-1-stratified-sampling">
<h3 class="anchored" data-anchor-id="r-script-1-stratified-sampling">R Script 1: Stratified Sampling</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a aria-hidden="true" href="#cb2-1" tabindex="-1"></a><span class="co"># Function for stratified sampling</span></span>
<span id="cb2-2"><a aria-hidden="true" href="#cb2-2" tabindex="-1"></a>stratified_sampling <span class="ot">&lt;-</span> <span class="cf">function</span>(data, strata_var, sample_sizes) {</span>
<span id="cb2-3"><a aria-hidden="true" href="#cb2-3" tabindex="-1"></a>  <span class="co"># Check if inputs are valid</span></span>
<span id="cb2-4"><a aria-hidden="true" href="#cb2-4" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.data.frame</span>(data)) {</span>
<span id="cb2-5"><a aria-hidden="true" href="#cb2-5" tabindex="-1"></a>    <span class="fu">stop</span>(<span class="st">"Input 'data' must be a data frame."</span>)</span>
<span id="cb2-6"><a aria-hidden="true" href="#cb2-6" tabindex="-1"></a>  }</span>
<span id="cb2-7"><a aria-hidden="true" href="#cb2-7" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span>strata_var <span class="sc">%in%</span> <span class="fu">names</span>(data)) {</span>
<span id="cb2-8"><a aria-hidden="true" href="#cb2-8" tabindex="-1"></a>    <span class="fu">stop</span>(<span class="st">"Stratification variable '"</span>, strata_var, <span class="st">"' not found in data."</span>)</span>
<span id="cb2-9"><a aria-hidden="true" href="#cb2-9" tabindex="-1"></a>  }</span>
<span id="cb2-10"><a aria-hidden="true" href="#cb2-10" tabindex="-1"></a>  <span class="cf">if</span> (<span class="sc">!</span><span class="fu">is.numeric</span>(sample_sizes) <span class="sc">||</span> <span class="fu">any</span>(sample_sizes <span class="sc">&lt;=</span> <span class="dv">0</span>)) {</span>
<span id="cb2-11"><a aria-hidden="true" href="#cb2-11" tabindex="-1"></a>    <span class="fu">stop</span>(<span class="st">"'sample_sizes' must be a numeric vector with positive values."</span>)</span>
<span id="cb2-12"><a aria-hidden="true" href="#cb2-12" tabindex="-1"></a>  }</span>
<span id="cb2-13"><a aria-hidden="true" href="#cb2-13" tabindex="-1"></a>  </span>
<span id="cb2-14"><a aria-hidden="true" href="#cb2-14" tabindex="-1"></a>  <span class="co"># Get the unique strata levels</span></span>
<span id="cb2-15"><a aria-hidden="true" href="#cb2-15" tabindex="-1"></a>  strata_levels <span class="ot">&lt;-</span> <span class="fu">unique</span>(data[[strata_var]])</span>
<span id="cb2-16"><a aria-hidden="true" href="#cb2-16" tabindex="-1"></a>  </span>
<span id="cb2-17"><a aria-hidden="true" href="#cb2-17" tabindex="-1"></a>  <span class="co"># Check if the number of sample sizes matches the number of strata</span></span>
<span id="cb2-18"><a aria-hidden="true" href="#cb2-18" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(sample_sizes) <span class="sc">!=</span> <span class="dv">1</span> <span class="sc">&amp;&amp;</span> <span class="fu">length</span>(sample_sizes) <span class="sc">!=</span> <span class="fu">length</span>(strata_levels)) {</span>
<span id="cb2-19"><a aria-hidden="true" href="#cb2-19" tabindex="-1"></a>    <span class="fu">stop</span>(<span class="st">"Length of 'sample_sizes' must be 1 or equal to the number of strata levels."</span>)</span>
<span id="cb2-20"><a aria-hidden="true" href="#cb2-20" tabindex="-1"></a>  }</span>
<span id="cb2-21"><a aria-hidden="true" href="#cb2-21" tabindex="-1"></a>  </span>
<span id="cb2-22"><a aria-hidden="true" href="#cb2-22" tabindex="-1"></a>  <span class="co"># If a single sample size is provided, use it for all strata</span></span>
<span id="cb2-23"><a aria-hidden="true" href="#cb2-23" tabindex="-1"></a>  <span class="cf">if</span> (<span class="fu">length</span>(sample_sizes) <span class="sc">==</span> <span class="dv">1</span>) {</span>
<span id="cb2-24"><a aria-hidden="true" href="#cb2-24" tabindex="-1"></a>    sample_sizes <span class="ot">&lt;-</span> <span class="fu">rep</span>(sample_sizes, <span class="fu">length</span>(strata_levels))</span>
<span id="cb2-25"><a aria-hidden="true" href="#cb2-25" tabindex="-1"></a>  }</span>
<span id="cb2-26"><a aria-hidden="true" href="#cb2-26" tabindex="-1"></a>  </span>
<span id="cb2-27"><a aria-hidden="true" href="#cb2-27" tabindex="-1"></a>  <span class="co"># Create an empty list to store the samples</span></span>
<span id="cb2-28"><a aria-hidden="true" href="#cb2-28" tabindex="-1"></a>  stratified_sample <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb2-29"><a aria-hidden="true" href="#cb2-29" tabindex="-1"></a>  </span>
<span id="cb2-30"><a aria-hidden="true" href="#cb2-30" tabindex="-1"></a>  <span class="co"># Loop through each stratum</span></span>
<span id="cb2-31"><a aria-hidden="true" href="#cb2-31" tabindex="-1"></a>  <span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span><span class="fu">length</span>(strata_levels)) {</span>
<span id="cb2-32"><a aria-hidden="true" href="#cb2-32" tabindex="-1"></a>    stratum <span class="ot">&lt;-</span> strata_levels[i]</span>
<span id="cb2-33"><a aria-hidden="true" href="#cb2-33" tabindex="-1"></a>    stratum_data <span class="ot">&lt;-</span> data[data[[strata_var]] <span class="sc">==</span> stratum, ]</span>
<span id="cb2-34"><a aria-hidden="true" href="#cb2-34" tabindex="-1"></a>    </span>
<span id="cb2-35"><a aria-hidden="true" href="#cb2-35" tabindex="-1"></a>    <span class="co"># Check if sample size is larger than the stratum size</span></span>
<span id="cb2-36"><a aria-hidden="true" href="#cb2-36" tabindex="-1"></a>    <span class="cf">if</span> (sample_sizes[i] <span class="sc">&gt;</span> <span class="fu">nrow</span>(stratum_data)) {</span>
<span id="cb2-37"><a aria-hidden="true" href="#cb2-37" tabindex="-1"></a>      <span class="fu">warning</span>(<span class="fu">paste</span>(<span class="st">"Sample size for stratum '"</span>, stratum, <span class="st">"' is larger than the stratum size. Taking the entire stratum."</span>, <span class="at">sep =</span> <span class="st">""</span>))</span>
<span id="cb2-38"><a aria-hidden="true" href="#cb2-38" tabindex="-1"></a>      stratified_sample[[i]] <span class="ot">&lt;-</span> stratum_data</span>
<span id="cb2-39"><a aria-hidden="true" href="#cb2-39" tabindex="-1"></a>    } <span class="cf">else</span> {</span>
<span id="cb2-40"><a aria-hidden="true" href="#cb2-40" tabindex="-1"></a>      <span class="co"># Randomly sample from the stratum</span></span>
<span id="cb2-41"><a aria-hidden="true" href="#cb2-41" tabindex="-1"></a>      stratum_sample <span class="ot">&lt;-</span> stratum_data[<span class="fu">sample</span>(<span class="fu">nrow</span>(stratum_data), sample_sizes[i]), ]</span>
<span id="cb2-42"><a aria-hidden="true" href="#cb2-42" tabindex="-1"></a>      stratified_sample[[i]] <span class="ot">&lt;-</span> stratum_sample</span>
<span id="cb2-43"><a aria-hidden="true" href="#cb2-43" tabindex="-1"></a>    }</span>
<span id="cb2-44"><a aria-hidden="true" href="#cb2-44" tabindex="-1"></a>  }</span>
<span id="cb2-45"><a aria-hidden="true" href="#cb2-45" tabindex="-1"></a>  </span>
<span id="cb2-46"><a aria-hidden="true" href="#cb2-46" tabindex="-1"></a>  <span class="co"># Combine the samples from each stratum</span></span>
<span id="cb2-47"><a aria-hidden="true" href="#cb2-47" tabindex="-1"></a>  stratified_sample <span class="ot">&lt;-</span> <span class="fu">do.call</span>(rbind, stratified_sample)</span>
<span id="cb2-48"><a aria-hidden="true" href="#cb2-48" tabindex="-1"></a>  </span>
<span id="cb2-49"><a aria-hidden="true" href="#cb2-49" tabindex="-1"></a>  <span class="fu">return</span>(stratified_sample)</span>
<span id="cb2-50"><a aria-hidden="true" href="#cb2-50" tabindex="-1"></a>}</span>
<span id="cb2-51"><a aria-hidden="true" href="#cb2-51" tabindex="-1"></a></span>
<span id="cb2-52"><a aria-hidden="true" href="#cb2-52" tabindex="-1"></a><span class="co"># Example usage:</span></span>
<span id="cb2-53"><a aria-hidden="true" href="#cb2-53" tabindex="-1"></a></span>
<span id="cb2-54"><a aria-hidden="true" href="#cb2-54" tabindex="-1"></a><span class="co"># Create a sample data frame</span></span>
<span id="cb2-55"><a aria-hidden="true" href="#cb2-55" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb2-56"><a aria-hidden="true" href="#cb2-56" tabindex="-1"></a>population <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb2-57"><a aria-hidden="true" href="#cb2-57" tabindex="-1"></a>  <span class="at">ID =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">100</span>,</span>
<span id="cb2-58"><a aria-hidden="true" href="#cb2-58" tabindex="-1"></a>  <span class="at">Group =</span> <span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"A"</span>, <span class="st">"B"</span>, <span class="st">"C"</span>, <span class="st">"D"</span>), <span class="at">each =</span> <span class="dv">25</span>),</span>
<span id="cb2-59"><a aria-hidden="true" href="#cb2-59" tabindex="-1"></a>  <span class="at">Value =</span> <span class="fu">rnorm</span>(<span class="dv">100</span>)</span>
<span id="cb2-60"><a aria-hidden="true" href="#cb2-60" tabindex="-1"></a>)</span>
<span id="cb2-61"><a aria-hidden="true" href="#cb2-61" tabindex="-1"></a></span>
<span id="cb2-62"><a aria-hidden="true" href="#cb2-62" tabindex="-1"></a><span class="co"># Perform stratified sampling with different sample sizes for each group</span></span>
<span id="cb2-63"><a aria-hidden="true" href="#cb2-63" tabindex="-1"></a>sample_sizes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">15</span>, <span class="dv">20</span>) <span class="co"># Sample sizes for groups A, B, C, and D respectively</span></span>
<span id="cb2-64"><a aria-hidden="true" href="#cb2-64" tabindex="-1"></a>stratified_sample_diff <span class="ot">&lt;-</span> <span class="fu">stratified_sampling</span>(population, <span class="st">"Group"</span>, sample_sizes)</span>
<span id="cb2-65"><a aria-hidden="true" href="#cb2-65" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Stratified Sample (Different Sizes):"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Stratified Sample (Different Sizes):"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a aria-hidden="true" href="#cb4-1" tabindex="-1"></a><span class="fu">print</span>(stratified_sample_diff)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     ID Group        Value
20   20     A -0.472791408
16   16     A  1.786913137
22   22     A -0.217974915
11   11     A  1.224081797
24   24     A -0.728891229
45   45     B  1.207961998
33   33     B  0.895125661
28   28     B  0.153373118
29   29     B -1.138136937
50   50     B -0.083369066
37   37     B  0.553917654
42   42     B -0.207917278
35   35     B  0.821581082
36   36     B  0.688640254
34   34     B  0.878133488
73   73     C  1.005738524
58   58     C  0.584613750
64   64     C -1.018575383
71   71     C -0.491031166
63   63     C -0.333207384
52   52     C -0.028546755
61   61     C  0.379639483
72   72     C -2.309168876
75   75     C -0.688008616
56   56     C  1.516470604
59   59     C  0.123854244
74   74     C -0.709200763
62   62     C -0.502323453
60   60     C  0.215941569
54   54     C  1.368602284
88   88     D  0.435181491
89   89     D -0.325931586
96   96     D -0.600259587
91   91     D  0.993503856
76   76     D  1.025571370
83   83     D -0.370660032
95   95     D  1.360652449
85   85     D -0.220486562
94   94     D -0.627906076
77   77     D -0.284773007
100 100     D -1.026420900
80   80     D -0.138891362
84   84     D  0.644376549
82   82     D  0.385280401
87   87     D  1.096839013
93   93     D  0.238731735
92   92     D  0.548396960
86   86     D  0.331781964
81   81     D  0.005764186
97   97     D  2.187332993</code></pre>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a aria-hidden="true" href="#cb6-1" tabindex="-1"></a><span class="co"># Perform stratified sampling with the same sample size for all groups</span></span>
<span id="cb6-2"><a aria-hidden="true" href="#cb6-2" tabindex="-1"></a>sample_size <span class="ot">&lt;-</span> <span class="dv">10</span>  <span class="co"># Sample size for all groups</span></span>
<span id="cb6-3"><a aria-hidden="true" href="#cb6-3" tabindex="-1"></a>stratified_sample_same <span class="ot">&lt;-</span> <span class="fu">stratified_sampling</span>(population, <span class="st">"Group"</span>, sample_size)</span>
<span id="cb6-4"><a aria-hidden="true" href="#cb6-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Stratified Sample (Same Size):"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Stratified Sample (Same Size):"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a aria-hidden="true" href="#cb8-1" tabindex="-1"></a><span class="fu">print</span>(stratified_sample_same)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     ID Group       Value
11   11     A  1.22408180
1     1     A -0.56047565
19   19     A  0.70135590
10   10     A -0.44566197
21   21     A -1.06782371
13   13     A  0.40077145
25   25     A -0.62503927
23   23     A -1.02600445
7     7     A  0.46091621
9     9     A -0.68685285
48   48     B -0.46665535
45   45     B  1.20796200
34   34     B  0.87813349
50   50     B -0.08336907
30   30     B  1.25381492
39   39     B -0.30596266
49   49     B  0.77996512
31   31     B  0.42646422
26   26     B -1.68669331
35   35     B  0.82158108
60   60     C  0.21594157
67   67     C  0.44820978
74   74     C -0.70920076
71   71     C -0.49103117
57   57     C -1.54875280
59   59     C  0.12385424
56   56     C  1.51647060
68   68     C  0.05300423
73   73     C  1.00573852
66   66     C  0.30352864
80   80     D -0.13889136
95   95     D  1.36065245
78   78     D -1.22071771
89   89     D -0.32593159
76   76     D  1.02557137
77   77     D -0.28477301
79   79     D  0.18130348
85   85     D -0.22048656
96   96     D -0.60025959
100 100     D -1.02642090</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a aria-hidden="true" href="#cb10-1" tabindex="-1"></a><span class="co"># Example with sample size larger than stratum size</span></span>
<span id="cb10-2"><a aria-hidden="true" href="#cb10-2" tabindex="-1"></a>sample_sizes_large <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>, <span class="dv">30</span>, <span class="dv">20</span>) <span class="co"># Sample size for group C is larger than its size</span></span>
<span id="cb10-3"><a aria-hidden="true" href="#cb10-3" tabindex="-1"></a>stratified_sample_large <span class="ot">&lt;-</span> <span class="fu">stratified_sampling</span>(population, <span class="st">"Group"</span>, sample_sizes_large)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in stratified_sampling(population, "Group", sample_sizes_large): Sample
size for stratum 'C' is larger than the stratum size. Taking the entire
stratum.</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a aria-hidden="true" href="#cb12-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Stratified Sample (Large Sample Size in One Stratum):"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Stratified Sample (Large Sample Size in One Stratum):"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a aria-hidden="true" href="#cb14-1" tabindex="-1"></a><span class="fu">print</span>(stratified_sample_large)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>   ID Group        Value
21 21     A -1.067823706
8   8     A -1.265061235
23 23     A -1.026004448
25 25     A -0.625039268
13 13     A  0.400771451
43 43     B -1.265396352
35 35     B  0.821581082
31 31     B  0.426464221
32 32     B -0.295071483
34 34     B  0.878133488
41 41     B -0.694706979
42 42     B -0.207917278
38 38     B -0.061911711
33 33     B  0.895125661
30 30     B  1.253814921
51 51     C  0.253318514
52 52     C -0.028546755
53 53     C -0.042870457
54 54     C  1.368602284
55 55     C -0.225770986
56 56     C  1.516470604
57 57     C -1.548752804
58 58     C  0.584613750
59 59     C  0.123854244
60 60     C  0.215941569
61 61     C  0.379639483
62 62     C -0.502323453
63 63     C -0.333207384
64 64     C -1.018575383
65 65     C -1.071791226
66 66     C  0.303528641
67 67     C  0.448209779
68 68     C  0.053004227
69 69     C  0.922267468
70 70     C  2.050084686
71 71     C -0.491031166
72 72     C -2.309168876
73 73     C  1.005738524
74 74     C -0.709200763
75 75     C -0.688008616
82 82     D  0.385280401
95 95     D  1.360652449
93 93     D  0.238731735
92 92     D  0.548396960
76 76     D  1.025571370
77 77     D -0.284773007
97 97     D  2.187332993
99 99     D -0.235700359
88 88     D  0.435181491
83 83     D -0.370660032
96 96     D -0.600259587
78 78     D -1.220717712
98 98     D  1.532610626
94 94     D -0.627906076
80 80     D -0.138891362
84 84     D  0.644376549
81 81     D  0.005764186
87 87     D  1.096839013
91 91     D  0.993503856
85 85     D -0.220486562</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><p><strong><code>stratified_sampling(data, strata_var, sample_sizes)</code> function:</strong></p>
<ul>
<li><code>data</code>: The input data frame containing the population.</li>
<li><code>strata_var</code>: The name of the column in <code>data</code> that defines the strata (e.g., “Group”, “Region”, “AgeGroup”).</li>
<li><code>sample_sizes</code>: A numeric vector specifying the desired sample size for each stratum. If a single number is provided, it’s used as the sample size for all strata.</li>
</ul></li>
<li><p><strong>Input Validation:</strong> The function checks if the inputs are valid (e.g., <code>data</code> is a data frame, <code>strata_var</code> exists, <code>sample_sizes</code> are positive).</p></li>
<li><p><strong>Strata Levels:</strong> It identifies the unique values in the <code>strata_var</code> column to determine the different strata.</p></li>
<li><p><strong>Sample Size Handling:</strong></p>
<ul>
<li>If <code>sample_sizes</code> has length 1, it replicates the single sample size for all strata.</li>
<li>It checks if the number of sample sizes matches the number of strata (if not a single size is provided).</li>
</ul></li>
<li><p><strong>Stratified Sampling Loop:</strong></p>
<ul>
<li>It iterates through each stratum level.</li>
<li>It subsets the <code>data</code> to get the data for the current stratum.</li>
<li><strong>Sample Size Check:</strong> It checks if the requested sample size for the stratum is larger than the stratum’s population size. If it is, it takes the entire stratum as the sample (and issues a warning).</li>
<li><strong>Random Sampling:</strong> It uses <code>sample()</code> to randomly select rows from the stratum data without replacement.</li>
<li>It stores the stratum sample in the <code>stratified_sample</code> list.</li>
</ul></li>
<li><p><strong>Combine Samples:</strong> <code>do.call(rbind, stratified_sample)</code> combines the samples from each stratum into a single data frame.</p></li>
<li><p><strong>Return Value:</strong> The function returns the combined stratified sample as a data frame.</p></li>
</ol>
<p><strong>Example Usage:</strong></p>
<ul>
<li>A sample data frame <code>population</code> is created.</li>
<li><strong>Different Sample Sizes:</strong> The <code>stratified_sampling</code> function is called with different sample sizes for each stratum.</li>
<li><strong>Same Sample Size:</strong> The function is called with the same sample size for all strata.</li>
<li><strong>Large Sample Size Example:</strong> Demonstrates the case where a sample size is larger than the stratum size.</li>
</ul>
<p><strong>How to use in your project</strong></p>
<ol type="1">
<li><p><strong>Install necessary packages (if not already installed):</strong></p>
<div class="sourceCode" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a aria-hidden="true" href="#cb16-1" tabindex="-1"></a><span class="co"># No specific packages needed for basic stratified sampling</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Prepare your data:</strong></p>
<ul>
<li>Make sure your data is in a data frame.</li>
<li>Identify the column that represents your stratification variable.</li>
</ul></li>
<li><p><strong>Determine sample sizes:</strong></p>
<ul>
<li>Decide on the sample size you want for each stratum.</li>
</ul></li>
<li><p><strong>Run the script:</strong></p>
<ul>
<li>Copy and paste the <code>stratified_sampling</code> function into your R script.</li>
<li>Call the function with your data, stratification variable, and sample sizes.</li>
</ul>
<div class="sourceCode" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a aria-hidden="true" href="#cb17-1" tabindex="-1"></a><span class="co"># Example:</span></span>
<span id="cb17-2"><a aria-hidden="true" href="#cb17-2" tabindex="-1"></a>my_data <span class="ot">&lt;-</span> <span class="fu">read.csv</span>(<span class="st">"your_data.csv"</span>) <span class="co"># Replace with your data file</span></span>
<span id="cb17-3"><a aria-hidden="true" href="#cb17-3" tabindex="-1"></a>stratification_variable <span class="ot">&lt;-</span> <span class="st">"Region"</span> <span class="co"># Replace with your stratification column</span></span>
<span id="cb17-4"><a aria-hidden="true" href="#cb17-4" tabindex="-1"></a>my_sample_sizes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">50</span>, <span class="dv">75</span>, <span class="dv">100</span>) <span class="co"># Replace with your desired sample sizes</span></span>
<span id="cb17-5"><a aria-hidden="true" href="#cb17-5" tabindex="-1"></a>my_stratified_sample <span class="ot">&lt;-</span> <span class="fu">stratified_sampling</span>(my_data, stratification_variable, my_sample_sizes)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div></li>
<li><p><strong>Analyze your sample:</strong></p>
<ul>
<li><code>my_stratified_sample</code> now contains your stratified sample. You can proceed with your analysis using this sample.</li>
<li>Consider saving this sample using write.csv() to reuse it later.</li>
</ul></li>
</ol>
<p>This script provides a flexible and robust way to perform stratified sampling in R, handling various scenarios and providing clear error/warning messages. Remember to adjust the example usage to fit your specific data and requirements.</p>
</section>
<section class="level3" id="r-script-2-sample-mean-and-central-limit-theorem">
<h3 class="anchored" data-anchor-id="r-script-2-sample-mean-and-central-limit-theorem">R Script 2: Sample Mean and Central Limit Theorem</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a aria-hidden="true" href="#cb18-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb18-2"><a aria-hidden="true" href="#cb18-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a aria-hidden="true" href="#cb20-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb20-2"><a aria-hidden="true" href="#cb20-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb20-3"><a aria-hidden="true" href="#cb20-3" tabindex="-1"></a></span>
<span id="cb20-4"><a aria-hidden="true" href="#cb20-4" tabindex="-1"></a><span class="co"># Set parameters for the population distribution</span></span>
<span id="cb20-5"><a aria-hidden="true" href="#cb20-5" tabindex="-1"></a>population_mean <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb20-6"><a aria-hidden="true" href="#cb20-6" tabindex="-1"></a>population_sd <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb20-7"><a aria-hidden="true" href="#cb20-7" tabindex="-1"></a></span>
<span id="cb20-8"><a aria-hidden="true" href="#cb20-8" tabindex="-1"></a><span class="co"># Create a function to simulate sample means</span></span>
<span id="cb20-9"><a aria-hidden="true" href="#cb20-9" tabindex="-1"></a>simulate_sample_means <span class="ot">&lt;-</span> <span class="cf">function</span>(sample_size, num_samples) {</span>
<span id="cb20-10"><a aria-hidden="true" href="#cb20-10" tabindex="-1"></a>  <span class="fu">rerun</span>(num_samples, {</span>
<span id="cb20-11"><a aria-hidden="true" href="#cb20-11" tabindex="-1"></a>    sample <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(sample_size, <span class="at">mean =</span> population_mean, <span class="at">sd =</span> population_sd)</span>
<span id="cb20-12"><a aria-hidden="true" href="#cb20-12" tabindex="-1"></a>    <span class="fu">mean</span>(sample)</span>
<span id="cb20-13"><a aria-hidden="true" href="#cb20-13" tabindex="-1"></a>  }) <span class="sc">%&gt;%</span></span>
<span id="cb20-14"><a aria-hidden="true" href="#cb20-14" tabindex="-1"></a>    <span class="fu">bind_cols</span>() <span class="sc">%&gt;%</span></span>
<span id="cb20-15"><a aria-hidden="true" href="#cb20-15" tabindex="-1"></a>    <span class="fu">gather</span>(sample_number, sample_mean) <span class="sc">%&gt;%</span></span>
<span id="cb20-16"><a aria-hidden="true" href="#cb20-16" tabindex="-1"></a>    <span class="fu">select</span>(<span class="sc">-</span>sample_number)</span>
<span id="cb20-17"><a aria-hidden="true" href="#cb20-17" tabindex="-1"></a>}</span>
<span id="cb20-18"><a aria-hidden="true" href="#cb20-18" tabindex="-1"></a></span>
<span id="cb20-19"><a aria-hidden="true" href="#cb20-19" tabindex="-1"></a><span class="co"># Simulate sample means for different sample sizes</span></span>
<span id="cb20-20"><a aria-hidden="true" href="#cb20-20" tabindex="-1"></a>sample_means_10 <span class="ot">&lt;-</span> <span class="fu">simulate_sample_means</span>(<span class="at">sample_size =</span> <span class="dv">10</span>, <span class="at">num_samples =</span> <span class="dv">1000</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: `rerun()` was deprecated in purrr 1.0.0.
ℹ Please use `map()` instead.
  # Previously
rerun(1000, {
sample &lt;- rnorm(sample_size, mean = population_mean, sd = population_sd)
mean(sample)
})

  # Now
map(1:1000, ~ {
sample &lt;- rnorm(sample_size, mean = population_mean, sd = population_sd)
mean(sample)
})</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>New names:
• `` -&gt; `...1`
• `` -&gt; `...2`
• `` -&gt; `...3`
• `` -&gt; `...4`
• `` -&gt; `...5`
• `` -&gt; `...6`
• `` -&gt; `...7`
• `` -&gt; `...8`
• `` -&gt; `...9`
• `` -&gt; `...10`
• `` -&gt; `...11`
• `` -&gt; `...12`
• `` -&gt; `...13`
• `` -&gt; `...14`
• `` -&gt; `...15`
• `` -&gt; `...16`
• `` -&gt; `...17`
• `` -&gt; `...18`
• `` -&gt; `...19`
• `` -&gt; `...20`
• `` -&gt; `...21`
• `` -&gt; `...22`
• `` -&gt; `...23`
• `` -&gt; `...24`
• `` -&gt; `...25`
• `` -&gt; `...26`
• `` -&gt; `...27`
• `` -&gt; `...28`
• `` -&gt; `...29`
• `` -&gt; `...30`
• `` -&gt; `...31`
• `` -&gt; `...32`
• `` -&gt; `...33`
• `` -&gt; `...34`
• `` -&gt; `...35`
• `` -&gt; `...36`
• `` -&gt; `...37`
• `` -&gt; `...38`
• `` -&gt; `...39`
• `` -&gt; `...40`
• `` -&gt; `...41`
• `` -&gt; `...42`
• `` -&gt; `...43`
• `` -&gt; `...44`
• `` -&gt; `...45`
• `` -&gt; `...46`
• `` -&gt; `...47`
• `` -&gt; `...48`
• `` -&gt; `...49`
• `` -&gt; `...50`
• `` -&gt; `...51`
• `` -&gt; `...52`
• `` -&gt; `...53`
• `` -&gt; `...54`
• `` -&gt; `...55`
• `` -&gt; `...56`
• `` -&gt; `...57`
• `` -&gt; `...58`
• `` -&gt; `...59`
• `` -&gt; `...60`
• `` -&gt; `...61`
• `` -&gt; `...62`
• `` -&gt; `...63`
• `` -&gt; `...64`
• `` -&gt; `...65`
• `` -&gt; `...66`
• `` -&gt; `...67`
• `` -&gt; `...68`
• `` -&gt; `...69`
• `` -&gt; `...70`
• `` -&gt; `...71`
• `` -&gt; `...72`
• `` -&gt; `...73`
• `` -&gt; `...74`
• `` -&gt; `...75`
• `` -&gt; `...76`
• `` -&gt; `...77`
• `` -&gt; `...78`
• `` -&gt; `...79`
• `` -&gt; `...80`
• `` -&gt; `...81`
• `` -&gt; `...82`
• `` -&gt; `...83`
• `` -&gt; `...84`
• `` -&gt; `...85`
• `` -&gt; `...86`
• `` -&gt; `...87`
• `` -&gt; `...88`
• `` -&gt; `...89`
• `` -&gt; `...90`
• `` -&gt; `...91`
• `` -&gt; `...92`
• `` -&gt; `...93`
• `` -&gt; `...94`
• `` -&gt; `...95`
• `` -&gt; `...96`
• `` -&gt; `...97`
• `` -&gt; `...98`
• `` -&gt; `...99`
• `` -&gt; `...100`
• `` -&gt; `...101`
• `` -&gt; `...102`
• `` -&gt; `...103`
• `` -&gt; `...104`
• `` -&gt; `...105`
• `` -&gt; `...106`
• `` -&gt; `...107`
• `` -&gt; `...108`
• `` -&gt; `...109`
• `` -&gt; `...110`
• `` -&gt; `...111`
• `` -&gt; `...112`
• `` -&gt; `...113`
• `` -&gt; `...114`
• `` -&gt; `...115`
• `` -&gt; `...116`
• `` -&gt; `...117`
• `` -&gt; `...118`
• `` -&gt; `...119`
• `` -&gt; `...120`
• `` -&gt; `...121`
• `` -&gt; `...122`
• `` -&gt; `...123`
• `` -&gt; `...124`
• `` -&gt; `...125`
• `` -&gt; `...126`
• `` -&gt; `...127`
• `` -&gt; `...128`
• `` -&gt; `...129`
• `` -&gt; `...130`
• `` -&gt; `...131`
• `` -&gt; `...132`
• `` -&gt; `...133`
• `` -&gt; `...134`
• `` -&gt; `...135`
• `` -&gt; `...136`
• `` -&gt; `...137`
• `` -&gt; `...138`
• `` -&gt; `...139`
• `` -&gt; `...140`
• `` -&gt; `...141`
• `` -&gt; `...142`
• `` -&gt; `...143`
• `` -&gt; `...144`
• `` -&gt; `...145`
• `` -&gt; `...146`
• `` -&gt; `...147`
• `` -&gt; `...148`
• `` -&gt; `...149`
• `` -&gt; `...150`
• `` -&gt; `...151`
• `` -&gt; `...152`
• `` -&gt; `...153`
• `` -&gt; `...154`
• `` -&gt; `...155`
• `` -&gt; `...156`
• `` -&gt; `...157`
• `` -&gt; `...158`
• `` -&gt; `...159`
• `` -&gt; `...160`
• `` -&gt; `...161`
• `` -&gt; `...162`
• `` -&gt; `...163`
• `` -&gt; `...164`
• `` -&gt; `...165`
• `` -&gt; `...166`
• `` -&gt; `...167`
• `` -&gt; `...168`
• `` -&gt; `...169`
• `` -&gt; `...170`
• `` -&gt; `...171`
• `` -&gt; `...172`
• `` -&gt; `...173`
• `` -&gt; `...174`
• `` -&gt; `...175`
• `` -&gt; `...176`
• `` -&gt; `...177`
• `` -&gt; `...178`
• `` -&gt; `...179`
• `` -&gt; `...180`
• `` -&gt; `...181`
• `` -&gt; `...182`
• `` -&gt; `...183`
• `` -&gt; `...184`
• `` -&gt; `...185`
• `` -&gt; `...186`
• `` -&gt; `...187`
• `` -&gt; `...188`
• `` -&gt; `...189`
• `` -&gt; `...190`
• `` -&gt; `...191`
• `` -&gt; `...192`
• `` -&gt; `...193`
• `` -&gt; `...194`
• `` -&gt; `...195`
• `` -&gt; `...196`
• `` -&gt; `...197`
• `` -&gt; `...198`
• `` -&gt; `...199`
• `` -&gt; `...200`
• `` -&gt; `...201`
• `` -&gt; `...202`
• `` -&gt; `...203`
• `` -&gt; `...204`
• `` -&gt; `...205`
• `` -&gt; `...206`
• `` -&gt; `...207`
• `` -&gt; `...208`
• `` -&gt; `...209`
• `` -&gt; `...210`
• `` -&gt; `...211`
• `` -&gt; `...212`
• `` -&gt; `...213`
• `` -&gt; `...214`
• `` -&gt; `...215`
• `` -&gt; `...216`
• `` -&gt; `...217`
• `` -&gt; `...218`
• `` -&gt; `...219`
• `` -&gt; `...220`
• `` -&gt; `...221`
• `` -&gt; `...222`
• `` -&gt; `...223`
• `` -&gt; `...224`
• `` -&gt; `...225`
• `` -&gt; `...226`
• `` -&gt; `...227`
• `` -&gt; `...228`
• `` -&gt; `...229`
• `` -&gt; `...230`
• `` -&gt; `...231`
• `` -&gt; `...232`
• `` -&gt; `...233`
• `` -&gt; `...234`
• `` -&gt; `...235`
• `` -&gt; `...236`
• `` -&gt; `...237`
• `` -&gt; `...238`
• `` -&gt; `...239`
• `` -&gt; `...240`
• `` -&gt; `...241`
• `` -&gt; `...242`
• `` -&gt; `...243`
• `` -&gt; `...244`
• `` -&gt; `...245`
• `` -&gt; `...246`
• `` -&gt; `...247`
• `` -&gt; `...248`
• `` -&gt; `...249`
• `` -&gt; `...250`
• `` -&gt; `...251`
• `` -&gt; `...252`
• `` -&gt; `...253`
• `` -&gt; `...254`
• `` -&gt; `...255`
• `` -&gt; `...256`
• `` -&gt; `...257`
• `` -&gt; `...258`
• `` -&gt; `...259`
• `` -&gt; `...260`
• `` -&gt; `...261`
• `` -&gt; `...262`
• `` -&gt; `...263`
• `` -&gt; `...264`
• `` -&gt; `...265`
• `` -&gt; `...266`
• `` -&gt; `...267`
• `` -&gt; `...268`
• `` -&gt; `...269`
• `` -&gt; `...270`
• `` -&gt; `...271`
• `` -&gt; `...272`
• `` -&gt; `...273`
• `` -&gt; `...274`
• `` -&gt; `...275`
• `` -&gt; `...276`
• `` -&gt; `...277`
• `` -&gt; `...278`
• `` -&gt; `...279`
• `` -&gt; `...280`
• `` -&gt; `...281`
• `` -&gt; `...282`
• `` -&gt; `...283`
• `` -&gt; `...284`
• `` -&gt; `...285`
• `` -&gt; `...286`
• `` -&gt; `...287`
• `` -&gt; `...288`
• `` -&gt; `...289`
• `` -&gt; `...290`
• `` -&gt; `...291`
• `` -&gt; `...292`
• `` -&gt; `...293`
• `` -&gt; `...294`
• `` -&gt; `...295`
• `` -&gt; `...296`
• `` -&gt; `...297`
• `` -&gt; `...298`
• `` -&gt; `...299`
• `` -&gt; `...300`
• `` -&gt; `...301`
• `` -&gt; `...302`
• `` -&gt; `...303`
• `` -&gt; `...304`
• `` -&gt; `...305`
• `` -&gt; `...306`
• `` -&gt; `...307`
• `` -&gt; `...308`
• `` -&gt; `...309`
• `` -&gt; `...310`
• `` -&gt; `...311`
• `` -&gt; `...312`
• `` -&gt; `...313`
• `` -&gt; `...314`
• `` -&gt; `...315`
• `` -&gt; `...316`
• `` -&gt; `...317`
• `` -&gt; `...318`
• `` -&gt; `...319`
• `` -&gt; `...320`
• `` -&gt; `...321`
• `` -&gt; `...322`
• `` -&gt; `...323`
• `` -&gt; `...324`
• `` -&gt; `...325`
• `` -&gt; `...326`
• `` -&gt; `...327`
• `` -&gt; `...328`
• `` -&gt; `...329`
• `` -&gt; `...330`
• `` -&gt; `...331`
• `` -&gt; `...332`
• `` -&gt; `...333`
• `` -&gt; `...334`
• `` -&gt; `...335`
• `` -&gt; `...336`
• `` -&gt; `...337`
• `` -&gt; `...338`
• `` -&gt; `...339`
• `` -&gt; `...340`
• `` -&gt; `...341`
• `` -&gt; `...342`
• `` -&gt; `...343`
• `` -&gt; `...344`
• `` -&gt; `...345`
• `` -&gt; `...346`
• `` -&gt; `...347`
• `` -&gt; `...348`
• `` -&gt; `...349`
• `` -&gt; `...350`
• `` -&gt; `...351`
• `` -&gt; `...352`
• `` -&gt; `...353`
• `` -&gt; `...354`
• `` -&gt; `...355`
• `` -&gt; `...356`
• `` -&gt; `...357`
• `` -&gt; `...358`
• `` -&gt; `...359`
• `` -&gt; `...360`
• `` -&gt; `...361`
• `` -&gt; `...362`
• `` -&gt; `...363`
• `` -&gt; `...364`
• `` -&gt; `...365`
• `` -&gt; `...366`
• `` -&gt; `...367`
• `` -&gt; `...368`
• `` -&gt; `...369`
• `` -&gt; `...370`
• `` -&gt; `...371`
• `` -&gt; `...372`
• `` -&gt; `...373`
• `` -&gt; `...374`
• `` -&gt; `...375`
• `` -&gt; `...376`
• `` -&gt; `...377`
• `` -&gt; `...378`
• `` -&gt; `...379`
• `` -&gt; `...380`
• `` -&gt; `...381`
• `` -&gt; `...382`
• `` -&gt; `...383`
• `` -&gt; `...384`
• `` -&gt; `...385`
• `` -&gt; `...386`
• `` -&gt; `...387`
• `` -&gt; `...388`
• `` -&gt; `...389`
• `` -&gt; `...390`
• `` -&gt; `...391`
• `` -&gt; `...392`
• `` -&gt; `...393`
• `` -&gt; `...394`
• `` -&gt; `...395`
• `` -&gt; `...396`
• `` -&gt; `...397`
• `` -&gt; `...398`
• `` -&gt; `...399`
• `` -&gt; `...400`
• `` -&gt; `...401`
• `` -&gt; `...402`
• `` -&gt; `...403`
• `` -&gt; `...404`
• `` -&gt; `...405`
• `` -&gt; `...406`
• `` -&gt; `...407`
• `` -&gt; `...408`
• `` -&gt; `...409`
• `` -&gt; `...410`
• `` -&gt; `...411`
• `` -&gt; `...412`
• `` -&gt; `...413`
• `` -&gt; `...414`
• `` -&gt; `...415`
• `` -&gt; `...416`
• `` -&gt; `...417`
• `` -&gt; `...418`
• `` -&gt; `...419`
• `` -&gt; `...420`
• `` -&gt; `...421`
• `` -&gt; `...422`
• `` -&gt; `...423`
• `` -&gt; `...424`
• `` -&gt; `...425`
• `` -&gt; `...426`
• `` -&gt; `...427`
• `` -&gt; `...428`
• `` -&gt; `...429`
• `` -&gt; `...430`
• `` -&gt; `...431`
• `` -&gt; `...432`
• `` -&gt; `...433`
• `` -&gt; `...434`
• `` -&gt; `...435`
• `` -&gt; `...436`
• `` -&gt; `...437`
• `` -&gt; `...438`
• `` -&gt; `...439`
• `` -&gt; `...440`
• `` -&gt; `...441`
• `` -&gt; `...442`
• `` -&gt; `...443`
• `` -&gt; `...444`
• `` -&gt; `...445`
• `` -&gt; `...446`
• `` -&gt; `...447`
• `` -&gt; `...448`
• `` -&gt; `...449`
• `` -&gt; `...450`
• `` -&gt; `...451`
• `` -&gt; `...452`
• `` -&gt; `...453`
• `` -&gt; `...454`
• `` -&gt; `...455`
• `` -&gt; `...456`
• `` -&gt; `...457`
• `` -&gt; `...458`
• `` -&gt; `...459`
• `` -&gt; `...460`
• `` -&gt; `...461`
• `` -&gt; `...462`
• `` -&gt; `...463`
• `` -&gt; `...464`
• `` -&gt; `...465`
• `` -&gt; `...466`
• `` -&gt; `...467`
• `` -&gt; `...468`
• `` -&gt; `...469`
• `` -&gt; `...470`
• `` -&gt; `...471`
• `` -&gt; `...472`
• `` -&gt; `...473`
• `` -&gt; `...474`
• `` -&gt; `...475`
• `` -&gt; `...476`
• `` -&gt; `...477`
• `` -&gt; `...478`
• `` -&gt; `...479`
• `` -&gt; `...480`
• `` -&gt; `...481`
• `` -&gt; `...482`
• `` -&gt; `...483`
• `` -&gt; `...484`
• `` -&gt; `...485`
• `` -&gt; `...486`
• `` -&gt; `...487`
• `` -&gt; `...488`
• `` -&gt; `...489`
• `` -&gt; `...490`
• `` -&gt; `...491`
• `` -&gt; `...492`
• `` -&gt; `...493`
• `` -&gt; `...494`
• `` -&gt; `...495`
• `` -&gt; `...496`
• `` -&gt; `...497`
• `` -&gt; `...498`
• `` -&gt; `...499`
• `` -&gt; `...500`
• `` -&gt; `...501`
• `` -&gt; `...502`
• `` -&gt; `...503`
• `` -&gt; `...504`
• `` -&gt; `...505`
• `` -&gt; `...506`
• `` -&gt; `...507`
• `` -&gt; `...508`
• `` -&gt; `...509`
• `` -&gt; `...510`
• `` -&gt; `...511`
• `` -&gt; `...512`
• `` -&gt; `...513`
• `` -&gt; `...514`
• `` -&gt; `...515`
• `` -&gt; `...516`
• `` -&gt; `...517`
• `` -&gt; `...518`
• `` -&gt; `...519`
• `` -&gt; `...520`
• `` -&gt; `...521`
• `` -&gt; `...522`
• `` -&gt; `...523`
• `` -&gt; `...524`
• `` -&gt; `...525`
• `` -&gt; `...526`
• `` -&gt; `...527`
• `` -&gt; `...528`
• `` -&gt; `...529`
• `` -&gt; `...530`
• `` -&gt; `...531`
• `` -&gt; `...532`
• `` -&gt; `...533`
• `` -&gt; `...534`
• `` -&gt; `...535`
• `` -&gt; `...536`
• `` -&gt; `...537`
• `` -&gt; `...538`
• `` -&gt; `...539`
• `` -&gt; `...540`
• `` -&gt; `...541`
• `` -&gt; `...542`
• `` -&gt; `...543`
• `` -&gt; `...544`
• `` -&gt; `...545`
• `` -&gt; `...546`
• `` -&gt; `...547`
• `` -&gt; `...548`
• `` -&gt; `...549`
• `` -&gt; `...550`
• `` -&gt; `...551`
• `` -&gt; `...552`
• `` -&gt; `...553`
• `` -&gt; `...554`
• `` -&gt; `...555`
• `` -&gt; `...556`
• `` -&gt; `...557`
• `` -&gt; `...558`
• `` -&gt; `...559`
• `` -&gt; `...560`
• `` -&gt; `...561`
• `` -&gt; `...562`
• `` -&gt; `...563`
• `` -&gt; `...564`
• `` -&gt; `...565`
• `` -&gt; `...566`
• `` -&gt; `...567`
• `` -&gt; `...568`
• `` -&gt; `...569`
• `` -&gt; `...570`
• `` -&gt; `...571`
• `` -&gt; `...572`
• `` -&gt; `...573`
• `` -&gt; `...574`
• `` -&gt; `...575`
• `` -&gt; `...576`
• `` -&gt; `...577`
• `` -&gt; `...578`
• `` -&gt; `...579`
• `` -&gt; `...580`
• `` -&gt; `...581`
• `` -&gt; `...582`
• `` -&gt; `...583`
• `` -&gt; `...584`
• `` -&gt; `...585`
• `` -&gt; `...586`
• `` -&gt; `...587`
• `` -&gt; `...588`
• `` -&gt; `...589`
• `` -&gt; `...590`
• `` -&gt; `...591`
• `` -&gt; `...592`
• `` -&gt; `...593`
• `` -&gt; `...594`
• `` -&gt; `...595`
• `` -&gt; `...596`
• `` -&gt; `...597`
• `` -&gt; `...598`
• `` -&gt; `...599`
• `` -&gt; `...600`
• `` -&gt; `...601`
• `` -&gt; `...602`
• `` -&gt; `...603`
• `` -&gt; `...604`
• `` -&gt; `...605`
• `` -&gt; `...606`
• `` -&gt; `...607`
• `` -&gt; `...608`
• `` -&gt; `...609`
• `` -&gt; `...610`
• `` -&gt; `...611`
• `` -&gt; `...612`
• `` -&gt; `...613`
• `` -&gt; `...614`
• `` -&gt; `...615`
• `` -&gt; `...616`
• `` -&gt; `...617`
• `` -&gt; `...618`
• `` -&gt; `...619`
• `` -&gt; `...620`
• `` -&gt; `...621`
• `` -&gt; `...622`
• `` -&gt; `...623`
• `` -&gt; `...624`
• `` -&gt; `...625`
• `` -&gt; `...626`
• `` -&gt; `...627`
• `` -&gt; `...628`
• `` -&gt; `...629`
• `` -&gt; `...630`
• `` -&gt; `...631`
• `` -&gt; `...632`
• `` -&gt; `...633`
• `` -&gt; `...634`
• `` -&gt; `...635`
• `` -&gt; `...636`
• `` -&gt; `...637`
• `` -&gt; `...638`
• `` -&gt; `...639`
• `` -&gt; `...640`
• `` -&gt; `...641`
• `` -&gt; `...642`
• `` -&gt; `...643`
• `` -&gt; `...644`
• `` -&gt; `...645`
• `` -&gt; `...646`
• `` -&gt; `...647`
• `` -&gt; `...648`
• `` -&gt; `...649`
• `` -&gt; `...650`
• `` -&gt; `...651`
• `` -&gt; `...652`
• `` -&gt; `...653`
• `` -&gt; `...654`
• `` -&gt; `...655`
• `` -&gt; `...656`
• `` -&gt; `...657`
• `` -&gt; `...658`
• `` -&gt; `...659`
• `` -&gt; `...660`
• `` -&gt; `...661`
• `` -&gt; `...662`
• `` -&gt; `...663`
• `` -&gt; `...664`
• `` -&gt; `...665`
• `` -&gt; `...666`
• `` -&gt; `...667`
• `` -&gt; `...668`
• `` -&gt; `...669`
• `` -&gt; `...670`
• `` -&gt; `...671`
• `` -&gt; `...672`
• `` -&gt; `...673`
• `` -&gt; `...674`
• `` -&gt; `...675`
• `` -&gt; `...676`
• `` -&gt; `...677`
• `` -&gt; `...678`
• `` -&gt; `...679`
• `` -&gt; `...680`
• `` -&gt; `...681`
• `` -&gt; `...682`
• `` -&gt; `...683`
• `` -&gt; `...684`
• `` -&gt; `...685`
• `` -&gt; `...686`
• `` -&gt; `...687`
• `` -&gt; `...688`
• `` -&gt; `...689`
• `` -&gt; `...690`
• `` -&gt; `...691`
• `` -&gt; `...692`
• `` -&gt; `...693`
• `` -&gt; `...694`
• `` -&gt; `...695`
• `` -&gt; `...696`
• `` -&gt; `...697`
• `` -&gt; `...698`
• `` -&gt; `...699`
• `` -&gt; `...700`
• `` -&gt; `...701`
• `` -&gt; `...702`
• `` -&gt; `...703`
• `` -&gt; `...704`
• `` -&gt; `...705`
• `` -&gt; `...706`
• `` -&gt; `...707`
• `` -&gt; `...708`
• `` -&gt; `...709`
• `` -&gt; `...710`
• `` -&gt; `...711`
• `` -&gt; `...712`
• `` -&gt; `...713`
• `` -&gt; `...714`
• `` -&gt; `...715`
• `` -&gt; `...716`
• `` -&gt; `...717`
• `` -&gt; `...718`
• `` -&gt; `...719`
• `` -&gt; `...720`
• `` -&gt; `...721`
• `` -&gt; `...722`
• `` -&gt; `...723`
• `` -&gt; `...724`
• `` -&gt; `...725`
• `` -&gt; `...726`
• `` -&gt; `...727`
• `` -&gt; `...728`
• `` -&gt; `...729`
• `` -&gt; `...730`
• `` -&gt; `...731`
• `` -&gt; `...732`
• `` -&gt; `...733`
• `` -&gt; `...734`
• `` -&gt; `...735`
• `` -&gt; `...736`
• `` -&gt; `...737`
• `` -&gt; `...738`
• `` -&gt; `...739`
• `` -&gt; `...740`
• `` -&gt; `...741`
• `` -&gt; `...742`
• `` -&gt; `...743`
• `` -&gt; `...744`
• `` -&gt; `...745`
• `` -&gt; `...746`
• `` -&gt; `...747`
• `` -&gt; `...748`
• `` -&gt; `...749`
• `` -&gt; `...750`
• `` -&gt; `...751`
• `` -&gt; `...752`
• `` -&gt; `...753`
• `` -&gt; `...754`
• `` -&gt; `...755`
• `` -&gt; `...756`
• `` -&gt; `...757`
• `` -&gt; `...758`
• `` -&gt; `...759`
• `` -&gt; `...760`
• `` -&gt; `...761`
• `` -&gt; `...762`
• `` -&gt; `...763`
• `` -&gt; `...764`
• `` -&gt; `...765`
• `` -&gt; `...766`
• `` -&gt; `...767`
• `` -&gt; `...768`
• `` -&gt; `...769`
• `` -&gt; `...770`
• `` -&gt; `...771`
• `` -&gt; `...772`
• `` -&gt; `...773`
• `` -&gt; `...774`
• `` -&gt; `...775`
• `` -&gt; `...776`
• `` -&gt; `...777`
• `` -&gt; `...778`
• `` -&gt; `...779`
• `` -&gt; `...780`
• `` -&gt; `...781`
• `` -&gt; `...782`
• `` -&gt; `...783`
• `` -&gt; `...784`
• `` -&gt; `...785`
• `` -&gt; `...786`
• `` -&gt; `...787`
• `` -&gt; `...788`
• `` -&gt; `...789`
• `` -&gt; `...790`
• `` -&gt; `...791`
• `` -&gt; `...792`
• `` -&gt; `...793`
• `` -&gt; `...794`
• `` -&gt; `...795`
• `` -&gt; `...796`
• `` -&gt; `...797`
• `` -&gt; `...798`
• `` -&gt; `...799`
• `` -&gt; `...800`
• `` -&gt; `...801`
• `` -&gt; `...802`
• `` -&gt; `...803`
• `` -&gt; `...804`
• `` -&gt; `...805`
• `` -&gt; `...806`
• `` -&gt; `...807`
• `` -&gt; `...808`
• `` -&gt; `...809`
• `` -&gt; `...810`
• `` -&gt; `...811`
• `` -&gt; `...812`
• `` -&gt; `...813`
• `` -&gt; `...814`
• `` -&gt; `...815`
• `` -&gt; `...816`
• `` -&gt; `...817`
• `` -&gt; `...818`
• `` -&gt; `...819`
• `` -&gt; `...820`
• `` -&gt; `...821`
• `` -&gt; `...822`
• `` -&gt; `...823`
• `` -&gt; `...824`
• `` -&gt; `...825`
• `` -&gt; `...826`
• `` -&gt; `...827`
• `` -&gt; `...828`
• `` -&gt; `...829`
• `` -&gt; `...830`
• `` -&gt; `...831`
• `` -&gt; `...832`
• `` -&gt; `...833`
• `` -&gt; `...834`
• `` -&gt; `...835`
• `` -&gt; `...836`
• `` -&gt; `...837`
• `` -&gt; `...838`
• `` -&gt; `...839`
• `` -&gt; `...840`
• `` -&gt; `...841`
• `` -&gt; `...842`
• `` -&gt; `...843`
• `` -&gt; `...844`
• `` -&gt; `...845`
• `` -&gt; `...846`
• `` -&gt; `...847`
• `` -&gt; `...848`
• `` -&gt; `...849`
• `` -&gt; `...850`
• `` -&gt; `...851`
• `` -&gt; `...852`
• `` -&gt; `...853`
• `` -&gt; `...854`
• `` -&gt; `...855`
• `` -&gt; `...856`
• `` -&gt; `...857`
• `` -&gt; `...858`
• `` -&gt; `...859`
• `` -&gt; `...860`
• `` -&gt; `...861`
• `` -&gt; `...862`
• `` -&gt; `...863`
• `` -&gt; `...864`
• `` -&gt; `...865`
• `` -&gt; `...866`
• `` -&gt; `...867`
• `` -&gt; `...868`
• `` -&gt; `...869`
• `` -&gt; `...870`
• `` -&gt; `...871`
• `` -&gt; `...872`
• `` -&gt; `...873`
• `` -&gt; `...874`
• `` -&gt; `...875`
• `` -&gt; `...876`
• `` -&gt; `...877`
• `` -&gt; `...878`
• `` -&gt; `...879`
• `` -&gt; `...880`
• `` -&gt; `...881`
• `` -&gt; `...882`
• `` -&gt; `...883`
• `` -&gt; `...884`
• `` -&gt; `...885`
• `` -&gt; `...886`
• `` -&gt; `...887`
• `` -&gt; `...888`
• `` -&gt; `...889`
• `` -&gt; `...890`
• `` -&gt; `...891`
• `` -&gt; `...892`
• `` -&gt; `...893`
• `` -&gt; `...894`
• `` -&gt; `...895`
• `` -&gt; `...896`
• `` -&gt; `...897`
• `` -&gt; `...898`
• `` -&gt; `...899`
• `` -&gt; `...900`
• `` -&gt; `...901`
• `` -&gt; `...902`
• `` -&gt; `...903`
• `` -&gt; `...904`
• `` -&gt; `...905`
• `` -&gt; `...906`
• `` -&gt; `...907`
• `` -&gt; `...908`
• `` -&gt; `...909`
• `` -&gt; `...910`
• `` -&gt; `...911`
• `` -&gt; `...912`
• `` -&gt; `...913`
• `` -&gt; `...914`
• `` -&gt; `...915`
• `` -&gt; `...916`
• `` -&gt; `...917`
• `` -&gt; `...918`
• `` -&gt; `...919`
• `` -&gt; `...920`
• `` -&gt; `...921`
• `` -&gt; `...922`
• `` -&gt; `...923`
• `` -&gt; `...924`
• `` -&gt; `...925`
• `` -&gt; `...926`
• `` -&gt; `...927`
• `` -&gt; `...928`
• `` -&gt; `...929`
• `` -&gt; `...930`
• `` -&gt; `...931`
• `` -&gt; `...932`
• `` -&gt; `...933`
• `` -&gt; `...934`
• `` -&gt; `...935`
• `` -&gt; `...936`
• `` -&gt; `...937`
• `` -&gt; `...938`
• `` -&gt; `...939`
• `` -&gt; `...940`
• `` -&gt; `...941`
• `` -&gt; `...942`
• `` -&gt; `...943`
• `` -&gt; `...944`
• `` -&gt; `...945`
• `` -&gt; `...946`
• `` -&gt; `...947`
• `` -&gt; `...948`
• `` -&gt; `...949`
• `` -&gt; `...950`
• `` -&gt; `...951`
• `` -&gt; `...952`
• `` -&gt; `...953`
• `` -&gt; `...954`
• `` -&gt; `...955`
• `` -&gt; `...956`
• `` -&gt; `...957`
• `` -&gt; `...958`
• `` -&gt; `...959`
• `` -&gt; `...960`
• `` -&gt; `...961`
• `` -&gt; `...962`
• `` -&gt; `...963`
• `` -&gt; `...964`
• `` -&gt; `...965`
• `` -&gt; `...966`
• `` -&gt; `...967`
• `` -&gt; `...968`
• `` -&gt; `...969`
• `` -&gt; `...970`
• `` -&gt; `...971`
• `` -&gt; `...972`
• `` -&gt; `...973`
• `` -&gt; `...974`
• `` -&gt; `...975`
• `` -&gt; `...976`
• `` -&gt; `...977`
• `` -&gt; `...978`
• `` -&gt; `...979`
• `` -&gt; `...980`
• `` -&gt; `...981`
• `` -&gt; `...982`
• `` -&gt; `...983`
• `` -&gt; `...984`
• `` -&gt; `...985`
• `` -&gt; `...986`
• `` -&gt; `...987`
• `` -&gt; `...988`
• `` -&gt; `...989`
• `` -&gt; `...990`
• `` -&gt; `...991`
• `` -&gt; `...992`
• `` -&gt; `...993`
• `` -&gt; `...994`
• `` -&gt; `...995`
• `` -&gt; `...996`
• `` -&gt; `...997`
• `` -&gt; `...998`
• `` -&gt; `...999`
• `` -&gt; `...1000`</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a aria-hidden="true" href="#cb23-1" tabindex="-1"></a>sample_means_30 <span class="ot">&lt;-</span> <span class="fu">simulate_sample_means</span>(<span class="at">sample_size =</span> <span class="dv">30</span>, <span class="at">num_samples =</span> <span class="dv">1000</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>New names:
• `` -&gt; `...1`
• `` -&gt; `...2`
• `` -&gt; `...3`
• `` -&gt; `...4`
• `` -&gt; `...5`
• `` -&gt; `...6`
• `` -&gt; `...7`
• `` -&gt; `...8`
• `` -&gt; `...9`
• `` -&gt; `...10`
• `` -&gt; `...11`
• `` -&gt; `...12`
• `` -&gt; `...13`
• `` -&gt; `...14`
• `` -&gt; `...15`
• `` -&gt; `...16`
• `` -&gt; `...17`
• `` -&gt; `...18`
• `` -&gt; `...19`
• `` -&gt; `...20`
• `` -&gt; `...21`
• `` -&gt; `...22`
• `` -&gt; `...23`
• `` -&gt; `...24`
• `` -&gt; `...25`
• `` -&gt; `...26`
• `` -&gt; `...27`
• `` -&gt; `...28`
• `` -&gt; `...29`
• `` -&gt; `...30`
• `` -&gt; `...31`
• `` -&gt; `...32`
• `` -&gt; `...33`
• `` -&gt; `...34`
• `` -&gt; `...35`
• `` -&gt; `...36`
• `` -&gt; `...37`
• `` -&gt; `...38`
• `` -&gt; `...39`
• `` -&gt; `...40`
• `` -&gt; `...41`
• `` -&gt; `...42`
• `` -&gt; `...43`
• `` -&gt; `...44`
• `` -&gt; `...45`
• `` -&gt; `...46`
• `` -&gt; `...47`
• `` -&gt; `...48`
• `` -&gt; `...49`
• `` -&gt; `...50`
• `` -&gt; `...51`
• `` -&gt; `...52`
• `` -&gt; `...53`
• `` -&gt; `...54`
• `` -&gt; `...55`
• `` -&gt; `...56`
• `` -&gt; `...57`
• `` -&gt; `...58`
• `` -&gt; `...59`
• `` -&gt; `...60`
• `` -&gt; `...61`
• `` -&gt; `...62`
• `` -&gt; `...63`
• `` -&gt; `...64`
• `` -&gt; `...65`
• `` -&gt; `...66`
• `` -&gt; `...67`
• `` -&gt; `...68`
• `` -&gt; `...69`
• `` -&gt; `...70`
• `` -&gt; `...71`
• `` -&gt; `...72`
• `` -&gt; `...73`
• `` -&gt; `...74`
• `` -&gt; `...75`
• `` -&gt; `...76`
• `` -&gt; `...77`
• `` -&gt; `...78`
• `` -&gt; `...79`
• `` -&gt; `...80`
• `` -&gt; `...81`
• `` -&gt; `...82`
• `` -&gt; `...83`
• `` -&gt; `...84`
• `` -&gt; `...85`
• `` -&gt; `...86`
• `` -&gt; `...87`
• `` -&gt; `...88`
• `` -&gt; `...89`
• `` -&gt; `...90`
• `` -&gt; `...91`
• `` -&gt; `...92`
• `` -&gt; `...93`
• `` -&gt; `...94`
• `` -&gt; `...95`
• `` -&gt; `...96`
• `` -&gt; `...97`
• `` -&gt; `...98`
• `` -&gt; `...99`
• `` -&gt; `...100`
• `` -&gt; `...101`
• `` -&gt; `...102`
• `` -&gt; `...103`
• `` -&gt; `...104`
• `` -&gt; `...105`
• `` -&gt; `...106`
• `` -&gt; `...107`
• `` -&gt; `...108`
• `` -&gt; `...109`
• `` -&gt; `...110`
• `` -&gt; `...111`
• `` -&gt; `...112`
• `` -&gt; `...113`
• `` -&gt; `...114`
• `` -&gt; `...115`
• `` -&gt; `...116`
• `` -&gt; `...117`
• `` -&gt; `...118`
• `` -&gt; `...119`
• `` -&gt; `...120`
• `` -&gt; `...121`
• `` -&gt; `...122`
• `` -&gt; `...123`
• `` -&gt; `...124`
• `` -&gt; `...125`
• `` -&gt; `...126`
• `` -&gt; `...127`
• `` -&gt; `...128`
• `` -&gt; `...129`
• `` -&gt; `...130`
• `` -&gt; `...131`
• `` -&gt; `...132`
• `` -&gt; `...133`
• `` -&gt; `...134`
• `` -&gt; `...135`
• `` -&gt; `...136`
• `` -&gt; `...137`
• `` -&gt; `...138`
• `` -&gt; `...139`
• `` -&gt; `...140`
• `` -&gt; `...141`
• `` -&gt; `...142`
• `` -&gt; `...143`
• `` -&gt; `...144`
• `` -&gt; `...145`
• `` -&gt; `...146`
• `` -&gt; `...147`
• `` -&gt; `...148`
• `` -&gt; `...149`
• `` -&gt; `...150`
• `` -&gt; `...151`
• `` -&gt; `...152`
• `` -&gt; `...153`
• `` -&gt; `...154`
• `` -&gt; `...155`
• `` -&gt; `...156`
• `` -&gt; `...157`
• `` -&gt; `...158`
• `` -&gt; `...159`
• `` -&gt; `...160`
• `` -&gt; `...161`
• `` -&gt; `...162`
• `` -&gt; `...163`
• `` -&gt; `...164`
• `` -&gt; `...165`
• `` -&gt; `...166`
• `` -&gt; `...167`
• `` -&gt; `...168`
• `` -&gt; `...169`
• `` -&gt; `...170`
• `` -&gt; `...171`
• `` -&gt; `...172`
• `` -&gt; `...173`
• `` -&gt; `...174`
• `` -&gt; `...175`
• `` -&gt; `...176`
• `` -&gt; `...177`
• `` -&gt; `...178`
• `` -&gt; `...179`
• `` -&gt; `...180`
• `` -&gt; `...181`
• `` -&gt; `...182`
• `` -&gt; `...183`
• `` -&gt; `...184`
• `` -&gt; `...185`
• `` -&gt; `...186`
• `` -&gt; `...187`
• `` -&gt; `...188`
• `` -&gt; `...189`
• `` -&gt; `...190`
• `` -&gt; `...191`
• `` -&gt; `...192`
• `` -&gt; `...193`
• `` -&gt; `...194`
• `` -&gt; `...195`
• `` -&gt; `...196`
• `` -&gt; `...197`
• `` -&gt; `...198`
• `` -&gt; `...199`
• `` -&gt; `...200`
• `` -&gt; `...201`
• `` -&gt; `...202`
• `` -&gt; `...203`
• `` -&gt; `...204`
• `` -&gt; `...205`
• `` -&gt; `...206`
• `` -&gt; `...207`
• `` -&gt; `...208`
• `` -&gt; `...209`
• `` -&gt; `...210`
• `` -&gt; `...211`
• `` -&gt; `...212`
• `` -&gt; `...213`
• `` -&gt; `...214`
• `` -&gt; `...215`
• `` -&gt; `...216`
• `` -&gt; `...217`
• `` -&gt; `...218`
• `` -&gt; `...219`
• `` -&gt; `...220`
• `` -&gt; `...221`
• `` -&gt; `...222`
• `` -&gt; `...223`
• `` -&gt; `...224`
• `` -&gt; `...225`
• `` -&gt; `...226`
• `` -&gt; `...227`
• `` -&gt; `...228`
• `` -&gt; `...229`
• `` -&gt; `...230`
• `` -&gt; `...231`
• `` -&gt; `...232`
• `` -&gt; `...233`
• `` -&gt; `...234`
• `` -&gt; `...235`
• `` -&gt; `...236`
• `` -&gt; `...237`
• `` -&gt; `...238`
• `` -&gt; `...239`
• `` -&gt; `...240`
• `` -&gt; `...241`
• `` -&gt; `...242`
• `` -&gt; `...243`
• `` -&gt; `...244`
• `` -&gt; `...245`
• `` -&gt; `...246`
• `` -&gt; `...247`
• `` -&gt; `...248`
• `` -&gt; `...249`
• `` -&gt; `...250`
• `` -&gt; `...251`
• `` -&gt; `...252`
• `` -&gt; `...253`
• `` -&gt; `...254`
• `` -&gt; `...255`
• `` -&gt; `...256`
• `` -&gt; `...257`
• `` -&gt; `...258`
• `` -&gt; `...259`
• `` -&gt; `...260`
• `` -&gt; `...261`
• `` -&gt; `...262`
• `` -&gt; `...263`
• `` -&gt; `...264`
• `` -&gt; `...265`
• `` -&gt; `...266`
• `` -&gt; `...267`
• `` -&gt; `...268`
• `` -&gt; `...269`
• `` -&gt; `...270`
• `` -&gt; `...271`
• `` -&gt; `...272`
• `` -&gt; `...273`
• `` -&gt; `...274`
• `` -&gt; `...275`
• `` -&gt; `...276`
• `` -&gt; `...277`
• `` -&gt; `...278`
• `` -&gt; `...279`
• `` -&gt; `...280`
• `` -&gt; `...281`
• `` -&gt; `...282`
• `` -&gt; `...283`
• `` -&gt; `...284`
• `` -&gt; `...285`
• `` -&gt; `...286`
• `` -&gt; `...287`
• `` -&gt; `...288`
• `` -&gt; `...289`
• `` -&gt; `...290`
• `` -&gt; `...291`
• `` -&gt; `...292`
• `` -&gt; `...293`
• `` -&gt; `...294`
• `` -&gt; `...295`
• `` -&gt; `...296`
• `` -&gt; `...297`
• `` -&gt; `...298`
• `` -&gt; `...299`
• `` -&gt; `...300`
• `` -&gt; `...301`
• `` -&gt; `...302`
• `` -&gt; `...303`
• `` -&gt; `...304`
• `` -&gt; `...305`
• `` -&gt; `...306`
• `` -&gt; `...307`
• `` -&gt; `...308`
• `` -&gt; `...309`
• `` -&gt; `...310`
• `` -&gt; `...311`
• `` -&gt; `...312`
• `` -&gt; `...313`
• `` -&gt; `...314`
• `` -&gt; `...315`
• `` -&gt; `...316`
• `` -&gt; `...317`
• `` -&gt; `...318`
• `` -&gt; `...319`
• `` -&gt; `...320`
• `` -&gt; `...321`
• `` -&gt; `...322`
• `` -&gt; `...323`
• `` -&gt; `...324`
• `` -&gt; `...325`
• `` -&gt; `...326`
• `` -&gt; `...327`
• `` -&gt; `...328`
• `` -&gt; `...329`
• `` -&gt; `...330`
• `` -&gt; `...331`
• `` -&gt; `...332`
• `` -&gt; `...333`
• `` -&gt; `...334`
• `` -&gt; `...335`
• `` -&gt; `...336`
• `` -&gt; `...337`
• `` -&gt; `...338`
• `` -&gt; `...339`
• `` -&gt; `...340`
• `` -&gt; `...341`
• `` -&gt; `...342`
• `` -&gt; `...343`
• `` -&gt; `...344`
• `` -&gt; `...345`
• `` -&gt; `...346`
• `` -&gt; `...347`
• `` -&gt; `...348`
• `` -&gt; `...349`
• `` -&gt; `...350`
• `` -&gt; `...351`
• `` -&gt; `...352`
• `` -&gt; `...353`
• `` -&gt; `...354`
• `` -&gt; `...355`
• `` -&gt; `...356`
• `` -&gt; `...357`
• `` -&gt; `...358`
• `` -&gt; `...359`
• `` -&gt; `...360`
• `` -&gt; `...361`
• `` -&gt; `...362`
• `` -&gt; `...363`
• `` -&gt; `...364`
• `` -&gt; `...365`
• `` -&gt; `...366`
• `` -&gt; `...367`
• `` -&gt; `...368`
• `` -&gt; `...369`
• `` -&gt; `...370`
• `` -&gt; `...371`
• `` -&gt; `...372`
• `` -&gt; `...373`
• `` -&gt; `...374`
• `` -&gt; `...375`
• `` -&gt; `...376`
• `` -&gt; `...377`
• `` -&gt; `...378`
• `` -&gt; `...379`
• `` -&gt; `...380`
• `` -&gt; `...381`
• `` -&gt; `...382`
• `` -&gt; `...383`
• `` -&gt; `...384`
• `` -&gt; `...385`
• `` -&gt; `...386`
• `` -&gt; `...387`
• `` -&gt; `...388`
• `` -&gt; `...389`
• `` -&gt; `...390`
• `` -&gt; `...391`
• `` -&gt; `...392`
• `` -&gt; `...393`
• `` -&gt; `...394`
• `` -&gt; `...395`
• `` -&gt; `...396`
• `` -&gt; `...397`
• `` -&gt; `...398`
• `` -&gt; `...399`
• `` -&gt; `...400`
• `` -&gt; `...401`
• `` -&gt; `...402`
• `` -&gt; `...403`
• `` -&gt; `...404`
• `` -&gt; `...405`
• `` -&gt; `...406`
• `` -&gt; `...407`
• `` -&gt; `...408`
• `` -&gt; `...409`
• `` -&gt; `...410`
• `` -&gt; `...411`
• `` -&gt; `...412`
• `` -&gt; `...413`
• `` -&gt; `...414`
• `` -&gt; `...415`
• `` -&gt; `...416`
• `` -&gt; `...417`
• `` -&gt; `...418`
• `` -&gt; `...419`
• `` -&gt; `...420`
• `` -&gt; `...421`
• `` -&gt; `...422`
• `` -&gt; `...423`
• `` -&gt; `...424`
• `` -&gt; `...425`
• `` -&gt; `...426`
• `` -&gt; `...427`
• `` -&gt; `...428`
• `` -&gt; `...429`
• `` -&gt; `...430`
• `` -&gt; `...431`
• `` -&gt; `...432`
• `` -&gt; `...433`
• `` -&gt; `...434`
• `` -&gt; `...435`
• `` -&gt; `...436`
• `` -&gt; `...437`
• `` -&gt; `...438`
• `` -&gt; `...439`
• `` -&gt; `...440`
• `` -&gt; `...441`
• `` -&gt; `...442`
• `` -&gt; `...443`
• `` -&gt; `...444`
• `` -&gt; `...445`
• `` -&gt; `...446`
• `` -&gt; `...447`
• `` -&gt; `...448`
• `` -&gt; `...449`
• `` -&gt; `...450`
• `` -&gt; `...451`
• `` -&gt; `...452`
• `` -&gt; `...453`
• `` -&gt; `...454`
• `` -&gt; `...455`
• `` -&gt; `...456`
• `` -&gt; `...457`
• `` -&gt; `...458`
• `` -&gt; `...459`
• `` -&gt; `...460`
• `` -&gt; `...461`
• `` -&gt; `...462`
• `` -&gt; `...463`
• `` -&gt; `...464`
• `` -&gt; `...465`
• `` -&gt; `...466`
• `` -&gt; `...467`
• `` -&gt; `...468`
• `` -&gt; `...469`
• `` -&gt; `...470`
• `` -&gt; `...471`
• `` -&gt; `...472`
• `` -&gt; `...473`
• `` -&gt; `...474`
• `` -&gt; `...475`
• `` -&gt; `...476`
• `` -&gt; `...477`
• `` -&gt; `...478`
• `` -&gt; `...479`
• `` -&gt; `...480`
• `` -&gt; `...481`
• `` -&gt; `...482`
• `` -&gt; `...483`
• `` -&gt; `...484`
• `` -&gt; `...485`
• `` -&gt; `...486`
• `` -&gt; `...487`
• `` -&gt; `...488`
• `` -&gt; `...489`
• `` -&gt; `...490`
• `` -&gt; `...491`
• `` -&gt; `...492`
• `` -&gt; `...493`
• `` -&gt; `...494`
• `` -&gt; `...495`
• `` -&gt; `...496`
• `` -&gt; `...497`
• `` -&gt; `...498`
• `` -&gt; `...499`
• `` -&gt; `...500`
• `` -&gt; `...501`
• `` -&gt; `...502`
• `` -&gt; `...503`
• `` -&gt; `...504`
• `` -&gt; `...505`
• `` -&gt; `...506`
• `` -&gt; `...507`
• `` -&gt; `...508`
• `` -&gt; `...509`
• `` -&gt; `...510`
• `` -&gt; `...511`
• `` -&gt; `...512`
• `` -&gt; `...513`
• `` -&gt; `...514`
• `` -&gt; `...515`
• `` -&gt; `...516`
• `` -&gt; `...517`
• `` -&gt; `...518`
• `` -&gt; `...519`
• `` -&gt; `...520`
• `` -&gt; `...521`
• `` -&gt; `...522`
• `` -&gt; `...523`
• `` -&gt; `...524`
• `` -&gt; `...525`
• `` -&gt; `...526`
• `` -&gt; `...527`
• `` -&gt; `...528`
• `` -&gt; `...529`
• `` -&gt; `...530`
• `` -&gt; `...531`
• `` -&gt; `...532`
• `` -&gt; `...533`
• `` -&gt; `...534`
• `` -&gt; `...535`
• `` -&gt; `...536`
• `` -&gt; `...537`
• `` -&gt; `...538`
• `` -&gt; `...539`
• `` -&gt; `...540`
• `` -&gt; `...541`
• `` -&gt; `...542`
• `` -&gt; `...543`
• `` -&gt; `...544`
• `` -&gt; `...545`
• `` -&gt; `...546`
• `` -&gt; `...547`
• `` -&gt; `...548`
• `` -&gt; `...549`
• `` -&gt; `...550`
• `` -&gt; `...551`
• `` -&gt; `...552`
• `` -&gt; `...553`
• `` -&gt; `...554`
• `` -&gt; `...555`
• `` -&gt; `...556`
• `` -&gt; `...557`
• `` -&gt; `...558`
• `` -&gt; `...559`
• `` -&gt; `...560`
• `` -&gt; `...561`
• `` -&gt; `...562`
• `` -&gt; `...563`
• `` -&gt; `...564`
• `` -&gt; `...565`
• `` -&gt; `...566`
• `` -&gt; `...567`
• `` -&gt; `...568`
• `` -&gt; `...569`
• `` -&gt; `...570`
• `` -&gt; `...571`
• `` -&gt; `...572`
• `` -&gt; `...573`
• `` -&gt; `...574`
• `` -&gt; `...575`
• `` -&gt; `...576`
• `` -&gt; `...577`
• `` -&gt; `...578`
• `` -&gt; `...579`
• `` -&gt; `...580`
• `` -&gt; `...581`
• `` -&gt; `...582`
• `` -&gt; `...583`
• `` -&gt; `...584`
• `` -&gt; `...585`
• `` -&gt; `...586`
• `` -&gt; `...587`
• `` -&gt; `...588`
• `` -&gt; `...589`
• `` -&gt; `...590`
• `` -&gt; `...591`
• `` -&gt; `...592`
• `` -&gt; `...593`
• `` -&gt; `...594`
• `` -&gt; `...595`
• `` -&gt; `...596`
• `` -&gt; `...597`
• `` -&gt; `...598`
• `` -&gt; `...599`
• `` -&gt; `...600`
• `` -&gt; `...601`
• `` -&gt; `...602`
• `` -&gt; `...603`
• `` -&gt; `...604`
• `` -&gt; `...605`
• `` -&gt; `...606`
• `` -&gt; `...607`
• `` -&gt; `...608`
• `` -&gt; `...609`
• `` -&gt; `...610`
• `` -&gt; `...611`
• `` -&gt; `...612`
• `` -&gt; `...613`
• `` -&gt; `...614`
• `` -&gt; `...615`
• `` -&gt; `...616`
• `` -&gt; `...617`
• `` -&gt; `...618`
• `` -&gt; `...619`
• `` -&gt; `...620`
• `` -&gt; `...621`
• `` -&gt; `...622`
• `` -&gt; `...623`
• `` -&gt; `...624`
• `` -&gt; `...625`
• `` -&gt; `...626`
• `` -&gt; `...627`
• `` -&gt; `...628`
• `` -&gt; `...629`
• `` -&gt; `...630`
• `` -&gt; `...631`
• `` -&gt; `...632`
• `` -&gt; `...633`
• `` -&gt; `...634`
• `` -&gt; `...635`
• `` -&gt; `...636`
• `` -&gt; `...637`
• `` -&gt; `...638`
• `` -&gt; `...639`
• `` -&gt; `...640`
• `` -&gt; `...641`
• `` -&gt; `...642`
• `` -&gt; `...643`
• `` -&gt; `...644`
• `` -&gt; `...645`
• `` -&gt; `...646`
• `` -&gt; `...647`
• `` -&gt; `...648`
• `` -&gt; `...649`
• `` -&gt; `...650`
• `` -&gt; `...651`
• `` -&gt; `...652`
• `` -&gt; `...653`
• `` -&gt; `...654`
• `` -&gt; `...655`
• `` -&gt; `...656`
• `` -&gt; `...657`
• `` -&gt; `...658`
• `` -&gt; `...659`
• `` -&gt; `...660`
• `` -&gt; `...661`
• `` -&gt; `...662`
• `` -&gt; `...663`
• `` -&gt; `...664`
• `` -&gt; `...665`
• `` -&gt; `...666`
• `` -&gt; `...667`
• `` -&gt; `...668`
• `` -&gt; `...669`
• `` -&gt; `...670`
• `` -&gt; `...671`
• `` -&gt; `...672`
• `` -&gt; `...673`
• `` -&gt; `...674`
• `` -&gt; `...675`
• `` -&gt; `...676`
• `` -&gt; `...677`
• `` -&gt; `...678`
• `` -&gt; `...679`
• `` -&gt; `...680`
• `` -&gt; `...681`
• `` -&gt; `...682`
• `` -&gt; `...683`
• `` -&gt; `...684`
• `` -&gt; `...685`
• `` -&gt; `...686`
• `` -&gt; `...687`
• `` -&gt; `...688`
• `` -&gt; `...689`
• `` -&gt; `...690`
• `` -&gt; `...691`
• `` -&gt; `...692`
• `` -&gt; `...693`
• `` -&gt; `...694`
• `` -&gt; `...695`
• `` -&gt; `...696`
• `` -&gt; `...697`
• `` -&gt; `...698`
• `` -&gt; `...699`
• `` -&gt; `...700`
• `` -&gt; `...701`
• `` -&gt; `...702`
• `` -&gt; `...703`
• `` -&gt; `...704`
• `` -&gt; `...705`
• `` -&gt; `...706`
• `` -&gt; `...707`
• `` -&gt; `...708`
• `` -&gt; `...709`
• `` -&gt; `...710`
• `` -&gt; `...711`
• `` -&gt; `...712`
• `` -&gt; `...713`
• `` -&gt; `...714`
• `` -&gt; `...715`
• `` -&gt; `...716`
• `` -&gt; `...717`
• `` -&gt; `...718`
• `` -&gt; `...719`
• `` -&gt; `...720`
• `` -&gt; `...721`
• `` -&gt; `...722`
• `` -&gt; `...723`
• `` -&gt; `...724`
• `` -&gt; `...725`
• `` -&gt; `...726`
• `` -&gt; `...727`
• `` -&gt; `...728`
• `` -&gt; `...729`
• `` -&gt; `...730`
• `` -&gt; `...731`
• `` -&gt; `...732`
• `` -&gt; `...733`
• `` -&gt; `...734`
• `` -&gt; `...735`
• `` -&gt; `...736`
• `` -&gt; `...737`
• `` -&gt; `...738`
• `` -&gt; `...739`
• `` -&gt; `...740`
• `` -&gt; `...741`
• `` -&gt; `...742`
• `` -&gt; `...743`
• `` -&gt; `...744`
• `` -&gt; `...745`
• `` -&gt; `...746`
• `` -&gt; `...747`
• `` -&gt; `...748`
• `` -&gt; `...749`
• `` -&gt; `...750`
• `` -&gt; `...751`
• `` -&gt; `...752`
• `` -&gt; `...753`
• `` -&gt; `...754`
• `` -&gt; `...755`
• `` -&gt; `...756`
• `` -&gt; `...757`
• `` -&gt; `...758`
• `` -&gt; `...759`
• `` -&gt; `...760`
• `` -&gt; `...761`
• `` -&gt; `...762`
• `` -&gt; `...763`
• `` -&gt; `...764`
• `` -&gt; `...765`
• `` -&gt; `...766`
• `` -&gt; `...767`
• `` -&gt; `...768`
• `` -&gt; `...769`
• `` -&gt; `...770`
• `` -&gt; `...771`
• `` -&gt; `...772`
• `` -&gt; `...773`
• `` -&gt; `...774`
• `` -&gt; `...775`
• `` -&gt; `...776`
• `` -&gt; `...777`
• `` -&gt; `...778`
• `` -&gt; `...779`
• `` -&gt; `...780`
• `` -&gt; `...781`
• `` -&gt; `...782`
• `` -&gt; `...783`
• `` -&gt; `...784`
• `` -&gt; `...785`
• `` -&gt; `...786`
• `` -&gt; `...787`
• `` -&gt; `...788`
• `` -&gt; `...789`
• `` -&gt; `...790`
• `` -&gt; `...791`
• `` -&gt; `...792`
• `` -&gt; `...793`
• `` -&gt; `...794`
• `` -&gt; `...795`
• `` -&gt; `...796`
• `` -&gt; `...797`
• `` -&gt; `...798`
• `` -&gt; `...799`
• `` -&gt; `...800`
• `` -&gt; `...801`
• `` -&gt; `...802`
• `` -&gt; `...803`
• `` -&gt; `...804`
• `` -&gt; `...805`
• `` -&gt; `...806`
• `` -&gt; `...807`
• `` -&gt; `...808`
• `` -&gt; `...809`
• `` -&gt; `...810`
• `` -&gt; `...811`
• `` -&gt; `...812`
• `` -&gt; `...813`
• `` -&gt; `...814`
• `` -&gt; `...815`
• `` -&gt; `...816`
• `` -&gt; `...817`
• `` -&gt; `...818`
• `` -&gt; `...819`
• `` -&gt; `...820`
• `` -&gt; `...821`
• `` -&gt; `...822`
• `` -&gt; `...823`
• `` -&gt; `...824`
• `` -&gt; `...825`
• `` -&gt; `...826`
• `` -&gt; `...827`
• `` -&gt; `...828`
• `` -&gt; `...829`
• `` -&gt; `...830`
• `` -&gt; `...831`
• `` -&gt; `...832`
• `` -&gt; `...833`
• `` -&gt; `...834`
• `` -&gt; `...835`
• `` -&gt; `...836`
• `` -&gt; `...837`
• `` -&gt; `...838`
• `` -&gt; `...839`
• `` -&gt; `...840`
• `` -&gt; `...841`
• `` -&gt; `...842`
• `` -&gt; `...843`
• `` -&gt; `...844`
• `` -&gt; `...845`
• `` -&gt; `...846`
• `` -&gt; `...847`
• `` -&gt; `...848`
• `` -&gt; `...849`
• `` -&gt; `...850`
• `` -&gt; `...851`
• `` -&gt; `...852`
• `` -&gt; `...853`
• `` -&gt; `...854`
• `` -&gt; `...855`
• `` -&gt; `...856`
• `` -&gt; `...857`
• `` -&gt; `...858`
• `` -&gt; `...859`
• `` -&gt; `...860`
• `` -&gt; `...861`
• `` -&gt; `...862`
• `` -&gt; `...863`
• `` -&gt; `...864`
• `` -&gt; `...865`
• `` -&gt; `...866`
• `` -&gt; `...867`
• `` -&gt; `...868`
• `` -&gt; `...869`
• `` -&gt; `...870`
• `` -&gt; `...871`
• `` -&gt; `...872`
• `` -&gt; `...873`
• `` -&gt; `...874`
• `` -&gt; `...875`
• `` -&gt; `...876`
• `` -&gt; `...877`
• `` -&gt; `...878`
• `` -&gt; `...879`
• `` -&gt; `...880`
• `` -&gt; `...881`
• `` -&gt; `...882`
• `` -&gt; `...883`
• `` -&gt; `...884`
• `` -&gt; `...885`
• `` -&gt; `...886`
• `` -&gt; `...887`
• `` -&gt; `...888`
• `` -&gt; `...889`
• `` -&gt; `...890`
• `` -&gt; `...891`
• `` -&gt; `...892`
• `` -&gt; `...893`
• `` -&gt; `...894`
• `` -&gt; `...895`
• `` -&gt; `...896`
• `` -&gt; `...897`
• `` -&gt; `...898`
• `` -&gt; `...899`
• `` -&gt; `...900`
• `` -&gt; `...901`
• `` -&gt; `...902`
• `` -&gt; `...903`
• `` -&gt; `...904`
• `` -&gt; `...905`
• `` -&gt; `...906`
• `` -&gt; `...907`
• `` -&gt; `...908`
• `` -&gt; `...909`
• `` -&gt; `...910`
• `` -&gt; `...911`
• `` -&gt; `...912`
• `` -&gt; `...913`
• `` -&gt; `...914`
• `` -&gt; `...915`
• `` -&gt; `...916`
• `` -&gt; `...917`
• `` -&gt; `...918`
• `` -&gt; `...919`
• `` -&gt; `...920`
• `` -&gt; `...921`
• `` -&gt; `...922`
• `` -&gt; `...923`
• `` -&gt; `...924`
• `` -&gt; `...925`
• `` -&gt; `...926`
• `` -&gt; `...927`
• `` -&gt; `...928`
• `` -&gt; `...929`
• `` -&gt; `...930`
• `` -&gt; `...931`
• `` -&gt; `...932`
• `` -&gt; `...933`
• `` -&gt; `...934`
• `` -&gt; `...935`
• `` -&gt; `...936`
• `` -&gt; `...937`
• `` -&gt; `...938`
• `` -&gt; `...939`
• `` -&gt; `...940`
• `` -&gt; `...941`
• `` -&gt; `...942`
• `` -&gt; `...943`
• `` -&gt; `...944`
• `` -&gt; `...945`
• `` -&gt; `...946`
• `` -&gt; `...947`
• `` -&gt; `...948`
• `` -&gt; `...949`
• `` -&gt; `...950`
• `` -&gt; `...951`
• `` -&gt; `...952`
• `` -&gt; `...953`
• `` -&gt; `...954`
• `` -&gt; `...955`
• `` -&gt; `...956`
• `` -&gt; `...957`
• `` -&gt; `...958`
• `` -&gt; `...959`
• `` -&gt; `...960`
• `` -&gt; `...961`
• `` -&gt; `...962`
• `` -&gt; `...963`
• `` -&gt; `...964`
• `` -&gt; `...965`
• `` -&gt; `...966`
• `` -&gt; `...967`
• `` -&gt; `...968`
• `` -&gt; `...969`
• `` -&gt; `...970`
• `` -&gt; `...971`
• `` -&gt; `...972`
• `` -&gt; `...973`
• `` -&gt; `...974`
• `` -&gt; `...975`
• `` -&gt; `...976`
• `` -&gt; `...977`
• `` -&gt; `...978`
• `` -&gt; `...979`
• `` -&gt; `...980`
• `` -&gt; `...981`
• `` -&gt; `...982`
• `` -&gt; `...983`
• `` -&gt; `...984`
• `` -&gt; `...985`
• `` -&gt; `...986`
• `` -&gt; `...987`
• `` -&gt; `...988`
• `` -&gt; `...989`
• `` -&gt; `...990`
• `` -&gt; `...991`
• `` -&gt; `...992`
• `` -&gt; `...993`
• `` -&gt; `...994`
• `` -&gt; `...995`
• `` -&gt; `...996`
• `` -&gt; `...997`
• `` -&gt; `...998`
• `` -&gt; `...999`
• `` -&gt; `...1000`</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a aria-hidden="true" href="#cb25-1" tabindex="-1"></a>sample_means_100 <span class="ot">&lt;-</span> <span class="fu">simulate_sample_means</span>(<span class="at">sample_size =</span> <span class="dv">100</span>, <span class="at">num_samples =</span> <span class="dv">1000</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>New names:
• `` -&gt; `...1`
• `` -&gt; `...2`
• `` -&gt; `...3`
• `` -&gt; `...4`
• `` -&gt; `...5`
• `` -&gt; `...6`
• `` -&gt; `...7`
• `` -&gt; `...8`
• `` -&gt; `...9`
• `` -&gt; `...10`
• `` -&gt; `...11`
• `` -&gt; `...12`
• `` -&gt; `...13`
• `` -&gt; `...14`
• `` -&gt; `...15`
• `` -&gt; `...16`
• `` -&gt; `...17`
• `` -&gt; `...18`
• `` -&gt; `...19`
• `` -&gt; `...20`
• `` -&gt; `...21`
• `` -&gt; `...22`
• `` -&gt; `...23`
• `` -&gt; `...24`
• `` -&gt; `...25`
• `` -&gt; `...26`
• `` -&gt; `...27`
• `` -&gt; `...28`
• `` -&gt; `...29`
• `` -&gt; `...30`
• `` -&gt; `...31`
• `` -&gt; `...32`
• `` -&gt; `...33`
• `` -&gt; `...34`
• `` -&gt; `...35`
• `` -&gt; `...36`
• `` -&gt; `...37`
• `` -&gt; `...38`
• `` -&gt; `...39`
• `` -&gt; `...40`
• `` -&gt; `...41`
• `` -&gt; `...42`
• `` -&gt; `...43`
• `` -&gt; `...44`
• `` -&gt; `...45`
• `` -&gt; `...46`
• `` -&gt; `...47`
• `` -&gt; `...48`
• `` -&gt; `...49`
• `` -&gt; `...50`
• `` -&gt; `...51`
• `` -&gt; `...52`
• `` -&gt; `...53`
• `` -&gt; `...54`
• `` -&gt; `...55`
• `` -&gt; `...56`
• `` -&gt; `...57`
• `` -&gt; `...58`
• `` -&gt; `...59`
• `` -&gt; `...60`
• `` -&gt; `...61`
• `` -&gt; `...62`
• `` -&gt; `...63`
• `` -&gt; `...64`
• `` -&gt; `...65`
• `` -&gt; `...66`
• `` -&gt; `...67`
• `` -&gt; `...68`
• `` -&gt; `...69`
• `` -&gt; `...70`
• `` -&gt; `...71`
• `` -&gt; `...72`
• `` -&gt; `...73`
• `` -&gt; `...74`
• `` -&gt; `...75`
• `` -&gt; `...76`
• `` -&gt; `...77`
• `` -&gt; `...78`
• `` -&gt; `...79`
• `` -&gt; `...80`
• `` -&gt; `...81`
• `` -&gt; `...82`
• `` -&gt; `...83`
• `` -&gt; `...84`
• `` -&gt; `...85`
• `` -&gt; `...86`
• `` -&gt; `...87`
• `` -&gt; `...88`
• `` -&gt; `...89`
• `` -&gt; `...90`
• `` -&gt; `...91`
• `` -&gt; `...92`
• `` -&gt; `...93`
• `` -&gt; `...94`
• `` -&gt; `...95`
• `` -&gt; `...96`
• `` -&gt; `...97`
• `` -&gt; `...98`
• `` -&gt; `...99`
• `` -&gt; `...100`
• `` -&gt; `...101`
• `` -&gt; `...102`
• `` -&gt; `...103`
• `` -&gt; `...104`
• `` -&gt; `...105`
• `` -&gt; `...106`
• `` -&gt; `...107`
• `` -&gt; `...108`
• `` -&gt; `...109`
• `` -&gt; `...110`
• `` -&gt; `...111`
• `` -&gt; `...112`
• `` -&gt; `...113`
• `` -&gt; `...114`
• `` -&gt; `...115`
• `` -&gt; `...116`
• `` -&gt; `...117`
• `` -&gt; `...118`
• `` -&gt; `...119`
• `` -&gt; `...120`
• `` -&gt; `...121`
• `` -&gt; `...122`
• `` -&gt; `...123`
• `` -&gt; `...124`
• `` -&gt; `...125`
• `` -&gt; `...126`
• `` -&gt; `...127`
• `` -&gt; `...128`
• `` -&gt; `...129`
• `` -&gt; `...130`
• `` -&gt; `...131`
• `` -&gt; `...132`
• `` -&gt; `...133`
• `` -&gt; `...134`
• `` -&gt; `...135`
• `` -&gt; `...136`
• `` -&gt; `...137`
• `` -&gt; `...138`
• `` -&gt; `...139`
• `` -&gt; `...140`
• `` -&gt; `...141`
• `` -&gt; `...142`
• `` -&gt; `...143`
• `` -&gt; `...144`
• `` -&gt; `...145`
• `` -&gt; `...146`
• `` -&gt; `...147`
• `` -&gt; `...148`
• `` -&gt; `...149`
• `` -&gt; `...150`
• `` -&gt; `...151`
• `` -&gt; `...152`
• `` -&gt; `...153`
• `` -&gt; `...154`
• `` -&gt; `...155`
• `` -&gt; `...156`
• `` -&gt; `...157`
• `` -&gt; `...158`
• `` -&gt; `...159`
• `` -&gt; `...160`
• `` -&gt; `...161`
• `` -&gt; `...162`
• `` -&gt; `...163`
• `` -&gt; `...164`
• `` -&gt; `...165`
• `` -&gt; `...166`
• `` -&gt; `...167`
• `` -&gt; `...168`
• `` -&gt; `...169`
• `` -&gt; `...170`
• `` -&gt; `...171`
• `` -&gt; `...172`
• `` -&gt; `...173`
• `` -&gt; `...174`
• `` -&gt; `...175`
• `` -&gt; `...176`
• `` -&gt; `...177`
• `` -&gt; `...178`
• `` -&gt; `...179`
• `` -&gt; `...180`
• `` -&gt; `...181`
• `` -&gt; `...182`
• `` -&gt; `...183`
• `` -&gt; `...184`
• `` -&gt; `...185`
• `` -&gt; `...186`
• `` -&gt; `...187`
• `` -&gt; `...188`
• `` -&gt; `...189`
• `` -&gt; `...190`
• `` -&gt; `...191`
• `` -&gt; `...192`
• `` -&gt; `...193`
• `` -&gt; `...194`
• `` -&gt; `...195`
• `` -&gt; `...196`
• `` -&gt; `...197`
• `` -&gt; `...198`
• `` -&gt; `...199`
• `` -&gt; `...200`
• `` -&gt; `...201`
• `` -&gt; `...202`
• `` -&gt; `...203`
• `` -&gt; `...204`
• `` -&gt; `...205`
• `` -&gt; `...206`
• `` -&gt; `...207`
• `` -&gt; `...208`
• `` -&gt; `...209`
• `` -&gt; `...210`
• `` -&gt; `...211`
• `` -&gt; `...212`
• `` -&gt; `...213`
• `` -&gt; `...214`
• `` -&gt; `...215`
• `` -&gt; `...216`
• `` -&gt; `...217`
• `` -&gt; `...218`
• `` -&gt; `...219`
• `` -&gt; `...220`
• `` -&gt; `...221`
• `` -&gt; `...222`
• `` -&gt; `...223`
• `` -&gt; `...224`
• `` -&gt; `...225`
• `` -&gt; `...226`
• `` -&gt; `...227`
• `` -&gt; `...228`
• `` -&gt; `...229`
• `` -&gt; `...230`
• `` -&gt; `...231`
• `` -&gt; `...232`
• `` -&gt; `...233`
• `` -&gt; `...234`
• `` -&gt; `...235`
• `` -&gt; `...236`
• `` -&gt; `...237`
• `` -&gt; `...238`
• `` -&gt; `...239`
• `` -&gt; `...240`
• `` -&gt; `...241`
• `` -&gt; `...242`
• `` -&gt; `...243`
• `` -&gt; `...244`
• `` -&gt; `...245`
• `` -&gt; `...246`
• `` -&gt; `...247`
• `` -&gt; `...248`
• `` -&gt; `...249`
• `` -&gt; `...250`
• `` -&gt; `...251`
• `` -&gt; `...252`
• `` -&gt; `...253`
• `` -&gt; `...254`
• `` -&gt; `...255`
• `` -&gt; `...256`
• `` -&gt; `...257`
• `` -&gt; `...258`
• `` -&gt; `...259`
• `` -&gt; `...260`
• `` -&gt; `...261`
• `` -&gt; `...262`
• `` -&gt; `...263`
• `` -&gt; `...264`
• `` -&gt; `...265`
• `` -&gt; `...266`
• `` -&gt; `...267`
• `` -&gt; `...268`
• `` -&gt; `...269`
• `` -&gt; `...270`
• `` -&gt; `...271`
• `` -&gt; `...272`
• `` -&gt; `...273`
• `` -&gt; `...274`
• `` -&gt; `...275`
• `` -&gt; `...276`
• `` -&gt; `...277`
• `` -&gt; `...278`
• `` -&gt; `...279`
• `` -&gt; `...280`
• `` -&gt; `...281`
• `` -&gt; `...282`
• `` -&gt; `...283`
• `` -&gt; `...284`
• `` -&gt; `...285`
• `` -&gt; `...286`
• `` -&gt; `...287`
• `` -&gt; `...288`
• `` -&gt; `...289`
• `` -&gt; `...290`
• `` -&gt; `...291`
• `` -&gt; `...292`
• `` -&gt; `...293`
• `` -&gt; `...294`
• `` -&gt; `...295`
• `` -&gt; `...296`
• `` -&gt; `...297`
• `` -&gt; `...298`
• `` -&gt; `...299`
• `` -&gt; `...300`
• `` -&gt; `...301`
• `` -&gt; `...302`
• `` -&gt; `...303`
• `` -&gt; `...304`
• `` -&gt; `...305`
• `` -&gt; `...306`
• `` -&gt; `...307`
• `` -&gt; `...308`
• `` -&gt; `...309`
• `` -&gt; `...310`
• `` -&gt; `...311`
• `` -&gt; `...312`
• `` -&gt; `...313`
• `` -&gt; `...314`
• `` -&gt; `...315`
• `` -&gt; `...316`
• `` -&gt; `...317`
• `` -&gt; `...318`
• `` -&gt; `...319`
• `` -&gt; `...320`
• `` -&gt; `...321`
• `` -&gt; `...322`
• `` -&gt; `...323`
• `` -&gt; `...324`
• `` -&gt; `...325`
• `` -&gt; `...326`
• `` -&gt; `...327`
• `` -&gt; `...328`
• `` -&gt; `...329`
• `` -&gt; `...330`
• `` -&gt; `...331`
• `` -&gt; `...332`
• `` -&gt; `...333`
• `` -&gt; `...334`
• `` -&gt; `...335`
• `` -&gt; `...336`
• `` -&gt; `...337`
• `` -&gt; `...338`
• `` -&gt; `...339`
• `` -&gt; `...340`
• `` -&gt; `...341`
• `` -&gt; `...342`
• `` -&gt; `...343`
• `` -&gt; `...344`
• `` -&gt; `...345`
• `` -&gt; `...346`
• `` -&gt; `...347`
• `` -&gt; `...348`
• `` -&gt; `...349`
• `` -&gt; `...350`
• `` -&gt; `...351`
• `` -&gt; `...352`
• `` -&gt; `...353`
• `` -&gt; `...354`
• `` -&gt; `...355`
• `` -&gt; `...356`
• `` -&gt; `...357`
• `` -&gt; `...358`
• `` -&gt; `...359`
• `` -&gt; `...360`
• `` -&gt; `...361`
• `` -&gt; `...362`
• `` -&gt; `...363`
• `` -&gt; `...364`
• `` -&gt; `...365`
• `` -&gt; `...366`
• `` -&gt; `...367`
• `` -&gt; `...368`
• `` -&gt; `...369`
• `` -&gt; `...370`
• `` -&gt; `...371`
• `` -&gt; `...372`
• `` -&gt; `...373`
• `` -&gt; `...374`
• `` -&gt; `...375`
• `` -&gt; `...376`
• `` -&gt; `...377`
• `` -&gt; `...378`
• `` -&gt; `...379`
• `` -&gt; `...380`
• `` -&gt; `...381`
• `` -&gt; `...382`
• `` -&gt; `...383`
• `` -&gt; `...384`
• `` -&gt; `...385`
• `` -&gt; `...386`
• `` -&gt; `...387`
• `` -&gt; `...388`
• `` -&gt; `...389`
• `` -&gt; `...390`
• `` -&gt; `...391`
• `` -&gt; `...392`
• `` -&gt; `...393`
• `` -&gt; `...394`
• `` -&gt; `...395`
• `` -&gt; `...396`
• `` -&gt; `...397`
• `` -&gt; `...398`
• `` -&gt; `...399`
• `` -&gt; `...400`
• `` -&gt; `...401`
• `` -&gt; `...402`
• `` -&gt; `...403`
• `` -&gt; `...404`
• `` -&gt; `...405`
• `` -&gt; `...406`
• `` -&gt; `...407`
• `` -&gt; `...408`
• `` -&gt; `...409`
• `` -&gt; `...410`
• `` -&gt; `...411`
• `` -&gt; `...412`
• `` -&gt; `...413`
• `` -&gt; `...414`
• `` -&gt; `...415`
• `` -&gt; `...416`
• `` -&gt; `...417`
• `` -&gt; `...418`
• `` -&gt; `...419`
• `` -&gt; `...420`
• `` -&gt; `...421`
• `` -&gt; `...422`
• `` -&gt; `...423`
• `` -&gt; `...424`
• `` -&gt; `...425`
• `` -&gt; `...426`
• `` -&gt; `...427`
• `` -&gt; `...428`
• `` -&gt; `...429`
• `` -&gt; `...430`
• `` -&gt; `...431`
• `` -&gt; `...432`
• `` -&gt; `...433`
• `` -&gt; `...434`
• `` -&gt; `...435`
• `` -&gt; `...436`
• `` -&gt; `...437`
• `` -&gt; `...438`
• `` -&gt; `...439`
• `` -&gt; `...440`
• `` -&gt; `...441`
• `` -&gt; `...442`
• `` -&gt; `...443`
• `` -&gt; `...444`
• `` -&gt; `...445`
• `` -&gt; `...446`
• `` -&gt; `...447`
• `` -&gt; `...448`
• `` -&gt; `...449`
• `` -&gt; `...450`
• `` -&gt; `...451`
• `` -&gt; `...452`
• `` -&gt; `...453`
• `` -&gt; `...454`
• `` -&gt; `...455`
• `` -&gt; `...456`
• `` -&gt; `...457`
• `` -&gt; `...458`
• `` -&gt; `...459`
• `` -&gt; `...460`
• `` -&gt; `...461`
• `` -&gt; `...462`
• `` -&gt; `...463`
• `` -&gt; `...464`
• `` -&gt; `...465`
• `` -&gt; `...466`
• `` -&gt; `...467`
• `` -&gt; `...468`
• `` -&gt; `...469`
• `` -&gt; `...470`
• `` -&gt; `...471`
• `` -&gt; `...472`
• `` -&gt; `...473`
• `` -&gt; `...474`
• `` -&gt; `...475`
• `` -&gt; `...476`
• `` -&gt; `...477`
• `` -&gt; `...478`
• `` -&gt; `...479`
• `` -&gt; `...480`
• `` -&gt; `...481`
• `` -&gt; `...482`
• `` -&gt; `...483`
• `` -&gt; `...484`
• `` -&gt; `...485`
• `` -&gt; `...486`
• `` -&gt; `...487`
• `` -&gt; `...488`
• `` -&gt; `...489`
• `` -&gt; `...490`
• `` -&gt; `...491`
• `` -&gt; `...492`
• `` -&gt; `...493`
• `` -&gt; `...494`
• `` -&gt; `...495`
• `` -&gt; `...496`
• `` -&gt; `...497`
• `` -&gt; `...498`
• `` -&gt; `...499`
• `` -&gt; `...500`
• `` -&gt; `...501`
• `` -&gt; `...502`
• `` -&gt; `...503`
• `` -&gt; `...504`
• `` -&gt; `...505`
• `` -&gt; `...506`
• `` -&gt; `...507`
• `` -&gt; `...508`
• `` -&gt; `...509`
• `` -&gt; `...510`
• `` -&gt; `...511`
• `` -&gt; `...512`
• `` -&gt; `...513`
• `` -&gt; `...514`
• `` -&gt; `...515`
• `` -&gt; `...516`
• `` -&gt; `...517`
• `` -&gt; `...518`
• `` -&gt; `...519`
• `` -&gt; `...520`
• `` -&gt; `...521`
• `` -&gt; `...522`
• `` -&gt; `...523`
• `` -&gt; `...524`
• `` -&gt; `...525`
• `` -&gt; `...526`
• `` -&gt; `...527`
• `` -&gt; `...528`
• `` -&gt; `...529`
• `` -&gt; `...530`
• `` -&gt; `...531`
• `` -&gt; `...532`
• `` -&gt; `...533`
• `` -&gt; `...534`
• `` -&gt; `...535`
• `` -&gt; `...536`
• `` -&gt; `...537`
• `` -&gt; `...538`
• `` -&gt; `...539`
• `` -&gt; `...540`
• `` -&gt; `...541`
• `` -&gt; `...542`
• `` -&gt; `...543`
• `` -&gt; `...544`
• `` -&gt; `...545`
• `` -&gt; `...546`
• `` -&gt; `...547`
• `` -&gt; `...548`
• `` -&gt; `...549`
• `` -&gt; `...550`
• `` -&gt; `...551`
• `` -&gt; `...552`
• `` -&gt; `...553`
• `` -&gt; `...554`
• `` -&gt; `...555`
• `` -&gt; `...556`
• `` -&gt; `...557`
• `` -&gt; `...558`
• `` -&gt; `...559`
• `` -&gt; `...560`
• `` -&gt; `...561`
• `` -&gt; `...562`
• `` -&gt; `...563`
• `` -&gt; `...564`
• `` -&gt; `...565`
• `` -&gt; `...566`
• `` -&gt; `...567`
• `` -&gt; `...568`
• `` -&gt; `...569`
• `` -&gt; `...570`
• `` -&gt; `...571`
• `` -&gt; `...572`
• `` -&gt; `...573`
• `` -&gt; `...574`
• `` -&gt; `...575`
• `` -&gt; `...576`
• `` -&gt; `...577`
• `` -&gt; `...578`
• `` -&gt; `...579`
• `` -&gt; `...580`
• `` -&gt; `...581`
• `` -&gt; `...582`
• `` -&gt; `...583`
• `` -&gt; `...584`
• `` -&gt; `...585`
• `` -&gt; `...586`
• `` -&gt; `...587`
• `` -&gt; `...588`
• `` -&gt; `...589`
• `` -&gt; `...590`
• `` -&gt; `...591`
• `` -&gt; `...592`
• `` -&gt; `...593`
• `` -&gt; `...594`
• `` -&gt; `...595`
• `` -&gt; `...596`
• `` -&gt; `...597`
• `` -&gt; `...598`
• `` -&gt; `...599`
• `` -&gt; `...600`
• `` -&gt; `...601`
• `` -&gt; `...602`
• `` -&gt; `...603`
• `` -&gt; `...604`
• `` -&gt; `...605`
• `` -&gt; `...606`
• `` -&gt; `...607`
• `` -&gt; `...608`
• `` -&gt; `...609`
• `` -&gt; `...610`
• `` -&gt; `...611`
• `` -&gt; `...612`
• `` -&gt; `...613`
• `` -&gt; `...614`
• `` -&gt; `...615`
• `` -&gt; `...616`
• `` -&gt; `...617`
• `` -&gt; `...618`
• `` -&gt; `...619`
• `` -&gt; `...620`
• `` -&gt; `...621`
• `` -&gt; `...622`
• `` -&gt; `...623`
• `` -&gt; `...624`
• `` -&gt; `...625`
• `` -&gt; `...626`
• `` -&gt; `...627`
• `` -&gt; `...628`
• `` -&gt; `...629`
• `` -&gt; `...630`
• `` -&gt; `...631`
• `` -&gt; `...632`
• `` -&gt; `...633`
• `` -&gt; `...634`
• `` -&gt; `...635`
• `` -&gt; `...636`
• `` -&gt; `...637`
• `` -&gt; `...638`
• `` -&gt; `...639`
• `` -&gt; `...640`
• `` -&gt; `...641`
• `` -&gt; `...642`
• `` -&gt; `...643`
• `` -&gt; `...644`
• `` -&gt; `...645`
• `` -&gt; `...646`
• `` -&gt; `...647`
• `` -&gt; `...648`
• `` -&gt; `...649`
• `` -&gt; `...650`
• `` -&gt; `...651`
• `` -&gt; `...652`
• `` -&gt; `...653`
• `` -&gt; `...654`
• `` -&gt; `...655`
• `` -&gt; `...656`
• `` -&gt; `...657`
• `` -&gt; `...658`
• `` -&gt; `...659`
• `` -&gt; `...660`
• `` -&gt; `...661`
• `` -&gt; `...662`
• `` -&gt; `...663`
• `` -&gt; `...664`
• `` -&gt; `...665`
• `` -&gt; `...666`
• `` -&gt; `...667`
• `` -&gt; `...668`
• `` -&gt; `...669`
• `` -&gt; `...670`
• `` -&gt; `...671`
• `` -&gt; `...672`
• `` -&gt; `...673`
• `` -&gt; `...674`
• `` -&gt; `...675`
• `` -&gt; `...676`
• `` -&gt; `...677`
• `` -&gt; `...678`
• `` -&gt; `...679`
• `` -&gt; `...680`
• `` -&gt; `...681`
• `` -&gt; `...682`
• `` -&gt; `...683`
• `` -&gt; `...684`
• `` -&gt; `...685`
• `` -&gt; `...686`
• `` -&gt; `...687`
• `` -&gt; `...688`
• `` -&gt; `...689`
• `` -&gt; `...690`
• `` -&gt; `...691`
• `` -&gt; `...692`
• `` -&gt; `...693`
• `` -&gt; `...694`
• `` -&gt; `...695`
• `` -&gt; `...696`
• `` -&gt; `...697`
• `` -&gt; `...698`
• `` -&gt; `...699`
• `` -&gt; `...700`
• `` -&gt; `...701`
• `` -&gt; `...702`
• `` -&gt; `...703`
• `` -&gt; `...704`
• `` -&gt; `...705`
• `` -&gt; `...706`
• `` -&gt; `...707`
• `` -&gt; `...708`
• `` -&gt; `...709`
• `` -&gt; `...710`
• `` -&gt; `...711`
• `` -&gt; `...712`
• `` -&gt; `...713`
• `` -&gt; `...714`
• `` -&gt; `...715`
• `` -&gt; `...716`
• `` -&gt; `...717`
• `` -&gt; `...718`
• `` -&gt; `...719`
• `` -&gt; `...720`
• `` -&gt; `...721`
• `` -&gt; `...722`
• `` -&gt; `...723`
• `` -&gt; `...724`
• `` -&gt; `...725`
• `` -&gt; `...726`
• `` -&gt; `...727`
• `` -&gt; `...728`
• `` -&gt; `...729`
• `` -&gt; `...730`
• `` -&gt; `...731`
• `` -&gt; `...732`
• `` -&gt; `...733`
• `` -&gt; `...734`
• `` -&gt; `...735`
• `` -&gt; `...736`
• `` -&gt; `...737`
• `` -&gt; `...738`
• `` -&gt; `...739`
• `` -&gt; `...740`
• `` -&gt; `...741`
• `` -&gt; `...742`
• `` -&gt; `...743`
• `` -&gt; `...744`
• `` -&gt; `...745`
• `` -&gt; `...746`
• `` -&gt; `...747`
• `` -&gt; `...748`
• `` -&gt; `...749`
• `` -&gt; `...750`
• `` -&gt; `...751`
• `` -&gt; `...752`
• `` -&gt; `...753`
• `` -&gt; `...754`
• `` -&gt; `...755`
• `` -&gt; `...756`
• `` -&gt; `...757`
• `` -&gt; `...758`
• `` -&gt; `...759`
• `` -&gt; `...760`
• `` -&gt; `...761`
• `` -&gt; `...762`
• `` -&gt; `...763`
• `` -&gt; `...764`
• `` -&gt; `...765`
• `` -&gt; `...766`
• `` -&gt; `...767`
• `` -&gt; `...768`
• `` -&gt; `...769`
• `` -&gt; `...770`
• `` -&gt; `...771`
• `` -&gt; `...772`
• `` -&gt; `...773`
• `` -&gt; `...774`
• `` -&gt; `...775`
• `` -&gt; `...776`
• `` -&gt; `...777`
• `` -&gt; `...778`
• `` -&gt; `...779`
• `` -&gt; `...780`
• `` -&gt; `...781`
• `` -&gt; `...782`
• `` -&gt; `...783`
• `` -&gt; `...784`
• `` -&gt; `...785`
• `` -&gt; `...786`
• `` -&gt; `...787`
• `` -&gt; `...788`
• `` -&gt; `...789`
• `` -&gt; `...790`
• `` -&gt; `...791`
• `` -&gt; `...792`
• `` -&gt; `...793`
• `` -&gt; `...794`
• `` -&gt; `...795`
• `` -&gt; `...796`
• `` -&gt; `...797`
• `` -&gt; `...798`
• `` -&gt; `...799`
• `` -&gt; `...800`
• `` -&gt; `...801`
• `` -&gt; `...802`
• `` -&gt; `...803`
• `` -&gt; `...804`
• `` -&gt; `...805`
• `` -&gt; `...806`
• `` -&gt; `...807`
• `` -&gt; `...808`
• `` -&gt; `...809`
• `` -&gt; `...810`
• `` -&gt; `...811`
• `` -&gt; `...812`
• `` -&gt; `...813`
• `` -&gt; `...814`
• `` -&gt; `...815`
• `` -&gt; `...816`
• `` -&gt; `...817`
• `` -&gt; `...818`
• `` -&gt; `...819`
• `` -&gt; `...820`
• `` -&gt; `...821`
• `` -&gt; `...822`
• `` -&gt; `...823`
• `` -&gt; `...824`
• `` -&gt; `...825`
• `` -&gt; `...826`
• `` -&gt; `...827`
• `` -&gt; `...828`
• `` -&gt; `...829`
• `` -&gt; `...830`
• `` -&gt; `...831`
• `` -&gt; `...832`
• `` -&gt; `...833`
• `` -&gt; `...834`
• `` -&gt; `...835`
• `` -&gt; `...836`
• `` -&gt; `...837`
• `` -&gt; `...838`
• `` -&gt; `...839`
• `` -&gt; `...840`
• `` -&gt; `...841`
• `` -&gt; `...842`
• `` -&gt; `...843`
• `` -&gt; `...844`
• `` -&gt; `...845`
• `` -&gt; `...846`
• `` -&gt; `...847`
• `` -&gt; `...848`
• `` -&gt; `...849`
• `` -&gt; `...850`
• `` -&gt; `...851`
• `` -&gt; `...852`
• `` -&gt; `...853`
• `` -&gt; `...854`
• `` -&gt; `...855`
• `` -&gt; `...856`
• `` -&gt; `...857`
• `` -&gt; `...858`
• `` -&gt; `...859`
• `` -&gt; `...860`
• `` -&gt; `...861`
• `` -&gt; `...862`
• `` -&gt; `...863`
• `` -&gt; `...864`
• `` -&gt; `...865`
• `` -&gt; `...866`
• `` -&gt; `...867`
• `` -&gt; `...868`
• `` -&gt; `...869`
• `` -&gt; `...870`
• `` -&gt; `...871`
• `` -&gt; `...872`
• `` -&gt; `...873`
• `` -&gt; `...874`
• `` -&gt; `...875`
• `` -&gt; `...876`
• `` -&gt; `...877`
• `` -&gt; `...878`
• `` -&gt; `...879`
• `` -&gt; `...880`
• `` -&gt; `...881`
• `` -&gt; `...882`
• `` -&gt; `...883`
• `` -&gt; `...884`
• `` -&gt; `...885`
• `` -&gt; `...886`
• `` -&gt; `...887`
• `` -&gt; `...888`
• `` -&gt; `...889`
• `` -&gt; `...890`
• `` -&gt; `...891`
• `` -&gt; `...892`
• `` -&gt; `...893`
• `` -&gt; `...894`
• `` -&gt; `...895`
• `` -&gt; `...896`
• `` -&gt; `...897`
• `` -&gt; `...898`
• `` -&gt; `...899`
• `` -&gt; `...900`
• `` -&gt; `...901`
• `` -&gt; `...902`
• `` -&gt; `...903`
• `` -&gt; `...904`
• `` -&gt; `...905`
• `` -&gt; `...906`
• `` -&gt; `...907`
• `` -&gt; `...908`
• `` -&gt; `...909`
• `` -&gt; `...910`
• `` -&gt; `...911`
• `` -&gt; `...912`
• `` -&gt; `...913`
• `` -&gt; `...914`
• `` -&gt; `...915`
• `` -&gt; `...916`
• `` -&gt; `...917`
• `` -&gt; `...918`
• `` -&gt; `...919`
• `` -&gt; `...920`
• `` -&gt; `...921`
• `` -&gt; `...922`
• `` -&gt; `...923`
• `` -&gt; `...924`
• `` -&gt; `...925`
• `` -&gt; `...926`
• `` -&gt; `...927`
• `` -&gt; `...928`
• `` -&gt; `...929`
• `` -&gt; `...930`
• `` -&gt; `...931`
• `` -&gt; `...932`
• `` -&gt; `...933`
• `` -&gt; `...934`
• `` -&gt; `...935`
• `` -&gt; `...936`
• `` -&gt; `...937`
• `` -&gt; `...938`
• `` -&gt; `...939`
• `` -&gt; `...940`
• `` -&gt; `...941`
• `` -&gt; `...942`
• `` -&gt; `...943`
• `` -&gt; `...944`
• `` -&gt; `...945`
• `` -&gt; `...946`
• `` -&gt; `...947`
• `` -&gt; `...948`
• `` -&gt; `...949`
• `` -&gt; `...950`
• `` -&gt; `...951`
• `` -&gt; `...952`
• `` -&gt; `...953`
• `` -&gt; `...954`
• `` -&gt; `...955`
• `` -&gt; `...956`
• `` -&gt; `...957`
• `` -&gt; `...958`
• `` -&gt; `...959`
• `` -&gt; `...960`
• `` -&gt; `...961`
• `` -&gt; `...962`
• `` -&gt; `...963`
• `` -&gt; `...964`
• `` -&gt; `...965`
• `` -&gt; `...966`
• `` -&gt; `...967`
• `` -&gt; `...968`
• `` -&gt; `...969`
• `` -&gt; `...970`
• `` -&gt; `...971`
• `` -&gt; `...972`
• `` -&gt; `...973`
• `` -&gt; `...974`
• `` -&gt; `...975`
• `` -&gt; `...976`
• `` -&gt; `...977`
• `` -&gt; `...978`
• `` -&gt; `...979`
• `` -&gt; `...980`
• `` -&gt; `...981`
• `` -&gt; `...982`
• `` -&gt; `...983`
• `` -&gt; `...984`
• `` -&gt; `...985`
• `` -&gt; `...986`
• `` -&gt; `...987`
• `` -&gt; `...988`
• `` -&gt; `...989`
• `` -&gt; `...990`
• `` -&gt; `...991`
• `` -&gt; `...992`
• `` -&gt; `...993`
• `` -&gt; `...994`
• `` -&gt; `...995`
• `` -&gt; `...996`
• `` -&gt; `...997`
• `` -&gt; `...998`
• `` -&gt; `...999`
• `` -&gt; `...1000`</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a aria-hidden="true" href="#cb27-1" tabindex="-1"></a><span class="co"># Combine the results</span></span>
<span id="cb27-2"><a aria-hidden="true" href="#cb27-2" tabindex="-1"></a>all_sample_means <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(</span>
<span id="cb27-3"><a aria-hidden="true" href="#cb27-3" tabindex="-1"></a>  <span class="fu">mutate</span>(sample_means_10, <span class="at">sample_size =</span> <span class="st">"10"</span>),</span>
<span id="cb27-4"><a aria-hidden="true" href="#cb27-4" tabindex="-1"></a>  <span class="fu">mutate</span>(sample_means_30, <span class="at">sample_size =</span> <span class="st">"30"</span>),</span>
<span id="cb27-5"><a aria-hidden="true" href="#cb27-5" tabindex="-1"></a>  <span class="fu">mutate</span>(sample_means_100, <span class="at">sample_size =</span> <span class="st">"100"</span>)</span>
<span id="cb27-6"><a aria-hidden="true" href="#cb27-6" tabindex="-1"></a>)</span>
<span id="cb27-7"><a aria-hidden="true" href="#cb27-7" tabindex="-1"></a></span>
<span id="cb27-8"><a aria-hidden="true" href="#cb27-8" tabindex="-1"></a><span class="co"># Create a plot of the sample means</span></span>
<span id="cb27-9"><a aria-hidden="true" href="#cb27-9" tabindex="-1"></a><span class="fu">ggplot</span>(all_sample_means, <span class="fu">aes</span>(<span class="at">x =</span> sample_mean, <span class="at">fill =</span> sample_size)) <span class="sc">+</span></span>
<span id="cb27-10"><a aria-hidden="true" href="#cb27-10" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">binwidth =</span> <span class="fl">0.2</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">position =</span> <span class="st">"identity"</span>) <span class="sc">+</span></span>
<span id="cb27-11"><a aria-hidden="true" href="#cb27-11" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>sample_size, <span class="at">scales =</span> <span class="st">"free_y"</span>) <span class="sc">+</span></span>
<span id="cb27-12"><a aria-hidden="true" href="#cb27-12" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb27-13"><a aria-hidden="true" href="#cb27-13" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Distribution of Sample Means for Different Sample Sizes"</span>,</span>
<span id="cb27-14"><a aria-hidden="true" href="#cb27-14" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Sample Mean"</span>,</span>
<span id="cb27-15"><a aria-hidden="true" href="#cb27-15" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Frequency"</span></span>
<span id="cb27-16"><a aria-hidden="true" href="#cb27-16" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb27-17"><a aria-hidden="true" href="#cb27-17" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap10_files/figure-html/unnamed-chunk-2-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>This script illustrates the properties of the <strong>sample mean</strong> and the <strong>Central Limit Theorem (CLT)</strong>, as discussed in Section 10.2.1 of the text.</p>
<ol type="1">
<li>We load the <code>tidyverse</code> package for data manipulation and plotting.</li>
<li>We set the population mean and standard deviation to 5 and 2, respectively.</li>
<li>We define a function <code>simulate_sample_means</code> that takes the sample size and the number of samples as input. It simulates drawing samples of the specified size from a normal distribution with the given population parameters, calculates the mean of each sample, and returns a tibble with the sample means.</li>
<li>We use this function to simulate 1000 sample means for sample sizes of 10, 30, and 100.</li>
<li>We combine the results into a single tibble, adding a column to indicate the sample size for each set of sample means.</li>
<li>We create a histogram of the sample means using <code>ggplot2</code>, with separate panels for each sample size. The histograms show the distribution of the sample means for each sample size.</li>
</ol>
<p>The histograms demonstrate that:</p>
<ul>
<li>The sample mean is an unbiased estimator of the population mean (as discussed in Theorem 10.1 of the text), as the distributions are centered around the true population mean of 5.</li>
<li>The variance of the sample mean decreases as the sample size increases (as shown in Theorem 10.1 of the text), as the histograms become narrower for larger sample sizes.</li>
<li>The distribution of the sample mean approaches a normal distribution as the sample size increases, illustrating the Central Limit Theorem (Theorem 10.1 of the text).</li>
</ul>
</section>
<section class="level3" id="r-script-3-kernel-density-estimation">
<h3 class="anchored" data-anchor-id="r-script-3-kernel-density-estimation">R Script 3: Kernel Density Estimation</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb28-1"><a aria-hidden="true" href="#cb28-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb28-2"><a aria-hidden="true" href="#cb28-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb28-3"><a aria-hidden="true" href="#cb28-3" tabindex="-1"></a></span>
<span id="cb28-4"><a aria-hidden="true" href="#cb28-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb28-5"><a aria-hidden="true" href="#cb28-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb28-6"><a aria-hidden="true" href="#cb28-6" tabindex="-1"></a></span>
<span id="cb28-7"><a aria-hidden="true" href="#cb28-7" tabindex="-1"></a><span class="co"># Generate sample data from a mixture of two normal distributions</span></span>
<span id="cb28-8"><a aria-hidden="true" href="#cb28-8" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rnorm</span>(<span class="dv">500</span>, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>), <span class="fu">rnorm</span>(<span class="dv">500</span>, <span class="at">mean =</span> <span class="dv">3</span>, <span class="at">sd =</span> <span class="fl">0.5</span>))</span>
<span id="cb28-9"><a aria-hidden="true" href="#cb28-9" tabindex="-1"></a></span>
<span id="cb28-10"><a aria-hidden="true" href="#cb28-10" tabindex="-1"></a><span class="co"># Create a tibble with the sample data</span></span>
<span id="cb28-11"><a aria-hidden="true" href="#cb28-11" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(<span class="at">x =</span> sample_data)</span>
<span id="cb28-12"><a aria-hidden="true" href="#cb28-12" tabindex="-1"></a></span>
<span id="cb28-13"><a aria-hidden="true" href="#cb28-13" tabindex="-1"></a><span class="co"># Create a plot with histogram and kernel density estimates</span></span>
<span id="cb28-14"><a aria-hidden="true" href="#cb28-14" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb28-15"><a aria-hidden="true" href="#cb28-15" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">binwidth =</span> <span class="fl">0.5</span>, <span class="at">fill =</span> <span class="st">"lightblue"</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb28-16"><a aria-hidden="true" href="#cb28-16" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">kernel =</span> <span class="st">"gaussian"</span>, <span class="at">bw =</span> <span class="fl">0.2</span>, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb28-17"><a aria-hidden="true" href="#cb28-17" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">kernel =</span> <span class="st">"gaussian"</span>, <span class="at">bw =</span> <span class="fl">0.5</span>, <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb28-18"><a aria-hidden="true" href="#cb28-18" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb28-19"><a aria-hidden="true" href="#cb28-19" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Histogram and Kernel Density Estimates"</span>,</span>
<span id="cb28-20"><a aria-hidden="true" href="#cb28-20" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"x"</span>,</span>
<span id="cb28-21"><a aria-hidden="true" href="#cb28-21" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Density"</span></span>
<span id="cb28-22"><a aria-hidden="true" href="#cb28-22" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb28-23"><a aria-hidden="true" href="#cb28-23" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
ℹ Please use `linewidth` instead.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.
ℹ Please use `after_stat(density)` instead.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap10_files/figure-html/unnamed-chunk-3-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>This script demonstrates <strong>kernel density estimation</strong>, as discussed in Section 10.2 of the text.</p>
<ol type="1">
<li>We load the <code>tidyverse</code> package for data manipulation and plotting.</li>
<li>We generate sample data from a mixture of two normal distributions to create a non-standard distribution.</li>
<li>We create a tibble with the sample data.</li>
<li>We create a plot using <code>ggplot2</code> that includes:
<ul>
<li>A histogram of the sample data, with the y-axis scaled to represent density.</li>
<li>A kernel density estimate using a Gaussian kernel with a bandwidth of 0.2 (red curve).</li>
<li>A kernel density estimate using a Gaussian kernel with a bandwidth of 0.5 (blue curve).</li>
</ul></li>
</ol>
<p>The plot shows how kernel density estimation provides a smooth estimate of the probability density function of the data. The bandwidth parameter (<code>bw</code>) controls the smoothness of the estimate, with smaller bandwidths resulting in more wiggly curves and larger bandwidths resulting in smoother curves, as discussed in the text.</p>
</section>
<section class="level3" id="r-script-4-likelihood-function-for-bernoulli-distribution">
<h3 class="anchored" data-anchor-id="r-script-4-likelihood-function-for-bernoulli-distribution">R Script 4: Likelihood Function for Bernoulli Distribution</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a aria-hidden="true" href="#cb31-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb31-2"><a aria-hidden="true" href="#cb31-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb31-3"><a aria-hidden="true" href="#cb31-3" tabindex="-1"></a></span>
<span id="cb31-4"><a aria-hidden="true" href="#cb31-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb31-5"><a aria-hidden="true" href="#cb31-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101112</span>)</span>
<span id="cb31-6"><a aria-hidden="true" href="#cb31-6" tabindex="-1"></a></span>
<span id="cb31-7"><a aria-hidden="true" href="#cb31-7" tabindex="-1"></a><span class="co"># Generate sample data from a Bernoulli distribution</span></span>
<span id="cb31-8"><a aria-hidden="true" href="#cb31-8" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># Sample size</span></span>
<span id="cb31-9"><a aria-hidden="true" href="#cb31-9" tabindex="-1"></a>theta <span class="ot">&lt;-</span> <span class="fl">0.6</span> <span class="co"># True parameter value</span></span>
<span id="cb31-10"><a aria-hidden="true" href="#cb31-10" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> theta)</span>
<span id="cb31-11"><a aria-hidden="true" href="#cb31-11" tabindex="-1"></a></span>
<span id="cb31-12"><a aria-hidden="true" href="#cb31-12" tabindex="-1"></a><span class="co"># Calculate the number of successes (k)</span></span>
<span id="cb31-13"><a aria-hidden="true" href="#cb31-13" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">sum</span>(sample_data)</span>
<span id="cb31-14"><a aria-hidden="true" href="#cb31-14" tabindex="-1"></a></span>
<span id="cb31-15"><a aria-hidden="true" href="#cb31-15" tabindex="-1"></a><span class="co"># Define the likelihood function</span></span>
<span id="cb31-16"><a aria-hidden="true" href="#cb31-16" tabindex="-1"></a>likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, k, n) {</span>
<span id="cb31-17"><a aria-hidden="true" href="#cb31-17" tabindex="-1"></a>  theta<span class="sc">^</span>k <span class="sc">*</span> (<span class="dv">1</span> <span class="sc">-</span> theta)<span class="sc">^</span>(n <span class="sc">-</span> k)</span>
<span id="cb31-18"><a aria-hidden="true" href="#cb31-18" tabindex="-1"></a>}</span>
<span id="cb31-19"><a aria-hidden="true" href="#cb31-19" tabindex="-1"></a></span>
<span id="cb31-20"><a aria-hidden="true" href="#cb31-20" tabindex="-1"></a><span class="co"># Create a sequence of theta values</span></span>
<span id="cb31-21"><a aria-hidden="true" href="#cb31-21" tabindex="-1"></a>theta_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb31-22"><a aria-hidden="true" href="#cb31-22" tabindex="-1"></a></span>
<span id="cb31-23"><a aria-hidden="true" href="#cb31-23" tabindex="-1"></a><span class="co"># Calculate the likelihood for each theta value</span></span>
<span id="cb31-24"><a aria-hidden="true" href="#cb31-24" tabindex="-1"></a>likelihood_values <span class="ot">&lt;-</span> <span class="fu">likelihood</span>(theta_values, k, n)</span>
<span id="cb31-25"><a aria-hidden="true" href="#cb31-25" tabindex="-1"></a></span>
<span id="cb31-26"><a aria-hidden="true" href="#cb31-26" tabindex="-1"></a><span class="co"># Create a tibble with theta and likelihood values</span></span>
<span id="cb31-27"><a aria-hidden="true" href="#cb31-27" tabindex="-1"></a>likelihood_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb31-28"><a aria-hidden="true" href="#cb31-28" tabindex="-1"></a>  <span class="at">theta =</span> theta_values,</span>
<span id="cb31-29"><a aria-hidden="true" href="#cb31-29" tabindex="-1"></a>  <span class="at">likelihood =</span> likelihood_values</span>
<span id="cb31-30"><a aria-hidden="true" href="#cb31-30" tabindex="-1"></a>)</span>
<span id="cb31-31"><a aria-hidden="true" href="#cb31-31" tabindex="-1"></a></span>
<span id="cb31-32"><a aria-hidden="true" href="#cb31-32" tabindex="-1"></a><span class="co"># Find the maximum likelihood estimate</span></span>
<span id="cb31-33"><a aria-hidden="true" href="#cb31-33" tabindex="-1"></a>mle <span class="ot">&lt;-</span> theta_values[<span class="fu">which.max</span>(likelihood_values)]</span>
<span id="cb31-34"><a aria-hidden="true" href="#cb31-34" tabindex="-1"></a></span>
<span id="cb31-35"><a aria-hidden="true" href="#cb31-35" tabindex="-1"></a><span class="co"># Create a plot of the likelihood function</span></span>
<span id="cb31-36"><a aria-hidden="true" href="#cb31-36" tabindex="-1"></a><span class="fu">ggplot</span>(likelihood_data, <span class="fu">aes</span>(<span class="at">x =</span> theta, <span class="at">y =</span> likelihood)) <span class="sc">+</span></span>
<span id="cb31-37"><a aria-hidden="true" href="#cb31-37" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb31-38"><a aria-hidden="true" href="#cb31-38" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> mle, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb31-39"><a aria-hidden="true" href="#cb31-39" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb31-40"><a aria-hidden="true" href="#cb31-40" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Likelihood Function for Bernoulli Distribution"</span>,</span>
<span id="cb31-41"><a aria-hidden="true" href="#cb31-41" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Theta"</span>,</span>
<span id="cb31-42"><a aria-hidden="true" href="#cb31-42" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Likelihood"</span></span>
<span id="cb31-43"><a aria-hidden="true" href="#cb31-43" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb31-44"><a aria-hidden="true" href="#cb31-44" tabindex="-1"></a>  <span class="fu">annotate</span>(<span class="st">"text"</span>, <span class="at">x =</span> mle <span class="sc">+</span> <span class="fl">0.1</span>, <span class="at">y =</span> <span class="fu">max</span>(likelihood_values) <span class="sc">*</span> <span class="fl">0.8</span>, <span class="at">label =</span> <span class="fu">paste</span>(<span class="st">"MLE ="</span>, <span class="fu">round</span>(mle, <span class="dv">3</span>)), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb31-45"><a aria-hidden="true" href="#cb31-45" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap10_files/figure-html/unnamed-chunk-4-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>This script illustrates the concept of the <strong>likelihood function</strong> and <strong>maximum likelihood estimation</strong> for a Bernoulli distribution, as discussed in Section 10.3 and Example 10.3 of the text.</p>
<ol type="1">
<li>We load the <code>tidyverse</code> package for data manipulation and plotting.</li>
<li>We generate sample data from a Bernoulli distribution with a true parameter value of <span class="math inline">\(\theta = 0.6\)</span>.</li>
<li>We calculate the number of successes (<span class="math inline">\(k\)</span>) in the sample.</li>
<li>We define the likelihood function for a Bernoulli distribution as described in the text: <span class="math inline">\(L(\theta|X^n) = \theta^k (1-\theta)^{n-k}\)</span></li>
<li>We create a sequence of <span class="math inline">\(\theta\)</span> values from 0 to 1.</li>
<li>We calculate the likelihood for each <span class="math inline">\(\theta\)</span> value using the defined likelihood function.</li>
<li>We create a tibble with <span class="math inline">\(\theta\)</span> and likelihood values.</li>
<li>We find the maximum likelihood estimate (MLE) by identifying the <span class="math inline">\(\theta\)</span> value that corresponds to the maximum likelihood.</li>
<li>We create a plot of the likelihood function using <code>ggplot2</code>, with a vertical dashed line indicating the MLE.</li>
</ol>
<p>The plot shows how the likelihood function varies with <span class="math inline">\(\theta\)</span> for the given sample data. The MLE is the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood function, representing the parameter value that makes the observed data most probable under the assumed Bernoulli model.</p>
</section>
<section class="level3" id="r-script-5-bayesian-inference-for-normal-mean-with-known-variance">
<h3 class="anchored" data-anchor-id="r-script-5-bayesian-inference-for-normal-mean-with-known-variance">R Script 5: Bayesian Inference for Normal Mean with Known Variance</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a aria-hidden="true" href="#cb32-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb32-2"><a aria-hidden="true" href="#cb32-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb32-3"><a aria-hidden="true" href="#cb32-3" tabindex="-1"></a></span>
<span id="cb32-4"><a aria-hidden="true" href="#cb32-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb32-5"><a aria-hidden="true" href="#cb32-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">131415</span>)</span>
<span id="cb32-6"><a aria-hidden="true" href="#cb32-6" tabindex="-1"></a></span>
<span id="cb32-7"><a aria-hidden="true" href="#cb32-7" tabindex="-1"></a><span class="co"># Set parameters for the prior distribution</span></span>
<span id="cb32-8"><a aria-hidden="true" href="#cb32-8" tabindex="-1"></a>mu_prior <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb32-9"><a aria-hidden="true" href="#cb32-9" tabindex="-1"></a>sigma_prior <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb32-10"><a aria-hidden="true" href="#cb32-10" tabindex="-1"></a></span>
<span id="cb32-11"><a aria-hidden="true" href="#cb32-11" tabindex="-1"></a><span class="co"># Set parameters for the data distribution</span></span>
<span id="cb32-12"><a aria-hidden="true" href="#cb32-12" tabindex="-1"></a>mu_data <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb32-13"><a aria-hidden="true" href="#cb32-13" tabindex="-1"></a>sigma_data <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb32-14"><a aria-hidden="true" href="#cb32-14" tabindex="-1"></a>n_data <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb32-15"><a aria-hidden="true" href="#cb32-15" tabindex="-1"></a></span>
<span id="cb32-16"><a aria-hidden="true" href="#cb32-16" tabindex="-1"></a><span class="co"># Generate sample data</span></span>
<span id="cb32-17"><a aria-hidden="true" href="#cb32-17" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_data, <span class="at">mean =</span> mu_data, <span class="at">sd =</span> sigma_data)</span>
<span id="cb32-18"><a aria-hidden="true" href="#cb32-18" tabindex="-1"></a></span>
<span id="cb32-19"><a aria-hidden="true" href="#cb32-19" tabindex="-1"></a><span class="co"># Calculate sample mean</span></span>
<span id="cb32-20"><a aria-hidden="true" href="#cb32-20" tabindex="-1"></a>x_bar <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample_data)</span>
<span id="cb32-21"><a aria-hidden="true" href="#cb32-21" tabindex="-1"></a></span>
<span id="cb32-22"><a aria-hidden="true" href="#cb32-22" tabindex="-1"></a><span class="co"># Calculate parameters for the posterior distribution</span></span>
<span id="cb32-23"><a aria-hidden="true" href="#cb32-23" tabindex="-1"></a>mu_posterior <span class="ot">&lt;-</span> (sigma_data<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (n_data <span class="sc">*</span> sigma_prior<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> sigma_data<span class="sc">^</span><span class="dv">2</span>)) <span class="sc">*</span> mu_prior <span class="sc">+</span></span>
<span id="cb32-24"><a aria-hidden="true" href="#cb32-24" tabindex="-1"></a>  (n_data <span class="sc">*</span> sigma_prior<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> (n_data <span class="sc">*</span> sigma_prior<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> sigma_data<span class="sc">^</span><span class="dv">2</span>)) <span class="sc">*</span> x_bar</span>
<span id="cb32-25"><a aria-hidden="true" href="#cb32-25" tabindex="-1"></a>sigma_posterior <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="dv">1</span> <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">/</span> sigma_prior<span class="sc">^</span><span class="dv">2</span> <span class="sc">+</span> n_data <span class="sc">/</span> sigma_data<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb32-26"><a aria-hidden="true" href="#cb32-26" tabindex="-1"></a></span>
<span id="cb32-27"><a aria-hidden="true" href="#cb32-27" tabindex="-1"></a><span class="co"># Create a sequence of mu values</span></span>
<span id="cb32-28"><a aria-hidden="true" href="#cb32-28" tabindex="-1"></a>mu_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="dv">5</span>, <span class="at">length.out =</span> <span class="dv">1000</span>)</span>
<span id="cb32-29"><a aria-hidden="true" href="#cb32-29" tabindex="-1"></a></span>
<span id="cb32-30"><a aria-hidden="true" href="#cb32-30" tabindex="-1"></a><span class="co"># Calculate prior, likelihood, and posterior densities</span></span>
<span id="cb32-31"><a aria-hidden="true" href="#cb32-31" tabindex="-1"></a>prior_density <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu_values, <span class="at">mean =</span> mu_prior, <span class="at">sd =</span> sigma_prior)</span>
<span id="cb32-32"><a aria-hidden="true" href="#cb32-32" tabindex="-1"></a>likelihood_density <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu_values, <span class="at">mean =</span> x_bar, <span class="at">sd =</span> sigma_data <span class="sc">/</span> <span class="fu">sqrt</span>(n_data))</span>
<span id="cb32-33"><a aria-hidden="true" href="#cb32-33" tabindex="-1"></a>posterior_density <span class="ot">&lt;-</span> <span class="fu">dnorm</span>(mu_values, <span class="at">mean =</span> mu_posterior, <span class="at">sd =</span> sigma_posterior)</span>
<span id="cb32-34"><a aria-hidden="true" href="#cb32-34" tabindex="-1"></a></span>
<span id="cb32-35"><a aria-hidden="true" href="#cb32-35" tabindex="-1"></a><span class="co"># Create a tibble with mu, prior, likelihood, and posterior values</span></span>
<span id="cb32-36"><a aria-hidden="true" href="#cb32-36" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">tibble</span>(</span>
<span id="cb32-37"><a aria-hidden="true" href="#cb32-37" tabindex="-1"></a>  <span class="at">mu =</span> mu_values,</span>
<span id="cb32-38"><a aria-hidden="true" href="#cb32-38" tabindex="-1"></a>  <span class="at">prior =</span> prior_density,</span>
<span id="cb32-39"><a aria-hidden="true" href="#cb32-39" tabindex="-1"></a>  <span class="at">likelihood =</span> likelihood_density,</span>
<span id="cb32-40"><a aria-hidden="true" href="#cb32-40" tabindex="-1"></a>  <span class="at">posterior =</span> posterior_density</span>
<span id="cb32-41"><a aria-hidden="true" href="#cb32-41" tabindex="-1"></a>) <span class="sc">%&gt;%</span></span>
<span id="cb32-42"><a aria-hidden="true" href="#cb32-42" tabindex="-1"></a>  <span class="fu">gather</span>(<span class="at">key =</span> <span class="st">"distribution"</span>, <span class="at">value =</span> <span class="st">"density"</span>, <span class="sc">-</span>mu)</span>
<span id="cb32-43"><a aria-hidden="true" href="#cb32-43" tabindex="-1"></a></span>
<span id="cb32-44"><a aria-hidden="true" href="#cb32-44" tabindex="-1"></a><span class="co"># Create a plot of prior, likelihood, and posterior distributions</span></span>
<span id="cb32-45"><a aria-hidden="true" href="#cb32-45" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> mu, <span class="at">y =</span> density, <span class="at">color =</span> distribution)) <span class="sc">+</span></span>
<span id="cb32-46"><a aria-hidden="true" href="#cb32-46" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb32-47"><a aria-hidden="true" href="#cb32-47" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb32-48"><a aria-hidden="true" href="#cb32-48" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Bayesian Inference for Normal Mean (Known Variance)"</span>,</span>
<span id="cb32-49"><a aria-hidden="true" href="#cb32-49" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Mu"</span>,</span>
<span id="cb32-50"><a aria-hidden="true" href="#cb32-50" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Density"</span></span>
<span id="cb32-51"><a aria-hidden="true" href="#cb32-51" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb32-52"><a aria-hidden="true" href="#cb32-52" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"prior"</span> <span class="ot">=</span> <span class="st">"blue"</span>, <span class="st">"likelihood"</span> <span class="ot">=</span> <span class="st">"red"</span>, <span class="st">"posterior"</span> <span class="ot">=</span> <span class="st">"purple"</span>)) <span class="sc">+</span></span>
<span id="cb32-53"><a aria-hidden="true" href="#cb32-53" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap10_files/figure-html/unnamed-chunk-5-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<p>This script demonstrates <strong>Bayesian inference</strong> for the mean of a normal distribution with known variance, as discussed in Section 10.3.2 and Examples 10.9 and 10.10 of the text.</p>
<ol type="1">
<li><p>We load the <code>tidyverse</code> package for data manipulation and plotting.</p></li>
<li><p>We set the parameters for the prior distribution of the mean (<span class="math inline">\(\mu\)</span>), which is assumed to be normally distributed with mean <code>mu_prior</code> and standard deviation <code>sigma_prior</code>.</p></li>
<li><p>We set the parameters for the data distribution, which is assumed to be normally distributed with mean <code>mu_data</code> and standard deviation <code>sigma_data</code>. We also specify the sample size <code>n_data</code>.</p></li>
<li><p>We generate sample data from the data distribution.</p></li>
<li><p>We calculate the sample mean <code>x_bar</code>.</p></li>
<li><p>We calculate the parameters for the posterior distribution of <span class="math inline">\(\mu\)</span> using the formulas derived from Bayes’ theorem, as shown in Example 10.10 of the text:</p>
<p><span class="math inline">\(m_n = \dfrac{\sigma_X^2/n}{\sigma^2 + \sigma_X^2/n} \mu + \dfrac{\sigma^2}{\sigma^2 + \sigma_X^2/n} \bar{X}\)</span></p>
<p><span class="math inline">\(v_n = \left( \dfrac{1}{\sigma^2} + \dfrac{n}{\sigma_X^2} \right)^{-1}\)</span></p></li>
</ol>
<p>where <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> are the prior mean and standard deviation, <span class="math inline">\(\sigma_X\)</span> is the data standard deviation (known), <span class="math inline">\(n\)</span> is the sample size, and <span class="math inline">\(\bar{X}\)</span> is the sample mean. In our code, these are denoted as <code>mu_posterior</code> and <code>sigma_posterior</code>, respectively. 7. We create a sequence of <span class="math inline">\(\mu\)</span> values. 8. We calculate the prior, likelihood, and posterior densities for each <span class="math inline">\(\mu\)</span> value using the <code>dnorm</code> function. 9. We create a tibble with <span class="math inline">\(\mu\)</span>, prior, likelihood, and posterior values and transform it into a long format for plotting. 10. We create a plot using <code>ggplot2</code> showing the prior, likelihood, and posterior distributions.</p>
<p>The plot illustrates how the prior distribution and the likelihood function are combined to obtain the posterior distribution in Bayesian inference. The posterior distribution represents our updated beliefs about the parameter <span class="math inline">\(\mu\)</span> after observing the data. The plot shows that the posterior distribution is a compromise between the prior distribution and the likelihood function, with its mean being a weighted average of the prior mean and the sample mean, as discussed in the text.</p>
</section>
</section>
<section class="level2" id="youtube-videos">
<h2 class="anchored" data-anchor-id="youtube-videos">YouTube Videos</h2>
<p>Here are some YouTube videos that explain the concepts mentioned in the attached text, along with explanations of how they relate to the text:</p>
<section class="level3" id="stratified-sampling">
<h3 class="anchored" data-anchor-id="stratified-sampling">1. Stratified Sampling</h3>
<ul>
<li><p><strong>Video Title:</strong> Stratified Random Sampling</p></li>
<li><p><strong>Channel:</strong> Steve Mays</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=sYRUYJYOpG0">https://www.youtube.com/watch?v=sYRUYJYOpG0</a></p></li>
<li><p><strong>Relation to Text:</strong> This video explains the concept of <strong>stratified sampling</strong>, which is discussed in Section 10.1 of the text. It provides a clear definition of stratified sampling, explains how to create strata, and discusses the advantages of using this method compared to simple random sampling. The video also covers how to determine the sample size for each stratum and how to combine the results from different strata to obtain an overall estimate for the population.</p></li>
</ul>
</section>
<section class="level3" id="sample-mean-and-central-limit-theorem">
<h3 class="anchored" data-anchor-id="sample-mean-and-central-limit-theorem">2. Sample Mean and Central Limit Theorem</h3>
<ul>
<li><p><strong>Video Title:</strong> Central Limit Theorem - Sampling Distribution of Sample Means - Stats &amp; Probability</p></li>
<li><p><strong>Channel:</strong> The Organic Chemistry Tutor</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=JNm3M9cqWyc">https://www.youtube.com/watch?v=JNm3M9cqWyc</a></p></li>
<li><p><strong>Relation to Text:</strong> This video provides a comprehensive explanation of the <strong>Central Limit Theorem (CLT)</strong> and the properties of the <strong>sample mean</strong>, which are discussed in Section 10.2.1 and Theorem 10.1 of the text. It illustrates how the distribution of the sample mean approaches a normal distribution as the sample size increases, regardless of the shape of the population distribution. The video also demonstrates that the mean of the sample means is equal to the population mean (<span class="math inline">\(\mu\)</span>) and that the standard deviation of the sample means (standard error) is equal to the population standard deviation divided by the square root of the sample size (<span class="math inline">\(\sigma/\sqrt{n}\)</span>). These concepts are clearly presented in Theorem 10.1 of the text, with equations for the expected value and variance of the sample mean:</p>
<p><span class="math inline">\(E\bar{X} = \mu\)</span></p>
<p><span class="math inline">\(\text{var}(\bar{X}) = \dfrac{\sigma^2}{n}\)</span></p></li>
</ul>
</section>
<section class="level3" id="kernel-density-estimation">
<h3 class="anchored" data-anchor-id="kernel-density-estimation">3. Kernel Density Estimation</h3>
<ul>
<li><p><strong>Video Title:</strong> Statistics 101: Kernel Density Estimation</p></li>
<li><p><strong>Channel:</strong> Brandon Foltz</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=fJoR3QsfXa0">https://www.youtube.com/watch?v=fJoR3QsfXa0</a></p></li>
<li><p><strong>Relation to Text:</strong> This video explains the concept of <strong>kernel density estimation</strong>, which is introduced in Section 10.2 of the text. It provides an intuitive understanding of how kernel density estimation works by visualizing the process of placing a kernel function at each data point and then summing these functions to obtain a smooth estimate of the probability density function. The video also discusses the role of the bandwidth parameter and how it affects the smoothness of the density estimate, which is consistent with the discussion in the text.</p></li>
</ul>
</section>
<section class="level3" id="likelihood-function-and-maximum-likelihood-estimation">
<h3 class="anchored" data-anchor-id="likelihood-function-and-maximum-likelihood-estimation">4. Likelihood Function and Maximum Likelihood Estimation</h3>
<ul>
<li><p><strong>Video Title:</strong> (ML 3.1) Maximum likelihood estimation: intuition and example</p></li>
<li><p><strong>Channel:</strong> mathematicalmonk</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=XepXtl9YKwc">https://www.youtube.com/watch?v=XepXtl9YKwc</a></p></li>
<li><p><strong>Relation to Text:</strong> This video provides an intuitive explanation of the <strong>likelihood function</strong> and <strong>maximum likelihood estimation (MLE)</strong>, which are key concepts in Section 10.3 of the text. It demonstrates how the likelihood function is used to find the parameter values that make the observed data most probable under the assumed statistical model. The video uses a simple example to illustrate the concept and shows how to find the MLE by maximizing the likelihood function. The likelihood function is defined in the text as:</p>
<p><span class="math inline">\(L(\theta|X^n) = f(X_1, \dots, X_n|\theta)\)</span></p>
<p>and the MLE is defined as the value of <span class="math inline">\(\theta\)</span> that maximizes this function.</p></li>
</ul>
</section>
<section class="level3" id="bayesian-inference">
<h3 class="anchored" data-anchor-id="bayesian-inference">5. Bayesian Inference</h3>
<ul>
<li><p><strong>Video Title:</strong> Introduction to Bayesian statistics, part 1: The basic concepts</p></li>
<li><p><strong>Channel:</strong> StatQuest with Josh Starmer</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=3OJEae7Qb_o">https://www.youtube.com/watch?v=3OJEae7Qb_o</a></p></li>
<li><p><strong>Relation to Text:</strong> This video introduces the basic concepts of <strong>Bayesian inference</strong>, which is discussed in Section 10.3.2 of the text. It explains the difference between the Frequentist and Bayesian approaches to statistical inference and introduces the key components of Bayesian inference: the prior distribution, the likelihood function, and the posterior distribution. The video also provides a simple example to illustrate how Bayes’ theorem is used to update prior beliefs based on observed data. The text presents Bayes’ theorem as:</p>
<p><span class="math inline">\(\pi(\theta|X^n) \propto L(\theta|X^n) \times \pi(\theta)\)</span></p>
<p>where <span class="math inline">\(\pi(\theta)\)</span> is the prior distribution, <span class="math inline">\(L(\theta|X^n)\)</span> is the likelihood function, and <span class="math inline">\(\pi(\theta|X^n)\)</span> is the posterior distribution.</p></li>
</ul>
</section>
<section class="level3" id="sufficient-statistics">
<h3 class="anchored" data-anchor-id="sufficient-statistics">6. Sufficient Statistics</h3>
<ul>
<li><p><strong>Video Title:</strong> 3.5.1 Sufficiency Principle</p></li>
<li><p><strong>Channel:</strong> Bionic Turtle</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=4dE46x-DI4w">https://www.youtube.com/watch?v=4dE46x-DI4w</a></p></li>
<li><p><strong>Relation to Text:</strong> This video explains the concepts of <strong>sufficient statistics</strong> and the <strong>sufficiency principle</strong>, which are defined in Section 10.3.1.1 of the text. The video explains that a sufficient statistic for a parameter captures all information about that parameter from the sample. In other words, no other statistic can provide more information about the parameter’s value than the sufficient statistic.</p></li>
</ul>
</section>
<section class="level3" id="ancillary-statistics">
<h3 class="anchored" data-anchor-id="ancillary-statistics">7. Ancillary Statistics</h3>
<ul>
<li><p><strong>Video Title:</strong> Ancillary statistics</p></li>
<li><p><strong>Channel:</strong> MarinStatsLectures-R Programming &amp; Statistics</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=Y5sVbHn3-9w">https://www.youtube.com/watch?v=Y5sVbHn3-9w</a></p></li>
<li><p><strong>Relation to Text:</strong> This video explains the concept of <strong>ancillary statistics</strong>, which is introduced in Section 10.3.1.2 of the text. It defines an ancillary statistic as a statistic whose distribution does not depend on the parameter of interest. The video uses several examples to illustrate the concept and highlights that an ancillary statistic on its own does not provide information about the unknown parameter.</p></li>
</ul>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch10mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch10mcsolution1">MC Solution 1</a></p>
<p><strong>Stratified sampling</strong> is a sampling technique where:</p>
<ol type="a">
<li>The population is divided into subgroups, and a random sample is taken from each subgroup.</li>
<li>Every member of the population has an equal chance of being selected.</li>
<li>The sample is chosen based on convenience or availability.</li>
<li>The sample is selected to match the characteristics of the population.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch10mcsolution2">MC Solution 2</a></p>
<p>Which of the following is an advantage of using <strong>stratified sampling</strong> over simple random sampling?</p>
<ol type="a">
<li>It is always less expensive.</li>
<li>It can ensure that subgroups of the population are adequately represented.</li>
<li>It is always more accurate.</li>
<li>It eliminates the need for randomization.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch10mcsolution3">MC Solution 3</a></p>
<p><strong>Sample selection bias</strong> can occur when:</p>
<ol type="a">
<li>The sample size is too small.</li>
<li>The sample is not representative of the population due to differences between responders and nonresponders.</li>
<li>The data are not analyzed correctly.</li>
<li>The population is too large.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch10mcsolution4">MC Solution 4</a></p>
<p>The <strong>sample mean</strong> is calculated as:</p>
<ol type="a">
<li>The sum of all values in the sample divided by the number of values.</li>
<li>The middle value in the sample when the values are arranged in order.</li>
<li>The most frequent value in the sample.</li>
<li>The difference between the largest and smallest values in the sample.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch10mcsolution5">MC Solution 5</a></p>
<p>The <strong>sample variance</strong> measures:</p>
<ol type="a">
<li>The average value of the sample.</li>
<li>The spread or dispersion of the sample values around the sample mean.</li>
<li>The most frequent value in the sample.</li>
<li>The difference between the largest and smallest values in the sample.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch10mcsolution6">MC Solution 6</a></p>
<p>The <strong>Central Limit Theorem</strong> states that, for a large sample size, the distribution of the sample mean is approximately:</p>
<ol type="a">
<li>Uniform</li>
<li>Normal</li>
<li>Exponential</li>
<li>Chi-squared</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch10mcsolution7">MC Solution 7</a></p>
<p>The <strong>Central Limit Theorem</strong> applies:</p>
<ol type="a">
<li>Only to normally distributed populations.</li>
<li>To any population, regardless of its distribution.</li>
<li>Only to populations with a large variance.</li>
<li>Only to populations with a small variance.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch10mcsolution8">MC Solution 8</a></p>
<p>A <strong>kernel density estimate</strong> is:</p>
<ol type="a">
<li>A parametric method for estimating the probability density function.</li>
<li>A non-parametric method for estimating the probability density function.</li>
<li>A method for calculating the sample mean.</li>
<li>A method for calculating the sample variance.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch10mcsolution9">MC Solution 9</a></p>
<p>In <strong>kernel density estimation</strong>, the bandwidth parameter controls:</p>
<ol type="a">
<li>The number of data points.</li>
<li>The smoothness of the density estimate.</li>
<li>The scale of the x-axis.</li>
<li>The scale of the y-axis.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch10mcsolution10">MC Solution 10</a></p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, the statistic <span class="math inline">\(\dfrac{(n-1)s_*^2}{\sigma^2}\)</span> follows a:</p>
<ol type="a">
<li>Normal distribution</li>
<li>t-distribution</li>
<li>Chi-squared distribution</li>
<li>F-distribution</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch10mcsolution11">MC Solution 11</a></p>
<p>If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, the statistic <span class="math inline">\(\dfrac{\bar{X} - \mu}{s_*/\sqrt{n}}\)</span> follows a:</p>
<ol type="a">
<li>Normal distribution</li>
<li>t-distribution</li>
<li>Chi-squared distribution</li>
<li>F-distribution</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch10mcsolution12">MC Solution 12</a></p>
<p>A parameter is said to be <strong>unidentified</strong> if:</p>
<ol type="a">
<li>Its value is unknown.</li>
<li>Different values of the parameter lead to the same distribution of the observed data.</li>
<li>Its value can be uniquely determined from the data.</li>
<li>Its value is equal to zero.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch10mcsolution13">MC Solution 13</a></p>
<p>The <strong>likelihood function</strong> represents:</p>
<ol type="a">
<li>The probability of observing the data given the parameter values.</li>
<li>The probability of the parameter values given the data.</li>
<li>The joint distribution of the data and the parameters.</li>
<li>The marginal distribution of the data.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch10mcsolution14">MC Solution 14</a></p>
<p>The <strong>maximum likelihood estimator (MLE)</strong> is the parameter value that:</p>
<ol type="a">
<li>Minimizes the likelihood function.</li>
<li>Maximizes the likelihood function.</li>
<li>Is equal to the sample mean.</li>
<li>Is equal to the sample variance.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch10mcsolution15">MC Solution 15</a></p>
<p>A <strong>sufficient statistic</strong> for a parameter <span class="math inline">\(\theta\)</span> is a statistic that:</p>
<ol type="a">
<li>Is equal to the parameter value.</li>
<li>Captures all the information about <span class="math inline">\(\theta\)</span> contained in the sample data.</li>
<li>Has a normal distribution.</li>
<li>Is always unbiased.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch10mcsolution16">MC Solution 16</a></p>
<p>The <strong>sufficiency principle</strong> states that:</p>
<ol type="a">
<li>Any inference about a parameter should only depend on a sufficient statistic.</li>
<li>The sample size should always be as large as possible.</li>
<li>The data should always be normally distributed.</li>
<li>The maximum likelihood estimator should always be used.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch10mcsolution17">MC Solution 17</a></p>
<p>An <strong>ancillary statistic</strong> is a statistic whose distribution:</p>
<ol type="a">
<li>Depends on the parameter of interest.</li>
<li>Does not depend on the parameter of interest.</li>
<li>Is always normal.</li>
<li>Is always uniform.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch10mcsolution18">MC Solution 18</a></p>
<p>In <strong>Bayesian inference</strong>, the <strong>prior distribution</strong> represents:</p>
<ol type="a">
<li>The distribution of the data before observing any data.</li>
<li>The distribution of the parameter before observing any data.</li>
<li>The distribution of the data after observing the data.</li>
<li>The distribution of the parameter after observing the data.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch10mcsolution19">MC Solution 19</a></p>
<p>In <strong>Bayesian inference</strong>, the <strong>posterior distribution</strong> is obtained by:</p>
<ol type="a">
<li>Multiplying the prior distribution by the likelihood function and normalizing.</li>
<li>Dividing the prior distribution by the likelihood function.</li>
<li>Adding the prior distribution and the likelihood function.</li>
<li>Subtracting the likelihood function from the prior distribution.</li>
</ol>
</section>
<section class="level3" id="sec-ch10mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch10mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch10mcsolution20">MC Solution 20</a></p>
<p>An <strong>improper prior</strong> is a prior distribution that:</p>
<ol type="a">
<li>Is not a valid probability distribution because it does not integrate to 1.</li>
<li>Is uniform over the entire parameter space.</li>
<li>Is based on subjective beliefs.</li>
<li>Is only used in Frequentist inference.</li>
</ol>
</section>
</section>
<section class="level2" id="solutions-1">
<h2 class="anchored" data-anchor-id="solutions-1">Solutions</h2>
<section class="level3" id="sec-ch10mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch10mcexercise1">MC Exercise 1</a></p>
<p><strong>a. The population is divided into subgroups, and a random sample is taken from each subgroup.</strong></p>
<p><strong>Explanation:</strong> This is the definition of <strong>stratified sampling</strong>, as described in Section 10.1 of the text. The population is divided into strata (subgroups), and a random sample is taken from each stratum.</p>
</section>
<section class="level3" id="sec-ch10mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch10mcexercise2">MC Exercise 2</a></p>
<p><strong>b. It can ensure that subgroups of the population are adequately represented.</strong></p>
<p><strong>Explanation:</strong> One of the main advantages of <strong>stratified sampling</strong> is that it can ensure that each subgroup (stratum) of the population is adequately represented in the sample, as discussed in Section 10.1 of the text. This can lead to more precise estimates than simple random sampling, especially when the subgroups are different from each other.</p>
</section>
<section class="level3" id="sec-ch10mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch10mcexercise3">MC Exercise 3</a></p>
<p><strong>b. The sample is not representative of the population due to differences between responders and nonresponders.</strong></p>
<p><strong>Explanation:</strong> <strong>Sample selection bias</strong> occurs when the individuals who choose to participate in a study (or respond to a survey) are systematically different from those who do not, and this difference is related to the study’s outcome of interest, as discussed in Section 10.1 of the text. This can lead to biased estimates because the sample is not representative of the population.</p>
</section>
<section class="level3" id="sec-ch10mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch10mcexercise4">MC Exercise 4</a></p>
<p><strong>a. The sum of all values in the sample divided by the number of values.</strong></p>
<p><strong>Explanation:</strong> This is the definition of the <strong>sample mean</strong>, denoted by <span class="math inline">\(\bar{X}\)</span>. The formula is given in Section 10.2 of the text as:</p>
<p><span class="math inline">\(\bar{X} = \dfrac{1}{n} \sum_{i=1}^{n} X_i\)</span></p>
</section>
<section class="level3" id="sec-ch10mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch10mcexercise5">MC Exercise 5</a></p>
<p><strong>b. The spread or dispersion of the sample values around the sample mean.</strong></p>
<p><strong>Explanation:</strong> The <strong>sample variance</strong>, denoted by <span class="math inline">\(s^2\)</span>, measures the average squared deviation of the sample values from the sample mean. It is a measure of the spread or dispersion of the data. The formula is given in Section 10.2 of the text as:</p>
<p><span class="math inline">\(s^2 = \dfrac{1}{n} \sum_{i=1}^{n} (X_i - \bar{X})^2\)</span></p>
</section>
<section class="level3" id="sec-ch10mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch10mcexercise6">MC Exercise 6</a></p>
<p><strong>b. Normal</strong></p>
<p><strong>Explanation:</strong> The <strong>Central Limit Theorem (CLT)</strong> states that, for a large sample size, the distribution of the sample mean is approximately normal, regardless of the shape of the population distribution. This is stated in Theorem 10.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch10mcexercise7">MC Exercise 7</a></p>
<p><strong>b. To any population, regardless of its distribution.</strong></p>
<p><strong>Explanation:</strong> The <strong>Central Limit Theorem (CLT)</strong> applies to any population, regardless of its distribution, as long as the sample size is sufficiently large. This is a key aspect of the CLT and is implied in the discussion in Section 10.2.1 and Theorem 10.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch10mcexercise8">MC Exercise 8</a></p>
<p><strong>b. A non-parametric method for estimating the probability density function.</strong></p>
<p><strong>Explanation:</strong> A <strong>kernel density estimate</strong> is a non-parametric method for estimating the probability density function of a random variable. It is a way to smooth out a histogram and get a continuous estimate of the density, as discussed in Section 10.2 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch10mcexercise9">MC Exercise 9</a></p>
<p><strong>b. The smoothness of the density estimate.</strong></p>
<p><strong>Explanation:</strong> In <strong>kernel density estimation</strong>, the bandwidth parameter, denoted by <span class="math inline">\(h\)</span>, controls the smoothness of the density estimate. A larger bandwidth results in a smoother estimate, while a smaller bandwidth results in a more wiggly estimate. This is explained in Section 10.2 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch10mcexercise10">MC Exercise 10</a></p>
<p><strong>c. Chi-squared distribution</strong></p>
<p><strong>Explanation:</strong> If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the statistic <span class="math inline">\(\dfrac{(n-1)s_*^2}{\sigma^2}\)</span> follows a chi-squared distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. This is stated in Theorem 10.2, part (3) of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch10mcexercise11">MC Exercise 11</a></p>
<p><strong>b. t-distribution</strong></p>
<p><strong>Explanation:</strong> If <span class="math inline">\(X_1, \dots, X_n\)</span> is a random sample from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the statistic <span class="math inline">\(\dfrac{\bar{X} - \mu}{s_*/\sqrt{n}}\)</span> follows a t-distribution with <span class="math inline">\(n-1\)</span> degrees of freedom. This is stated in Theorem 10.2, part (4) of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch10mcexercise12">MC Exercise 12</a></p>
<p><strong>b. Different values of the parameter lead to the same distribution of the observed data.</strong></p>
<p><strong>Explanation:</strong> A parameter is said to be <strong>unidentified</strong> if different values of the parameter give rise to the same distribution of the observed data. This means that it is impossible to distinguish between these parameter values based on the data alone. This concept is discussed in Section 10.3 and defined in Definition 10.2 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch10mcexercise13">MC Exercise 13</a></p>
<p><strong>a. The probability of observing the data given the parameter values.</strong></p>
<p><strong>Explanation:</strong> The <strong>likelihood function</strong> represents the probability (or probability density) of observing the data given the parameter values. It is a function of the parameters, with the data held fixed. It is defined in Section 10.3 of the text as:</p>
<p><span class="math inline">\(L(\theta|X^n) = f(X_1, \dots, X_n|\theta)\)</span></p>
</section>
<section class="level3" id="sec-ch10mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch10mcexercise14">MC Exercise 14</a></p>
<p><strong>b. Maximizes the likelihood function.</strong></p>
<p><strong>Explanation:</strong> The <strong>maximum likelihood estimator (MLE)</strong> is the parameter value that maximizes the likelihood function. In other words, it is the parameter value that makes the observed data most probable under the assumed statistical model. This is discussed in Section 10.3 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch10mcexercise15">MC Exercise 15</a></p>
<p><strong>b. Captures all the information about <span class="math inline">\(\theta\)</span> contained in the sample data.</strong></p>
<p><strong>Explanation:</strong> A <strong>sufficient statistic</strong> for a parameter <span class="math inline">\(\theta\)</span> is a statistic that captures all the information about <span class="math inline">\(\theta\)</span> that is contained in the sample data. Once the sufficient statistic is known, no other function of the sample data can provide any additional information about <span class="math inline">\(\theta\)</span>. This concept is defined in Section 10.3.1.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch10mcexercise16">MC Exercise 16</a></p>
<p><strong>a. Any inference about a parameter should only depend on a sufficient statistic.</strong></p>
<p><strong>Explanation:</strong> The <strong>sufficiency principle</strong> states that if <span class="math inline">\(T(X)\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span>, then any inference about <span class="math inline">\(\theta\)</span> should depend on the sample <span class="math inline">\(X\)</span> only through the value of <span class="math inline">\(T(X)\)</span>. This principle is discussed in Section 10.3.1.1 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch10mcexercise17">MC Exercise 17</a></p>
<p><strong>b. Does not depend on the parameter of interest.</strong></p>
<p><strong>Explanation:</strong> An <strong>ancillary statistic</strong> is a statistic whose distribution does not depend on the parameter of interest. It provides no information about the parameter on its own. This is explained in Section 10.3.1.2 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch10mcexercise18">MC Exercise 18</a></p>
<p><strong>b. The distribution of the parameter before observing any data.</strong></p>
<p><strong>Explanation:</strong> In <strong>Bayesian inference</strong>, the <strong>prior distribution</strong> represents our initial beliefs about the possible values of a parameter <em>before</em> observing any data. This is discussed in Section 10.3.2 of the text.</p>
</section>
<section class="level3" id="sec-ch10mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch10mcexercise19">MC Exercise 19</a></p>
<p><strong>a. Multiplying the prior distribution by the likelihood function and normalizing.</strong></p>
<p><strong>Explanation:</strong> In <strong>Bayesian inference</strong>, the <strong>posterior distribution</strong> is obtained by multiplying the prior distribution by the likelihood function and then normalizing the result so that it integrates to 1. This is expressed by Bayes’ theorem, as shown in Section 10.3.2 of the text:</p>
<p><span class="math inline">\(\pi(\theta|X^n) \propto L(\theta|X^n) \times \pi(\theta)\)</span></p>
</section>
<section class="level3" id="sec-ch10mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch10mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch10mcexercise20">MC Exercise 20</a></p>
<p><strong>a. Is not a valid probability distribution because it does not integrate to 1.</strong></p>
<p><strong>Explanation:</strong> An <strong>improper prior</strong> is a prior distribution that does not integrate to 1 and therefore is not a proper probability distribution. Improper priors are often used when there is no prior information about the parameter, as discussed in the text following Example 10.10.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>