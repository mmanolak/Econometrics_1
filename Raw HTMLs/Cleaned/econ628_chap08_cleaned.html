<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 8: Asymptotic Theory – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap09.html" rel="next"/>
<link href="../chapters/chap07alt.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 8: Asymptotic Theory</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level2" id="inequalities">
<h2 class="anchored" data-anchor-id="inequalities">8.1 INEQUALITIES</h2>
<p>This section gives some inequalities that are useful tools in establishing a variety of probabilistic results.</p>
<section class="level3" id="theorem-8.1.-triangle-inequality.">
<h3 class="anchored" data-anchor-id="theorem-8.1.-triangle-inequality.">Theorem 8.1. TRIANGLE INEQUALITY.</h3>
<p>For random variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span></p>
<p><span class="math display">\[
E|X+Y| \leq E|X|+E|Y| .
\]</span></p>
<p><strong>Proof</strong>. This just follows from the property of real numbers and the additivity of expectation.</p>
<p><strong>Intuitive explanation</strong>: The expected value of the absolute value of the sum of two random variables is less than or equal to the sum of the expected values of the absolute values of each random variable. The intuition behind this is that the absolute value function is convex, and by Jensen’s inequality, the expectation of a convex function of a random variable is greater than or equal to the convex function of the expectation of the random variable.</p>
</section>
<section class="level3" id="theorem-8.2.-cauchy-schwarz-inequality.">
<h3 class="anchored" data-anchor-id="theorem-8.2.-cauchy-schwarz-inequality.">Theorem 8.2. CAUCHY-SCHWARZ INEQUALITY.</h3>
<p>For random variables <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, <span class="math display">\[
(E(X Y))^2 \leq E\left(X^2\right) \times E\left(Y^2\right)
\]</span> with equality if and only if <span class="math inline">\(Y=a X\)</span> for some <span class="math inline">\(a\)</span>.</p>
<p><strong>Proof</strong>. Let <span class="math display">\[
0 \leq h(t)=E\left[(t X-Y)^2\right]=t^2 E\left(X^2\right)+E\left(Y^2\right)-2 t E(X Y) .
\]</span></p>
<p>Then, <span class="math inline">\(h(t)\)</span> is a quadratic function in <span class="math inline">\(t\)</span> which increases as <span class="math inline">\(t \rightarrow \pm \infty\)</span>. It has a minimum at</p>
<p><span class="math display">\[
\begin{aligned}
&amp; h^{\prime}(t)=2 t E\left(X^2\right)-2 E(X Y) \\
&amp; \Rightarrow t=\frac{E(X Y)}{E\left(X^2\right)} .
\end{aligned}
\]</span></p>
<p>Substituting into <span class="math inline">\(h(t)\)</span> we obtain</p>
<p><span class="math display">\[
0 \leq \frac{E^2 X Y}{E\left(X^2\right)}+E\left(Y^2\right)-\frac{2 E^2(X Y)}{E\left(X^2\right)},
\]</span></p>
<p>which implies that</p>
<p><span class="math display">\[
0 \leq E\left(Y^2\right)-\frac{E^2(X Y)}{E\left(X^2\right)},
\]</span></p>
<p>which implies the result.</p>
<p><strong>Intuitive explanation</strong>: The Cauchy-Schwarz inequality states that the absolute value of the expected value of the product of two random variables is less than or equal to the square root of the product of the expected values of the squares of each random variable. In simpler terms, it provides an upper bound for the covariance between two random variables.</p>
<p>The <strong>Cauchy-Schwarz inequality</strong> is also used in other nonprobabilistic contexts, so that for numbers <span class="math inline">\(a_i, b_i\)</span></p>
<p><span class="math display">\[
\left(\sum a_i b_i\right)^2 \leq\left(\sum a_i^2\right) \cdot\left(\sum b_i^2\right),
\]</span></p>
<p>and for functions <span class="math inline">\(f, g\)</span></p>
<p><span class="math display">\[
\left(\int f g\right)^2 \leq \int f^2 \cdot \int g^2 .
\]</span></p>
</section>
<section class="level3" id="theorem-8.3.-hölders-inequality.">
<h3 class="anchored" data-anchor-id="theorem-8.3.-hölders-inequality.">Theorem 8.3. HÖLDER’S INEQUALITY.</h3>
<p>For any <span class="math inline">\(p, q\)</span> satisfying <span class="math inline">\(\frac{1}{p}+\frac{1}{q}=1\)</span>, we have</p>
<p><span class="math display">\[
E|X Y| \leq\left(E|X|^p\right)^{1 / p}\left(E|Y|^q\right)^{1 / q} .
\]</span></p>
<p>For example, <span class="math display">\[
E|X Y| \leq\left(E|X|^8\right)^{1 / 8}\left(E|Y|^{8 / 7}\right)^{7 / 8} .
\]</span></p>
<p><strong>Intuitive explanation</strong>: Hölder’s inequality is a generalization of the Cauchy-Schwarz inequality. It states that the expected value of the product of two random variables is less than or equal to the product of the <span class="math inline">\(p\)</span>-norm of one random variable and the <span class="math inline">\(q\)</span>-norm of the other random variable, where <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are conjugate exponents.</p>
</section>
<section class="level3" id="theorem-8.4.-markovs-inequality.">
<h3 class="anchored" data-anchor-id="theorem-8.4.-markovs-inequality.">Theorem 8.4. MARKOV’S INEQUALITY.</h3>
<p>For <span class="math inline">\(\eta&gt;0\)</span></p>
<p><span class="math display">\[
\operatorname{Pr}[|X| \geq \eta] \leq \frac{E|X|}{\eta} .
\]</span></p>
<p><strong>Proof</strong>. First, we have</p>
<p><span class="math display">\[
\operatorname{Pr}[|X| \geq \eta]=\int_{|x| \geq \eta} f(x) d x=\int \mathbb{1}(|x| \geq \eta) f(x) d x=E \mathbb{1}(|X| \geq \eta)
\]</span></p>
<p>Clearly (see Fig. 8.1), <span class="math display">\[
\text{indicator } \mathbb{1}(|X| \geq \eta) \leq \frac{|X|}{\eta} .
\]</span> This implies that</p>
<p><span class="math display">\[
E \mathbb{1}(|X| \geq \eta) \leq \frac{E|X|}{\eta} .
\]</span></p>
<p><strong>Intuitive explanation</strong>: Markov’s inequality provides an upper bound for the probability that a non-negative random variable exceeds a certain value. It states that the probability that a non-negative random variable <span class="math inline">\(X\)</span> is greater than or equal to a positive constant <span class="math inline">\(\eta\)</span> is less than or equal to the expected value of <span class="math inline">\(X\)</span> divided by <span class="math inline">\(\eta\)</span>.</p>
<p><strong>Real-world example</strong>: Suppose you are analyzing the income distribution of a population. Let <span class="math inline">\(X\)</span> be a random variable representing the income of an individual, and let <span class="math inline">\(\eta\)</span> be a certain income threshold, say $100,000. Markov’s inequality states that the proportion of individuals with income greater than or equal to $100,000 is at most the average income divided by $100,000.</p>
</section>
<section class="level3" id="theorem-8.5.-chebychevs-inequality.">
<h3 class="anchored" data-anchor-id="theorem-8.5.-chebychevs-inequality.">Theorem 8.5. CHEBYCHEV’S INEQUALITY.</h3>
<p>For <span class="math inline">\(\eta&gt;0\)</span></p>
<p><span class="math display">\[
\operatorname{Pr}[|X-E X| \geq \eta] \leq \frac{\operatorname{var}(X)}{\eta^2} .
\]</span></p>
<p><strong>Proof</strong>. Assume that <span class="math inline">\(E X=0\)</span> and compare</p>
<p><span class="math display">\[
\mathbb{1}(|X| \geq \eta) \text { with } \frac{X^2}{\eta^2} .
\]</span></p>
<p><strong>Intuitive explanation</strong>: Chebyshev’s inequality provides a bound on the probability that a random variable deviates from its mean by more than a certain number of standard deviations. It states that the probability that a random variable <span class="math inline">\(X\)</span> deviates from its mean <span class="math inline">\(EX\)</span> by at least <span class="math inline">\(\eta\)</span> is less than or equal to the variance of <span class="math inline">\(X\)</span> divided by <span class="math inline">\(\eta^2\)</span>.</p>
<p><strong>Real-world example</strong>: Consider a manufacturing process that produces items with a certain average weight and standard deviation. Chebyshev’s inequality can be used to estimate the proportion of items whose weight deviates from the average weight by more than a certain amount. For example, if the average weight is 10 grams and the standard deviation is 1 gram, Chebyshev’s inequality states that the proportion of items with weight less than 7 grams or more than 13 grams is at most 1/9.</p>
<p>The Markov and Chebychev inequalities have many statistical applications, but they have also been useful in investment theory.</p>
</section>
<section class="level3" id="example-8.1.">
<h3 class="anchored" data-anchor-id="example-8.1.">Example 8.1.</h3>
<p><strong>Roy (1952)</strong> proposed the <strong>safety first rule</strong> of investment, which involved minimizing the probability of disaster. If <span class="math inline">\(d\)</span> is the disaster level and <span class="math inline">\(X\)</span> is the investment return (which can be controlled up to a point by portfolio choice decisions), then Roy advocated minimizing the disaster probability</p>
<p><span class="math display">\[
\operatorname{Pr}(X&lt;d) .
\]</span></p>
<p>If <span class="math inline">\(X\)</span> were normally distributed, then <span class="math inline">\(\operatorname{Pr}(X&lt;d)=\Phi((d-\mu) / \sigma)\)</span>, where <span class="math inline">\(\mu=E(X)\)</span> and <span class="math inline">\(\sigma^2=\operatorname{var}(X)\)</span>, and in that case, the portfolio <span class="math inline">\(X\)</span> that maximizes <span class="math inline">\((\mu-d) / \sigma\)</span> also minimizes the disaster risk. When <span class="math inline">\(d\)</span> is chosen to be the risk free rate <span class="math inline">\(r\)</span>, the prescription is to maximize the <strong>Sharpe ratio</strong>. When <span class="math inline">\(X\)</span> is not normally distributed we can infer from Chebychev’s inequality that</p>
<p><span class="math display">\[
\operatorname{Pr}(|X-\mu| \geq \mu-d) \leq \frac{\sigma^2}{(\mu-d)^2},
\]</span></p>
<p>from which we obtain</p>
<p><span class="math display">\[
\operatorname{Pr}(X&lt;d) \leq \frac{\sigma^2}{(\mu-d)^2} .
\]</span></p>
<p>Roy proposed to minimize this upper bound, which is valid for any distribution with finite variance.</p>
<p>These inequalities can be quite “conservative”. Suppose that <span class="math inline">\(X \sim N(0,1)\)</span>, then we know that <span class="math inline">\(\operatorname{Pr}(|X| \geq 1.96)=0.05\)</span>, whereas Chebychev’s inequality says only that <span class="math inline">\(\operatorname{Pr}(|X| \geq 1.96) \leq 0.26\)</span>.</p>
</section>
</section>
<section class="level2" id="notions-of-convergence">
<h2 class="anchored" data-anchor-id="notions-of-convergence">8.2 NOTIONS OF CONVERGENCE</h2>
<p>Asymptotic theory involves generalizing the usual notions of convergence for real sequences to allow for random variables.</p>
<section class="level3" id="definition-8.1.">
<h3 class="anchored" data-anchor-id="definition-8.1.">Definition 8.1.</h3>
<p>We say that a sequence of real numbers <span class="math inline">\(\left\{x_n\right\}_{n=1}^{\infty}\)</span> converges to a limit <span class="math inline">\(x_{\infty}, x_n \rightarrow x_{\infty}\)</span>, also denoted <span class="math display">\[
\lim _{n \rightarrow \infty} x_n=x_{\infty},
\]</span> if for all <span class="math inline">\(\epsilon&gt;0\)</span> there exists an integer <span class="math inline">\(n_0\)</span> such that <span class="math inline">\(\left|x_n-x_{\infty}\right|&lt;\epsilon\)</span> for all <span class="math inline">\(n \geq n_0\)</span>.</p>
</section>
<section class="level3" id="example-8.2.">
<h3 class="anchored" data-anchor-id="example-8.2.">Example 8.2.</h3>
<p><span class="math inline">\(x_n=1 / n, \lim _{n \rightarrow \infty} x_n=0\)</span>.</p>
</section>
<section class="level3" id="example-8.3.">
<h3 class="anchored" data-anchor-id="example-8.3.">Example 8.3.</h3>
<p><span class="math inline">\(x_n=\sin (\pi n)\)</span> or <span class="math inline">\(x_n=\sin (\pi / n)\)</span> do not converge, i.e., <span class="math inline">\(\lim _{n \rightarrow \infty} x_n\)</span> does not exist.</p>
<p>In the case where a sequence does not converge there are always subsequences that converge. The <span class="math inline">\(\liminf\)</span> is the smallest possible limit and <span class="math inline">\(\limsup\)</span> is the largest possible limit. In the previous example, <span class="math inline">\(\liminf _{n \rightarrow \infty} \sin (\pi n)=-1\)</span> and <span class="math inline">\(\limsup _{n \rightarrow \infty} \sin (\pi n)=1\)</span>.</p>
<p>Random variables are more complicated than real numbers, and for sequences of random variables there are several different notions of convergence.</p>
</section>
<section class="level3" id="definition-8.2.">
<h3 class="anchored" data-anchor-id="definition-8.2.">Definition 8.2.</h3>
<p>We say that a sequence of random variables <span class="math inline">\(\left\{X_n\right\}_{n=1}^{\infty}\)</span> <strong>converges with probability one</strong> (or <strong>almost surely</strong>) to a random variable <span class="math inline">\(X\)</span>, denoted, <span class="math display">\[
X_n \stackrel{\text { a.s. }}{\rightarrow} X,
\]</span> if <span class="math display">\[
\operatorname{Pr}\left(\lim _{n \rightarrow \infty} X_n=X\right)=1 .
\]</span></p>
<p>We might ask what to say if <span class="math inline">\(X_n \rightarrow X\)</span> with probability one half, say, but this is not possible according to the <strong>zero-one law</strong>, i.e., either a sequence of random variables converges with probability one or with probability zero.</p>
<p>A real-valued random variable <span class="math inline">\(X\)</span> satisfies for all <span class="math inline">\(\epsilon&gt;0\)</span> there exists an <span class="math inline">\(M\)</span> such that <span class="math display">\[
\operatorname{Pr}(|X|&gt;M) \leq \epsilon,
\]</span> that is, it is <strong>stochastically bounded</strong>.</p>
</section>
<section class="level3" id="definition-8.3.">
<h3 class="anchored" data-anchor-id="definition-8.3.">Definition 8.3.</h3>
<p>A sequence <span class="math inline">\(\left\{X_n\right\}_{n=1}^{\infty}\)</span> of random variables is <strong>stochastically bounded</strong> if for all <span class="math inline">\(\epsilon&gt;0\)</span> there exists an <span class="math inline">\(M\)</span> and <span class="math inline">\(n_0\)</span> such that for all <span class="math inline">\(n \geq n_0\)</span> <span class="math display">\[
\operatorname{Pr}\left(\left|X_n\right|&gt;M\right) \leq \epsilon .
\]</span> In this case we write <span class="math inline">\(X_n=O_p(1)\)</span>.</p>
</section>
<section class="level3" id="example-8.4.">
<h3 class="anchored" data-anchor-id="example-8.4.">Example 8.4.</h3>
<p>Suppose that <span class="math inline">\(X_n \sim N(\sin (n), 1)\)</span> for each <span class="math inline">\(n\)</span>. This sequence does not converge in any useful way but it is stochastically bounded.</p>
</section>
<section class="level3" id="definition-8.4.">
<h3 class="anchored" data-anchor-id="definition-8.4.">Definition 8.4.</h3>
<p>We say that a sequence of random variables <span class="math inline">\(\left\{X_n\right\}_{n=1}^{\infty}\)</span> <strong>converges in probability</strong> to a random variable <span class="math inline">\(X\)</span>, denoted, <span class="math display">\[
X_n \stackrel{p}{\rightarrow} X,
\]</span> if for all <span class="math inline">\(\epsilon&gt;0\)</span>, <span class="math display">\[
\lim _{n \rightarrow \infty} G_n(\epsilon)=0,
\]</span> where <span class="math inline">\(G_n(\epsilon)=\operatorname{Pr}\left(\left|X_n-X\right|&gt;\epsilon\right)\)</span>. The limit <span class="math inline">\(X\)</span> could be a constant or a random variable. We sometimes write <span class="math display">\[
p \lim _{n \rightarrow \infty} X_n=X
\]</span> or <span class="math inline">\(X_n=X+o_p(1)\)</span>.</p>
<p><strong>Intuitive explanation</strong>: Convergence in probability means that as the sample size increases, the probability that the sequence of random variables takes on a value close to the limiting random variable approaches 1. In other words, the sequence of random variables becomes increasingly concentrated around the limiting random variable.</p>
<p>Convergence in probability is weaker than convergence almost surely (convergence almost surely implies convergence in probability) and easier to establish. The quantity <span class="math inline">\(G_n(\epsilon)\)</span> depends on the joint distribution of <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="example-8.5.">
<h3 class="anchored" data-anchor-id="example-8.5.">Example 8.5.</h3>
<p>Suppose that <span class="math display">\[
X_n= \begin{cases}1 &amp; \text { with probability } \dfrac{1}{n} \\ 0 &amp; \text { with probability } 1-\dfrac{1}{n}\end{cases}
\]</span> Then, for any <span class="math inline">\(\epsilon&gt;0\)</span> <span class="math display">\[
\operatorname{Pr}\left(X_n \geq \epsilon\right)=\frac{1}{n} \rightarrow 0,
\]</span> so that <span class="math inline">\(X_n \stackrel{p}{\rightarrow} 0\)</span>.</p>
</section>
<section class="level3" id="example-8.6.">
<h3 class="anchored" data-anchor-id="example-8.6.">Example 8.6.</h3>
<p><span class="math inline">\(X_n \sim U[0,1 / n]\)</span>. We have for any <span class="math inline">\(\epsilon&gt;0\)</span> that there exists an <span class="math inline">\(n_0\)</span> such that <span class="math inline">\(\epsilon&gt;1 / n_0\)</span>. Then for all <span class="math inline">\(n \geq n_0\)</span> <span class="math display">\[
\operatorname{Pr}\left(X_n \geq \epsilon\right)=0,
\]</span> so that <span class="math inline">\(X_n \stackrel{p}{\rightarrow} 0\)</span>.</p>
<p>Suppose that <span class="math display">\[
X_n=X+R_n,
\]</span> where <span class="math inline">\(R_n \stackrel{p}{\rightarrow} 0\)</span>. Then <span class="math display">\[
\operatorname{Pr}\left(\left|X_n-X\right|&gt;\epsilon\right)=\operatorname{Pr}\left(\left|R_n\right|&gt;\epsilon\right) \rightarrow 0,
\]</span> and it follows that the random variable <span class="math inline">\(X\)</span> is the probability limit of <span class="math inline">\(X_n\)</span>, i.e., <span class="math inline">\(X=p \lim _{n \rightarrow \infty} X_n\)</span>.</p>
</section>
<section class="level3" id="definition-8.5.">
<h3 class="anchored" data-anchor-id="definition-8.5.">Definition 8.5.</h3>
<p>We say that a sequence of random variables <span class="math inline">\(\left\{X_n\right\}_{n=1}^{\infty}\)</span> <strong>converges in mean square</strong> to a random variable <span class="math inline">\(X\)</span>, denoted <span class="math inline">\(X_n \stackrel{m . s .}{\rightarrow} X\)</span>, if <span class="math display">\[
\lim _{n \rightarrow \infty} E\left[\left(X_n-X\right)^2\right]=0 .
\]</span></p>
<p>This presumes of course that <span class="math inline">\(E X_n^2&lt;\infty\)</span> and <span class="math inline">\(E X^2&lt;\infty\)</span>. When <span class="math inline">\(X\)</span> is a constant, <span class="math display">\[
E\left[\left(X_n-X\right)^2\right]=E\left[\left(X_n-E X_n\right)^2\right]+\left(E X_n-X\right)^2=\operatorname{var}\left(X_n\right)+\left(E X_n-X\right)^2,
\]</span> and it is necessary and sufficient that <span class="math inline">\(E X_n \rightarrow X\)</span> and <span class="math inline">\(\operatorname{var}\left(X_n\right) \rightarrow 0\)</span>. Mean square convergence implies convergence in probability. This follows from the Chebychev inequality. Convergence in probability does not imply convergence in mean square.</p>
</section>
<section class="level3" id="example-8.7.">
<h3 class="anchored" data-anchor-id="example-8.7.">Example 8.7.</h3>
<p>Suppose that <span class="math display">\[
X_n= \begin{cases}n &amp; \text { with probability } \dfrac{1}{n} \\ 0 &amp; \text { with probability } 1-\dfrac{1}{n}\end{cases}
\]</span> Then for any <span class="math inline">\(\epsilon&gt;0\)</span> <span class="math display">\[
\operatorname{Pr}\left(X_n \geq \epsilon\right)=\frac{1}{n} \rightarrow 0 .
\]</span> But <span class="math display">\[
E\left(X_n^2\right)=n^2 \frac{1}{n}=n \rightarrow \infty .
\]</span></p>
</section>
<section class="level3" id="definition-8.6.">
<h3 class="anchored" data-anchor-id="definition-8.6.">Definition 8.6.</h3>
<p>We say that a sequence of random variables <span class="math inline">\(\left\{X_n\right\}_{n=1}^{\infty}\)</span> <strong>converges in distribution</strong> to a random variable <span class="math inline">\(X\)</span>, denoted, <span class="math display">\[
X_n \stackrel{D}{\rightarrow} X,
\]</span> if for all <span class="math inline">\(x\)</span> at which <span class="math inline">\(F(x)=\operatorname{Pr}[X \leq x]\)</span> is continuous, <span class="math display">\[
\lim _{n \rightarrow \infty} F_n(x)=\operatorname{Pr}(X \leq x),
\]</span> where <span class="math inline">\(F_n(x)=\operatorname{Pr}\left(X_n \leq x\right)\)</span>.</p>
<p><strong>Intuitive explanation</strong>: Convergence in distribution means that the cumulative distribution function (CDF) of the sequence of random variables converges to the CDF of the limiting random variable at all points where the limiting CDF is continuous. In other words, the probability that the sequence of random variables takes on a value less than or equal to a certain value approaches the probability that the limiting random variable takes on a value less than or equal to that value.</p>
<p>Specifically, we often have</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\theta}-\theta) \stackrel{D}{\rightarrow} X \sim N\left(0, \sigma^2\right) \text{, shorthand notation } \sqrt{n}(\hat{\theta}-\theta) \rightarrow N\left(0, \sigma^2\right) .
\]</span></p>
<p>The limiting normal distribution is continuous everywhere and so in this case we must check the convergence for all <span class="math inline">\(x \in \mathbb{R}\)</span>. Convergence in distribution does not restrict at all the relationship between <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> : specifically, <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> could be independent in which case <span class="math display">\[
\operatorname{Pr}\left(\left|X_n-X\right|&gt;\epsilon\right)
\]</span> does not go to zero. Note that convergence in probability is stronger than convergence in distribution, but they are equivalent when <span class="math inline">\(X\)</span> is a constant (i.e., not random).</p>
</section>
<section class="level3" id="theorem-8.6.">
<h3 class="anchored" data-anchor-id="theorem-8.6.">Theorem 8.6.</h3>
<p>Suppose that the sequence of random variables <span class="math inline">\(\left\{X_n\right\}_{n=1}^{\infty}\)</span> satisfies <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> for some random variable <span class="math inline">\(X\)</span>. Then <span class="math inline">\(X_n \stackrel{D}{\rightarrow} X\)</span>. If <span class="math inline">\(X\)</span> is constant, then vice versa.</p>
<p><strong>Proof</strong>. Let <span class="math inline">\(x\)</span> be a point of continuity of <span class="math inline">\(X\)</span>, i.e., <span class="math inline">\(\operatorname{Pr}(X=x)=0\)</span>. For any <span class="math inline">\(\epsilon&gt;0\)</span> we have by the law of total probability that <span class="math display">\[
\begin{aligned}
\operatorname{Pr}\left(X_n \leq x\right) &amp; \leq \operatorname{Pr}\left(X_n \leq x,\left|X_n-X\right|&lt;\epsilon\right)+\operatorname{Pr}\left(\left|X_n-X\right| \geq \epsilon\right) \\
&amp; \leq \operatorname{Pr}(X \leq x+\epsilon)+\operatorname{Pr}\left(\left|X_n-X\right| \geq \epsilon\right) .
\end{aligned}
\]</span> Similarly we obtain a lower bound to yield <span class="math display">\[
\operatorname{Pr}(X \leq x-\epsilon)-\operatorname{Pr}\left(\left|X_n-X\right|&gt;\epsilon\right) \leq \operatorname{Pr}\left(X_n \leq x\right) \leq \operatorname{Pr}(X \leq x+\epsilon)+\operatorname{Pr}\left(\left|X_n-X\right| \geq \epsilon\right) .
\]</span> Then letting <span class="math inline">\(n \rightarrow \infty\)</span> and <span class="math inline">\(\epsilon \rightarrow 0\)</span> we obtain the result.</p>
</section>
<section class="level3" id="example-8.8.">
<h3 class="anchored" data-anchor-id="example-8.8.">Example 8.8.</h3>
<p>Suppose that <span class="math display">\[
X_n= \begin{cases}\dfrac{1}{n} &amp; \text { with probability } \dfrac{1}{2} \\ -\dfrac{1}{n} &amp; \text { with probability } \dfrac{1}{2}\end{cases}
\]</span> Then <span class="math inline">\(X_n \stackrel{P}{\rightarrow} 0\)</span> and so <span class="math inline">\(X_n \stackrel{D}{\rightarrow} 0\)</span>, i.e., <span class="math display">\[
\operatorname{Pr}\left(X_n \leq x\right) \rightarrow \begin{cases}1 &amp; x \geq 0 \\ 0 &amp; x&lt;0\end{cases}
\]</span> But note that for all <span class="math inline">\(n, \operatorname{Pr}\left(X_n=0\right)=0\)</span>.</p>
</section>
</section>
<section class="level2" id="laws-of-large-numbers-and-clt">
<h2 class="anchored" data-anchor-id="laws-of-large-numbers-and-clt">8.3 LAWS OF LARGE NUMBERS AND CLT</h2>
<p>We now come to the two main theorems, the <strong>Law of Large Numbers</strong> (LLN), and the <strong>Central Limit Theorem</strong> (CLT).</p>
<section class="level3" id="theorem-8.7.">
<h3 class="anchored" data-anchor-id="theorem-8.7.">Theorem 8.7.</h3>
<p>(Kolmogorov) Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be independent and identically distributed (i.i.d.). Then a necessary and sufficient condition for <span class="math display">\[
T_n=\frac{1}{n} \sum_{i=1}^n X_i \stackrel{a . s .}{\rightarrow} \mu=E\left(X_1\right)
\]</span> is that <span class="math inline">\(E\left(\left|X_i\right|\right)&lt;\infty\)</span>.</p>
<p>This is called the <strong>strong law of large numbers</strong> and implies the <strong>weak law of large numbers</strong> (convergence in probability). The hypothesis that <span class="math inline">\(E\left(\left|X_i\right|\right)&lt;\infty\)</span> is necessary, i.e., if <span class="math inline">\(E|X|=\infty\)</span>, for example <span class="math inline">\(X\)</span> is Cauchy, then the LLN does not hold. What happens in that case? In fact, we may show in that case that <span class="math inline">\(\sum_{i=1}^n X_i / a_n\)</span> converges in distribution to a random variable, called a stable distribution for some sequence <span class="math inline">\(a_n\)</span> with <span class="math inline">\(a_n / \sqrt{n} \rightarrow \infty\)</span>. Under stronger moment conditions we can establish the CLT.</p>
</section>
<section class="level3" id="theorem-8.8.">
<h3 class="anchored" data-anchor-id="theorem-8.8.">Theorem 8.8.</h3>
<p>(Lindeberg-Levy) Let <span class="math inline">\(X_1, \ldots, X_n\)</span> be i.i.d. with <span class="math inline">\(E\left(X_i\right)=\mu\)</span> and <span class="math inline">\(\operatorname{var}\left(X_i\right)=\sigma^2\)</span>. Then <span class="math display">\[
\frac{1}{\sqrt{n}} \sum_{i=1}^n\left(X_i-\mu\right) \stackrel{D}{\rightarrow} N\left(0, \sigma^2\right) .
\]</span></p>
<p><strong>Proof</strong>. A proof of the result can be based on the characteristic function. Thus <span class="math display">\[
\phi_n(t)=E\left(e^{i t \sum_{i=1}^n X_i / \sqrt{n}}\right)=\varphi(t / \sqrt{n})^n,
\]</span> where <span class="math inline">\(\varphi(t)=E\left(e^{i t X_i}\right)\)</span>. We may expand the right hand side as follows <span class="math display">\[
\varphi(t / \sqrt{n})^n=\left[\varphi(0)+\frac{t}{\sqrt{n}} \varphi^{\prime}(0)+\frac{t^2}{2 n} \varphi^{\prime \prime}(0)+\ldots\right]^n,
\]</span> where <span class="math inline">\(\varphi(0)=1, \varphi^{\prime}(0)=i E(X)=0\)</span>, and <span class="math inline">\(\varphi^{\prime \prime}(0)=i^2 E\left(X^2\right)=-\sigma^2\)</span>. Therefore, <span class="math display">\[
\phi_n(t)=\left[1-\frac{t^2 \sigma^2}{2 n}+o\left(t^2 / n^2\right)\right]^n \rightarrow e^{-t^2 \sigma^2 / 2},
\]</span> which is the characteristic function of a normal random variable. This is a bit heuristic.</p>
<p>The LLN and CLT are important results because they show that no matter what the distribution of <span class="math inline">\(X_i\)</span>, the average of the <span class="math inline">\(X^{\prime}\)</span> ’s has a distribution that can be approximated by a potentially much simpler normal distribution. Example suppose <span class="math inline">\(X_i \sim U[0,1]\)</span>, Fig. 8.2 is distribution of <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n X_i\)</span> from 1000000 simulations.</p>
<p>There are now many generalizations of LLN and CLT for data that are not i.i.d., e.g., heterogeneous, dependent weighted sums. We will consider <strong>triangular arrays</strong> of random variables <span class="math inline">\(\left\{X_{n i}\right\}_{i=1}^n\)</span>, which allow a little more generality than sequences; this generality is needed in a number of situations.</p>
</section>
<section class="level3" id="theorem-8.9.">
<h3 class="anchored" data-anchor-id="theorem-8.9.">Theorem 8.9.</h3>
<p>Chebychev. Let <span class="math inline">\(X_{n 1}, \ldots, X_{n n}\)</span> be pairwise uncorrelated random variables, i.e., <span class="math inline">\(\operatorname{cov}\left(X_{n i}, X_{n j}\right)=0\)</span> for all <span class="math inline">\(i \neq j\)</span> for all <span class="math inline">\(n\)</span>, with <span class="math inline">\(E X_{n i}=0\)</span> and <span class="math inline">\(\operatorname{var}\left(X_{n i}\right) \leq C&lt;\infty\)</span>. Then <span class="math display">\[
T_n=\frac{1}{n} \sum_{i=1}^n X_{n i} \stackrel{P}{\rightarrow} 0 .
\]</span></p>
<p><strong>Proof</strong>. We have <span class="math display">\[
E\left(T_n^2\right)=\operatorname{var}\left(\frac{1}{n} \sum_{i=1}^n X_{n i}\right)
\]</span></p>
<p><span class="math display">\[
\begin{gathered}
=\frac{1}{n^2} \sum_{i=1}^n \operatorname{var}\left(X_{n i}\right)+\frac{1}{n^2} \sum_{i=1}^n \sum_{\substack{j=1 \\
i \neq j}}^n \operatorname{cov}\left(X_{n i}, X_{n j}\right) \\
=\frac{1}{n^2} \sum_{i=1}^n \operatorname{var}\left(X_{n i}\right) \leq \frac{C}{n} \rightarrow 0 .
\end{gathered}
\]</span></p>
<p>This establishes mean square convergence, which implies convergence in probability.</p>
</section>
<section class="level3" id="theorem-8.10.">
<h3 class="anchored" data-anchor-id="theorem-8.10.">Theorem 8.10.</h3>
<p>(Lindeberg-Feller) Let <span class="math inline">\(X_{n 1}, \ldots, X_{n n}\)</span> be independent random variables with <span class="math inline">\(E\left(X_{n i}\right)=0\)</span> and <span class="math inline">\(\operatorname{var}\left(X_{n i}\right)=\sigma_{n i}^2\)</span>, and let <span class="math inline">\(s_n^2=\sum_{i=1}^n \sigma_{n i}^2\)</span>. Suppose also that <strong>Lindeberg’s condition</strong> <span class="math display">\[
\frac{1}{s_n^2} \sum_{i=1}^n E\left[X_{n i}^2 \mathbb{1}\left(X_{n i}^2&gt;\epsilon s_n^2\right)\right] \rightarrow 0 \text { for all } \epsilon&gt;0
\]</span> holds. Then <span class="math display">\[
\frac{1}{s_n} \sum_{i=1}^n X_{n i} \stackrel{D}{\rightarrow} N(0,1) .
\]</span></p>
<p>A sufficient condition is that <span class="math inline">\(\sum_{i=1}^n E\left[\left|X_{n i}\right|^3\right] / s_n^3 \rightarrow 0\)</span> (<strong>Lyapunov condition</strong>). In the case that <span class="math inline">\(s_n^2\)</span> is bounded away from zero and infinity, the Lindeberg condition implies that <span class="math inline">\(\max _{1 \leq i \leq n} \operatorname{Pr}\left(\left|X_{n, i}\right|&gt;\epsilon\right) \rightarrow 0\)</span> for all <span class="math inline">\(\epsilon&gt;0\)</span>; this is a version of <strong>asymptotic negligibility</strong>, i.e., it says that individual terms in the sum are small compared to the sum itself. If <span class="math inline">\(X_i\)</span> is i.i.d. then the Lindeberg condition is automatically satisfied.</p>
</section>
<section class="level3" id="example-8.9.">
<h3 class="anchored" data-anchor-id="example-8.9.">Example 8.9.</h3>
<p>Suppose that <span class="math inline">\(U_i \sim U[-1 / 2,1 / 2]\)</span> <span class="math display">\[
X_i=\sqrt{i} \times U_i .
\]</span> Then <span class="math inline">\(X_i\)</span> are independent with <span class="math inline">\(E X_i=0\)</span> but not identically distributed. In fact, <span class="math display">\[
s_n^2=\sum_{i=1}^n \sigma_i^2=\frac{1}{12} \sum_{i=1}^n i=\frac{n(n+1)}{24}
\]</span></p>
<p><span class="math display">\[
\begin{gathered}
\frac{1}{s_n^2} \sum_{i=1}^n E\left[X_{n i}^2 \mathbb{1}\left(X_i^2&gt;\epsilon s_n^2\right)\right]=\frac{24}{n(n+1)} \sum_{i=1}^n E\left[i U_i^2 \mathbb{1}\left(U_i^2&gt;\epsilon \frac{n(n+1)}{24 i}\right)\right] \\
=\frac{24}{n(n+1)} \sum_{i=1}^n i E\left[U_i^2 \mathbb{1}\left(U_i^2&gt;\epsilon \frac{n(n+1)}{24 i}\right)\right] \\
\leq \frac{24}{n(n+1)} \sum_{i=1}^n i E\left[U_i^2 \mathbb{1}\left(U_i^2&gt;\frac{\epsilon(n+1)}{24}\right)\right] \\
=0 \text { for all } n&gt;6 / \epsilon-1 .
\end{gathered}
\]</span></p>
<p>So Lindeberg’s condition is satisfied.</p>
<p>Not all sequences of random variables satisfy the CLT.</p>
</section>
<section class="level3" id="example-8.10.">
<h3 class="anchored" data-anchor-id="example-8.10.">Example 8.10.</h3>
<p>Suppose that <span class="math inline">\(\left\{X_i\right\}_{i=1}^{\infty}\)</span> are independent with <span class="math display">\[
X_i= \begin{cases}m^2 &amp; p_m \\ -m &amp; 1-p_m\end{cases}
\]</span> where <span class="math inline">\(p_m=m /\left(m^2+m\right)\)</span>. Then <span class="math inline">\(E X_i=0\)</span> for all <span class="math inline">\(i\)</span> and <span class="math inline">\(\operatorname{var}\left(X_i\right)=m^2 p_m+m^2\left(1-p_m\right)\)</span>. If the CLT were to hold, the sample average should be less than zero approximately half the time when the sample size is large. This is not the case. We have <span class="math display">\[
\operatorname{Pr}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i&lt;0\right) \geq\left(1-p_m\right)^n,
\]</span> where <span class="math inline">\(s_n^2=n \operatorname{var}\left(X_i\right)\)</span>. If we take <span class="math inline">\(m=c \times n\)</span>, then <span class="math inline">\(\left(1-p_m\right)^n \geq \exp \left(-c^{-1}\right)\)</span>, which can be arbitrarily close to one for some <span class="math inline">\(c\)</span>. So the CLT can’t hold in this case. Intuitively, the sequence is dominated by the <span class="math inline">\(-m\)</span> value.</p>
</section>
<section class="level3" id="gambling-model">
<h3 class="anchored" data-anchor-id="gambling-model">8.3.1 Gambling Model</h3>
<p>Suppose that you have initial wealth <span class="math inline">\(W_0\)</span> that you can divide into a risky asset with payoff <span class="math inline">\(R \geq 0\)</span> per dollar invested and a riskless asset that pays off <span class="math inline">\(R_f \geq 1\)</span> per dollar invested. You choose a fraction <span class="math inline">\(\omega \in[0,1]\)</span> to invest in the risky asset. The wealth next period is <span class="math display">\[
W_1=W_0\left(\omega R+(1-\omega) R_f\right) .
\]</span> The payoff at period <span class="math inline">\(n\)</span> if the same proportion is invested every period and if the risky asset payoffs are i.i.d. is <span class="math display">\[
W_n=W_0 \prod_{i=1}^n\left(\omega R_i+(1-\omega) R_f\right),
\]</span> where <span class="math inline">\(R_i\)</span> is the risky payoff in period <span class="math inline">\(i\)</span>. We take logs and assume “without loss of generality” that <span class="math inline">\(W_0=1\)</span>, then <span class="math display">\[
\log W_n=\sum_{i=1}^n \log \left(\omega R_i+(1-\omega) R_f\right) .
\]</span> Let <span class="math display">\[
X_i(\omega)=\log \left(\omega R_i+(1-\omega) R_f\right)
\]</span> and suppose that <span class="math inline">\(E\left|X_i(\omega)\right|&lt;C&lt;\infty\)</span> exists and let <span class="math inline">\(\mu(\omega)=E\left(X_i(\omega)\right)\)</span>. It follows that by the law of large numbers that <span class="math display">\[
\frac{1}{n} \log W_n \stackrel{P}{\rightarrow} \mu(\omega) .
\]</span> If also <span class="math inline">\(E\left|X_i^2(\omega)\right| \leq C&lt;\infty\)</span> exists and let <span class="math inline">\(\sigma^2(\omega)=\operatorname{var}\left(X_i(\omega)\right)\)</span>. It follows that by the CLT that <span class="math display">\[
\sqrt{n}\left(\frac{1}{n} \log W_n-\mu(\omega)\right) \stackrel{D}{\rightarrow} N\left(0, \sigma^2(\omega)\right) .
\]</span></p>
<p>There are three cases:</p>
<ol type="1">
<li>If <span class="math inline">\(\mu(\omega)&gt;0\)</span>, then <span class="math inline">\(W_n \rightarrow \infty\)</span> with probability one.</li>
<li>If <span class="math inline">\(\mu(\omega)&lt;0\)</span>, then <span class="math inline">\(W_n \rightarrow 0\)</span> with probability one.</li>
<li>If <span class="math inline">\(\mu(\omega)=0\)</span>, then actually provided <span class="math inline">\(E\left[X_i(\omega)^2\right]&lt;\infty\)</span> <span class="math display">\[
\frac{1}{\sqrt{n}} \log W_n \stackrel{D}{\rightarrow} N\left(0, \operatorname{var}\left[X_i(\omega)\right]\right) .
\]</span></li>
</ol>
<p>What is the condition for <span class="math inline">\(\omega \in(0,1)\)</span> ? We optimize <span class="math display">\[
E\left[\log \left(\omega R_i+(1-\omega) R_f\right)\right]
\]</span> with respect to <span class="math inline">\(\omega\)</span>. The first order condition and second order condition <span class="math display">\[
\begin{gathered}
-E\left[\frac{R_i-R_f}{\omega\left(R_i-R_f\right)+R_f}\right]=0 \\
-E\left[\frac{\left(R_i-R_f\right)^2}{\left(\omega R_i+(1-\omega) R_f\right)^2}\right]&lt;0 .
\end{gathered}
\]</span> If <span class="math inline">\(E\left(R_i\right)&gt;R_f\)</span>, then <span class="math inline">\(\omega=0\)</span> would coincide with a positive value to the first derivative of the objective function so that <span class="math inline">\(\omega=0\)</span> could not be a solution. If <span class="math inline">\(E\left(R_i^{-1}\right)&lt;R_f\)</span>, then <span class="math inline">\(\omega=1\)</span> would coincide with a negative value to the first derivative of the objective function so that <span class="math inline">\(\omega=1\)</span> could not be a solution. So provided these two conditions are satisfied the investor will put some money in both stocks and bonds.</p>
<p>For example, suppose that <span class="math inline">\(R_f=1\)</span> and <span class="math display">\[
R= \begin{cases}\theta &amp; \text { with probability } 1 / 2 \\ 0 &amp; \text { with probability } 1 / 2,\end{cases}
\]</span> for <span class="math inline">\(\theta \in(0,1)\)</span>. Then <span class="math inline">\(E(R)=\left(1+\theta^2\right) / 2 \theta\)</span> and <span class="math inline">\(\operatorname{var}(R)=\left(1-\theta^2\right)^2 / 4 \theta^2\)</span>. The first order condition is <span class="math display">\[
\frac{1-\theta}{\omega(\theta-1)+1}-\frac{\frac{1}{\theta}}{\omega\left(\frac{1}{\theta}-1\right)+1}=0,
\]</span> which has a unique solution <span class="math inline">\(\omega=1 / 2\)</span>. In this case <span class="math display">\[
\mu=\frac{1}{2} \ln (\theta+1)+\frac{1}{2} \ln \left(\frac{1}{\theta}+1\right)-\ln (2),
\]</span> which is decreasing in <span class="math inline">\(\theta\)</span>. In fact, <span class="math inline">\(\mu \geq 0\)</span>, and <span class="math inline">\(\mu \rightarrow 0\)</span> as <span class="math inline">\(\theta \rightarrow 1\)</span> because in that case one just has a riskless asset paying off 1 .</p>
</section>
</section>
<section class="level2" id="some-additional-tools">
<h2 class="anchored" data-anchor-id="some-additional-tools">8.4 SOME ADDITIONAL TOOLS</h2>
<p>The following results are very useful in conjunction with the LLN and CLT:</p>
<section class="level3" id="theorem-8.11.">
<h3 class="anchored" data-anchor-id="theorem-8.11.">Theorem 8.11.</h3>
<p><strong>Mann-Wald</strong> or <strong>Continuous Mapping Theorem</strong>. Suppose that <span class="math inline">\(X_n \stackrel{D}{\rightarrow} X\)</span> and that <span class="math inline">\(g\)</span> is continuous. Then <span class="math inline">\(g\left(X_n\right) \stackrel{D}{\rightarrow} g(X)\)</span>. If <span class="math inline">\(X_n \stackrel{P}{\rightarrow} \alpha\)</span>, then <span class="math inline">\(g(X) \stackrel{P}{\rightarrow} g(\alpha)\)</span>.</p>
<p><strong>Proof</strong>. We show the second result. Let <span class="math inline">\(\epsilon&gt;0\)</span> be given. By continuity of <span class="math inline">\(g\)</span> at <span class="math inline">\(\alpha, \exists \eta&gt;0\)</span> such that if <span class="math display">\[
|x-\alpha|&lt;\eta \Rightarrow|g(x)-g(\alpha)|&lt;\epsilon .
\]</span> Let <span class="math inline">\(A_n=\left\{\left|X_n-\alpha\right|&lt;\eta\right\}\)</span> and <span class="math inline">\(B_n=\left\{\left|g\left(X_n\right)-g(\alpha)\right|&lt;\epsilon\right\}\)</span>. But when <span class="math inline">\(A_n\)</span> is true, we know that <span class="math inline">\(B_n\)</span> is true, i.e., <span class="math inline">\(A_n \subset B_n\)</span>. Since <span class="math inline">\(\operatorname{Pr}\left[A_n\right] \rightarrow 1\)</span>, we must have <span class="math inline">\(\operatorname{Pr}\left[B_n\right] \rightarrow 1\)</span>.</p>
<p>In the Gamblers ruin problem. We can apply the Mann-Wald theorem to obtain the limiting behaviour of <span class="math inline">\(W_n\)</span>. Specifically, <span class="math display">\[
W_n^{1 / n}=\exp \left(\frac{1}{n} \log W_n\right)=\exp \left(\log W_n^{1 / n}\right) \stackrel{P}{\rightarrow} \exp (\mu(\omega)) .
\]</span></p>
<p>There are three cases:</p>
<ol type="1">
<li>If <span class="math inline">\(\mu(\omega)&gt;0\)</span>, then <span class="math inline">\(W_n \simeq \exp (n \mu(\omega)) \rightarrow \infty\)</span>.</li>
<li>If <span class="math inline">\(\mu(\omega)&lt;0\)</span>, then <span class="math inline">\(W_n \simeq \exp (n \mu(\omega)) \rightarrow 0\)</span>. You are ruined in probability.</li>
<li>If <span class="math inline">\(\mu(\omega)=0\)</span>, then see below</li>
</ol>
</section>
<section class="level3" id="theorem-8.12.">
<h3 class="anchored" data-anchor-id="theorem-8.12.">Theorem 8.12.</h3>
<p><strong>Slutsky</strong>. Suppose that <span class="math inline">\(X_n \stackrel{D}{\rightarrow} X, Y_n \stackrel{P}{\rightarrow} \alpha\)</span>. Then:</p>
<ol type="i">
<li><p><span class="math inline">\(X_n+Y_n \stackrel{D}{\rightarrow} X+\alpha\)</span>;</p></li>
<li><p><span class="math inline">\(X_n Y_n \stackrel{D}{\rightarrow} \alpha X\)</span>; and</p></li>
<li><p><span class="math inline">\(X_n / Y_n \stackrel{D}{\rightarrow} X / \alpha\)</span>, provided <span class="math inline">\(\alpha \neq 0\)</span>.</p></li>
</ol>
<p>This result is a natural generalization of the corresponding properties of real sequences.</p>
</section>
<section class="level3" id="example-8.11.">
<h3 class="anchored" data-anchor-id="example-8.11.">Example 8.11.</h3>
<p>Suppose that <span class="math inline">\(X_n=\sum_{i=1}^n A_i / \sqrt{n}\)</span> and <span class="math inline">\(Y_n=\sum_{i=1}^n B_i / n\)</span>, where <span class="math inline">\(A_i, B_i\)</span> are i.i.d. with <span class="math inline">\(E(A)=0, \operatorname{var}(A)=\sigma_A^2\)</span> and <span class="math inline">\(E\left(B_i\right)=\mu_B\)</span>. Then <span class="math display">\[
X_n+Y_n=\frac{1}{\sqrt{n}} \sum_{i=1}^n A_i+\frac{1}{n} \sum_{i=1}^n B_i \stackrel{D}{\rightarrow} X+\mu_B \sim N\left(\mu_B, \sigma_A^2\right)
\]</span> <span class="math display">\[
X_n Y_n=\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n A_i\right)\left(\frac{1}{n} \sum_{i=1}^n B_i\right) \stackrel{D}{\rightarrow} X \times \mu_B \sim N\left(0, \mu_B^2 \sigma_A^2\right)
\]</span></p>
<p><span class="math display">\[
\frac{X_n}{Y_n} \stackrel{D}{\rightarrow} X / \mu_B \sim N\left(0, \frac{\sigma_A^2}{\mu_B^2}\right)
\]</span></p>
<p>Finally, we point out that when dealing with a vector <span class="math inline">\(X_n=\left(X_{n 1}, \ldots, X_{n k}\right)\)</span>, we have the result that <span class="math display">\[
\sum_{j=1}^k\left(X_{n j}-X_j\right)^2 \stackrel{P}{\rightarrow} 0,
\]</span> if and only if for all <span class="math inline">\(j=1, \ldots, k\)</span>: <span class="math display">\[
\left|X_{n j}-X_j\right| \stackrel{P}{\rightarrow} 0 .
\]</span> The if part is no surprise and follows from the continuous mapping theorem. The only if part follows because if <span class="math inline">\(\sum_{j=1}^k\left(X_{n j}-X_j\right)^2&lt;\epsilon^2\)</span> then <span class="math inline">\(\left|X_{n j}-X_j\right|&lt;\epsilon\)</span> for each <span class="math inline">\(j\)</span>. In this case, we say <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span>. As regards convergence in distribution, we have the <strong>Cramér-Wald</strong> Theorem</p>
</section>
<section class="level3" id="theorem-8.13.">
<h3 class="anchored" data-anchor-id="theorem-8.13.">Theorem 8.13.</h3>
<p>A vector <span class="math inline">\(X_n\)</span> converges in distribution to a random vector <span class="math inline">\(X\)</span>, i.e., <span class="math display">\[
\lim _{n \rightarrow \infty} \operatorname{Pr}\left(X_n \leq x\right)=\operatorname{Pr}(X \leq x),
\]</span> for any <span class="math inline">\(x \in \mathbb{R}^k\)</span> a continuity point of the c.d.f. of <span class="math inline">\(X\)</span>, if and only if <span class="math inline">\(c^T X_n\)</span> converges in distribution to <span class="math inline">\(c^T X\)</span> for every vector <span class="math inline">\(c\)</span>.</p>
<p>It is not sufficient that each marginal distribution converges in distribution because this does not restrict the joint distribution.</p>
</section>
<section class="level3" id="example-8.12.">
<h3 class="anchored" data-anchor-id="example-8.12.">Example 8.12.</h3>
<p>Suppose that <span class="math inline">\(X_n \sim N(0,1)\)</span> and <span class="math inline">\(Y_n=-X_n\)</span> for all <span class="math inline">\(n\)</span>, then <span class="math inline">\(X_n \stackrel{D}{\rightarrow} X\)</span> and <span class="math inline">\(Y_n \stackrel{D}{\rightarrow} Y\)</span>, where <span class="math inline">\(X, Y \sim N(0,1)\)</span>, but <span class="math inline">\(\left(X_n, Y_n\right) \stackrel{D}{\rightarrow}(X,-X)\)</span>, which is a degenerate distribution. In this case <span class="math inline">\(c_1 X_n+c_2 Y_n=\left(c_1-c_2\right) X \sim N\left(0,\left(c_1-c_2\right)^2\right)\)</span> for all <span class="math inline">\(\left(c_1, c_2\right)\)</span>.</p>
</section>
<section class="level3" id="theorem-8.14.">
<h3 class="anchored" data-anchor-id="theorem-8.14.">Theorem 8.14.</h3>
<p>Suppose that <span class="math inline">\(\sqrt{n}\left(\hat{\theta}-\theta_0\right) \stackrel{D}{\rightarrow} X\)</span>, where <span class="math inline">\(X\)</span> is some random variable and suppose that <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(\theta_0 \in \mathbb{R}\)</span> with <span class="math inline">\(f^{\prime}\left(\theta_0\right) \neq 0\)</span>. Then <span class="math display">\[
\sqrt{n}\left(f(\hat{\theta})-f\left(\theta_0\right)\right) \stackrel{D}{\rightarrow}\left(f^{\prime}\left(\theta_0\right)\right) \times X .
\]</span></p>
<p><strong>Proof</strong>. The function <span class="math inline">\(f\)</span> is differentiable at <span class="math inline">\(\theta_0\)</span>, which means that for all <span class="math inline">\(\epsilon&gt;0\)</span> there exists <span class="math inline">\(\delta&gt;0\)</span> such that <span class="math display">\[
\left|\theta-\theta_0\right| \leq \delta \Rightarrow\left|f(\theta)-f\left(\theta_0\right)-f^{\prime}\left(\theta_0\right)\left(\theta-\theta_0\right)\right| \leq \epsilon\left|\theta-\theta_0\right| .
\]</span> Since <span class="math inline">\(\hat{\theta} \stackrel{P}{\rightarrow} \theta_0\)</span>, we have for any <span class="math inline">\(\delta&gt;0, \operatorname{Pr}\left(\left|\hat{\theta}-\theta_0\right| \leq \delta\right) \rightarrow 1\)</span>. Define the events <span class="math display">\[
A_n=\left\{\left|\hat{\theta}-\theta_0\right| \leq \delta\right\} \quad ; \quad B_n=\left\{\left|\sqrt{n} R_n\right| \leq x\right\},
\]</span> where <span class="math inline">\(R_n=f(\hat{\theta})-f\left(\theta_0\right)-f^{\prime}\left(\theta_0\right)\left(\hat{\theta}-\theta_0\right)\)</span>. It follows that <span class="math display">\[
\operatorname{Pr}\left(\left|\sqrt{n} R_n\right| \leq x\right)=\operatorname{Pr}\left(B_n\right)=\operatorname{Pr}\left(B_n \cap A_n\right)+\operatorname{Pr}\left(B_n \cap A_n^c\right) \leq \operatorname{Pr}\left(B_n \cap A_n\right)+\operatorname{Pr}\left(A_n^c\right)
\]</span> by the law of total probability and the fact that <span class="math inline">\(B_n \cap A_n^c \subset A_n^c\)</span>. We have <span class="math display">\[
\operatorname{Pr}\left(B_n \cap A_n\right) \leq \operatorname{Pr}\left(\epsilon \sqrt{n}\left|\hat{\theta}-\theta_0\right| \leq x\right) \rightarrow F_Y(x / \epsilon),
\]</span> where <span class="math inline">\(F_Y\)</span> is the c.d.f. of the random variable <span class="math inline">\(Y=|X|\)</span> so that <span class="math inline">\(F_Y(x / \epsilon)\)</span> is arbitrarily close to one as <span class="math inline">\(\epsilon \rightarrow 0\)</span> for any <span class="math inline">\(x&gt;0\)</span> (the c.d.f. of <span class="math inline">\(Y\)</span> is continuous on <span class="math inline">\(\left.\mathbb{R}_{+}\right)\)</span>. Then <span class="math inline">\(\operatorname{Pr}\left(A_n^c\right) \rightarrow 0\)</span> by the fact that <span class="math inline">\(\sqrt{n}\left(\hat{\theta}-\theta_0\right) \stackrel{D}{\rightarrow} X\)</span>. We have shown that <span class="math display">\[
\sqrt{n}\left(f(\hat{\theta})-f\left(\theta_0\right)\right)=f^{\prime}\left(\theta_0\right) \sqrt{n}\left(\hat{\theta}-\theta_0\right)+\sqrt{n} R_n,
\]</span> where <span class="math inline">\(\sqrt{n} R_n \stackrel{P}{\rightarrow} 0\)</span>. The result now follows.</p>
</section>
<section class="level3" id="corollary-8.1.">
<h3 class="anchored" data-anchor-id="corollary-8.1.">Corollary 8.1.</h3>
<p>Suppose that <span class="math inline">\(D_n\left(\hat{\theta}-\theta_0\right) \stackrel{D}{\rightarrow} X\)</span>, where <span class="math inline">\(X\)</span> is some random variable and <span class="math inline">\(D_n \rightarrow \infty\)</span>. Suppose that <span class="math inline">\(f\)</span> is twice differentiable at <span class="math inline">\(\theta_0 \in \mathbb{R}\)</span> with <span class="math inline">\(f^{\prime}\left(\theta_0\right)=0\)</span> and <span class="math inline">\(f^{\prime \prime}\left(\theta_0\right) \neq 0\)</span>. Then <span class="math display">\[
D_n^2\left(f(\hat{\theta})-f\left(\theta_0\right)\right) \stackrel{D}{\rightarrow} \frac{1}{2} f^{\prime \prime}\left(\theta_0\right) \times X^2 .
\]</span></p>
<p><strong>Proof</strong>. Follows by similar arguments to the theorem.</p>
<p>In the Gamblers ruin problem, let <span class="math display">\[
\hat{\theta}=\frac{1}{n} \log W_n \quad ; \quad \theta_0=\mu(\omega) .
\]</span> Then we have <span class="math display">\[
\sqrt{n}\left(\hat{\theta}-\theta_0\right) \stackrel{D}{\rightarrow} N\left(0, \sigma^2(\omega)\right) .
\]</span> By the Delta method we have <span class="math display">\[
\sqrt{n}\left(\exp (\hat{\theta})-\exp \left(\theta_0\right)\right) \stackrel{D}{\rightarrow} N\left(0, \exp \left(2 \theta_0\right) \sigma^2(\omega)\right),
\]</span> where <span class="math inline">\(\exp (\hat{\theta})=\exp \left(\log \left(W_n^{1 / n}\right)\right)=W_n^{1 / n}\)</span>. Suppose that <span class="math inline">\(\mu(\omega)=0\)</span>. Then this says that <span class="math display">\[
\begin{gathered}
\sqrt{n}\left(W_n^{1 / n}-1\right) \stackrel{D}{\rightarrow} N\left(0, \sigma^2(\omega)\right), \\
W_n \simeq\left(1+\frac{N\left(0, \sigma^2(\omega)\right)}{\sqrt{n}}\right)^n .
\end{gathered}
\]</span></p>
<p>Exact distribution theory is limited to very special cases [normal i.i.d. errors plus linear estimators], or involves very difficult calculations. This is too restrictive for applications. By making approximations based on large sample sizes, we can obtain distribution theory that is applicable in a much wider range of circumstances. It is important to note that infinity is an abstract mathematical concept, and that no known physical mechanism can ever entertain infinity. We use the theorems to provide approximations, that is all. Finally, we give an advanced inequality, called <strong>Bernstein’s inequality</strong>.</p>
</section>
<section class="level3" id="theorem-8.15.">
<h3 class="anchored" data-anchor-id="theorem-8.15.">Theorem 8.15.</h3>
<p>Suppose that <span class="math inline">\(X_i\)</span> are i.i.d. with mean zero, variance <span class="math inline">\(\sigma^2\)</span>, and satisfy <span class="math inline">\(\left|X_i\right| \leq M&lt;\infty\)</span>. Then <span class="math display">\[
\operatorname{Pr}\left(\frac{1}{\sqrt{n}}\left|\sum_{i=1}^n X_i\right|&gt;x\right) \leq 2 \exp \left(-\frac{\frac{1}{2} x^2}{4 \sigma^2+x M / \sqrt{n}}\right) .
\]</span></p>
<p>We do not provide a proof of this result. It can be useful in some contexts. Specifically, it gives an explicit bound on the c.d.f. of the sample average, which holds for all <span class="math inline">\(n\)</span>. Suppose that we take <span class="math inline">\(x=(c \log n)^{1 / 2}\)</span> in this formula, then the right hand side of (8.2) is <span class="math display">\[
2 \exp \left(-\frac{\frac{1}{2} c \log n}{4 \sigma^2+(c \log n)^{1 / 2} M / \sqrt{n}}\right) \sim 2 \exp \left(-\frac{c}{4 \sigma^2} \log n\right) \sim 2 n^{-c / 4 \sigma^2},
\]</span> which goes to zero rapidly for <span class="math inline">\(c&gt;4 \sigma^2\)</span>. Suppose that <span class="math inline">\(X_i \sim N(0,1)\)</span>, then <span class="math inline">\(\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i\)</span> is also standard normal and we have by symmetry that <span class="math display">\[
\operatorname{Pr}\left(\frac{1}{\sqrt{n}}\left|\sum_{i=1}^n X_i\right|&gt;x\right)=2 \operatorname{Pr}(Z&gt;x),
\]</span> where <span class="math inline">\(Z\)</span> is a standard normal random variable. The following bound can be shown by elementary arguments: for all <span class="math inline">\(x&gt;0\)</span> <span class="math display">\[
\frac{x \exp \left(-\frac{1}{2} x^2\right)}{\left(1+x^2\right) \sqrt{2 \pi}} \leq \operatorname{Pr}(Z \geq x) \leq \frac{\exp \left(-\frac{1}{2} x^2\right)}{x \sqrt{2 \pi}} .
\]</span> Suppose we take <span class="math inline">\(x=(c \log n)^{1 / 2}\)</span> in this formula, then we obtain that <span class="math display">\[
\frac{(c \log n)^{1 / 2} n^{-c / 2}}{(1+c \log n) \sqrt{2 \pi}} \leq \operatorname{Pr}\left(Z \geq(c \log n)^{1 / 2}\right) \leq \frac{n^{-c / 2}}{(c \log n)^{1 / 2} \sqrt{2 \pi}},
\]</span> which is a little tighter than given by (8.3).</p>
</section>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch08exercise1">
<h3 class="anchored" data-anchor-id="sec-ch08exercise1">Exercise 1</h3>
<p><a href="#sec-ch08solution1">Solution 1</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables with <span class="math inline">\(E(X) = 3\)</span>, <span class="math inline">\(E(Y) = 5\)</span>, <span class="math inline">\(\operatorname{Var}(X) = 4\)</span>, and <span class="math inline">\(\operatorname{Var}(Y) = 9\)</span>. Find an upper bound for <span class="math inline">\(\operatorname{Pr}(|X + Y - 8| \geq 5)\)</span> using <strong>Chebyshev’s inequality</strong>.</p>
</section>
<section class="level3" id="sec-ch08exercise2">
<h3 class="anchored" data-anchor-id="sec-ch08exercise2">Exercise 2</h3>
<p><a href="#sec-ch08solution2">Solution 2</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(X_n = 1\)</span> with probability <span class="math inline">\(\dfrac{1}{n^2}\)</span> and <span class="math inline">\(X_n = 0\)</span> with probability <span class="math inline">\(1 - \dfrac{1}{n^2}\)</span>. Does <span class="math inline">\(X_n\)</span> converge in probability to 0?</p>
</section>
<section class="level3" id="sec-ch08exercise3">
<h3 class="anchored" data-anchor-id="sec-ch08exercise3">Exercise 3</h3>
<p><a href="#sec-ch08solution3">Solution 3</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(X_n \sim \operatorname{Uniform}(0, \dfrac{1}{n})\)</span>. Does <span class="math inline">\(X_n\)</span> converge in mean square to 0?</p>
</section>
<section class="level3" id="sec-ch08exercise4">
<h3 class="anchored" data-anchor-id="sec-ch08exercise4">Exercise 4</h3>
<p><a href="#sec-ch08solution4">Solution 4</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Use the <strong>Central Limit Theorem</strong> to find an approximate expression for <span class="math inline">\(\operatorname{Pr}(\sum_{i=1}^n X_i &gt; c)\)</span> for some constant <span class="math inline">\(c\)</span>.</p>
</section>
<section class="level3" id="sec-ch08exercise5">
<h3 class="anchored" data-anchor-id="sec-ch08exercise5">Exercise 5</h3>
<p><a href="#sec-ch08solution5">Solution 5</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(\operatorname{Pr}(X_n = n) = \dfrac{1}{n}\)</span> and <span class="math inline">\(\operatorname{Pr}(X_n = 0) = 1 - \dfrac{1}{n}\)</span>. Does <span class="math inline">\(X_n\)</span> converge in distribution to a random variable <span class="math inline">\(X\)</span>? If so, what is the distribution of <span class="math inline">\(X\)</span>?</p>
</section>
<section class="level3" id="sec-ch08exercise6">
<h3 class="anchored" data-anchor-id="sec-ch08exercise6">Exercise 6</h3>
<p><a href="#sec-ch08solution6">Solution 6</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables such that <span class="math inline">\(E(X) = 2\)</span>, <span class="math inline">\(E(Y) = 4\)</span>, <span class="math inline">\(E(X^2) = 5\)</span>, and <span class="math inline">\(E(Y^2) = 20\)</span>. Find an upper bound for <span class="math inline">\(E(XY)\)</span> using the <strong>Cauchy-Schwarz inequality</strong>.</p>
</section>
<section class="level3" id="sec-ch08exercise7">
<h3 class="anchored" data-anchor-id="sec-ch08exercise7">Exercise 7</h3>
<p><a href="#sec-ch08solution7">Solution 7</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be independent and identically distributed random variables with <span class="math inline">\(\operatorname{Pr}(X_i = 1) = p\)</span> and <span class="math inline">\(\operatorname{Pr}(X_i = 0) = 1 - p\)</span>. Let <span class="math inline">\(\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^n X_i\)</span>. Use the <strong>Law of Large Numbers</strong> to determine the limit of <span class="math inline">\(\bar{X}_n\)</span> as <span class="math inline">\(n\)</span> approaches infinity.</p>
</section>
<section class="level3" id="sec-ch08exercise8">
<h3 class="anchored" data-anchor-id="sec-ch08exercise8">Exercise 8</h3>
<p><a href="#sec-ch08solution8">Solution 8</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(X_n \sim \operatorname{Binomial}(n, p)\)</span>. Use the <strong>Central Limit Theorem</strong> to find an approximate distribution for <span class="math inline">\(X_n\)</span> when <span class="math inline">\(n\)</span> is large.</p>
</section>
<section class="level3" id="sec-ch08exercise9">
<h3 class="anchored" data-anchor-id="sec-ch08exercise9">Exercise 9</h3>
<p><a href="#sec-ch08solution9">Solution 9</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables defined as <span class="math inline">\(X_n = \dfrac{Z}{\sqrt{n}}\)</span>, where <span class="math inline">\(Z \sim N(0, 1)\)</span>. Does <span class="math inline">\(X_n\)</span> converge in probability to 0?</p>
</section>
<section class="level3" id="sec-ch08exercise10">
<h3 class="anchored" data-anchor-id="sec-ch08exercise10">Exercise 10</h3>
<p><a href="#sec-ch08solution10">Solution 10</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a sequence of independent random variables with <span class="math inline">\(E(X_i) = \mu\)</span> and <span class="math inline">\(\operatorname{Var}(X_i) = \sigma^2_i\)</span>, where <span class="math inline">\(\sigma^2_i \leq C\)</span> for some constant <span class="math inline">\(C\)</span>. Let <span class="math inline">\(\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^n X_i\)</span>. Use <strong>Chebyshev’s inequality for triangular arrays</strong> to show that <span class="math inline">\(\bar{X}_n\)</span> converges in probability to <span class="math inline">\(\mu\)</span>.</p>
</section>
<section class="level3" id="sec-ch08exercise11">
<h3 class="anchored" data-anchor-id="sec-ch08exercise11">Exercise 11</h3>
<p><a href="#sec-ch08solution11">Solution 11</a></p>
<p>Let <span class="math inline">\(X\)</span> be a non-negative random variable with <span class="math inline">\(E(X) = 5\)</span>. Use <strong>Markov’s inequality</strong> to find an upper bound for <span class="math inline">\(\operatorname{Pr}(X \geq 10)\)</span>.</p>
</section>
<section class="level3" id="sec-ch08exercise12">
<h3 class="anchored" data-anchor-id="sec-ch08exercise12">Exercise 12</h3>
<p><a href="#sec-ch08solution12">Solution 12</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(X_n \sim N(0, \dfrac{1}{n})\)</span>. Does <span class="math inline">\(X_n\)</span> converge in distribution to a random variable <span class="math inline">\(X\)</span>? If so, what is the distribution of <span class="math inline">\(X\)</span>?</p>
</section>
<section class="level3" id="sec-ch08exercise13">
<h3 class="anchored" data-anchor-id="sec-ch08exercise13">Exercise 13</h3>
<p><a href="#sec-ch08solution13">Solution 13</a></p>
<p>Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> be sequences of random variables such that <span class="math inline">\(X_n \stackrel{P}{\rightarrow} a\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants. Use the <strong>Continuous Mapping Theorem</strong> to show that <span class="math inline">\(X_n + Y_n \stackrel{P}{\rightarrow} a + b\)</span>.</p>
</section>
<section class="level3" id="sec-ch08exercise14">
<h3 class="anchored" data-anchor-id="sec-ch08exercise14">Exercise 14</h3>
<p><a href="#sec-ch08solution14">Solution 14</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(\sqrt{n}(X_n - \mu) \stackrel{D}{\rightarrow} N(0, \sigma^2)\)</span>. Use the <strong>Delta method</strong> to find the limiting distribution of <span class="math inline">\(\sqrt{n}(X_n^2 - \mu^2)\)</span>.</p>
</section>
<section class="level3" id="sec-ch08exercise15">
<h3 class="anchored" data-anchor-id="sec-ch08exercise15">Exercise 15</h3>
<p><a href="#sec-ch08solution15">Solution 15</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(X_n = \dfrac{1}{n}\sum_{i=1}^n Z_i^2\)</span>, where <span class="math inline">\(Z_i\)</span> are independent and identically distributed with <span class="math inline">\(E(Z_i) = 0\)</span> and <span class="math inline">\(E(Z_i^2) = \sigma^2\)</span>. Use the <strong>Law of Large Numbers</strong> to determine the limit of <span class="math inline">\(X_n\)</span> as <span class="math inline">\(n\)</span> approaches infinity.</p>
</section>
<section class="level3" id="sec-ch08exercise16">
<h3 class="anchored" data-anchor-id="sec-ch08exercise16">Exercise 16</h3>
<p><a href="#sec-ch08solution16">Solution 16</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(X_n = \dfrac{1}{n}\sum_{i=1}^n Z_i\)</span>, where <span class="math inline">\(Z_i\)</span> are independent and identically distributed with <span class="math inline">\(E(Z_i) = 0\)</span> and <span class="math inline">\(\operatorname{Var}(Z_i) = 1\)</span>. Use the <strong>Central Limit Theorem</strong> to find the approximate distribution of <span class="math inline">\(\sqrt{n}X_n\)</span>.</p>
</section>
<section class="level3" id="sec-ch08exercise17">
<h3 class="anchored" data-anchor-id="sec-ch08exercise17">Exercise 17</h3>
<p><a href="#sec-ch08solution17">Solution 17</a></p>
<p>Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> be sequences of random variables such that <span class="math inline">\(X_n \stackrel{D}{\rightarrow} X\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} c\)</span>, where <span class="math inline">\(c\)</span> is a constant. Use <strong>Slutsky’s Theorem</strong> to determine the limiting distribution of <span class="math inline">\(X_n Y_n\)</span>.</p>
</section>
<section class="level3" id="sec-ch08exercise18">
<h3 class="anchored" data-anchor-id="sec-ch08exercise18">Exercise 18</h3>
<p><a href="#sec-ch08solution18">Solution 18</a></p>
<p>Let <span class="math inline">\(X_n\)</span> be a sequence of random variables such that <span class="math inline">\(X_n \sim \chi^2_n\)</span>. Use the <strong>Central Limit Theorem</strong> to find an approximate distribution for <span class="math inline">\(\dfrac{X_n - n}{\sqrt{2n}}\)</span>.</p>
</section>
<section class="level3" id="sec-ch08exercise19">
<h3 class="anchored" data-anchor-id="sec-ch08exercise19">Exercise 19</h3>
<p><a href="#sec-ch08solution19">Solution 19</a></p>
<p>Let <span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> be sequences of random variables such that <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} Y\)</span>. Does the sequence of random vectors <span class="math inline">\((X_n, Y_n)\)</span> converge in probability to <span class="math inline">\((X, Y)\)</span>?</p>
</section>
<section class="level3" id="sec-ch08exercise20">
<h3 class="anchored" data-anchor-id="sec-ch08exercise20">Exercise 20</h3>
<p><a href="#sec-ch08solution20">Solution 20</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be independent and identically distributed random variables with <span class="math inline">\(E(X_i) = \mu\)</span> and <span class="math inline">\(\operatorname{Var}(X_i) = \sigma^2\)</span>. Let <span class="math inline">\(S_n = \sum_{i=1}^n X_i\)</span>. Use <strong>Lindeberg’s condition</strong> to show that <span class="math inline">\(\dfrac{S_n - n\mu}{\sigma\sqrt{n}}\)</span> converges in distribution to a standard normal random variable.</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch08solution1">
<h3 class="anchored" data-anchor-id="sec-ch08solution1">Solution 1</h3>
<p><a href="#sec-ch08exercise1">Exercise 1</a></p>
<p>We are given that <span class="math inline">\(E(X) = 3\)</span>, <span class="math inline">\(E(Y) = 5\)</span>, <span class="math inline">\(\operatorname{Var}(X) = 4\)</span>, and <span class="math inline">\(\operatorname{Var}(Y) = 9\)</span>. We want to find an upper bound for <span class="math inline">\(\operatorname{Pr}(|X + Y - 8| \geq 5)\)</span> using <strong>Chebyshev’s inequality</strong>.</p>
<p>First, let <span class="math inline">\(Z = X + Y\)</span>. Then <span class="math inline">\(E(Z) = E(X + Y) = E(X) + E(Y) = 3 + 5 = 8\)</span>. Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, <span class="math inline">\(\operatorname{Var}(Z) = \operatorname{Var}(X + Y) = \operatorname{Var}(X) + \operatorname{Var}(Y) = 4 + 9 = 13\)</span>.</p>
<p><strong>Chebyshev’s inequality</strong> states that for any random variable <span class="math inline">\(Z\)</span> with finite mean <span class="math inline">\(E(Z)\)</span> and finite variance <span class="math inline">\(\operatorname{Var}(Z)\)</span>, and for any <span class="math inline">\(\eta &gt; 0\)</span>, <span class="math display">\[
\operatorname{Pr}(|Z - E(Z)| \geq \eta) \leq \frac{\operatorname{Var}(Z)}{\eta^2}.
\]</span></p>
<p>In our case, <span class="math inline">\(Z = X + Y\)</span>, <span class="math inline">\(E(Z) = 8\)</span>, <span class="math inline">\(\operatorname{Var}(Z) = 13\)</span>, and <span class="math inline">\(\eta = 5\)</span>. Applying <strong>Chebyshev’s inequality</strong>, we get <span class="math display">\[
\operatorname{Pr}(|X + Y - 8| \geq 5) \leq \frac{\operatorname{Var}(X + Y)}{5^2} = \frac{13}{25}.
\]</span> Thus, an upper bound for <span class="math inline">\(\operatorname{Pr}(|X + Y - 8| \geq 5)\)</span> is <span class="math inline">\(\dfrac{13}{25}\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We first find the mean and variance of the new random variable <span class="math inline">\(Z=X+Y\)</span>. The mean of the sum of two random variables is the sum of their means. The variance of the sum of two independent random variables is the sum of their variances. Then, we apply <strong>Chebyshev’s inequality</strong> which states that the probability of a random variable deviating from its mean by more than a certain value is bounded by its variance divided by the square of that value.</p>
</section>
<section class="level3" id="sec-ch08solution2">
<h3 class="anchored" data-anchor-id="sec-ch08solution2">Solution 2</h3>
<p><a href="#sec-ch08exercise2">Exercise 2</a></p>
<p>We are given that <span class="math inline">\(X_n = 1\)</span> with probability <span class="math inline">\(\dfrac{1}{n^2}\)</span> and <span class="math inline">\(X_n = 0\)</span> with probability <span class="math inline">\(1 - \dfrac{1}{n^2}\)</span>. We want to determine if <span class="math inline">\(X_n\)</span> converges in probability to 0.</p>
<p>A sequence of random variables <span class="math inline">\(X_n\)</span> converges in probability to a random variable <span class="math inline">\(X\)</span> if for every <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math display">\[
\lim_{n \to \infty} \operatorname{Pr}(|X_n - X| &gt; \epsilon) = 0.
\]</span></p>
<p>In this case, we want to check if <span class="math inline">\(X_n \stackrel{P}{\rightarrow} 0\)</span>. So we need to check if <span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(|X_n - 0| &gt; \epsilon) = 0\)</span> for every <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<p>For any <span class="math inline">\(\epsilon &gt; 0\)</span>, if <span class="math inline">\(0 &lt; \epsilon &lt; 1\)</span>, then <span class="math inline">\(|X_n - 0| &gt; \epsilon\)</span> if and only if <span class="math inline">\(X_n = 1\)</span>. Thus, <span class="math display">\[
\operatorname{Pr}(|X_n - 0| &gt; \epsilon) = \operatorname{Pr}(X_n = 1) = \frac{1}{n^2}.
\]</span></p>
<p>If <span class="math inline">\(\epsilon \geq 1\)</span>, then <span class="math inline">\(|X_n - 0| &gt; \epsilon\)</span> is impossible, so <span class="math inline">\(\operatorname{Pr}(|X_n - 0| &gt; \epsilon) = 0\)</span>.</p>
<p>In either case, we have <span class="math display">\[
\lim_{n \to \infty} \operatorname{Pr}(|X_n - 0| &gt; \epsilon) = \lim_{n \to \infty} \frac{1}{n^2} = 0.
\]</span></p>
<p>Therefore, <span class="math inline">\(X_n\)</span> converges in probability to 0.</p>
<p><strong>Intuitive explanation</strong>: We are checking if the sequence of random variables becomes increasingly concentrated around 0. For any small positive value <span class="math inline">\(\epsilon\)</span>, the probability that <span class="math inline">\(X_n\)</span> is further away from 0 than <span class="math inline">\(\epsilon\)</span> is the probability that <span class="math inline">\(X_n\)</span> equals 1, which is <span class="math inline">\(\dfrac{1}{n^2}\)</span>. As <span class="math inline">\(n\)</span> goes to infinity, this probability goes to 0, indicating that <span class="math inline">\(X_n\)</span> converges in probability to 0.</p>
</section>
<section class="level3" id="sec-ch08solution3">
<h3 class="anchored" data-anchor-id="sec-ch08solution3">Solution 3</h3>
<p><a href="#sec-ch08exercise3">Exercise 3</a></p>
<p>We are given that <span class="math inline">\(X_n \sim \operatorname{Uniform}(0, \dfrac{1}{n})\)</span>. We want to determine if <span class="math inline">\(X_n\)</span> converges in mean square to 0.</p>
<p>A sequence of random variables <span class="math inline">\(X_n\)</span> converges in mean square to a random variable <span class="math inline">\(X\)</span> if <span class="math display">\[
\lim_{n \to \infty} E[(X_n - X)^2] = 0.
\]</span></p>
<p>In this case, we want to check if <span class="math inline">\(\lim_{n \to \infty} E[(X_n - 0)^2] = 0\)</span>. For a uniform distribution <span class="math inline">\(U(a, b)\)</span>, the mean is <span class="math inline">\(\dfrac{a+b}{2}\)</span> and the variance is <span class="math inline">\(\dfrac{(b-a)^2}{12}\)</span>. Thus, for <span class="math inline">\(X_n \sim \operatorname{Uniform}(0, \dfrac{1}{n})\)</span>, we have <span class="math inline">\(E(X_n) = \dfrac{1}{2n}\)</span> and <span class="math inline">\(\operatorname{Var}(X_n) = \dfrac{1}{12n^2}\)</span>.</p>
<p>We have <span class="math inline">\(E(X_n^2) = \operatorname{Var}(X_n) + (E(X_n))^2 = \dfrac{1}{12n^2} + \left(\dfrac{1}{2n}\right)^2 = \dfrac{1}{12n^2} + \dfrac{1}{4n^2} = \dfrac{1}{3n^2}\)</span>. Then <span class="math inline">\(E[(X_n - 0)^2] = E(X_n^2) = \dfrac{1}{3n^2}\)</span>. Taking the limit as <span class="math inline">\(n\)</span> approaches infinity, we have <span class="math display">\[
\lim_{n \to \infty} E[(X_n - 0)^2] = \lim_{n \to \infty} \frac{1}{3n^2} = 0.
\]</span> Therefore, <span class="math inline">\(X_n\)</span> converges in mean square to 0.</p>
<p><strong>Intuitive explanation</strong>: We are checking if the expected squared difference between <span class="math inline">\(X_n\)</span> and 0 approaches 0 as <span class="math inline">\(n\)</span> goes to infinity. We first calculate the expected value of <span class="math inline">\(X_n^2\)</span> using the properties of the uniform distribution. Then we find that the limit of this expected squared difference is 0, indicating that <span class="math inline">\(X_n\)</span> converges in mean square to 0.</p>
</section>
<section class="level3" id="sec-ch08solution4">
<h3 class="anchored" data-anchor-id="sec-ch08solution4">Solution 4</h3>
<p><a href="#sec-ch08exercise4">Exercise 4</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. We want to use the <strong>Central Limit Theorem</strong> to find an approximate expression for <span class="math inline">\(\operatorname{Pr}(\sum_{i=1}^n X_i &gt; c)\)</span> for some constant <span class="math inline">\(c\)</span>.</p>
<p>The <strong>Central Limit Theorem</strong> states that if <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the distribution of the standardized sum <span class="math inline">\(\dfrac{\sum_{i=1}^n X_i - n\mu}{\sigma\sqrt{n}}\)</span> approaches a standard normal distribution as <span class="math inline">\(n\)</span> approaches infinity.</p>
<p>Let <span class="math inline">\(S_n = \sum_{i=1}^n X_i\)</span>. Then <span class="math inline">\(E(S_n) = n\mu\)</span> and <span class="math inline">\(\operatorname{Var}(S_n) = n\sigma^2\)</span>. We have <span class="math display">\[
\operatorname{Pr}\left(\sum_{i=1}^n X_i &gt; c\right) = \operatorname{Pr}(S_n &gt; c) = \operatorname{Pr}\left(\frac{S_n - n\mu}{\sigma\sqrt{n}} &gt; \frac{c - n\mu}{\sigma\sqrt{n}}\right).
\]</span> By the <strong>Central Limit Theorem</strong>, <span class="math inline">\(\dfrac{S_n - n\mu}{\sigma\sqrt{n}}\)</span> is approximately distributed as <span class="math inline">\(N(0, 1)\)</span> for large <span class="math inline">\(n\)</span>. Therefore, <span class="math display">\[
\operatorname{Pr}\left(\sum_{i=1}^n X_i &gt; c\right) \approx \operatorname{Pr}\left(Z &gt; \frac{c - n\mu}{\sigma\sqrt{n}}\right),
\]</span> where <span class="math inline">\(Z \sim N(0, 1)\)</span>. This can also be written as <span class="math display">\[
\operatorname{Pr}\left(\sum_{i=1}^n X_i &gt; c\right) \approx 1 - \Phi\left(\frac{c - n\mu}{\sigma\sqrt{n}}\right),
\]</span> where <span class="math inline">\(\Phi\)</span> is the cumulative distribution function of the standard normal distribution.</p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Central Limit Theorem</strong> to approximate the probability that the sum of the random variables exceeds a certain value. We standardize the sum by subtracting its mean and dividing by its standard deviation. Then we use the fact that the standardized sum is approximately normally distributed to express the probability in terms of the standard normal distribution.</p>
</section>
<section class="level3" id="sec-ch08solution5">
<h3 class="anchored" data-anchor-id="sec-ch08solution5">Solution 5</h3>
<p><a href="#sec-ch08exercise5">Exercise 5</a></p>
<p>We are given that <span class="math inline">\(\operatorname{Pr}(X_n = n) = \dfrac{1}{n}\)</span> and <span class="math inline">\(\operatorname{Pr}(X_n = 0) = 1 - \dfrac{1}{n}\)</span>. We want to determine if <span class="math inline">\(X_n\)</span> converges in distribution to a random variable <span class="math inline">\(X\)</span>, and if so, what the distribution of <span class="math inline">\(X\)</span> is.</p>
<p>Let <span class="math inline">\(F_n(x) = \operatorname{Pr}(X_n \leq x)\)</span> be the cumulative distribution function of <span class="math inline">\(X_n\)</span>. If <span class="math inline">\(x &lt; 0\)</span>, then <span class="math inline">\(F_n(x) = 0\)</span>. If <span class="math inline">\(0 \leq x &lt; n\)</span>, then <span class="math inline">\(F_n(x) = \operatorname{Pr}(X_n = 0) = 1 - \dfrac{1}{n}\)</span>. If <span class="math inline">\(x \geq n\)</span>, then <span class="math inline">\(F_n(x) = \operatorname{Pr}(X_n = 0) + \operatorname{Pr}(X_n = n) = 1 - \dfrac{1}{n} + \dfrac{1}{n} = 1\)</span>.</p>
<p>Now, let’s consider the limit of <span class="math inline">\(F_n(x)\)</span> as <span class="math inline">\(n \to \infty\)</span>. If <span class="math inline">\(x &lt; 0\)</span>, then <span class="math inline">\(\lim_{n \to \infty} F_n(x) = 0\)</span>. If <span class="math inline">\(x \geq 0\)</span>, then for any <span class="math inline">\(x\)</span>, there exists an <span class="math inline">\(N\)</span> such that <span class="math inline">\(n &gt; x\)</span> for all <span class="math inline">\(n &gt; N\)</span>. Thus, for <span class="math inline">\(n &gt; N\)</span>, <span class="math inline">\(F_n(x) = 1 - \dfrac{1}{n}\)</span>. So, <span class="math inline">\(\lim_{n \to \infty} F_n(x) = \lim_{n \to \infty} (1 - \dfrac{1}{n}) = 1\)</span>.</p>
<p>Let <span class="math inline">\(F(x)\)</span> be the limiting distribution function, defined as <span class="math display">\[
F(x) = \begin{cases}
0 &amp; \text{if } x &lt; 0 \\
1 &amp; \text{if } x \geq 0
\end{cases}
\]</span> This is the cumulative distribution function of a random variable <span class="math inline">\(X\)</span> that is always 0, i.e., <span class="math inline">\(\operatorname{Pr}(X = 0) = 1\)</span>. Therefore, <span class="math inline">\(X_n\)</span> converges in distribution to a random variable <span class="math inline">\(X\)</span> such that <span class="math inline">\(\operatorname{Pr}(X = 0) = 1\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are examining the cumulative distribution function of <span class="math inline">\(X_n\)</span> and its limit as <span class="math inline">\(n\)</span> goes to infinity. We find that the limiting distribution is a step function that jumps from 0 to 1 at <span class="math inline">\(x=0\)</span>, which corresponds to a degenerate random variable that is always 0.</p>
</section>
<section class="level3" id="sec-ch08solution6">
<h3 class="anchored" data-anchor-id="sec-ch08solution6">Solution 6</h3>
<p><a href="#sec-ch08exercise6">Exercise 6</a></p>
<p>We are given that <span class="math inline">\(E(X) = 2\)</span>, <span class="math inline">\(E(Y) = 4\)</span>, <span class="math inline">\(E(X^2) = 5\)</span>, and <span class="math inline">\(E(Y^2) = 20\)</span>. We want to find an upper bound for <span class="math inline">\(E(XY)\)</span> using the <strong>Cauchy-Schwarz inequality</strong>.</p>
<p>The <strong>Cauchy-Schwarz inequality</strong> states that for any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, <span class="math display">\[
(E(XY))^2 \leq E(X^2)E(Y^2).
\]</span> Taking the square root of both sides, we get <span class="math display">\[
|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}.
\]</span> In our case, we have <span class="math inline">\(E(X^2) = 5\)</span> and <span class="math inline">\(E(Y^2) = 20\)</span>. Plugging these values into the inequality, we get <span class="math display">\[
|E(XY)| \leq \sqrt{5 \cdot 20} = \sqrt{100} = 10.
\]</span> Therefore, we have <span class="math display">\[
-10 \leq E(XY) \leq 10.
\]</span> Thus, an upper bound for <span class="math inline">\(E(XY)\)</span> is 10.</p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Cauchy-Schwarz inequality</strong> to find a bound on the expected value of the product of two random variables. The inequality states that the squared expectation of the product is less than or equal to the product of the expectations of the squares. We take the square root to get a bound on the absolute value of the expectation of the product.</p>
</section>
<section class="level3" id="sec-ch08solution7">
<h3 class="anchored" data-anchor-id="sec-ch08solution7">Solution 7</h3>
<p><a href="#sec-ch08exercise7">Exercise 7</a></p>
<p>We are given that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent and identically distributed random variables with <span class="math inline">\(\operatorname{Pr}(X_i = 1) = p\)</span> and <span class="math inline">\(\operatorname{Pr}(X_i = 0) = 1 - p\)</span>. We are also given that <span class="math inline">\(\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^n X_i\)</span>. We want to use the <strong>Law of Large Numbers</strong> to determine the limit of <span class="math inline">\(\bar{X}_n\)</span> as <span class="math inline">\(n\)</span> approaches infinity.</p>
<p>The <strong>Law of Large Numbers</strong> states that if <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span>, then the sample mean <span class="math inline">\(\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^n X_i\)</span> converges in probability to the true mean <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n\)</span> approaches infinity, i.e., <span class="math inline">\(\bar{X}_n \stackrel{P}{\rightarrow} \mu\)</span>.</p>
<p>In this case, <span class="math inline">\(E(X_i) = 1 \cdot p + 0 \cdot (1 - p) = p\)</span>. Therefore, by the <strong>Law of Large Numbers</strong>, we have <span class="math display">\[
\bar{X}_n \stackrel{P}{\rightarrow} p.
\]</span> Thus, the limit of <span class="math inline">\(\bar{X}_n\)</span> as <span class="math inline">\(n\)</span> approaches infinity is <span class="math inline">\(p\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Law of Large Numbers</strong> to find the limit of the sample mean. The law states that the sample mean converges in probability to the true mean of the random variables. In this case, the true mean is <span class="math inline">\(p\)</span>, so the sample mean converges to <span class="math inline">\(p\)</span>.</p>
</section>
<section class="level3" id="sec-ch08solution8">
<h3 class="anchored" data-anchor-id="sec-ch08solution8">Solution 8</h3>
<p><a href="#sec-ch08exercise8">Exercise 8</a></p>
<p>We are given that <span class="math inline">\(X_n\)</span> is a sequence of random variables such that <span class="math inline">\(X_n \sim \operatorname{Binomial}(n, p)\)</span>. We want to use the <strong>Central Limit Theorem</strong> to find an approximate distribution for <span class="math inline">\(X_n\)</span> when <span class="math inline">\(n\)</span> is large.</p>
<p>Let <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span> be independent and identically distributed Bernoulli random variables with success probability <span class="math inline">\(p\)</span>, i.e., <span class="math inline">\(Y_i \sim \operatorname{Bernoulli}(p)\)</span>. Then <span class="math inline">\(X_n = \sum_{i=1}^n Y_i\)</span> follows a binomial distribution with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, i.e., <span class="math inline">\(X_n \sim \operatorname{Binomial}(n, p)\)</span>.</p>
<p>We have <span class="math inline">\(E(Y_i) = p\)</span> and <span class="math inline">\(\operatorname{Var}(Y_i) = p(1-p)\)</span>. By the <strong>Central Limit Theorem</strong>, we have that <span class="math display">\[
\frac{\sum_{i=1}^n Y_i - np}{\sqrt{n p(1-p)}} = \frac{X_n - np}{\sqrt{n p(1-p)}} \stackrel{D}{\rightarrow} Z \sim N(0, 1).
\]</span> Thus, for large <span class="math inline">\(n\)</span>, <span class="math inline">\(X_n\)</span> is approximately normally distributed with mean <span class="math inline">\(np\)</span> and variance <span class="math inline">\(np(1-p)\)</span>, i.e., <span class="math display">\[
X_n \approx N(np, np(1-p)).
\]</span></p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Central Limit Theorem</strong> to approximate the distribution of a binomial random variable. We express the binomial random variable as a sum of independent Bernoulli random variables. Then we apply the <strong>Central Limit Theorem</strong> to the sum, which tells us that the standardized sum converges to a standard normal distribution. This allows us to approximate the binomial distribution with a normal distribution.</p>
</section>
<section class="level3" id="sec-ch08solution9">
<h3 class="anchored" data-anchor-id="sec-ch08solution9">Solution 9</h3>
<p><a href="#sec-ch08exercise9">Exercise 9</a></p>
<p>We are given that <span class="math inline">\(X_n = \dfrac{Z}{\sqrt{n}}\)</span>, where <span class="math inline">\(Z \sim N(0, 1)\)</span>. We want to determine if <span class="math inline">\(X_n\)</span> converges in probability to 0.</p>
<p>A sequence of random variables <span class="math inline">\(X_n\)</span> converges in probability to a constant <span class="math inline">\(c\)</span> if for every <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math display">\[
\lim_{n \to \infty} \operatorname{Pr}(|X_n - c| &gt; \epsilon) = 0.
\]</span> In this case, we want to check if <span class="math inline">\(X_n \stackrel{P}{\rightarrow} 0\)</span>. So we need to check if <span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(|X_n - 0| &gt; \epsilon) = 0\)</span> for every <span class="math inline">\(\epsilon &gt; 0\)</span>.</p>
<p>We have <span class="math display">\[
\operatorname{Pr}(|X_n - 0| &gt; \epsilon) = \operatorname{Pr}\left(\left|\frac{Z}{\sqrt{n}}\right| &gt; \epsilon\right) = \operatorname{Pr}(|Z| &gt; \epsilon\sqrt{n}).
\]</span> Since <span class="math inline">\(Z \sim N(0, 1)\)</span>, we know that <span class="math inline">\(\operatorname{Pr}(|Z| &gt; x)\)</span> decreases as <span class="math inline">\(x\)</span> increases. As <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\epsilon\sqrt{n} \to \infty\)</span>. Therefore, <span class="math display">\[
\lim_{n \to \infty} \operatorname{Pr}(|X_n - 0| &gt; \epsilon) = \lim_{n \to \infty} \operatorname{Pr}(|Z| &gt; \epsilon\sqrt{n}) = 0.
\]</span> Thus, <span class="math inline">\(X_n\)</span> converges in probability to 0.</p>
<p><strong>Intuitive explanation</strong>: We are checking if the probability that <span class="math inline">\(X_n\)</span> deviates from 0 by more than <span class="math inline">\(\epsilon\)</span> goes to 0 as <span class="math inline">\(n\)</span> goes to infinity. Since <span class="math inline">\(X_n\)</span> is a standard normal random variable divided by <span class="math inline">\(\sqrt{n}\)</span>, as <span class="math inline">\(n\)</span> increases, <span class="math inline">\(X_n\)</span> becomes more concentrated around 0, and the probability of deviating from 0 by more than <span class="math inline">\(\epsilon\)</span> goes to 0.</p>
</section>
<section class="level3" id="sec-ch08solution10">
<h3 class="anchored" data-anchor-id="sec-ch08solution10">Solution 10</h3>
<p><a href="#sec-ch08exercise10">Exercise 10</a></p>
<p>We are given a sequence of independent random variables <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> with <span class="math inline">\(E(X_i) = \mu\)</span> and <span class="math inline">\(\operatorname{Var}(X_i) = \sigma^2_i\)</span>, where <span class="math inline">\(\sigma^2_i \leq C\)</span> for some constant <span class="math inline">\(C\)</span>. We are also given that <span class="math inline">\(\bar{X}_n = \dfrac{1}{n}\sum_{i=1}^n X_i\)</span>. We want to use <strong>Chebyshev’s inequality for triangular arrays</strong> to show that <span class="math inline">\(\bar{X}_n\)</span> converges in probability to <span class="math inline">\(\mu\)</span>.</p>
<p><strong>Chebyshev’s inequality for triangular arrays</strong> (Theorem 8.9) states that if <span class="math inline">\(X_{n1}, \dots, X_{nn}\)</span> are pairwise uncorrelated random variables with <span class="math inline">\(E(X_{ni}) = 0\)</span> and <span class="math inline">\(\operatorname{Var}(X_{ni}) \leq C &lt; \infty\)</span>, then <span class="math inline">\(T_n = \dfrac{1}{n}\sum_{i=1}^n X_{ni} \stackrel{P}{\rightarrow} 0\)</span>.</p>
<p>Let <span class="math inline">\(Y_i = X_i - \mu\)</span>. Then <span class="math inline">\(E(Y_i) = E(X_i) - \mu = 0\)</span> and <span class="math inline">\(\operatorname{Var}(Y_i) = \operatorname{Var}(X_i) = \sigma_i^2 \leq C\)</span>. We can consider the triangular array where each row consists of <span class="math inline">\(n\)</span> random variables <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span>. Then <span class="math inline">\(\bar{Y}_n = \dfrac{1}{n}\sum_{i=1}^n Y_i = \dfrac{1}{n}\sum_{i=1}^n (X_i - \mu) = \bar{X}_n - \mu\)</span>.</p>
<p>By <strong>Chebyshev’s inequality for triangular arrays</strong>, we have <span class="math inline">\(\bar{Y}_n \stackrel{P}{\rightarrow} 0\)</span>. This means that for any <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math display">\[
\lim_{n \to \infty} \operatorname{Pr}(|\bar{Y}_n - 0| &gt; \epsilon) = \lim_{n \to \infty} \operatorname{Pr}(|\bar{X}_n - \mu| &gt; \epsilon) = 0.
\]</span> Therefore, <span class="math inline">\(\bar{X}_n \stackrel{P}{\rightarrow} \mu\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using <strong>Chebyshev’s inequality for triangular arrays</strong> to show that the sample mean converges in probability to the true mean. We first center the random variables by subtracting the true mean, so that the centered variables have mean 0. Then we apply <strong>Chebyshev’s inequality</strong> to the sample mean of the centered variables, which shows that it converges in probability to 0. This implies that the sample mean of the original variables converges in probability to the true mean.</p>
</section>
<section class="level3" id="sec-ch08solution11">
<h3 class="anchored" data-anchor-id="sec-ch08solution11">Solution 11</h3>
<p><a href="#sec-ch08exercise11">Exercise 11</a></p>
<p>We are given a non-negative random variable <span class="math inline">\(X\)</span> with <span class="math inline">\(E(X) = 5\)</span>. We want to use <strong>Markov’s inequality</strong> to find an upper bound for <span class="math inline">\(\operatorname{Pr}(X \geq 10)\)</span>.</p>
<p><strong>Markov’s inequality</strong> states that for any non-negative random variable <span class="math inline">\(X\)</span> and any <span class="math inline">\(a &gt; 0\)</span>, <span class="math display">\[
\operatorname{Pr}(X \geq a) \leq \frac{E(X)}{a}.
\]</span></p>
<p>In this case, we have <span class="math inline">\(E(X) = 5\)</span> and we want to find an upper bound for <span class="math inline">\(\operatorname{Pr}(X \geq 10)\)</span>. Applying <strong>Markov’s inequality</strong> with <span class="math inline">\(a = 10\)</span>, we get <span class="math display">\[
\operatorname{Pr}(X \geq 10) \leq \frac{E(X)}{10} = \frac{5}{10} = \frac{1}{2}.
\]</span></p>
<p>Thus, an upper bound for <span class="math inline">\(\operatorname{Pr}(X \geq 10)\)</span> is <span class="math inline">\(\dfrac{1}{2}\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using <strong>Markov’s inequality</strong> to find a bound on the probability that a non-negative random variable is greater than or equal to a certain value. The inequality states that this probability is less than or equal to the expected value of the random variable divided by the given value.</p>
</section>
<section class="level3" id="sec-ch08solution12">
<h3 class="anchored" data-anchor-id="sec-ch08solution12">Solution 12</h3>
<p><a href="#sec-ch08exercise12">Exercise 12</a></p>
<p>We are given that <span class="math inline">\(X_n \sim N(0, \dfrac{1}{n})\)</span>. We want to determine if <span class="math inline">\(X_n\)</span> converges in distribution to a random variable <span class="math inline">\(X\)</span>, and if so, what the distribution of <span class="math inline">\(X\)</span> is.</p>
<p>Let <span class="math inline">\(F_n(x) = \operatorname{Pr}(X_n \leq x)\)</span> be the cumulative distribution function of <span class="math inline">\(X_n\)</span>. Since <span class="math inline">\(X_n \sim N(0, \dfrac{1}{n})\)</span>, we have <span class="math display">\[
F_n(x) = \Phi(\sqrt{n}x),
\]</span> where <span class="math inline">\(\Phi\)</span> is the cumulative distribution function of the standard normal distribution.</p>
<p>Now, let’s consider the limit of <span class="math inline">\(F_n(x)\)</span> as <span class="math inline">\(n \to \infty\)</span>. If <span class="math inline">\(x &lt; 0\)</span>, then <span class="math inline">\(\lim_{n \to \infty} \sqrt{n}x = -\infty\)</span>, so <span class="math inline">\(\lim_{n \to \infty} F_n(x) = \lim_{n \to \infty} \Phi(\sqrt{n}x) = 0\)</span>. If <span class="math inline">\(x &gt; 0\)</span>, then <span class="math inline">\(\lim_{n \to \infty} \sqrt{n}x = \infty\)</span>, so <span class="math inline">\(\lim_{n \to \infty} F_n(x) = \lim_{n \to \infty} \Phi(\sqrt{n}x) = 1\)</span>. If <span class="math inline">\(x = 0\)</span>, then <span class="math inline">\(\lim_{n \to \infty} F_n(x) = \lim_{n \to \infty} \Phi(0) = \dfrac{1}{2}\)</span>.</p>
<p>Let <span class="math inline">\(F(x)\)</span> be the limiting distribution function, defined as <span class="math display">\[
F(x) = \begin{cases}
0 &amp; \text{if } x &lt; 0 \\
1 &amp; \text{if } x &gt; 0
\end{cases}
\]</span> The limit is not defined at <span class="math inline">\(x=0\)</span>. However, since we are only concerned about convergence at continuity points, we can ignore <span class="math inline">\(x=0\)</span>. This is the cumulative distribution function of a random variable <span class="math inline">\(X\)</span> that is always 0, i.e., <span class="math inline">\(\operatorname{Pr}(X = 0) = 1\)</span>. Therefore, <span class="math inline">\(X_n\)</span> converges in distribution to a random variable <span class="math inline">\(X\)</span> such that <span class="math inline">\(\operatorname{Pr}(X = 0) = 1\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are examining the cumulative distribution function of <span class="math inline">\(X_n\)</span> and its limit as <span class="math inline">\(n\)</span> goes to infinity. We find that the limiting distribution is a step function that jumps from 0 to 1 at <span class="math inline">\(x=0\)</span>, which corresponds to a degenerate random variable that is always 0.</p>
</section>
<section class="level3" id="sec-ch08solution13">
<h3 class="anchored" data-anchor-id="sec-ch08solution13">Solution 13</h3>
<p><a href="#sec-ch08exercise13">Exercise 13</a></p>
<p>We are given that <span class="math inline">\(X_n \stackrel{P}{\rightarrow} a\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} b\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants. We want to use the <strong>Continuous Mapping Theorem</strong> to show that <span class="math inline">\(X_n + Y_n \stackrel{P}{\rightarrow} a + b\)</span>.</p>
<p>The <strong>Continuous Mapping Theorem</strong> states that if <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> and <span class="math inline">\(g\)</span> is a continuous function, then <span class="math inline">\(g(X_n) \stackrel{P}{\rightarrow} g(X)\)</span>. We can extend this to functions of multiple variables.</p>
<p>Let <span class="math inline">\(g(x, y) = x + y\)</span>. This is a continuous function. We are given that <span class="math inline">\(X_n \stackrel{P}{\rightarrow} a\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} b\)</span>. By <strong>Slutsky’s Theorem</strong>, we have that <span class="math inline">\((X_n, Y_n) \stackrel{P}{\rightarrow} (a, b)\)</span>. Applying the <strong>Continuous Mapping Theorem</strong> with the continuous function <span class="math inline">\(g(x, y) = x + y\)</span>, we have <span class="math display">\[
g(X_n, Y_n) = X_n + Y_n \stackrel{P}{\rightarrow} g(a, b) = a + b.
\]</span> Thus, <span class="math inline">\(X_n + Y_n \stackrel{P}{\rightarrow} a + b\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Continuous Mapping Theorem</strong> to show that if two sequences of random variables converge in probability to constants, then their sum converges in probability to the sum of the constants. The sum function is continuous, so we can apply the theorem.</p>
</section>
<section class="level3" id="sec-ch08solution14">
<h3 class="anchored" data-anchor-id="sec-ch08solution14">Solution 14</h3>
<p><a href="#sec-ch08exercise14">Exercise 14</a></p>
<p>We are given that <span class="math inline">\(\sqrt{n}(X_n - \mu) \stackrel{D}{\rightarrow} N(0, \sigma^2)\)</span>. We want to use the <strong>Delta method</strong> to find the limiting distribution of <span class="math inline">\(\sqrt{n}(X_n^2 - \mu^2)\)</span>.</p>
<p>The <strong>Delta method</strong> states that if <span class="math inline">\(\sqrt{n}(\theta_n - \theta) \stackrel{D}{\rightarrow} N(0, \sigma^2)\)</span> and <span class="math inline">\(f\)</span> is a differentiable function at <span class="math inline">\(\theta\)</span> with <span class="math inline">\(f'(\theta) \neq 0\)</span>, then <span class="math inline">\(\sqrt{n}(f(\theta_n) - f(\theta)) \stackrel{D}{\rightarrow} N(0, [f'(\theta)]^2\sigma^2)\)</span>.</p>
<p>In this case, we have <span class="math inline">\(\theta_n = X_n\)</span>, <span class="math inline">\(\theta = \mu\)</span>, and <span class="math inline">\(f(x) = x^2\)</span>. The derivative of <span class="math inline">\(f(x)\)</span> is <span class="math inline">\(f'(x) = 2x\)</span>. At <span class="math inline">\(x = \mu\)</span>, we have <span class="math inline">\(f'(\mu) = 2\mu\)</span>. If <span class="math inline">\(\mu \neq 0\)</span>, we can apply the <strong>Delta method</strong> to get <span class="math display">\[
\sqrt{n}(X_n^2 - \mu^2) \stackrel{D}{\rightarrow} N(0, (2\mu)^2\sigma^2) = N(0, 4\mu^2\sigma^2).
\]</span> If <span class="math inline">\(\mu = 0\)</span>, we have <span class="math inline">\(f'(\mu) = 0\)</span>, so we cannot directly apply the <strong>Delta method</strong>. However, we can use the second-order <strong>Delta method</strong>. If <span class="math inline">\(f'(\theta) = 0\)</span> and <span class="math inline">\(f''(\theta) \neq 0\)</span>, then <span class="math inline">\(n(f(\theta_n) - f(\theta)) \stackrel{D}{\rightarrow} \dfrac{1}{2}f''(\theta)X^2\)</span>, where <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>. In our case, <span class="math inline">\(f''(x) = 2\)</span>, so <span class="math inline">\(f''(\mu) = 2\)</span>. Thus, <span class="math display">\[
n(X_n^2 - 0^2) \stackrel{D}{\rightarrow} \frac{1}{2}(2)X^2 = X^2,
\]</span> where <span class="math inline">\(X \sim N(0, \sigma^2)\)</span>. Since <span class="math inline">\(X^2 = \sigma^2 Z^2\)</span> where <span class="math inline">\(Z \sim N(0, 1)\)</span>, we have <span class="math inline">\(X^2 \sim \sigma^2 \chi^2_1\)</span>. Therefore, <span class="math inline">\(n X_n^2 \stackrel{D}{\rightarrow} \sigma^2 \chi^2_1\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Delta method</strong> to find the limiting distribution of a function of a sequence of random variables. The <strong>Delta method</strong> allows us to approximate the distribution of a function of a random variable by using a linear approximation of the function.</p>
</section>
<section class="level3" id="sec-ch08solution15">
<h3 class="anchored" data-anchor-id="sec-ch08solution15">Solution 15</h3>
<p><a href="#sec-ch08exercise15">Exercise 15</a></p>
<p>We are given that <span class="math inline">\(X_n = \dfrac{1}{n}\sum_{i=1}^n Z_i^2\)</span>, where <span class="math inline">\(Z_i\)</span> are independent and identically distributed with <span class="math inline">\(E(Z_i) = 0\)</span> and <span class="math inline">\(E(Z_i^2) = \sigma^2\)</span>. We want to use the <strong>Law of Large Numbers</strong> to determine the limit of <span class="math inline">\(X_n\)</span> as <span class="math inline">\(n\)</span> approaches infinity.</p>
<p>The <strong>Law of Large Numbers</strong> states that if <span class="math inline">\(Y_1, Y_2, \dots, Y_n\)</span> are independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span>, then the sample mean <span class="math inline">\(\bar{Y}_n = \dfrac{1}{n}\sum_{i=1}^n Y_i\)</span> converges in probability to the true mean <span class="math inline">\(\mu\)</span> as <span class="math inline">\(n\)</span> approaches infinity, i.e., <span class="math inline">\(\bar{Y}_n \stackrel{P}{\rightarrow} \mu\)</span>.</p>
<p>In this case, let <span class="math inline">\(Y_i = Z_i^2\)</span>. Then <span class="math inline">\(E(Y_i) = E(Z_i^2) = \sigma^2\)</span>. We have <span class="math inline">\(X_n = \dfrac{1}{n}\sum_{i=1}^n Y_i = \bar{Y}_n\)</span>. By the <strong>Law of Large Numbers</strong>, we have <span class="math display">\[
X_n = \bar{Y}_n \stackrel{P}{\rightarrow} E(Y_i) = \sigma^2.
\]</span> Thus, the limit of <span class="math inline">\(X_n\)</span> as <span class="math inline">\(n\)</span> approaches infinity is <span class="math inline">\(\sigma^2\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Law of Large Numbers</strong> to find the limit of the sample mean of the squares of the random variables. The law states that the sample mean converges in probability to the true mean, which in this case is the expected value of the square of the random variables.</p>
</section>
<section class="level3" id="sec-ch08solution16">
<h3 class="anchored" data-anchor-id="sec-ch08solution16">Solution 16</h3>
<p><a href="#sec-ch08exercise16">Exercise 16</a></p>
<p>We are given that <span class="math inline">\(X_n = \dfrac{1}{n}\sum_{i=1}^n Z_i\)</span>, where <span class="math inline">\(Z_i\)</span> are independent and identically distributed with <span class="math inline">\(E(Z_i) = 0\)</span> and <span class="math inline">\(\operatorname{Var}(Z_i) = 1\)</span>. We want to use the <strong>Central Limit Theorem</strong> to find the approximate distribution of <span class="math inline">\(\sqrt{n}X_n\)</span>.</p>
<p>The <strong>Central Limit Theorem</strong> states that if <span class="math inline">\(Z_1, Z_2, \dots, Z_n\)</span> are independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then the distribution of the standardized sum <span class="math inline">\(\dfrac{\sum_{i=1}^n Z_i - n\mu}{\sigma\sqrt{n}}\)</span> approaches a standard normal distribution as <span class="math inline">\(n\)</span> approaches infinity.</p>
<p>In this case, we have <span class="math inline">\(E(Z_i) = 0\)</span> and <span class="math inline">\(\operatorname{Var}(Z_i) = 1\)</span>. We are interested in the distribution of <span class="math inline">\(\sqrt{n}X_n = \sqrt{n}\left(\dfrac{1}{n}\sum_{i=1}^n Z_i\right) = \dfrac{1}{\sqrt{n}}\sum_{i=1}^n Z_i\)</span>. By the <strong>Central Limit Theorem</strong>, we have <span class="math display">\[
\frac{\sum_{i=1}^n Z_i - n(0)}{1\cdot\sqrt{n}} = \frac{\sum_{i=1}^n Z_i}{\sqrt{n}} = \sqrt{n}X_n \stackrel{D}{\rightarrow} N(0, 1).
\]</span> Thus, the approximate distribution of <span class="math inline">\(\sqrt{n}X_n\)</span> is <span class="math inline">\(N(0, 1)\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Central Limit Theorem</strong> to find the approximate distribution of a scaled sample mean. The theorem states that the standardized sum of independent and identically distributed random variables converges to a standard normal distribution. In this case, the scaled sample mean is equivalent to the standardized sum, so it converges to a standard normal distribution.</p>
</section>
<section class="level3" id="sec-ch08solution17">
<h3 class="anchored" data-anchor-id="sec-ch08solution17">Solution 17</h3>
<p><a href="#sec-ch08exercise17">Exercise 17</a></p>
<p>We are given that <span class="math inline">\(X_n \stackrel{D}{\rightarrow} X\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} c\)</span>, where <span class="math inline">\(c\)</span> is a constant. We want to use <strong>Slutsky’s Theorem</strong> to determine the limiting distribution of <span class="math inline">\(X_n Y_n\)</span>.</p>
<p><strong>Slutsky’s Theorem</strong> states that if <span class="math inline">\(X_n \stackrel{D}{\rightarrow} X\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} c\)</span>, where <span class="math inline">\(c\)</span> is a constant, then <span class="math inline">\(X_n Y_n \stackrel{D}{\rightarrow} cX\)</span>.</p>
<p>In this case, we have <span class="math inline">\(X_n \stackrel{D}{\rightarrow} X\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} c\)</span>. By <strong>Slutsky’s Theorem</strong>, we have <span class="math display">\[
X_n Y_n \stackrel{D}{\rightarrow} cX.
\]</span> Thus, the limiting distribution of <span class="math inline">\(X_n Y_n\)</span> is the distribution of <span class="math inline">\(cX\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using <strong>Slutsky’s Theorem</strong> to find the limiting distribution of the product of two sequences of random variables, where one converges in distribution and the other converges in probability to a constant. The theorem states that the product converges in distribution to the product of the limiting random variable and the constant.</p>
</section>
<section class="level3" id="sec-ch08solution18">
<h3 class="anchored" data-anchor-id="sec-ch08solution18">Solution 18</h3>
<p><a href="#sec-ch08exercise18">Exercise 18</a></p>
<p>We are given that <span class="math inline">\(X_n \sim \chi^2_n\)</span>. We want to use the <strong>Central Limit Theorem</strong> to find an approximate distribution for <span class="math inline">\(\dfrac{X_n - n}{\sqrt{2n}}\)</span>.</p>
<p>A chi-squared distribution with <span class="math inline">\(n\)</span> degrees of freedom is the distribution of the sum of squares of <span class="math inline">\(n\)</span> independent standard normal random variables. Let <span class="math inline">\(Z_1, Z_2, \dots, Z_n\)</span> be independent standard normal random variables, i.e., <span class="math inline">\(Z_i \sim N(0, 1)\)</span>. Then <span class="math inline">\(X_n = \sum_{i=1}^n Z_i^2 \sim \chi^2_n\)</span>.</p>
<p>We have <span class="math inline">\(E(Z_i^2) = 1\)</span> and <span class="math inline">\(\operatorname{Var}(Z_i^2) = 2\)</span>. Thus, <span class="math inline">\(E(X_n) = \sum_{i=1}^n E(Z_i^2) = n\)</span> and <span class="math inline">\(\operatorname{Var}(X_n) = \sum_{i=1}^n \operatorname{Var}(Z_i^2) = 2n\)</span>.</p>
<p>By the <strong>Central Limit Theorem</strong>, we have <span class="math display">\[
\frac{\sum_{i=1}^n Z_i^2 - n}{\sqrt{2n}} = \frac{X_n - n}{\sqrt{2n}} \stackrel{D}{\rightarrow} N(0, 1).
\]</span> Thus, the approximate distribution of <span class="math inline">\(\dfrac{X_n - n}{\sqrt{2n}}\)</span> is <span class="math inline">\(N(0, 1)\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are using the <strong>Central Limit Theorem</strong> to approximate the distribution of a standardized chi-squared random variable. We express the chi-squared random variable as a sum of squares of independent standard normal random variables. Then we apply the <strong>Central Limit Theorem</strong> to the sum, which tells us that the standardized sum converges to a standard normal distribution.</p>
</section>
<section class="level3" id="sec-ch08solution19">
<h3 class="anchored" data-anchor-id="sec-ch08solution19">Solution 19</h3>
<p><a href="#sec-ch08exercise19">Exercise 19</a></p>
<p>We are given that <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} Y\)</span>. We want to determine if the sequence of random vectors <span class="math inline">\((X_n, Y_n)\)</span> converges in probability to <span class="math inline">\((X, Y)\)</span>.</p>
<p>We know that <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> if and only if for every <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math inline">\(\lim_{n\to\infty} \operatorname{Pr}(|X_n - X| &gt; \epsilon) = 0\)</span>. Similarly, <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} Y\)</span> if and only if for every <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math inline">\(\lim_{n\to\infty} \operatorname{Pr}(|Y_n - Y| &gt; \epsilon) = 0\)</span>.</p>
<p>The sequence of random vectors <span class="math inline">\((X_n, Y_n)\)</span> converges in probability to <span class="math inline">\((X, Y)\)</span> if for every <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math inline">\(\lim_{n\to\infty} \operatorname{Pr}(\|(X_n, Y_n) - (X, Y)\| &gt; \epsilon) = 0\)</span>, where <span class="math inline">\(\|\cdot\|\)</span> denotes the Euclidean norm.</p>
<p>We have <span class="math inline">\(\|(X_n, Y_n) - (X, Y)\| = \sqrt{(X_n - X)^2 + (Y_n - Y)^2}\)</span>. If <span class="math inline">\(|X_n - X| &lt; \dfrac{\epsilon}{\sqrt{2}}\)</span> and <span class="math inline">\(|Y_n - Y| &lt; \dfrac{\epsilon}{\sqrt{2}}\)</span>, then <span class="math inline">\(\|(X_n, Y_n) - (X, Y)\| &lt; \sqrt{\dfrac{\epsilon^2}{2} + \dfrac{\epsilon^2}{2}} = \epsilon\)</span>. Therefore, <span class="math display">\[
\operatorname{Pr}(\|(X_n, Y_n) - (X, Y)\| &gt; \epsilon) \leq \operatorname{Pr}\left(|X_n - X| &gt; \frac{\epsilon}{\sqrt{2}}\right) + \operatorname{Pr}\left(|Y_n - Y| &gt; \frac{\epsilon}{\sqrt{2}}\right).
\]</span> Since <span class="math inline">\(X_n \stackrel{P}{\rightarrow} X\)</span> and <span class="math inline">\(Y_n \stackrel{P}{\rightarrow} Y\)</span>, both terms on the right-hand side go to 0 as <span class="math inline">\(n \to \infty\)</span>. Thus, <span class="math inline">\(\lim_{n\to\infty} \operatorname{Pr}(\|(X_n, Y_n) - (X, Y)\| &gt; \epsilon) = 0\)</span>, which means <span class="math inline">\((X_n, Y_n) \stackrel{P}{\rightarrow} (X, Y)\)</span>.</p>
<p><strong>Intuitive explanation</strong>: We are showing that if two sequences of random variables converge in probability to their respective limits, then the sequence of random vectors formed by pairing the terms of the two sequences converges in probability to the vector formed by the limits. We use the definition of convergence in probability and the relationship between the Euclidean norm and the individual components of the vectors.</p>
</section>
<section class="level3" id="sec-ch08solution20">
<h3 class="anchored" data-anchor-id="sec-ch08solution20">Solution 20</h3>
<p><a href="#sec-ch08exercise20">Exercise 20</a></p>
<p>We are given that <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent and identically distributed random variables with <span class="math inline">\(E(X_i) = \mu\)</span> and <span class="math inline">\(\operatorname{Var}(X_i) = \sigma^2\)</span>. We are also given that <span class="math inline">\(S_n = \sum_{i=1}^n X_i\)</span>. We want to use <strong>Lindeberg’s condition</strong> to show that <span class="math inline">\(\dfrac{S_n - n\mu}{\sigma\sqrt{n}}\)</span> converges in distribution to a standard normal random variable.</p>
<p>Let <span class="math inline">\(Y_i = X_i - \mu\)</span>. Then <span class="math inline">\(E(Y_i) = 0\)</span> and <span class="math inline">\(\operatorname{Var}(Y_i) = \sigma^2\)</span>. We have <span class="math inline">\(\dfrac{S_n - n\mu}{\sigma\sqrt{n}} = \dfrac{\sum_{i=1}^n (X_i - \mu)}{\sigma\sqrt{n}} = \dfrac{\sum_{i=1}^n Y_i}{\sigma\sqrt{n}}\)</span>. Let <span class="math inline">\(Y_{ni} = \dfrac{Y_i}{\sigma\sqrt{n}}\)</span>. Then <span class="math inline">\(E(Y_{ni}) = 0\)</span> and <span class="math inline">\(\operatorname{Var}(Y_{ni}) = \dfrac{\sigma^2}{n\sigma^2} = \dfrac{1}{n}\)</span>. Let <span class="math inline">\(s_n^2 = \sum_{i=1}^n \operatorname{Var}(Y_{ni}) = \sum_{i=1}^n \dfrac{1}{n} = 1\)</span>.</p>
<p><strong>Lindeberg’s condition</strong> states that if for every <span class="math inline">\(\epsilon &gt; 0\)</span>, <span class="math display">\[
\lim_{n \to \infty} \frac{1}{s_n^2} \sum_{i=1}^n E[Y_{ni}^2 \mathbb{1}(|Y_{ni}| &gt; \epsilon s_n)] = 0,
\]</span> then <span class="math inline">\(\sum_{i=1}^n Y_{ni} \stackrel{D}{\rightarrow} N(0, 1)\)</span>.</p>
<p>In our case, we need to check if <span class="math display">\[
\lim_{n \to \infty} \sum_{i=1}^n E\left[\frac{Y_i^2}{n\sigma^2} \mathbb{1}\left(\frac{|Y_i|}{\sigma\sqrt{n}} &gt; \epsilon\right)\right] = 0.
\]</span> This simplifies to <span class="math display">\[
\lim_{n \to \infty} \frac{1}{n\sigma^2} \sum_{i=1}^n E[Y_i^2 \mathbb{1}(|Y_i| &gt; \epsilon\sigma\sqrt{n})] = 0.
\]</span> Since <span class="math inline">\(Y_i\)</span> are identically distributed, we have <span class="math display">\[
\lim_{n \to \infty} \frac{1}{\sigma^2} E[Y_1^2 \mathbb{1}(|Y_1| &gt; \epsilon\sigma\sqrt{n})] = 0.
\]</span> As <span class="math inline">\(n \to \infty\)</span>, <span class="math inline">\(\epsilon\sigma\sqrt{n} \to \infty\)</span>. Since <span class="math inline">\(E(Y_1^2) = \sigma^2 &lt; \infty\)</span>, the dominated convergence theorem implies that <span class="math inline">\(E[Y_1^2 \mathbb{1}(|Y_1| &gt; \epsilon\sigma\sqrt{n})] \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>. Thus, <strong>Lindeberg’s condition</strong> is satisfied, and we have <span class="math display">\[
\frac{S_n - n\mu}{\sigma\sqrt{n}} = \sum_{i=1}^n Y_{ni} \stackrel{D}{\rightarrow} N(0, 1).
\]</span></p>
<p><strong>Intuitive explanation</strong>: We are using <strong>Lindeberg’s condition</strong> to show that the standardized sum of independent and identically distributed random variables converges in distribution to a standard normal random variable. We first center the random variables by subtracting their mean and then scale them appropriately. Then we check if <strong>Lindeberg’s condition</strong> is satisfied, which essentially requires that the contribution of each individual term to the sum becomes negligible as <span class="math inline">\(n\)</span> increases. Since the condition is satisfied, we can conclude that the standardized sum converges to a standard normal distribution.</p>
</section>
</section>
<section class="level2" id="r-scripts">
<h2 class="anchored" data-anchor-id="r-scripts">R Scripts</h2>
<section class="level3" id="r-script-1-illustrating-the-law-of-large-numbers">
<h3 class="anchored" data-anchor-id="r-script-1-illustrating-the-law-of-large-numbers">R Script 1: Illustrating the Law of Large Numbers</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="co"># Set the seed for reproducibility</span></span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb3-3"><a aria-hidden="true" href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a aria-hidden="true" href="#cb3-4" tabindex="-1"></a><span class="co"># Define the number of simulations</span></span>
<span id="cb3-5"><a aria-hidden="true" href="#cb3-5" tabindex="-1"></a>n_simulations <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb3-6"><a aria-hidden="true" href="#cb3-6" tabindex="-1"></a></span>
<span id="cb3-7"><a aria-hidden="true" href="#cb3-7" tabindex="-1"></a><span class="co"># Define the sample sizes</span></span>
<span id="cb3-8"><a aria-hidden="true" href="#cb3-8" tabindex="-1"></a>sample_sizes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">5000</span>)</span>
<span id="cb3-9"><a aria-hidden="true" href="#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a aria-hidden="true" href="#cb3-10" tabindex="-1"></a><span class="co"># Create an empty list to store the results</span></span>
<span id="cb3-11"><a aria-hidden="true" href="#cb3-11" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb3-12"><a aria-hidden="true" href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a aria-hidden="true" href="#cb3-13" tabindex="-1"></a><span class="co"># Loop through the sample sizes</span></span>
<span id="cb3-14"><a aria-hidden="true" href="#cb3-14" tabindex="-1"></a><span class="cf">for</span> (size <span class="cf">in</span> sample_sizes) {</span>
<span id="cb3-15"><a aria-hidden="true" href="#cb3-15" tabindex="-1"></a>  <span class="co"># Simulate n_simulations samples of Bernoulli random variables</span></span>
<span id="cb3-16"><a aria-hidden="true" href="#cb3-16" tabindex="-1"></a>  samples <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n_simulations <span class="sc">*</span> size, <span class="dv">1</span>, <span class="fl">0.6</span>)</span>
<span id="cb3-17"><a aria-hidden="true" href="#cb3-17" tabindex="-1"></a></span>
<span id="cb3-18"><a aria-hidden="true" href="#cb3-18" tabindex="-1"></a>  <span class="co"># Reshape the samples into a matrix</span></span>
<span id="cb3-19"><a aria-hidden="true" href="#cb3-19" tabindex="-1"></a>  samples_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(samples, <span class="at">nrow =</span> n_simulations, <span class="at">ncol =</span> size)</span>
<span id="cb3-20"><a aria-hidden="true" href="#cb3-20" tabindex="-1"></a></span>
<span id="cb3-21"><a aria-hidden="true" href="#cb3-21" tabindex="-1"></a>  <span class="co"># Calculate the sample means</span></span>
<span id="cb3-22"><a aria-hidden="true" href="#cb3-22" tabindex="-1"></a>  sample_means <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(samples_matrix)</span>
<span id="cb3-23"><a aria-hidden="true" href="#cb3-23" tabindex="-1"></a></span>
<span id="cb3-24"><a aria-hidden="true" href="#cb3-24" tabindex="-1"></a>  <span class="co"># Store the results in the list</span></span>
<span id="cb3-25"><a aria-hidden="true" href="#cb3-25" tabindex="-1"></a>  results[[<span class="fu">as.character</span>(size)]] <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb3-26"><a aria-hidden="true" href="#cb3-26" tabindex="-1"></a>    <span class="at">sample_size =</span> size,</span>
<span id="cb3-27"><a aria-hidden="true" href="#cb3-27" tabindex="-1"></a>    <span class="at">sample_mean =</span> sample_means</span>
<span id="cb3-28"><a aria-hidden="true" href="#cb3-28" tabindex="-1"></a>  )</span>
<span id="cb3-29"><a aria-hidden="true" href="#cb3-29" tabindex="-1"></a>}</span>
<span id="cb3-30"><a aria-hidden="true" href="#cb3-30" tabindex="-1"></a></span>
<span id="cb3-31"><a aria-hidden="true" href="#cb3-31" tabindex="-1"></a><span class="co"># Combine the results into a single data frame</span></span>
<span id="cb3-32"><a aria-hidden="true" href="#cb3-32" tabindex="-1"></a>results_df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(results)</span>
<span id="cb3-33"><a aria-hidden="true" href="#cb3-33" tabindex="-1"></a></span>
<span id="cb3-34"><a aria-hidden="true" href="#cb3-34" tabindex="-1"></a><span class="co"># Create a plot of the sample means</span></span>
<span id="cb3-35"><a aria-hidden="true" href="#cb3-35" tabindex="-1"></a><span class="fu">ggplot</span>(results_df, <span class="fu">aes</span>(<span class="at">x =</span> sample_mean, <span class="at">fill =</span> <span class="fu">factor</span>(sample_size))) <span class="sc">+</span></span>
<span id="cb3-36"><a aria-hidden="true" href="#cb3-36" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position =</span> <span class="st">"identity"</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">bins =</span> <span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb3-37"><a aria-hidden="true" href="#cb3-37" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> <span class="fl">0.6</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb3-38"><a aria-hidden="true" href="#cb3-38" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb3-39"><a aria-hidden="true" href="#cb3-39" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Law of Large Numbers"</span>,</span>
<span id="cb3-40"><a aria-hidden="true" href="#cb3-40" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">"Sample means converge to the true mean as sample size increases"</span>,</span>
<span id="cb3-41"><a aria-hidden="true" href="#cb3-41" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Sample Mean"</span>,</span>
<span id="cb3-42"><a aria-hidden="true" href="#cb3-42" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Frequency"</span>,</span>
<span id="cb3-43"><a aria-hidden="true" href="#cb3-43" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">"Sample Size"</span></span>
<span id="cb3-44"><a aria-hidden="true" href="#cb3-44" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb3-45"><a aria-hidden="true" href="#cb3-45" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap08_files/figure-html/unnamed-chunk-1-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load necessary libraries:</strong> The code starts by loading the <code>tidyverse</code> library, which provides a collection of R packages designed for data science, including <code>ggplot2</code> for visualization and <code>dplyr</code> for data manipulation.</li>
<li><strong>Set the seed for reproducibility:</strong> <code>set.seed(123)</code> ensures that the results of the simulation are reproducible.</li>
<li><strong>Define the number of simulations and sample sizes:</strong> <code>n_simulations</code> specifies the number of times we will simulate a sample, and <code>sample_sizes</code> defines the different sample sizes we will use.</li>
<li><strong>Loop through the sample sizes:</strong> The code then loops through each sample size in <code>sample_sizes</code>.</li>
<li><strong>Simulate samples:</strong> Inside the loop, <code>rbinom(n_simulations * size, 1, 0.6)</code> simulates <code>n_simulations</code> samples of Bernoulli random variables with a success probability of 0.6. Each sample has a size of <code>size</code>.</li>
<li><strong>Reshape the samples into a matrix:</strong> <code>matrix(samples, nrow = n_simulations, ncol = size)</code> reshapes the simulated samples into a matrix where each row represents a simulation and each column represents an observation within a sample.</li>
<li><strong>Calculate the sample means:</strong> <code>rowMeans(samples_matrix)</code> calculates the mean of each row (i.e., the sample mean for each simulation).</li>
<li><strong>Store the results:</strong> The results (sample size and sample means) are stored in a list called <code>results</code>.</li>
<li><strong>Combine the results:</strong> <code>bind_rows(results)</code> combines the results from each sample size into a single data frame.</li>
<li><strong>Create a plot:</strong> <code>ggplot2</code> is used to create a histogram of the sample means for each sample size. The <code>geom_vline()</code> function adds a vertical dashed line at the true mean (0.6).</li>
</ol>
<p><strong>Relationship to the text:</strong></p>
<p>This script illustrates the <strong>Law of Large Numbers</strong>, which states that as the sample size increases, the sample mean converges in probability to the true mean of the population. In this script, we simulate samples of Bernoulli random variables and calculate their sample means. The plot shows that as the sample size increases, the distribution of the sample means becomes more concentrated around the true mean of 0.6, demonstrating the <strong>Law of Large Numbers</strong>.</p>
</section>
<section class="level3" id="r-script-2-illustrating-the-central-limit-theorem">
<h3 class="anchored" data-anchor-id="r-script-2-illustrating-the-central-limit-theorem">R Script 2: Illustrating the Central Limit Theorem</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a aria-hidden="true" href="#cb4-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb4-2"><a aria-hidden="true" href="#cb4-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb4-3"><a aria-hidden="true" href="#cb4-3" tabindex="-1"></a></span>
<span id="cb4-4"><a aria-hidden="true" href="#cb4-4" tabindex="-1"></a><span class="co"># Set the seed for reproducibility</span></span>
<span id="cb4-5"><a aria-hidden="true" href="#cb4-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb4-6"><a aria-hidden="true" href="#cb4-6" tabindex="-1"></a></span>
<span id="cb4-7"><a aria-hidden="true" href="#cb4-7" tabindex="-1"></a><span class="co"># Define the number of simulations</span></span>
<span id="cb4-8"><a aria-hidden="true" href="#cb4-8" tabindex="-1"></a>n_simulations <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb4-9"><a aria-hidden="true" href="#cb4-9" tabindex="-1"></a></span>
<span id="cb4-10"><a aria-hidden="true" href="#cb4-10" tabindex="-1"></a><span class="co"># Define the sample sizes</span></span>
<span id="cb4-11"><a aria-hidden="true" href="#cb4-11" tabindex="-1"></a>sample_sizes <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">30</span>, <span class="dv">100</span>, <span class="dv">500</span>)</span>
<span id="cb4-12"><a aria-hidden="true" href="#cb4-12" tabindex="-1"></a></span>
<span id="cb4-13"><a aria-hidden="true" href="#cb4-13" tabindex="-1"></a><span class="co"># Create an empty list to store the results</span></span>
<span id="cb4-14"><a aria-hidden="true" href="#cb4-14" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb4-15"><a aria-hidden="true" href="#cb4-15" tabindex="-1"></a></span>
<span id="cb4-16"><a aria-hidden="true" href="#cb4-16" tabindex="-1"></a><span class="co"># Loop through the sample sizes</span></span>
<span id="cb4-17"><a aria-hidden="true" href="#cb4-17" tabindex="-1"></a><span class="cf">for</span> (size <span class="cf">in</span> sample_sizes) {</span>
<span id="cb4-18"><a aria-hidden="true" href="#cb4-18" tabindex="-1"></a>  <span class="co"># Simulate n_simulations samples of exponential random variables</span></span>
<span id="cb4-19"><a aria-hidden="true" href="#cb4-19" tabindex="-1"></a>  samples <span class="ot">&lt;-</span> <span class="fu">rexp</span>(n_simulations <span class="sc">*</span> size, <span class="at">rate =</span> <span class="dv">1</span>)</span>
<span id="cb4-20"><a aria-hidden="true" href="#cb4-20" tabindex="-1"></a></span>
<span id="cb4-21"><a aria-hidden="true" href="#cb4-21" tabindex="-1"></a>  <span class="co"># Reshape the samples into a matrix</span></span>
<span id="cb4-22"><a aria-hidden="true" href="#cb4-22" tabindex="-1"></a>  samples_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(samples, <span class="at">nrow =</span> n_simulations, <span class="at">ncol =</span> size)</span>
<span id="cb4-23"><a aria-hidden="true" href="#cb4-23" tabindex="-1"></a></span>
<span id="cb4-24"><a aria-hidden="true" href="#cb4-24" tabindex="-1"></a>  <span class="co"># Calculate the standardized sample means</span></span>
<span id="cb4-25"><a aria-hidden="true" href="#cb4-25" tabindex="-1"></a>  sample_means <span class="ot">&lt;-</span> (<span class="fu">rowMeans</span>(samples_matrix) <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (<span class="dv">1</span> <span class="sc">/</span> <span class="fu">sqrt</span>(size))</span>
<span id="cb4-26"><a aria-hidden="true" href="#cb4-26" tabindex="-1"></a></span>
<span id="cb4-27"><a aria-hidden="true" href="#cb4-27" tabindex="-1"></a>  <span class="co"># Store the results in the list</span></span>
<span id="cb4-28"><a aria-hidden="true" href="#cb4-28" tabindex="-1"></a>  results[[<span class="fu">as.character</span>(size)]] <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb4-29"><a aria-hidden="true" href="#cb4-29" tabindex="-1"></a>    <span class="at">sample_size =</span> size,</span>
<span id="cb4-30"><a aria-hidden="true" href="#cb4-30" tabindex="-1"></a>    <span class="at">standardized_sample_mean =</span> sample_means</span>
<span id="cb4-31"><a aria-hidden="true" href="#cb4-31" tabindex="-1"></a>  )</span>
<span id="cb4-32"><a aria-hidden="true" href="#cb4-32" tabindex="-1"></a>}</span>
<span id="cb4-33"><a aria-hidden="true" href="#cb4-33" tabindex="-1"></a></span>
<span id="cb4-34"><a aria-hidden="true" href="#cb4-34" tabindex="-1"></a><span class="co"># Combine the results into a single data frame</span></span>
<span id="cb4-35"><a aria-hidden="true" href="#cb4-35" tabindex="-1"></a>results_df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(results)</span>
<span id="cb4-36"><a aria-hidden="true" href="#cb4-36" tabindex="-1"></a></span>
<span id="cb4-37"><a aria-hidden="true" href="#cb4-37" tabindex="-1"></a><span class="co"># Create a plot of the standardized sample means</span></span>
<span id="cb4-38"><a aria-hidden="true" href="#cb4-38" tabindex="-1"></a><span class="fu">ggplot</span>(results_df, <span class="fu">aes</span>(<span class="at">x =</span> standardized_sample_mean, <span class="at">fill =</span> <span class="fu">factor</span>(sample_size))) <span class="sc">+</span></span>
<span id="cb4-39"><a aria-hidden="true" href="#cb4-39" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">position =</span> <span class="st">"identity"</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">bins =</span> <span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb4-40"><a aria-hidden="true" href="#cb4-40" tabindex="-1"></a>  <span class="fu">stat_function</span>(</span>
<span id="cb4-41"><a aria-hidden="true" href="#cb4-41" tabindex="-1"></a>    <span class="at">fun =</span> dnorm, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>),</span>
<span id="cb4-42"><a aria-hidden="true" href="#cb4-42" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span></span>
<span id="cb4-43"><a aria-hidden="true" href="#cb4-43" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb4-44"><a aria-hidden="true" href="#cb4-44" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb4-45"><a aria-hidden="true" href="#cb4-45" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Central Limit Theorem"</span>,</span>
<span id="cb4-46"><a aria-hidden="true" href="#cb4-46" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">"Distribution of standardized sample means approaches N(0,1)"</span>,</span>
<span id="cb4-47"><a aria-hidden="true" href="#cb4-47" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Standardized Sample Mean"</span>,</span>
<span id="cb4-48"><a aria-hidden="true" href="#cb4-48" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Frequency"</span>,</span>
<span id="cb4-49"><a aria-hidden="true" href="#cb4-49" tabindex="-1"></a>    <span class="at">fill =</span> <span class="st">"Sample Size"</span></span>
<span id="cb4-50"><a aria-hidden="true" href="#cb4-50" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb4-51"><a aria-hidden="true" href="#cb4-51" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Multiple drawing groups in `geom_function()`
ℹ Did you use the correct group, colour, or fill aesthetics?</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap08_files/figure-html/unnamed-chunk-2-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load necessary libraries:</strong> The code loads the <code>tidyverse</code> library.</li>
<li><strong>Set the seed:</strong> <code>set.seed(456)</code> ensures reproducibility.</li>
<li><strong>Define the number of simulations and sample sizes:</strong> Similar to the previous script, these variables control the simulation parameters.</li>
<li><strong>Loop through the sample sizes:</strong> The code iterates through each sample size.</li>
<li><strong>Simulate samples:</strong> <code>rexp(n_simulations * size, rate = 1)</code> simulates samples of exponential random variables with a rate parameter of 1 (mean = 1, variance = 1).</li>
<li><strong>Reshape the samples:</strong> The samples are reshaped into a matrix.</li>
<li><strong>Calculate the standardized sample means:</strong> This is the crucial step for illustrating the CLT. <code>(rowMeans(samples_matrix) - 1) / (1 / sqrt(size))</code> calculates the standardized sample mean for each simulation. We subtract the true mean (1) and divide by the standard deviation of the sample mean, which is <span class="math inline">\(\dfrac{\sigma}{\sqrt{n}} = \dfrac{1}{\sqrt{size}}\)</span> for an exponential distribution with rate 1.</li>
<li><strong>Store the results:</strong> The results are stored in a list.</li>
<li><strong>Combine the results:</strong> The results are combined into a data frame.</li>
<li><strong>Create a plot:</strong> <code>ggplot2</code> creates a histogram of the standardized sample means. <code>stat_function()</code> adds the probability density function of the standard normal distribution (N(0, 1)) for comparison.</li>
</ol>
<p><strong>Relationship to the text:</strong></p>
<p>This script demonstrates the <strong>Central Limit Theorem (CLT)</strong>. The CLT states that the distribution of the standardized sample mean approaches a standard normal distribution as the sample size increases, regardless of the underlying population distribution (as long as the mean and variance are finite). Here, we simulate samples from an exponential distribution, which is non-normal. The plot shows that as the sample size increases, the distribution of the standardized sample means gets closer to the standard normal distribution, illustrating the <strong>CLT</strong>.</p>
</section>
<section class="level3" id="r-script-3-demonstrating-convergence-in-probability">
<h3 class="anchored" data-anchor-id="r-script-3-demonstrating-convergence-in-probability">R Script 3: Demonstrating Convergence in Probability</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a aria-hidden="true" href="#cb6-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb6-2"><a aria-hidden="true" href="#cb6-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb6-3"><a aria-hidden="true" href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a aria-hidden="true" href="#cb6-4" tabindex="-1"></a><span class="co"># Set the seed for reproducibility</span></span>
<span id="cb6-5"><a aria-hidden="true" href="#cb6-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb6-6"><a aria-hidden="true" href="#cb6-6" tabindex="-1"></a></span>
<span id="cb6-7"><a aria-hidden="true" href="#cb6-7" tabindex="-1"></a><span class="co"># Define the sequence of sample sizes</span></span>
<span id="cb6-8"><a aria-hidden="true" href="#cb6-8" tabindex="-1"></a>sample_sizes <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">10</span>, <span class="dv">1000</span>, <span class="at">by =</span> <span class="dv">10</span>)</span>
<span id="cb6-9"><a aria-hidden="true" href="#cb6-9" tabindex="-1"></a></span>
<span id="cb6-10"><a aria-hidden="true" href="#cb6-10" tabindex="-1"></a><span class="co"># Create an empty data frame to store the results</span></span>
<span id="cb6-11"><a aria-hidden="true" href="#cb6-11" tabindex="-1"></a>results_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>()</span>
<span id="cb6-12"><a aria-hidden="true" href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a aria-hidden="true" href="#cb6-13" tabindex="-1"></a><span class="co"># Loop through the sample sizes</span></span>
<span id="cb6-14"><a aria-hidden="true" href="#cb6-14" tabindex="-1"></a><span class="cf">for</span> (size <span class="cf">in</span> sample_sizes) {</span>
<span id="cb6-15"><a aria-hidden="true" href="#cb6-15" tabindex="-1"></a>  <span class="co"># Generate a random variable X_n = 1/n</span></span>
<span id="cb6-16"><a aria-hidden="true" href="#cb6-16" tabindex="-1"></a>  X_n <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> size</span>
<span id="cb6-17"><a aria-hidden="true" href="#cb6-17" tabindex="-1"></a></span>
<span id="cb6-18"><a aria-hidden="true" href="#cb6-18" tabindex="-1"></a>  <span class="co"># Store the results in the data frame</span></span>
<span id="cb6-19"><a aria-hidden="true" href="#cb6-19" tabindex="-1"></a>  results_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(</span>
<span id="cb6-20"><a aria-hidden="true" href="#cb6-20" tabindex="-1"></a>    results_df,</span>
<span id="cb6-21"><a aria-hidden="true" href="#cb6-21" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">sample_size =</span> size, <span class="at">X_n =</span> X_n)</span>
<span id="cb6-22"><a aria-hidden="true" href="#cb6-22" tabindex="-1"></a>  )</span>
<span id="cb6-23"><a aria-hidden="true" href="#cb6-23" tabindex="-1"></a>}</span>
<span id="cb6-24"><a aria-hidden="true" href="#cb6-24" tabindex="-1"></a></span>
<span id="cb6-25"><a aria-hidden="true" href="#cb6-25" tabindex="-1"></a><span class="co"># Create a plot of X_n against the sample size</span></span>
<span id="cb6-26"><a aria-hidden="true" href="#cb6-26" tabindex="-1"></a><span class="fu">ggplot</span>(results_df, <span class="fu">aes</span>(<span class="at">x =</span> sample_size, <span class="at">y =</span> X_n)) <span class="sc">+</span></span>
<span id="cb6-27"><a aria-hidden="true" href="#cb6-27" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb6-28"><a aria-hidden="true" href="#cb6-28" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb6-29"><a aria-hidden="true" href="#cb6-29" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb6-30"><a aria-hidden="true" href="#cb6-30" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Convergence in Probability"</span>,</span>
<span id="cb6-31"><a aria-hidden="true" href="#cb6-31" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">"X_n converges to 0 as n increases"</span>,</span>
<span id="cb6-32"><a aria-hidden="true" href="#cb6-32" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Sample Size (n)"</span>,</span>
<span id="cb6-33"><a aria-hidden="true" href="#cb6-33" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"X_n"</span></span>
<span id="cb6-34"><a aria-hidden="true" href="#cb6-34" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb6-35"><a aria-hidden="true" href="#cb6-35" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap08_files/figure-html/unnamed-chunk-3-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load necessary libraries:</strong> The code loads the <code>tidyverse</code> library.</li>
<li><strong>Set the seed:</strong> <code>set.seed(789)</code> ensures reproducibility.</li>
<li><strong>Define the sequence of sample sizes:</strong> <code>sample_sizes</code> creates a sequence of sample sizes from 10 to 1000.</li>
<li><strong>Create an empty data frame:</strong> <code>results_df</code> will store the results.</li>
<li><strong>Loop through the sample sizes:</strong> The code iterates through each sample size in the sequence.</li>
<li><strong>Generate <span class="math inline">\(X_n\)</span>:</strong> Inside the loop, <code>X_n &lt;- 1 / size</code> defines a deterministic sequence <span class="math inline">\(X_n\)</span> that depends on the sample size. Here we are deterministically defining <span class="math inline">\(X_n\)</span> rather than simulating it to illustrate the most basic notion of convergence in probability.</li>
<li><strong>Store the results:</strong> The sample size and the corresponding value of <span class="math inline">\(X_n\)</span> are stored in the <code>results_df</code> data frame.</li>
<li><strong>Create a plot:</strong> <code>ggplot2</code> creates a line plot showing how <span class="math inline">\(X_n\)</span> changes as the sample size increases. The <code>geom_hline()</code> function adds a horizontal dashed line at 0.</li>
</ol>
<p><strong>Relationship to the text:</strong></p>
<p>This script illustrates the concept of <strong>convergence in probability</strong>. Although in this example <span class="math inline">\(X_n\)</span> is defined deterministically, it can be shown that if <span class="math inline">\(\left\{X_n\right\}_{n=1}^{\infty}\)</span> is a sequence of real numbers that converges to a limit <span class="math inline">\(x_{\infty}\)</span>, then <span class="math inline">\(X_n\)</span> also converges to <span class="math inline">\(x_{\infty}\)</span> in probability. The plot shows that as <span class="math inline">\(n\)</span> increases, <span class="math inline">\(X_n\)</span> gets closer and closer to 0, illustrating convergence to 0.</p>
</section>
<section class="level3" id="r-script-4-illustrating-chebyshevs-inequality">
<h3 class="anchored" data-anchor-id="r-script-4-illustrating-chebyshevs-inequality">R Script 4: Illustrating Chebyshev’s Inequality</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a aria-hidden="true" href="#cb7-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb7-2"><a aria-hidden="true" href="#cb7-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb7-3"><a aria-hidden="true" href="#cb7-3" tabindex="-1"></a></span>
<span id="cb7-4"><a aria-hidden="true" href="#cb7-4" tabindex="-1"></a><span class="co"># Set the seed for reproducibility</span></span>
<span id="cb7-5"><a aria-hidden="true" href="#cb7-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101112</span>)</span>
<span id="cb7-6"><a aria-hidden="true" href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a aria-hidden="true" href="#cb7-7" tabindex="-1"></a><span class="co"># Define the number of simulations</span></span>
<span id="cb7-8"><a aria-hidden="true" href="#cb7-8" tabindex="-1"></a>n_simulations <span class="ot">&lt;-</span> <span class="dv">10000</span></span>
<span id="cb7-9"><a aria-hidden="true" href="#cb7-9" tabindex="-1"></a></span>
<span id="cb7-10"><a aria-hidden="true" href="#cb7-10" tabindex="-1"></a><span class="co"># Define the sample size</span></span>
<span id="cb7-11"><a aria-hidden="true" href="#cb7-11" tabindex="-1"></a>sample_size <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb7-12"><a aria-hidden="true" href="#cb7-12" tabindex="-1"></a></span>
<span id="cb7-13"><a aria-hidden="true" href="#cb7-13" tabindex="-1"></a><span class="co"># Generate samples from a normal distribution</span></span>
<span id="cb7-14"><a aria-hidden="true" href="#cb7-14" tabindex="-1"></a>samples <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_simulations <span class="sc">*</span> sample_size, <span class="at">mean =</span> <span class="dv">5</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb7-15"><a aria-hidden="true" href="#cb7-15" tabindex="-1"></a></span>
<span id="cb7-16"><a aria-hidden="true" href="#cb7-16" tabindex="-1"></a><span class="co"># Reshape the samples into a matrix</span></span>
<span id="cb7-17"><a aria-hidden="true" href="#cb7-17" tabindex="-1"></a>samples_matrix <span class="ot">&lt;-</span> <span class="fu">matrix</span>(samples, <span class="at">nrow =</span> n_simulations, <span class="at">ncol =</span> sample_size)</span>
<span id="cb7-18"><a aria-hidden="true" href="#cb7-18" tabindex="-1"></a></span>
<span id="cb7-19"><a aria-hidden="true" href="#cb7-19" tabindex="-1"></a><span class="co"># Calculate the sample means</span></span>
<span id="cb7-20"><a aria-hidden="true" href="#cb7-20" tabindex="-1"></a>sample_means <span class="ot">&lt;-</span> <span class="fu">rowMeans</span>(samples_matrix)</span>
<span id="cb7-21"><a aria-hidden="true" href="#cb7-21" tabindex="-1"></a></span>
<span id="cb7-22"><a aria-hidden="true" href="#cb7-22" tabindex="-1"></a><span class="co"># Calculate the deviations from the true mean</span></span>
<span id="cb7-23"><a aria-hidden="true" href="#cb7-23" tabindex="-1"></a>deviations <span class="ot">&lt;-</span> <span class="fu">abs</span>(sample_means <span class="sc">-</span> <span class="dv">5</span>)</span>
<span id="cb7-24"><a aria-hidden="true" href="#cb7-24" tabindex="-1"></a></span>
<span id="cb7-25"><a aria-hidden="true" href="#cb7-25" tabindex="-1"></a><span class="co"># Define a range of eta values</span></span>
<span id="cb7-26"><a aria-hidden="true" href="#cb7-26" tabindex="-1"></a>eta_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.1</span>, <span class="dv">5</span>, <span class="at">by =</span> <span class="fl">0.1</span>)</span>
<span id="cb7-27"><a aria-hidden="true" href="#cb7-27" tabindex="-1"></a></span>
<span id="cb7-28"><a aria-hidden="true" href="#cb7-28" tabindex="-1"></a><span class="co"># Create an empty data frame to store the results</span></span>
<span id="cb7-29"><a aria-hidden="true" href="#cb7-29" tabindex="-1"></a>results_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>()</span>
<span id="cb7-30"><a aria-hidden="true" href="#cb7-30" tabindex="-1"></a></span>
<span id="cb7-31"><a aria-hidden="true" href="#cb7-31" tabindex="-1"></a><span class="co"># Loop through the eta values</span></span>
<span id="cb7-32"><a aria-hidden="true" href="#cb7-32" tabindex="-1"></a><span class="cf">for</span> (eta <span class="cf">in</span> eta_values) {</span>
<span id="cb7-33"><a aria-hidden="true" href="#cb7-33" tabindex="-1"></a>  <span class="co"># Calculate the proportion of deviations greater than eta</span></span>
<span id="cb7-34"><a aria-hidden="true" href="#cb7-34" tabindex="-1"></a>  proportion_greater <span class="ot">&lt;-</span> <span class="fu">mean</span>(deviations <span class="sc">&gt;=</span> eta)</span>
<span id="cb7-35"><a aria-hidden="true" href="#cb7-35" tabindex="-1"></a></span>
<span id="cb7-36"><a aria-hidden="true" href="#cb7-36" tabindex="-1"></a>  <span class="co"># Calculate the Chebyshev bound</span></span>
<span id="cb7-37"><a aria-hidden="true" href="#cb7-37" tabindex="-1"></a>  chebyshev_bound <span class="ot">&lt;-</span> (<span class="dv">2</span><span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (eta<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb7-38"><a aria-hidden="true" href="#cb7-38" tabindex="-1"></a></span>
<span id="cb7-39"><a aria-hidden="true" href="#cb7-39" tabindex="-1"></a>  <span class="co"># Store the results in the data frame</span></span>
<span id="cb7-40"><a aria-hidden="true" href="#cb7-40" tabindex="-1"></a>  results_df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(</span>
<span id="cb7-41"><a aria-hidden="true" href="#cb7-41" tabindex="-1"></a>    results_df,</span>
<span id="cb7-42"><a aria-hidden="true" href="#cb7-42" tabindex="-1"></a>    <span class="fu">data.frame</span>(<span class="at">eta =</span> eta, <span class="at">proportion =</span> proportion_greater, <span class="at">bound =</span> chebyshev_bound)</span>
<span id="cb7-43"><a aria-hidden="true" href="#cb7-43" tabindex="-1"></a>  )</span>
<span id="cb7-44"><a aria-hidden="true" href="#cb7-44" tabindex="-1"></a>}</span>
<span id="cb7-45"><a aria-hidden="true" href="#cb7-45" tabindex="-1"></a></span>
<span id="cb7-46"><a aria-hidden="true" href="#cb7-46" tabindex="-1"></a><span class="co"># Create a plot of the proportions and the Chebyshev bound</span></span>
<span id="cb7-47"><a aria-hidden="true" href="#cb7-47" tabindex="-1"></a><span class="fu">ggplot</span>(results_df, <span class="fu">aes</span>(<span class="at">x =</span> eta)) <span class="sc">+</span></span>
<span id="cb7-48"><a aria-hidden="true" href="#cb7-48" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> proportion, <span class="at">color =</span> <span class="st">"Proportion"</span>)) <span class="sc">+</span></span>
<span id="cb7-49"><a aria-hidden="true" href="#cb7-49" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> bound, <span class="at">color =</span> <span class="st">"Chebyshev Bound"</span>)) <span class="sc">+</span></span>
<span id="cb7-50"><a aria-hidden="true" href="#cb7-50" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb7-51"><a aria-hidden="true" href="#cb7-51" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Chebyshev's Inequality"</span>,</span>
<span id="cb7-52"><a aria-hidden="true" href="#cb7-52" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">"Proportion of deviations &gt;= eta is below the Chebyshev bound"</span>,</span>
<span id="cb7-53"><a aria-hidden="true" href="#cb7-53" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Eta"</span>,</span>
<span id="cb7-54"><a aria-hidden="true" href="#cb7-54" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Value"</span>,</span>
<span id="cb7-55"><a aria-hidden="true" href="#cb7-55" tabindex="-1"></a>    <span class="at">color =</span> <span class="st">""</span></span>
<span id="cb7-56"><a aria-hidden="true" href="#cb7-56" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb7-57"><a aria-hidden="true" href="#cb7-57" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">"Proportion"</span> <span class="ot">=</span> <span class="st">"blue"</span>, <span class="st">"Chebyshev Bound"</span> <span class="ot">=</span> <span class="st">"red"</span>)) <span class="sc">+</span></span>
<span id="cb7-58"><a aria-hidden="true" href="#cb7-58" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap08_files/figure-html/unnamed-chunk-4-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load necessary libraries:</strong> The code loads the <code>tidyverse</code> library.</li>
<li><strong>Set the seed:</strong> <code>set.seed(101112)</code> ensures reproducibility.</li>
<li><strong>Define the number of simulations and sample size:</strong> These variables control the simulation parameters.</li>
<li><strong>Generate samples:</strong> <code>rnorm(n_simulations * sample_size, mean = 5, sd = 2)</code> simulates samples from a normal distribution with mean 5 and standard deviation 2.</li>
<li><strong>Reshape the samples:</strong> The samples are reshaped into a matrix.</li>
<li><strong>Calculate the sample means:</strong> <code>rowMeans(samples_matrix)</code> calculates the sample mean for each simulation.</li>
<li><strong>Calculate the deviations:</strong> <code>abs(sample_means - 5)</code> calculates the absolute deviations of the sample means from the true mean (5).</li>
<li><strong>Define a range of eta values:</strong> <code>eta_values</code> creates a sequence of <span class="math inline">\(\eta\)</span> values to test <strong>Chebyshev’s inequality</strong>.</li>
<li><strong>Loop through the eta values:</strong> The code iterates through each <span class="math inline">\(\eta\)</span> value.</li>
<li><strong>Calculate the proportion of deviations greater than eta:</strong> <code>mean(deviations &gt;= eta)</code> calculates the proportion of simulations where the absolute deviation of the sample mean from the true mean is greater than or equal to <span class="math inline">\(\eta\)</span>.</li>
<li><strong>Calculate the Chebyshev bound:</strong> <code>(2^2) / (eta^2)</code> calculates the <strong>Chebyshev bound</strong>, which is <span class="math inline">\(\dfrac{\operatorname{Var}(X)}{\eta^2}\)</span>. In this case <span class="math inline">\(\operatorname{Var}(X)\)</span> is the variance of the sample mean, which is the population variance divided by <span class="math inline">\(n\)</span>, <span class="math inline">\(\dfrac{4}{100}=0.04\)</span>. However, for the purposes of this example, we are treating the simulated values of <span class="math inline">\(X\)</span> as the population, so we use <span class="math inline">\(\operatorname{Var}(X)=\sigma^2=2^2=4\)</span>.</li>
<li><strong>Store the results:</strong> The <span class="math inline">\(\eta\)</span> value, the proportion, and the <strong>Chebyshev bound</strong> are stored in the <code>results_df</code> data frame.</li>
<li><strong>Create a plot:</strong> <code>ggplot2</code> creates a line plot showing the proportion of deviations greater than <span class="math inline">\(\eta\)</span> and the <strong>Chebyshev bound</strong> for different values of <span class="math inline">\(\eta\)</span>.</li>
</ol>
<p><strong>Relationship to the text:</strong></p>
<p>This script illustrates <strong>Chebyshev’s inequality</strong>, which provides an upper bound for the probability that a random variable deviates from its mean by more than a certain amount. The plot shows that the proportion of simulations where the sample mean deviates from the true mean by more than <span class="math inline">\(\eta\)</span> is always below the <strong>Chebyshev bound</strong>, confirming the inequality.</p>
</section>
<section class="level3" id="r-script-5-illustrating-the-cauchy-schwarz-inequality">
<h3 class="anchored" data-anchor-id="r-script-5-illustrating-the-cauchy-schwarz-inequality">R Script 5: Illustrating the Cauchy-Schwarz Inequality</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a aria-hidden="true" href="#cb8-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb8-2"><a aria-hidden="true" href="#cb8-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb8-3"><a aria-hidden="true" href="#cb8-3" tabindex="-1"></a></span>
<span id="cb8-4"><a aria-hidden="true" href="#cb8-4" tabindex="-1"></a><span class="co"># Set the seed for reproducibility</span></span>
<span id="cb8-5"><a aria-hidden="true" href="#cb8-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">12345</span>)</span>
<span id="cb8-6"><a aria-hidden="true" href="#cb8-6" tabindex="-1"></a></span>
<span id="cb8-7"><a aria-hidden="true" href="#cb8-7" tabindex="-1"></a><span class="co"># Define the number of simulations</span></span>
<span id="cb8-8"><a aria-hidden="true" href="#cb8-8" tabindex="-1"></a>n_simulations <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb8-9"><a aria-hidden="true" href="#cb8-9" tabindex="-1"></a></span>
<span id="cb8-10"><a aria-hidden="true" href="#cb8-10" tabindex="-1"></a><span class="co"># Generate two sets of random variables</span></span>
<span id="cb8-11"><a aria-hidden="true" href="#cb8-11" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n_simulations, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb8-12"><a aria-hidden="true" href="#cb8-12" tabindex="-1"></a>Y <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(n_simulations, <span class="at">shape =</span> <span class="dv">3</span>, <span class="at">rate =</span> <span class="dv">1</span>)</span>
<span id="cb8-13"><a aria-hidden="true" href="#cb8-13" tabindex="-1"></a></span>
<span id="cb8-14"><a aria-hidden="true" href="#cb8-14" tabindex="-1"></a><span class="co"># Calculate E(XY), E(X^2), and E(Y^2)</span></span>
<span id="cb8-15"><a aria-hidden="true" href="#cb8-15" tabindex="-1"></a>expected_XY <span class="ot">&lt;-</span> <span class="fu">mean</span>(X <span class="sc">*</span> Y)</span>
<span id="cb8-16"><a aria-hidden="true" href="#cb8-16" tabindex="-1"></a>expected_X2 <span class="ot">&lt;-</span> <span class="fu">mean</span>(X<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-17"><a aria-hidden="true" href="#cb8-17" tabindex="-1"></a>expected_Y2 <span class="ot">&lt;-</span> <span class="fu">mean</span>(Y<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb8-18"><a aria-hidden="true" href="#cb8-18" tabindex="-1"></a></span>
<span id="cb8-19"><a aria-hidden="true" href="#cb8-19" tabindex="-1"></a><span class="co"># Calculate the Cauchy-Schwarz bound</span></span>
<span id="cb8-20"><a aria-hidden="true" href="#cb8-20" tabindex="-1"></a>cs_bound <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(expected_X2 <span class="sc">*</span> expected_Y2)</span>
<span id="cb8-21"><a aria-hidden="true" href="#cb8-21" tabindex="-1"></a></span>
<span id="cb8-22"><a aria-hidden="true" href="#cb8-22" tabindex="-1"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb8-23"><a aria-hidden="true" href="#cb8-23" tabindex="-1"></a>results_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb8-24"><a aria-hidden="true" href="#cb8-24" tabindex="-1"></a>  <span class="at">Value =</span> <span class="fu">c</span>(expected_XY, cs_bound),</span>
<span id="cb8-25"><a aria-hidden="true" href="#cb8-25" tabindex="-1"></a>  <span class="at">Label =</span> <span class="fu">c</span>(<span class="st">"E(XY)"</span>, <span class="st">"sqrt(E(X^2)E(Y^2))"</span>)</span>
<span id="cb8-26"><a aria-hidden="true" href="#cb8-26" tabindex="-1"></a>)</span>
<span id="cb8-27"><a aria-hidden="true" href="#cb8-27" tabindex="-1"></a></span>
<span id="cb8-28"><a aria-hidden="true" href="#cb8-28" tabindex="-1"></a><span class="co"># Create a bar plot to compare E(XY) and the bound</span></span>
<span id="cb8-29"><a aria-hidden="true" href="#cb8-29" tabindex="-1"></a><span class="fu">ggplot</span>(results_df, <span class="fu">aes</span>(<span class="at">x =</span> Label, <span class="at">y =</span> Value, <span class="at">fill =</span> Label)) <span class="sc">+</span></span>
<span id="cb8-30"><a aria-hidden="true" href="#cb8-30" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">width =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb8-31"><a aria-hidden="true" href="#cb8-31" tabindex="-1"></a>  <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">label =</span> <span class="fu">round</span>(Value, <span class="dv">2</span>)), <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb8-32"><a aria-hidden="true" href="#cb8-32" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb8-33"><a aria-hidden="true" href="#cb8-33" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Cauchy-Schwarz Inequality"</span>,</span>
<span id="cb8-34"><a aria-hidden="true" href="#cb8-34" tabindex="-1"></a>    <span class="at">subtitle =</span> <span class="st">"|E(XY)| is less than or equal to sqrt(E(X^2)E(Y^2))"</span>,</span>
<span id="cb8-35"><a aria-hidden="true" href="#cb8-35" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">""</span>,</span>
<span id="cb8-36"><a aria-hidden="true" href="#cb8-36" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Value"</span></span>
<span id="cb8-37"><a aria-hidden="true" href="#cb8-37" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb8-38"><a aria-hidden="true" href="#cb8-38" tabindex="-1"></a>  <span class="fu">theme_minimal</span>() <span class="sc">+</span></span>
<span id="cb8-39"><a aria-hidden="true" href="#cb8-39" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">"none"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap08_files/figure-html/unnamed-chunk-5-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load necessary libraries:</strong> The code loads the <code>tidyverse</code> library.</li>
<li><strong>Set the seed:</strong> <code>set.seed(12345)</code> ensures reproducibility.</li>
<li><strong>Define the number of simulations:</strong> <code>n_simulations</code> specifies the number of random values to generate for each variable.</li>
<li><strong>Generate two sets of random variables:</strong> <code>rnorm()</code> generates random values from a normal distribution (for <span class="math inline">\(X\)</span>), and <code>rgamma()</code> generates random values from a gamma distribution (for <span class="math inline">\(Y\)</span>).</li>
<li><strong>Calculate <span class="math inline">\(E(XY)\)</span>, <span class="math inline">\(E(X^2)\)</span>, and <span class="math inline">\(E(Y^2)\)</span>:</strong> <code>mean(X * Y)</code>, <code>mean(X^2)</code>, and <code>mean(Y^2)</code> calculate the sample means of <span class="math inline">\(XY\)</span>, <span class="math inline">\(X^2\)</span>, and <span class="math inline">\(Y^2\)</span>, respectively, which serve as estimates of the expectations.</li>
<li><strong>Calculate the Cauchy-Schwarz bound:</strong> <code>sqrt(expected_X2 * expected_Y2)</code> calculates the bound provided by the <strong>Cauchy-Schwarz inequality</strong>.</li>
<li><strong>Create a data frame for plotting:</strong> <code>results_df</code> stores the calculated values and labels for plotting.</li>
<li><strong>Create a bar plot:</strong> <code>ggplot2</code> creates a bar plot comparing the estimated <span class="math inline">\(E(XY)\)</span> with the <strong>Cauchy-Schwarz bound</strong>. <code>geom_text()</code> adds the values as text labels above the bars.</li>
</ol>
<p><strong>Relationship to the text:</strong></p>
<p>This script illustrates the <strong>Cauchy-Schwarz inequality</strong>, which states that <span class="math inline">\(|E(XY)| \leq \sqrt{E(X^2)E(Y^2)}\)</span>. The bar plot shows that the estimated value of <span class="math inline">\(E(XY)\)</span> is indeed less than or equal to the calculated <strong>Cauchy-Schwarz bound</strong>, demonstrating the inequality.</p>
</section>
</section>
<section class="level2" id="youtube-videos-on-asymptotic-theory-concepts">
<h2 class="anchored" data-anchor-id="youtube-videos-on-asymptotic-theory-concepts">YouTube Videos on Asymptotic Theory Concepts</h2>
<p>Here are some YouTube videos that explain the concepts mentioned in the attached text, along with explanations of how they relate to the text:</p>
<section class="level3" id="videos-on-inequalities">
<h3 class="anchored" data-anchor-id="videos-on-inequalities">Videos on Inequalities</h3>
<ol type="1">
<li><p><strong>Title:</strong> Markov’s inequality</p>
<p><strong>Channel:</strong> Learn Statistics with Brian</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=e-nAr3MkAII">https://www.youtube.com/watch?v=e-nAr3MkAII</a></p>
<p><strong>Relevance to the text:</strong> This video provides a clear explanation of <strong>Markov’s inequality</strong>, including its proof and an example. It complements <strong>Theorem 8.4</strong> in the text, which states <strong>Markov’s inequality</strong>:</p>
<p><span class="math display">\[
\operatorname{Pr}[|X| \geq \eta] \leq \frac{E|X|}{\eta} .
\]</span></p></li>
<li><p><strong>Title:</strong> Chebyshev’s inequality</p>
<p><strong>Channel:</strong> Learn Statistics with Brian</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=mlelI1LA9o4">https://www.youtube.com/watch?v=mlelI1LA9o4</a></p>
<p><strong>Relevance to the text:</strong> This video explains <strong>Chebyshev’s inequality</strong> in detail, covering its proof and providing illustrative examples. It complements <strong>Theorem 8.5</strong> in the text, which states <strong>Chebyshev’s inequality</strong>:</p>
<p><span class="math display">\[
\operatorname{Pr}[|X-E X| \geq \eta] \leq \frac{\operatorname{var}(X)}{\eta^2} .
\]</span></p></li>
<li><p><strong>Title:</strong> The Cauchy-Schwarz Inequality - Visual Proof</p>
<p><strong>Channel:</strong> Mike, the Mathematician</p>
<p><strong>URL:</strong> https://www.youtube.com/watch?v=icIau4tp8_4](https://www.youtube.com/watch?v=icIau4tp8_4)</p>
<p><strong>Relevance to the text:</strong> This video provides a visual proof of the <strong>Cauchy-Schwarz inequality</strong>. It is directly related to <strong>Theorem 8.2</strong> in the text, which states the inequality as:</p>
<p><span class="math display">\[
(E(X Y))^2 \leq E\left(X^2\right) \times E\left(Y^2\right)
\]</span></p></li>
</ol>
</section>
<section class="level3" id="videos-on-convergence-concepts">
<h3 class="anchored" data-anchor-id="videos-on-convergence-concepts">Videos on Convergence Concepts</h3>
<ol type="1">
<li><p><strong>Title:</strong> Convergence in Probability</p>
<p><strong>Channel:</strong> Jochumzen</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=HsNVjCDI9ts">https://www.youtube.com/watch?v=HsNVjCDI9ts</a></p>
<p><strong>Relevance to the text:</strong> This video explains the concept of <strong>convergence in probability</strong> with examples. It relates to <strong>Definition 8.4</strong> in the text, which defines convergence in probability as:</p>
<p><span class="math display">\[
X_n \stackrel{p}{\rightarrow} X \text{ if for all } \epsilon&gt;0, \lim _{n \rightarrow \infty} \operatorname{Pr}\left(\left|X_n-X\right|&gt;\epsilon\right)=0 .
\]</span></p></li>
<li><p><strong>Title:</strong> Convergence in Distribution</p>
<p><strong>Channel:</strong> <code>Jochumzen</code></p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=lF5JMZVZ9fo">https://www.youtube.com/watch?v=lF5JMZVZ9fo</a></p>
<p><strong>Relevance to the text:</strong> This video clarifies the concept of <strong>convergence in distribution</strong>. It aligns with <strong>Definition 8.6</strong> in the text, which defines convergence in distribution as:</p>
<p><span class="math display">\[
X_n \stackrel{D}{\rightarrow} X \text{ if for all } x \text{ at which } F(x)=\operatorname{Pr}[X \leq x] \text{ is continuous, } \lim _{n \rightarrow \infty} F_n(x)=\operatorname{Pr}(X \leq x) ,
\]</span></p>
<p>where <span class="math inline">\(F_n(x)=\operatorname{Pr}\left(X_n \leq x\right)\)</span>.</p></li>
<li><p><strong>Title:</strong> Relationships Between Modes of Convergence</p>
<p><strong>Channel:</strong> Probability and Random Processes</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=U9g6CWmMF7I">https://www.youtube.com/watch?v=U9g6CWmMF7I</a></p>
<p><strong>Relevance to the text:</strong> This video provides a comprehensive overview of <strong>modes of convergence</strong>, with examples. It relates to <strong>Section 8.2</strong> in the text.</p></li>
</ol>
</section>
<section class="level3" id="videos-on-law-of-large-numbers-and-central-limit-theorem">
<h3 class="anchored" data-anchor-id="videos-on-law-of-large-numbers-and-central-limit-theorem">Videos on Law of Large Numbers and Central Limit Theorem</h3>
<ol type="1">
<li><p><strong>Title:</strong> The Law of Large Numbers - Explained</p>
<p><strong>Channel:</strong> NStatum</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=ycuPP72_DVU">https://www.youtube.com/watch?v=ycuPP72_DVU</a></p>
<p><strong>Relevance to the text:</strong> This video provides an intuitive understanding of the <strong>Law of Large Numbers (LLN)</strong>. It supports <strong>Theorem 8.7</strong> (Kolmogorov’s strong law of large numbers) in the text, which states the conditions for the LLN to hold.</p></li>
<li><p><strong>Title:</strong> But what is the Central Limit Theorem?</p>
<p><strong>Channel:</strong> 3Blue1Brown</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=zeJD6dqJ5lo">https://www.youtube.com/watch?v=zeJD6dqJ5lo</a></p>
<p><strong>Relevance to the text:</strong> This video offers a clear explanation of the <strong>Central Limit Theorem (CLT)</strong>, including examples and visualizations. It relates to <strong>Theorem 8.8</strong> (Lindeberg-Levy CLT) in the text, which states the CLT for independent and identically distributed random variables.</p></li>
</ol>
</section>
<section class="level3" id="videos-on-related-concepts">
<h3 class="anchored" data-anchor-id="videos-on-related-concepts">Videos on Related Concepts</h3>
<ol type="1">
<li><p><strong>Title:</strong> The Delta Method</p>
<p><strong>Channel:</strong> Justin Eloriaga</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=Tv1pEA8RngE">https://www.youtube.com/watch?v=Tv1pEA8RngE</a></p>
<p><strong>Relevance to the text:</strong> This video explains the <strong>Delta method</strong>, a technique for approximating the distribution of a function of a random variable. It supports <strong>Theorem 8.14</strong> and <strong>Corollary 8.1</strong> in the text, which discuss the Delta method and its extension to second-order approximations.</p></li>
<li><p><strong>Title:</strong> Slutsky’s Theorem &amp; Delta Method</p>
<p><strong>Channel:</strong> statisticsmatt</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=makNYdhVboE">https://www.youtube.com/watch?v=makNYdhVboE</a></p>
<p><strong>Relevance to the text:</strong> This video discusses <strong>Slutsky’s Theorem</strong>, which is useful for determining the limiting distribution of combinations of random variables. It aligns with <strong>Theorem 8.12</strong> in the text, which presents Slutsky’s Theorem.</p></li>
</ol>
<p>These videos provide a good starting point for understanding the concepts discussed in the text. They offer visual explanations, examples, and proofs that can help solidify your understanding of asymptotic theory. Remember that watching these videos should be combined with careful reading and practice to fully grasp the material.</p>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch08mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch08mcsolution1">MC Solution 1</a></p>
<p>Which of the following inequalities is known as the <strong>triangle inequality</strong> for expectations?</p>
<ol type="a">
<li><span class="math inline">\(E|X + Y| \leq E|X| + E|Y|\)</span></li>
<li><span class="math inline">\((E(XY))^2 \leq E(X^2)E(Y^2)\)</span></li>
<li><span class="math inline">\(\operatorname{Pr}[|X| \geq \eta] \leq \dfrac{E|X|}{\eta}\)</span></li>
<li><span class="math inline">\(\operatorname{Pr}[|X - EX| \geq \eta] \leq \dfrac{\operatorname{var}(X)}{\eta^2}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch08mcsolution2">MC Solution 2</a></p>
<p>The <strong>Cauchy-Schwarz inequality</strong> states that for any two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<ol type="a">
<li><span class="math inline">\(E|X + Y| \leq E|X| + E|Y|\)</span></li>
<li><span class="math inline">\((E(XY))^2 \leq E(X^2)E(Y^2)\)</span></li>
<li><span class="math inline">\(\operatorname{Pr}[|X| \geq \eta] \leq \dfrac{E|X|}{\eta}\)</span></li>
<li><span class="math inline">\(\operatorname{Pr}[|X - EX| \geq \eta] \leq \dfrac{\operatorname{var}(X)}{\eta^2}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch08mcsolution3">MC Solution 3</a></p>
<p><strong>Markov’s inequality</strong> provides an upper bound for which of the following probabilities?</p>
<ol type="a">
<li><span class="math inline">\(\operatorname{Pr}(X \geq a)\)</span> for a non-negative random variable <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(\operatorname{Pr}(|X - E(X)| \geq a)\)</span> for any random variable <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(\operatorname{Pr}(X \leq a)\)</span> for a non-negative random variable <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(\operatorname{Pr}(|X| \leq a)\)</span> for any random variable <span class="math inline">\(X\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch08mcsolution4">MC Solution 4</a></p>
<p><strong>Chebyshev’s inequality</strong> states that for any random variable <span class="math inline">\(X\)</span> with finite mean and variance, and any <span class="math inline">\(\eta &gt; 0\)</span>:</p>
<ol type="a">
<li><span class="math inline">\(\operatorname{Pr}[|X| \geq \eta] \leq \dfrac{E|X|}{\eta}\)</span></li>
<li><span class="math inline">\(\operatorname{Pr}[|X - EX| \geq \eta] \leq \dfrac{\operatorname{var}(X)}{\eta^2}\)</span></li>
<li><span class="math inline">\((E(XY))^2 \leq E(X^2)E(Y^2)\)</span></li>
<li><span class="math inline">\(E|X + Y| \leq E|X| + E|Y|\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch08mcsolution5">MC Solution 5</a></p>
<p>A sequence of random variables <span class="math inline">\(\{X_n\}\)</span> <strong>converges in probability</strong> to a random variable <span class="math inline">\(X\)</span> if for every <span class="math inline">\(\epsilon &gt; 0\)</span>:</p>
<ol type="a">
<li><span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(|X_n - X| &gt; \epsilon) = 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} E(|X_n - X|) = 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} E[(X_n - X)^2] = 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(X_n \leq x) = \operatorname{Pr}(X \leq x)\)</span> for all <span class="math inline">\(x\)</span> where <span class="math inline">\(\operatorname{Pr}(X \leq x)\)</span> is continuous</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch08mcsolution6">MC Solution 6</a></p>
<p>A sequence of random variables <span class="math inline">\(\{X_n\}\)</span> <strong>converges in mean square</strong> to a random variable <span class="math inline">\(X\)</span> if:</p>
<ol type="a">
<li><span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(|X_n - X| &gt; \epsilon) = 0\)</span> for every <span class="math inline">\(\epsilon &gt; 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} E(|X_n - X|) = 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} E[(X_n - X)^2] = 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(X_n \leq x) = \operatorname{Pr}(X \leq x)\)</span> for all <span class="math inline">\(x\)</span> where <span class="math inline">\(\operatorname{Pr}(X \leq x)\)</span> is continuous</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch08mcsolution7">MC Solution 7</a></p>
<p>A sequence of random variables <span class="math inline">\(\{X_n\}\)</span> <strong>converges in distribution</strong> to a random variable <span class="math inline">\(X\)</span> if:</p>
<ol type="a">
<li><span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(|X_n - X| &gt; \epsilon) = 0\)</span> for every <span class="math inline">\(\epsilon &gt; 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} E(|X_n - X|) = 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} E[(X_n - X)^2] = 0\)</span></li>
<li><span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(X_n \leq x) = \operatorname{Pr}(X \leq x)\)</span> for all <span class="math inline">\(x\)</span> where <span class="math inline">\(\operatorname{Pr}(X \leq x)\)</span> is continuous</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch08mcsolution8">MC Solution 8</a></p>
<p>The <strong>Law of Large Numbers</strong> states that the sample mean of a sequence of independent and identically distributed random variables converges to:</p>
<ol type="a">
<li>The true mean of the population</li>
<li>The true variance of the population</li>
<li>A standard normal distribution</li>
<li>Zero</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch08mcsolution9">MC Solution 9</a></p>
<p>The <strong>Central Limit Theorem</strong> states that the standardized sum of a sequence of independent and identically distributed random variables converges in distribution to:</p>
<ol type="a">
<li>The true mean of the population</li>
<li>The true variance of the population</li>
<li>A standard normal distribution</li>
<li>Zero</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch08mcsolution10">MC Solution 10</a></p>
<p><strong>Lindeberg’s condition</strong> is a sufficient condition for which of the following to hold?</p>
<ol type="a">
<li>The Law of Large Numbers</li>
<li>The Central Limit Theorem for independent but not identically distributed random variables</li>
<li>Convergence in probability</li>
<li>Convergence in mean square</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch08mcsolution11">MC Solution 11</a></p>
<p><strong>Hölder’s inequality</strong> is a generalization of which of the following inequalities?</p>
<ol type="a">
<li>Markov’s inequality</li>
<li>Chebyshev’s inequality</li>
<li>Cauchy-Schwarz inequality</li>
<li>Triangle inequality</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch08mcsolution12">MC Solution 12</a></p>
<p><strong>Almost sure convergence</strong> of a sequence of random variables <span class="math inline">\(X_n\)</span> to a random variable <span class="math inline">\(X\)</span> implies which of the following?</p>
<ol type="a">
<li>Convergence in probability of <span class="math inline">\(X_n\)</span> to <span class="math inline">\(X\)</span></li>
<li>Convergence in mean square of <span class="math inline">\(X_n\)</span> to <span class="math inline">\(X\)</span></li>
<li>Convergence in distribution of <span class="math inline">\(X_n\)</span> to <span class="math inline">\(X\)</span></li>
<li>Both (a) and (c)</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch08mcsolution13">MC Solution 13</a></p>
<p>The <strong>Continuous Mapping Theorem</strong> states that if <span class="math inline">\(X_n\)</span> converges in probability to <span class="math inline">\(X\)</span> and <span class="math inline">\(g\)</span> is a continuous function, then:</p>
<ol type="a">
<li><span class="math inline">\(g(X_n)\)</span> converges in probability to <span class="math inline">\(g(X)\)</span></li>
<li><span class="math inline">\(g(X_n)\)</span> converges in distribution to <span class="math inline">\(g(X)\)</span></li>
<li><span class="math inline">\(g(X_n)\)</span> converges almost surely to <span class="math inline">\(g(X)\)</span></li>
<li><span class="math inline">\(g(X_n)\)</span> converges in mean square to <span class="math inline">\(g(X)\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch08mcsolution14">MC Solution 14</a></p>
<p><strong>Slutsky’s Theorem</strong> is useful for determining the limiting distribution of which of the following?</p>
<ol type="a">
<li>A function of a single random variable</li>
<li>Combinations of random variables, where one converges in distribution and the other converges in probability to a constant</li>
<li>The sample mean of a sequence of random variables</li>
<li>The sample variance of a sequence of random variables</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch08mcsolution15">MC Solution 15</a></p>
<p>The <strong>Delta method</strong> is used to approximate the distribution of:</p>
<ol type="a">
<li>A function of a random variable</li>
<li>The sum of independent random variables</li>
<li>The product of independent random variables</li>
<li>The ratio of independent random variables</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch08mcsolution16">MC Solution 16</a></p>
<p>For a sequence of random vectors <span class="math inline">\((X_n, Y_n)\)</span> to converge in probability to <span class="math inline">\((X, Y)\)</span>, it is sufficient that:</p>
<ol type="a">
<li><span class="math inline">\(X_n\)</span> converges in probability to <span class="math inline">\(X\)</span></li>
<li><span class="math inline">\(Y_n\)</span> converges in probability to <span class="math inline">\(Y\)</span></li>
<li>Both <span class="math inline">\(X_n\)</span> converges in probability to <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_n\)</span> converges in probability to <span class="math inline">\(Y\)</span></li>
<li><span class="math inline">\(X_n\)</span> and <span class="math inline">\(Y_n\)</span> are independent</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch08mcsolution17">MC Solution 17</a></p>
<p>The <strong>Cramér-Wald device</strong> is used to:</p>
<ol type="a">
<li>Prove convergence in probability</li>
<li>Prove convergence in distribution for random vectors</li>
<li>Prove convergence in mean square</li>
<li>Prove almost sure convergence</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch08mcsolution18">MC Solution 18</a></p>
<p><strong>Roy’s safety-first rule</strong> in investment involves minimizing:</p>
<ol type="a">
<li>The expected loss</li>
<li>The variance of returns</li>
<li>The probability of a return falling below a certain threshold</li>
<li>The maximum possible loss</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch08mcsolution19">MC Solution 19</a></p>
<p>In the context of the <strong>gambling model</strong> discussed in the text, if <span class="math inline">\(\mu(\omega) &gt; 0\)</span>, what can be said about the gambler’s wealth <span class="math inline">\(W_n\)</span> as <span class="math inline">\(n\)</span> approaches infinity?</p>
<ol type="a">
<li><span class="math inline">\(W_n\)</span> converges to 0 with probability one</li>
<li><span class="math inline">\(W_n\)</span> converges to a constant with probability one</li>
<li><span class="math inline">\(W_n\)</span> converges to infinity with probability one</li>
<li>The behavior of <span class="math inline">\(W_n\)</span> cannot be determined</li>
</ol>
</section>
<section class="level3" id="sec-ch08mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch08mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch08mcsolution20">MC Solution 20</a></p>
<p><strong>Bernstein’s inequality</strong> provides a bound on the:</p>
<ol type="a">
<li>Expected value of a random variable</li>
<li>Variance of a random variable</li>
<li>Probability that the sample mean deviates from the true mean by more than a certain value</li>
<li>Mean squared error of an estimator</li>
</ol>
</section>
</section>
<section class="level2" id="solutions-1">
<h2 class="anchored" data-anchor-id="solutions-1">Solutions</h2>
<section class="level3" id="sec-ch08mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch08mcexercise1">MC Exercise 1</a></p>
<p><strong>Correct Answer:</strong> (a) <span class="math inline">\(E|X + Y| \leq E|X| + E|Y|\)</span></p>
<p><strong>Explanation:</strong></p>
<p>The <strong>triangle inequality</strong> for expectations, as stated in <strong>Theorem 8.1</strong> of the text, is given by:</p>
<p><span class="math display">\[
E|X + Y| \leq E|X| + E|Y|
\]</span></p>
<p>This inequality states that the expectation of the absolute value of the sum of two random variables is less than or equal to the sum of the expectations of their absolute values.</p>
<p><strong>(b)</strong> is the <strong>Cauchy-Schwarz inequality</strong>. <strong>(c)</strong> is <strong>Markov’s inequality</strong>. <strong>(d)</strong> is <strong>Chebyshev’s inequality</strong>.</p>
</section>
<section class="level3" id="sec-ch08mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch08mcexercise2">MC Exercise 2</a></p>
<p><strong>Correct Answer:</strong> (b) <span class="math inline">\((E(XY))^2 \leq E(X^2)E(Y^2)\)</span></p>
<p><strong>Explanation:</strong></p>
<p>The <strong>Cauchy-Schwarz inequality</strong>, as stated in <strong>Theorem 8.2</strong> of the text, is given by:</p>
<p><span class="math display">\[
(E(XY))^2 \leq E(X^2)E(Y^2)
\]</span></p>
<p>This inequality provides an upper bound for the squared expectation of the product of two random variables in terms of the product of their second moments.</p>
<p><strong>(a)</strong> is the <strong>triangle inequality</strong>. <strong>(c)</strong> is <strong>Markov’s inequality</strong>. <strong>(d)</strong> is <strong>Chebyshev’s inequality</strong>.</p>
</section>
<section class="level3" id="sec-ch08mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch08mcexercise3">MC Exercise 3</a></p>
<p><strong>Correct Answer:</strong> (a) <span class="math inline">\(\operatorname{Pr}(X \geq a)\)</span> for a non-negative random variable <span class="math inline">\(X\)</span></p>
<p><strong>Explanation:</strong></p>
<p><strong>Markov’s inequality</strong>, as stated in <strong>Theorem 8.4</strong> of the text, provides an upper bound for the probability that a non-negative random variable <span class="math inline">\(X\)</span> is greater than or equal to a positive constant <span class="math inline">\(a\)</span>. It is given by:</p>
<p><span class="math display">\[
\operatorname{Pr}(X \geq a) \leq \frac{E(X)}{a}
\]</span></p>
<p><strong>(b)</strong> is related to <strong>Chebyshev’s inequality</strong>. <strong>(c)</strong> and <strong>(d)</strong> are not directly related to <strong>Markov’s inequality</strong>.</p>
</section>
<section class="level3" id="sec-ch08mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch08mcexercise4">MC Exercise 4</a></p>
<p><strong>Correct Answer:</strong> (b) <span class="math inline">\(\operatorname{Pr}[|X - EX| \geq \eta] \leq \dfrac{\operatorname{var}(X)}{\eta^2}\)</span></p>
<p><strong>Explanation:</strong></p>
<p><strong>Chebyshev’s inequality</strong>, as stated in <strong>Theorem 8.5</strong> of the text, is given by:</p>
<p><span class="math display">\[
\operatorname{Pr}[|X - EX| \geq \eta] \leq \dfrac{\operatorname{var}(X)}{\eta^2}
\]</span></p>
<p>This inequality provides an upper bound for the probability that a random variable deviates from its mean by more than a certain value <span class="math inline">\(\eta\)</span>, in terms of its variance.</p>
<p><strong>(a)</strong> is <strong>Markov’s inequality</strong>. <strong>(c)</strong> is the <strong>Cauchy-Schwarz inequality</strong>. <strong>(d)</strong> is the <strong>triangle inequality</strong>.</p>
</section>
<section class="level3" id="sec-ch08mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch08mcexercise5">MC Exercise 5</a></p>
<p><strong>Correct Answer:</strong> (a) <span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(|X_n - X| &gt; \epsilon) = 0\)</span></p>
<p><strong>Explanation:</strong></p>
<p><strong>Convergence in probability</strong>, as defined in <strong>Definition 8.4</strong> of the text, means that for any small positive value <span class="math inline">\(\epsilon\)</span>, the probability that the difference between <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> is greater than <span class="math inline">\(\epsilon\)</span> approaches zero as <span class="math inline">\(n\)</span> goes to infinity. This is expressed as:</p>
<p><span class="math display">\[
\lim_{n \to \infty} \operatorname{Pr}(|X_n - X| &gt; \epsilon) = 0
\]</span></p>
<p><strong>(b)</strong> describes a concept related to convergence in mean, but not convergence in probability. <strong>(c)</strong> defines <strong>convergence in mean square</strong>. <strong>(d)</strong> defines <strong>convergence in distribution</strong>.</p>
</section>
<section class="level3" id="sec-ch08mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch08mcexercise6">MC Exercise 6</a></p>
<p><strong>Correct Answer:</strong> (c) <span class="math inline">\(\lim_{n \to \infty} E[(X_n - X)^2] = 0\)</span></p>
<p><strong>Explanation:</strong></p>
<p><strong>Convergence in mean square</strong>, as defined in <strong>Definition 8.5</strong> of the text, means that the limit of the expected squared difference between <span class="math inline">\(X_n\)</span> and <span class="math inline">\(X\)</span> is zero as <span class="math inline">\(n\)</span> goes to infinity. This is expressed as:</p>
<p><span class="math display">\[
\lim_{n \to \infty} E[(X_n - X)^2] = 0
\]</span></p>
<p><strong>(a)</strong> defines <strong>convergence in probability</strong>. <strong>(b)</strong> describes a concept related to convergence in mean, but not convergence in mean square. <strong>(d)</strong> defines <strong>convergence in distribution</strong>.</p>
</section>
<section class="level3" id="sec-ch08mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch08mcexercise7">MC Exercise 7</a></p>
<p><strong>Correct Answer:</strong> (d) <span class="math inline">\(\lim_{n \to \infty} \operatorname{Pr}(X_n \leq x) = \operatorname{Pr}(X \leq x)\)</span> for all <span class="math inline">\(x\)</span> where <span class="math inline">\(\operatorname{Pr}(X \leq x)\)</span> is continuous</p>
<p><strong>Explanation:</strong></p>
<p><strong>Convergence in distribution</strong>, as defined in <strong>Definition 8.6</strong> of the text, means that the cumulative distribution function (CDF) of <span class="math inline">\(X_n\)</span> converges to the CDF of <span class="math inline">\(X\)</span> at all points where the CDF of <span class="math inline">\(X\)</span> is continuous. This is expressed as:</p>
<p><span class="math display">\[
\lim_{n \to \infty} \operatorname{Pr}(X_n \leq x) = \operatorname{Pr}(X \leq x)
\]</span></p>
<p>for all <span class="math inline">\(x\)</span> where <span class="math inline">\(F(x) = \operatorname{Pr}(X \leq x)\)</span> is continuous.</p>
<p><strong>(a)</strong> defines <strong>convergence in probability</strong>. <strong>(b)</strong> describes a concept related to convergence in mean. <strong>(c)</strong> defines <strong>convergence in mean square</strong>.</p>
</section>
<section class="level3" id="sec-ch08mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch08mcexercise8">MC Exercise 8</a></p>
<p><strong>Correct Answer:</strong> (a) The true mean of the population</p>
<p><strong>Explanation:</strong></p>
<p>The <strong>Law of Large Numbers</strong>, as stated in <strong>Theorem 8.7</strong> of the text, states that the sample mean of a sequence of independent and identically distributed (i.i.d.) random variables converges in probability (and also almost surely, in the strong version) to the true mean of the population.</p>
<p><strong>(b)</strong> is not guaranteed by the Law of Large Numbers. <strong>(c)</strong> is related to the <strong>Central Limit Theorem</strong>. <strong>(d)</strong> is not generally true unless the true mean is zero.</p>
</section>
<section class="level3" id="sec-ch08mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch08mcexercise9">MC Exercise 9</a></p>
<p><strong>Correct Answer:</strong> (c) A standard normal distribution</p>
<p><strong>Explanation:</strong></p>
<p>The <strong>Central Limit Theorem</strong>, as stated in <strong>Theorem 8.8</strong> of the text, states that the standardized sum of a sequence of independent and identically distributed (i.i.d.) random variables converges in distribution to a standard normal distribution, <span class="math inline">\(N(0, 1)\)</span>.</p>
<p><strong>(a)</strong> and <strong>(b)</strong> are related to the <strong>Law of Large Numbers</strong>. <strong>(d)</strong> is not generally true.</p>
</section>
<section class="level3" id="sec-ch08mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch08mcexercise10">MC Exercise 10</a></p>
<p><strong>Correct Answer:</strong> (b) The Central Limit Theorem for independent but not identically distributed random variables</p>
<p><strong>Explanation:</strong></p>
<p><strong>Lindeberg’s condition</strong>, as described in <strong>Theorem 8.10</strong> of the text, is a sufficient condition for the <strong>Central Limit Theorem</strong> to hold for independent but not necessarily identically distributed random variables. It ensures that the individual variances of the random variables do not dominate the sum, allowing the CLT to apply.</p>
<p><strong>(a)</strong> The <strong>Law of Large Numbers</strong> has different conditions (e.g., finite mean). <strong>(c)</strong> and <strong>(d)</strong> Convergence in probability and mean square do not directly rely on Lindeberg’s condition.</p>
</section>
<section class="level3" id="sec-ch08mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch08mcexercise11">MC Exercise 11</a></p>
<p><strong>Correct Answer:</strong> (c) Cauchy-Schwarz inequality</p>
<p><strong>Explanation:</strong></p>
<p><strong>Hölder’s inequality</strong>, as stated in <strong>Theorem 8.3</strong> of the text, is a generalization of the <strong>Cauchy-Schwarz inequality</strong>. When <span class="math inline">\(p = q = 2\)</span>, <strong>Hölder’s inequality</strong> reduces to the <strong>Cauchy-Schwarz inequality</strong>.</p>
<p><strong>(a)</strong>, <strong>(b)</strong>, and <strong>(d)</strong> are different inequalities with different purposes.</p>
</section>
<section class="level3" id="sec-ch08mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch08mcexercise12">MC Exercise 12</a></p>
<p><strong>Correct Answer:</strong> (d) Both (a) and (c)</p>
<p><strong>Explanation:</strong></p>
<p><strong>Almost sure convergence</strong> is a stronger form of convergence than both <strong>convergence in probability</strong> and <strong>convergence in distribution</strong>. If a sequence of random variables converges almost surely to a random variable <span class="math inline">\(X\)</span>, it also converges to <span class="math inline">\(X\)</span> in probability and in distribution. This is mentioned in the text following <strong>Definition 8.4</strong>.</p>
<p><strong>(b)</strong> is not implied by almost sure convergence.</p>
</section>
<section class="level3" id="sec-ch08mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch08mcexercise13">MC Exercise 13</a></p>
<p><strong>Correct Answer:</strong> (a) <span class="math inline">\(g(X_n)\)</span> converges in probability to <span class="math inline">\(g(X)\)</span></p>
<p><strong>Explanation:</strong></p>
<p>The <strong>Continuous Mapping Theorem</strong>, as stated in <strong>Theorem 8.11</strong> of the text, states that if <span class="math inline">\(X_n\)</span> converges in probability to <span class="math inline">\(X\)</span> and <span class="math inline">\(g\)</span> is a continuous function, then <span class="math inline">\(g(X_n)\)</span> converges in probability to <span class="math inline">\(g(X)\)</span>. The theorem also covers convergence in distribution, but the question specifically asks about convergence in probability.</p>
<p><strong>(b)</strong> is a correct statement about convergence in distribution, but the question asks about convergence in probability. <strong>(c)</strong> and <strong>(d)</strong> are not guaranteed by the Continuous Mapping Theorem.</p>
</section>
<section class="level3" id="sec-ch08mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch08mcexercise14">MC Exercise 14</a></p>
<p><strong>Correct Answer:</strong> (b) Combinations of random variables, where one converges in distribution and the other converges in probability to a constant</p>
<p><strong>Explanation:</strong></p>
<p><strong>Slutsky’s Theorem</strong>, as stated in <strong>Theorem 8.12</strong> of the text, is useful for determining the limiting distribution of combinations (sums, products, ratios) of random variables, where one sequence converges in distribution and the other converges in probability to a constant.</p>
<p><strong>(a)</strong> The <strong>Delta method</strong> is more suitable for this. <strong>(c)</strong> and <strong>(d)</strong> The <strong>Law of Large Numbers</strong> and <strong>Central Limit Theorem</strong> are more relevant here.</p>
</section>
<section class="level3" id="sec-ch08mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch08mcexercise15">MC Exercise 15</a></p>
<p><strong>Correct Answer:</strong> (a) A function of a random variable</p>
<p><strong>Explanation:</strong></p>
<p>The <strong>Delta method</strong>, as discussed in <strong>Theorem 8.14</strong> and <strong>Corollary 8.1</strong> of the text, is used to approximate the distribution of a function of a random variable, especially when the distribution of the random variable itself is known or can be approximated (e.g., using the CLT).</p>
<p><strong>(b)</strong>, <strong>(c)</strong>, and <strong>(d)</strong> are not the primary applications of the Delta method.</p>
</section>
<section class="level3" id="sec-ch08mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch08mcexercise16">MC Exercise 16</a></p>
<p><strong>Correct Answer:</strong> (c) Both <span class="math inline">\(X_n\)</span> converges in probability to <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_n\)</span> converges in probability to <span class="math inline">\(Y\)</span></p>
<p><strong>Explanation:</strong></p>
<p>For a sequence of random vectors <span class="math inline">\((X_n, Y_n)\)</span> to converge in probability to <span class="math inline">\((X, Y)\)</span>, it is sufficient that both <span class="math inline">\(X_n\)</span> converges in probability to <span class="math inline">\(X\)</span> and <span class="math inline">\(Y_n\)</span> converges in probability to <span class="math inline">\(Y\)</span>. This is mentioned in the text following <strong>Example 8.11</strong>.</p>
<p><strong>(a)</strong> and <strong>(b)</strong> alone are not sufficient. <strong>(d)</strong> Independence is not a requirement for convergence in probability.</p>
</section>
<section class="level3" id="sec-ch08mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch08mcexercise17">MC Exercise 17</a></p>
<p><strong>Correct Answer:</strong> (b) Prove convergence in distribution for random vectors</p>
<p><strong>Explanation:</strong></p>
<p>The <strong>Cramér-Wald device</strong>, as stated in <strong>Theorem 8.13</strong> of the text, is a technique used to prove <strong>convergence in distribution for random vectors</strong>. It states that a sequence of random vectors converges in distribution to a random vector if and only if every linear combination of the components of the random vectors converges in distribution to the corresponding linear combination of the components of the limiting random vector.</p>
<p><strong>(a)</strong>, <strong>(c)</strong>, and <strong>(d)</strong> are not the primary applications of the Cramér-Wald device.</p>
</section>
<section class="level3" id="sec-ch08mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch08mcexercise18">MC Exercise 18</a></p>
<p><strong>Correct Answer:</strong> (c) The probability of a return falling below a certain threshold</p>
<p><strong>Explanation:</strong></p>
<p><strong>Roy’s safety-first rule</strong>, as discussed in <strong>Example 8.1</strong> of the text, is an investment strategy that involves minimizing the probability that the portfolio return falls below a certain threshold (the “disaster level”).</p>
<p><strong>(a)</strong>, <strong>(b)</strong>, and <strong>(d)</strong> are different investment criteria.</p>
</section>
<section class="level3" id="sec-ch08mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch08mcexercise19">MC Exercise 19</a></p>
<p><strong>Correct Answer:</strong> (c) <span class="math inline">\(W_n\)</span> converges to infinity with probability one</p>
<p><strong>Explanation:</strong></p>
<p>In the <strong>gambling model</strong> discussed in the text, if <span class="math inline">\(\mu(\omega) &gt; 0\)</span>, it means the expected growth rate of the gambler’s wealth is positive. In this case, the gambler’s wealth <span class="math inline">\(W_n\)</span> will converge to infinity with probability one as <span class="math inline">\(n\)</span> approaches infinity. This is mentioned in the text following the introduction of the <strong>gambling model</strong> equations.</p>
<p><strong>(a)</strong>, <strong>(b)</strong>, and <strong>(d)</strong> are incorrect in this case.</p>
</section>
<section class="level3" id="sec-ch08mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch08mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch08mcexercise20">MC Exercise 20</a></p>
<p><strong>Correct Answer:</strong> (c) The probability that the sample mean deviates from the true mean by more than a certain value</p>
<p><strong>Explanation:</strong></p>
<p><strong>Bernstein’s inequality</strong>, as stated in <strong>Theorem 8.15</strong> of the text, provides an upper bound on the probability that the sample mean of a sequence of independent and identically distributed random variables deviates from the true mean by more than a certain value. It is a concentration inequality that is particularly useful when dealing with bounded random variables.</p>
<p><strong>(a)</strong>, <strong>(b)</strong>, and <strong>(d)</strong> are not directly related to Bernstein’s inequality.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>