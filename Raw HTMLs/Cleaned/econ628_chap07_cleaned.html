<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 7: Multivariate Random Variables – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap07alt.html" rel="next"/>
<link href="../chapters/chap06.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 7: Multivariate Random Variables</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level2" id="multivariate-distributions">
<h2 class="anchored" data-anchor-id="multivariate-distributions">7.1 MULTIVARIATE DISTRIBUTIONS</h2>
<p>We now consider multiple random variables, <span class="math inline">\(X = (X_1, ..., X_k) \in \mathbb{R}^k\)</span>. This introduces concepts about how the coordinate random variables relate to each other.</p>
<section class="level3" id="definition-7.1">
<h3 class="anchored" data-anchor-id="definition-7.1">Definition 7.1</h3>
<p>A <strong>vector random variable</strong> <span class="math inline">\(X\)</span> is a measurable mapping from the sample space <span class="math inline">\(S\)</span></p>
<p><span class="math display">\[
X : S \rightarrow S_X \subset \mathbb{R}^k
\]</span></p>
<p>such that <span class="math inline">\(A_X = \{A \subset S : X(A) \in B\} = \{X^{-1}(B) : B \in \mathcal{B}\} \subseteq \mathcal{A}\)</span>, where <span class="math inline">\(\mathcal{B}\)</span> is the sigma algebra on <span class="math inline">\(S_X\)</span> and <span class="math inline">\(\mathcal{A}\)</span> is the sigma algebra on <span class="math inline">\(S\)</span>.</p>
<p><strong>Intuitive explanation:</strong> This definition states that a vector random variable is a function that maps outcomes from a sample space to a subset of a k-dimensional Euclidean space. The measurability condition ensures that for any set <span class="math inline">\(B\)</span> in the sigma algebra on <span class="math inline">\(S_X\)</span>, its preimage under <span class="math inline">\(X\)</span> is an event in the sigma algebra on <span class="math inline">\(S\)</span>. In simpler terms, it means that we can assign probabilities to events involving the random vector <span class="math inline">\(X\)</span> in a consistent way.</p>
</section>
<section class="level3" id="example-7.1">
<h3 class="anchored" data-anchor-id="example-7.1">Example 7.1</h3>
<p>Let <span class="math inline">\(S_X = \mathbb{R}^k\)</span>, and let</p>
<p><span class="math display">\[
B = \{(a_1, b_1) \times \cdots \times (a_k, b_k), a_i, b_i \in \mathbb{R}, i = 1, ..., k\}.
\]</span></p>
<p>Then let <span class="math inline">\(\mathcal{A} = \sigma(B)\)</span> consist of all countable unions of members of <span class="math inline">\(B\)</span> and complements thereof. This is called the <strong>product Borel sigma-algebra</strong>. The coordinate random variables <span class="math inline">\(X_i\)</span> have the usual Borel sigma algebra derived by marginalization.</p>
</section>
<section class="level3" id="definition-7.2">
<h3 class="anchored" data-anchor-id="definition-7.2">Definition 7.2</h3>
<p>The <strong>multivariate joint c.d.f.</strong> is defined for all <span class="math inline">\(x = (x_1, . . ., x_k) \in \mathbb{R}^k\)</span></p>
<p><span class="math display">\[
F_X(x) = \text{Pr}(X_1 \leq x_1, X_2 \leq x_2, ..., X_k \leq x_k).
\]</span></p>
<p>The Multivariate c.d.f. has similar coordinate-wise properties to a univariate c.d.f.</p>
<p><strong>Intuitive explanation:</strong> The multivariate joint c.d.f. gives the probability that each random variable <span class="math inline">\(X_i\)</span> in the vector <span class="math inline">\(X\)</span> is less than or equal to the corresponding value <span class="math inline">\(x_i\)</span>. It’s a generalization of the cumulative distribution function (c.d.f.) to multiple dimensions.</p>
</section>
<section class="level3" id="theorem-7.1">
<h3 class="anchored" data-anchor-id="theorem-7.1">Theorem 7.1</h3>
<p>The following properties hold:</p>
<ol type="i">
<li><span class="math inline">\(F_X(x) \leq F_X(x')\)</span> whenever <span class="math inline">\(x \leq x'\)</span> in the multivariate sense, i.e., <span class="math inline">\(x_i \leq x'_i\)</span> for <span class="math inline">\(i = 1, ..., k\)</span>;</li>
<li>It is right continuous: take any sequence <span class="math inline">\(x_n \geq x\)</span> with <span class="math inline">\(x_n \rightarrow x\)</span>, then <span class="math inline">\(F_X(x_n) \rightarrow F_X(x)\)</span> as <span class="math inline">\(n \rightarrow \infty\)</span>;</li>
<li><span class="math inline">\(F_X(x) \rightarrow 0\)</span> as <span class="math inline">\(x \rightarrow (-\infty, -\infty, ..., -\infty)\)</span> and <span class="math inline">\(F_X(x) \rightarrow 1\)</span> as <span class="math inline">\(x \rightarrow (+\infty, +\infty, ..., +\infty)\)</span>.</li>
</ol>
<p><strong>Intuitive explanation:</strong> (i) If each component of <span class="math inline">\(x\)</span> is less than or equal to the corresponding component of <span class="math inline">\(x'\)</span>, then the probability that <span class="math inline">\(X\)</span> is less than or equal to <span class="math inline">\(x\)</span> is less than or equal to the probability that <span class="math inline">\(X\)</span> is less than or equal to <span class="math inline">\(x'\)</span>. (ii) The c.d.f. is continuous from the right, meaning that if we approach a point <span class="math inline">\(x\)</span> from the right, the c.d.f. converges to the value at that point. (iii) As all components of <span class="math inline">\(x\)</span> go to negative infinity, the probability that <span class="math inline">\(X\)</span> is less than or equal to <span class="math inline">\(x\)</span> goes to 0. As all components go to positive infinity, the probability goes to 1.</p>
<p>Suppose that <span class="math inline">\(X = (X_1, X_2)\)</span>, where <span class="math inline">\(X_1 \in \mathbb{R}^{k_1}\)</span> and <span class="math inline">\(X_2 \in \mathbb{R}^{k_2}\)</span>, where <span class="math inline">\(k_1 + k_2 = k\)</span>. The <strong>Marginal c.d.f.</strong> (marginal distributions) of the subset <span class="math inline">\(X_1\)</span>,</p>
<p><span class="math display">\[
F_{X_1}(x_1) = \text{Pr}(X_1 \leq x_1),
\]</span></p>
<p>has a relationship with the joint c.d.f., <span class="math inline">\(F_X(x)\)</span>. In particular</p>
<p><span class="math display">\[
F_{X_1}(x_1) = \lim_{x_2 \rightarrow (+\infty, +\infty, ..., +\infty)} F_X(x_1, x_2). \qquad (7.1)
\]</span></p>
<p>This follows because we may write <span class="math inline">\(\text{Pr}(A) = \text{Pr}(A \cap B) + \text{Pr}(A \cap B^c)\)</span>, where <span class="math inline">\(B = \{X_2 \leq x_2\}\)</span>. Taking the limit as <span class="math inline">\(x_2 \rightarrow \infty\)</span>, the event <span class="math inline">\(B\)</span> increases to the whole sample space, while <span class="math inline">\(B^c\)</span> shrinks to zero. Therefore, the joint probability <span class="math inline">\(\text{Pr}(A \cap B)\)</span> converges to the marginal probability <span class="math inline">\(\text{Pr}(A)\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The marginal c.d.f. of <span class="math inline">\(X_1\)</span> is obtained by considering the joint c.d.f. of <span class="math inline">\((X_1, X_2)\)</span> and letting the components of <span class="math inline">\(x_2\)</span> go to infinity. This effectively removes the dependence on <span class="math inline">\(X_2\)</span>, leaving only the distribution of <span class="math inline">\(X_1\)</span>.</p>
<p>We next turn to the joint p.m.f. and p.d.f. It is possible to define the density function in a general way with respect to some measure on <span class="math inline">\(\mathbb{R}^k\)</span>, but we shall just consider the two polar cases.</p>
<p>In the discrete case the p.m.f. is defined as before</p>
<p><span class="math display">\[
f_X(x) = \text{Pr}(X = x) \qquad (7.2)
\]</span></p>
<p>for <span class="math inline">\(x \in \mathbb{R}^k\)</span>. In the continuous case, we may define the p.d.f. through its relation with the c.d.f. as follows</p>
<p><span class="math display">\[
F_X(x) = \int_{-\infty}^{x_1} \cdots \int_{-\infty}^{x_k} f_X(z_1, ..., z_k) dz_1 \cdots dz_k.
\]</span></p>
<p>We can then compute this integral recursively by first computing <span class="math inline">\(g_1(x_2, ..., x_k) = \int_{-\infty}^{x_1} f_X(x) dx_1\)</span> and then repeating the process. Under technical conditions [Fubini’s Theorem], we can reverse the order of integration, which may be useful for some computations.</p>
</section>
<section class="level3" id="theorem-7.2">
<h3 class="anchored" data-anchor-id="theorem-7.2">Theorem 7.2</h3>
<p>Suppose that the joint c.d.f. <span class="math inline">\(F\)</span> is <span class="math inline">\(k\)</span>-times partially differentiable at the point <span class="math inline">\(x = (x_1, ..., x_k) \in \mathbb{R}^k\)</span>. Then, the joint density function <span class="math inline">\(f\)</span> satisfies</p>
<p><span class="math display">\[
f_X(x) = \frac{\partial^k F_X(x_1, ..., x_k)}{\partial x_1, ..., \partial x_k}.
\]</span></p>
<p><strong>Proof:</strong> This is shown recursively. For <span class="math inline">\(l = 1, 2, . . ., k\)</span>, let,</p>
<p><span class="math display">\[
g_{1...l}(x) = \int_{-\infty}^{x_{l+1}} \cdots \int_{-\infty}^{x_k} f_X(x_1, ..., x_l, z_{l+1}, ..., z_k) dz_{l+1} \cdots dz_k.
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\begin{aligned}
F_X(x) &amp;= \int_{-\infty}^{x_1} g_1(z_1, x_2, ..., x_k) dz_1 \\
\frac{\partial F_X}{\partial x_1}(x) &amp;= g_1(x) = \int_{-\infty}^{x_2} \cdots \int_{-\infty}^{x_k} f_X(x_1, z_2, ..., z_k) dz_2 \cdots dz_k \\
&amp;= \int_{-\infty}^{x_2} g_{12}(x_1, z_2, x_3, ..., x_k) dz_2.
\end{aligned}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\frac{\partial^2 F_X}{\partial x_1 \partial x_2}(x) = \frac{\partial g_1(x)}{\partial x_2} = g_{12}(x),
\]</span></p>
<p>and so on. <span class="math inline">\(\square\)</span></p>
<p><strong>Intuitive explanation:</strong> This theorem states that if the joint c.d.f. is sufficiently smooth (k-times partially differentiable), then we can obtain the joint p.d.f. by taking the mixed partial derivative of the c.d.f. with respect to each variable.</p>
<p>Note that Lebesgue measure on <span class="math inline">\(\mathbb{R}^k\)</span> puts zero volume on all sets that are of lower dimension. It follows that for a continuous random variable <span class="math inline">\(X\)</span> (that is absolutely continuous with respect to Lebesgue measure on <span class="math inline">\(\mathbb{R}^k\)</span>) <span class="math inline">\(\text{Pr}(X = x) = 0\)</span> for any point <span class="math inline">\(x \in \mathbb{R}^k\)</span>, but even more holds. In fact,</p>
<p><span class="math display">\[
\text{Pr}(X \in \{x : f_1(x_1, ..., x_k) = 0, . . ., f_r(x_1, . . ., x_k) = 0\}) = 0 \qquad (7.3)
\]</span></p>
<p>for any measurable functions <span class="math inline">\(f_j : \mathbb{R}^k \rightarrow \mathbb{R}\)</span> for <span class="math inline">\(j = 1, ..., r\)</span> with <span class="math inline">\(r \geq 1\)</span>.</p>
</section>
<section class="level3" id="example-7.2">
<h3 class="anchored" data-anchor-id="example-7.2">Example 7.2</h3>
<p>Suppose that <span class="math inline">\(X = (X_1, X_2)\)</span> are continuously distributed on <span class="math inline">\(\mathbb{R}^2\)</span>, i.e., absolutely continuous with respect to Lebesgue measure on <span class="math inline">\(\mathbb{R}^2\)</span>. Then</p>
<p><span class="math display">\[
\text{Pr}(X_1^2 + X_2^2 = r) = 0
\]</span></p>
<p>for any <span class="math inline">\(r &gt; 0\)</span>. The probability that <span class="math inline">\(X\)</span> lies on the circumference of a circle is zero.</p>
<p>Suppose that <span class="math inline">\(X = (X_1, X_2)\)</span>, where <span class="math inline">\(X_1 \in \mathbb{R}^{k_1}\)</span> and <span class="math inline">\(X_2 \in \mathbb{R}^{k_2}\)</span>, where <span class="math inline">\(k_1 + k_2 = k\)</span>. The marginal densities of the subset <span class="math inline">\(X_1\)</span> have a relationship with the joint densities.</p>
</section>
<section class="level3" id="theorem-7.3">
<h3 class="anchored" data-anchor-id="theorem-7.3">Theorem 7.3</h3>
<p>For any joint p.m.f. or p.d.f. function, the functions</p>
<p><span class="math display">\[
f_{X_1}(x_1) = \sum_{x_2} f_X(x_1, x_2)
\]</span> <!-- PF NOTE: dropped one of the integrals --> <span class="math display">\[
f_{X_1}(x_1) = \int_{-\infty}^{\infty} f(x_1, x_2) dx_2
\]</span></p>
<p>are themselves mass functions or density functions, and are called the <strong>marginals</strong>.</p>
<p><strong>Intuitive explanation:</strong> The marginal p.m.f. or p.d.f. of <span class="math inline">\(X_1\)</span> is obtained by summing or integrating the joint p.m.f. or p.d.f. over all possible values of <span class="math inline">\(X_2\)</span>. This effectively eliminates the dependence on <span class="math inline">\(X_2\)</span>, leaving only the distribution of <span class="math inline">\(X_1\)</span>.</p>
<p>The proof of this result is straightforward. This result emphasizes that a random variable <span class="math inline">\(X_1\)</span> might be part of a bigger picture involving other random variables <span class="math inline">\(X_2\)</span>. It is easy to go from the joint distribution to the marginal but not in the other direction. The joint distribution can not be determined from the marginals alone: knowing <span class="math inline">\(f_{X_1}\)</span> and <span class="math inline">\(f_{X_2}\)</span> does not determine <span class="math inline">\(f_X\)</span> uniquely unless <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent.</p>
</section>
<section class="level3" id="example-7.3">
<h3 class="anchored" data-anchor-id="example-7.3">Example 7.3</h3>
<p>We return to a population version of the stock example. Suppose that <span class="math inline">\(X\)</span> is yesterday’s stock market outcome and <span class="math inline">\(Y\)</span> is today’s outcome with <span class="math inline">\(X = +1\)</span> or <span class="math inline">\(X = -1\)</span> and <span class="math inline">\(Y = +1\)</span> or <span class="math inline">\(Y = -1\)</span> with the following probabilities</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: left;"></th>
<th style="text-align: left;">Y = 1</th>
<th style="text-align: left;">Y = -1</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">X = 1</td>
<td style="text-align: left;"><span class="math inline">\(\pi_{1,1}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\pi_{1,-1}\)</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">X = -1</td>
<td style="text-align: left;"><span class="math inline">\(\pi_{-1,1}\)</span></td>
<td style="text-align: left;"><span class="math inline">\(\pi_{-1,-1}\)</span></td>
</tr>
</tbody>
</table>
<p>The table shows us</p>
<p><span class="math display">\[
\text{Pr}(X = i, Y = j) = \pi_{i,j},
\]</span></p>
<p>which satisfy <span class="math inline">\(\pi_{i,j} \in [0, 1]\)</span> for all <span class="math inline">\(i, j\)</span>, and <span class="math inline">\(\pi_{1,1} + \pi_{1,-1} + \pi_{-1,1} + \pi_{-1,-1} = 1\)</span>. The marginal probabilities are</p>
<p><span class="math display">\[
\begin{aligned}
p_1 &amp;= \text{Pr}(X = 1) = \pi_{1,1} + \pi_{1,-1} \\
p_{-1} &amp;= \text{Pr}(X = -1) = \pi_{-1,1} + \pi_{-1,-1} = 1 - p_1 \\
q_1 &amp;= \text{Pr}(Y = 1) = \pi_{1,1} + \pi_{-1,1} \\
q_{-1} &amp;= \text{Pr}(Y = -1) = \pi_{1,-1} + \pi_{-1,-1} = 1 - q_1.
\end{aligned}
\]</span></p>
<p>On the other hand, knowing <span class="math inline">\(p_1, p_{-1}, q_1\)</span>, and <span class="math inline">\(q_{-1}\)</span> does not determine <span class="math inline">\(\pi_{1,1}, \pi_{1,-1}, \pi_{-1,1}\)</span>, and <span class="math inline">\(\pi_{-1,-1}\)</span> uniquely. This can be seen by a straight count. There are three freely varying quantities in the <span class="math inline">\(\pi\)</span>’s but only two freely varying quantities amongst the <span class="math inline">\(p_1, p_{-1}, q_1\)</span>, and <span class="math inline">\(q_{-1}\)</span>.</p>
</section>
</section>
<section class="level2" id="conditional-distributions-and-independence">
<h2 class="anchored" data-anchor-id="conditional-distributions-and-independence">7.2 CONDITIONAL DISTRIBUTIONS AND INDEPENDENCE</h2>
<p>We defined conditional probability <span class="math inline">\(P(A|B) = P(A \cap B) / P(B)\)</span> for events <span class="math inline">\(A, B\)</span> with <span class="math inline">\(P(B) \neq 0\)</span>. We now want to define the conditional distribution of <span class="math inline">\(Y|X\)</span>. A general definition would be</p>
<p><span class="math display">\[
P(A|X = x) = \lim_{\epsilon \rightarrow 0} \frac{P(A \cap B_\epsilon)}{P(B_\epsilon)}, \quad B_\epsilon = \{X \in [x - \epsilon, x + \epsilon]\}. \qquad (7.4)
\]</span></p>
<p>We focus on the two leading cases where simple definitions are available. In the case where both <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are discrete we may define the conditional p.m.f. as</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = \text{Pr}(Y = y|X = x) = \frac{f_{Y,X}(y, x)}{f_X(x)},
\]</span></p>
<p>when the event <span class="math inline">\(\{X = x\}\)</span> has non-zero probability. Likewise, we can define the conditional c.d.f. as</p>
<p><span class="math display">\[
F_{Y|X}(y|x) = \text{Pr}(Y \leq y|X = x) = \frac{\sum_{y \leq y} f(y, x)}{f_X(x)}.
\]</span></p>
<p>Note that <span class="math inline">\(f_{Y|X}(y|x)\)</span> is a p.m.f. and <span class="math inline">\(F_{Y|X}(y|x)\)</span> is a c.d.f., i.e.,</p>
<section class="level3" id="theorem-7.4">
<h3 class="anchored" data-anchor-id="theorem-7.4">Theorem 7.4</h3>
<p>For all <span class="math inline">\(x\)</span><br/>
(1) <span class="math inline">\(f_{Y|X}(y|x) \geq 0\)</span> all <span class="math inline">\(y\)</span>;<br/>
(2) <span class="math inline">\(\sum_y f_{Y|X}(y|x) = \frac{\sum_y f(y,x)}{f_X(x)} = \frac{f_X(x)}{f_X(x)} = 1\)</span>.<br/>
</p>
<p>In the continuous case, it appears a bit anomalous to talk about the <span class="math inline">\(\text{Pr}(y \in A|X = x)\)</span>, since <span class="math inline">\(\{X = x\}\)</span> itself has zero probability of occurring. Still, we define the conditional density function</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = \frac{f_{Y,X}(y, x)}{f_X(x)} \qquad (7.5)
\]</span></p>
<p>in terms of the joint and marginal densities. It turns out that <span class="math inline">\(f_{Y|X}(y|x)\)</span> has the properties of a p.d.f.:</p>
</section>
<section class="level3" id="theorem-7.5">
<h3 class="anchored" data-anchor-id="theorem-7.5">Theorem 7.5</h3>
<p>For all <span class="math inline">\(x\)</span>,<br/>
(1) <span class="math inline">\(f_{Y|X}(y|x) \geq 0\)</span> for all <span class="math inline">\(y\)</span>;<br/>
(2) <span class="math inline">\(\int_{-\infty}^{\infty} f_{Y|X}(y|x) dy = \frac{\int_{-\infty}^{\infty} f_{Y,X}(y,x) dy}{f_X(x)} = \frac{f_X(x)}{f_X(x)} = 1\)</span>.<br/>
</p>
<p>We may define the conditional c.d.f. through the conditional density as</p>
<p><span class="math display">\[
F_{Y|X}(y|x) = \int_{-\infty}^{y} f_{Y|X}(y'|x) dy'.
\]</span></p>
<p>We may also start with a conditional c.d.f. and infer the existence of the conditional density through the integration formula. We may show <span class="math inline">\(\text{Pr}(Y \in [a, b]|X = x) = F_{Y|X}(b|x) - F_{Y|X}(a|x)\)</span>.</p>
<p>The relationship between the joint density and the conditional density</p>
<p><span class="math display">\[
f_{Y,X}(y, x) = f_{Y|X}(y|x) f_X(x) \qquad (7.6)
\]</span></p>
<p>turns out to be very useful in modelling and time series in particular. It generalizes to any number of random variables so that</p>
<p><span class="math display">\[
f_{Y,X,Z}(y, x, z) = f_{Y|X,Z}(y|x, z) f_{X|Z}(x|z) f_Z(z)
\]</span></p>
<p>etc. In some case we may specify a model for the conditional density and the marginal density and this relation implies the joint density, which itself is of central interest. It also shows a way that we might simulate outcomes on <span class="math inline">\(y, x\)</span> by first simulating <span class="math inline">\(x\)</span> from its marginal and then drawing <span class="math inline">\(y\)</span> from the conditional distribution. This sequential approach is very convenient in complicated situations.</p>
<p>We may also define a version of Bayes theorem for densities:</p>
<p><span class="math display">\[
f_{X|Y}(x|y) = \frac{f_{Y|X}(y|x) f_X(x)}{f_Y(y)} = \frac{f_{Y|X}(y|x) f_X(x)}{\int f_{Y,X}(y, x) dx} \qquad (7.7)
\]</span></p>
<p>This formula is very useful in statistical inference questions. It is also useful for the sort of signal extraction and learning problems widely encountered in economics.</p>
</section>
<section class="level3" id="definition-7.3">
<h3 class="anchored" data-anchor-id="definition-7.3">Definition 7.3</h3>
<p><strong>INDEPENDENCE.</strong> We say that <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are independent [denoted <span class="math inline">\(Y \perp X\)</span>] if</p>
<p><span class="math display">\[
\text{Pr}(Y \in A, X \in B) = \text{Pr}(Y \in A) \text{Pr}(x \in B)
\]</span></p>
<p>for all events <span class="math inline">\(A, B\)</span> in the relevant sigma-algebras. This is equivalent to the following definition in terms of the c.d.f.’s, which is simpler to state and apply:</p>
<p><span class="math display">\[
F_{Y,X}(y, x) = F_Y(y) \cdot F_X(x)
\]</span></p>
<p>for all <span class="math inline">\(x, y \in \mathbb{R}\)</span>.</p>
</section>
<section class="level3" id="example-7.4">
<h3 class="anchored" data-anchor-id="example-7.4">Example 7.4</h3>
<p>Stock market example again. The condition for independence is</p>
<p><span class="math display">\[
\text{Pr}(X = i, Y = j) = \text{Pr}(X = i) \text{Pr}(Y = j) \qquad (7.8)
\]</span></p>
<p>for all <span class="math inline">\(i, j \in \{-1, 1\}\)</span>. This requires</p>
<p><span class="math display">\[
\pi_{i,j} = (\pi_{i,j} + \pi_{i,-j}) \times (\pi_{i,j} + \pi_{-i,j})
\]</span></p>
<p>for all <span class="math inline">\(i, j\)</span>, although given the other restrictions it is sufficient to check this condition for only one combination. Under independence, there are only two free parameters in the <span class="math inline">\(\pi\)</span>’s, say <span class="math inline">\(\pi_{1,1}\)</span> and <span class="math inline">\(\pi_{-1,-1}\)</span>, since then</p>
<p><span class="math display">\[
\begin{aligned}
\pi_{1,-1} &amp;= (\pi_{1,1} + \pi_{1,-1}) \times (\pi_{1,-1} + \pi_{-1,-1}) \\
&amp;= \pi_{1,-1} (\pi_{-1,-1} + \pi_{1,1}) + \pi_{-1,-1} \pi_{1,1} + \pi_{1,-1}^2
\end{aligned}
\]</span></p>
<p>so that <span class="math inline">\(\pi_{1,-1}\)</span> satisfies the quadratic equation</p>
<p><span class="math display">\[
\pi_{1,-1}^2 + \pi_{1,-1} (\pi_{-1,-1} + \pi_{1,1} - 1) + \pi_{-1,-1} \pi_{1,1} = 0. \qquad (7.9)
\]</span></p>
<p>Note that <span class="math inline">\(\pi_{-1,-1} + \pi_{1,1} - 1 \leq 0\)</span>. In fact, <span class="math inline">\(\pi_{-1,-1} + \pi_{1,1} - 1 &lt; 0\)</span>, otherwise <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> cannot be independent. Eq. (7.9) has a unique solution if and only if there is independence. For example, suppose that <span class="math inline">\(\pi_{-1,-1} = \pi_{1,1} = 1/4\)</span>, then the unique solution to (7.9) is <span class="math inline">\(\pi_{1,-1} = 1/4\)</span>.</p>
<p>A special case of interest for sampling is where <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are independent and identically distributed (i.i.d.), i.e., <span class="math inline">\(F_Y(x) = F_X(x) = F(x)\)</span>, say, so that</p>
<p><span class="math display">\[
F_{Y,X}(y, x) = F(y) F(x).
\]</span></p>
<p>We sometimes work with the density (or p.m.f. for discrete random variables) version of this concept.</p>
</section>
<section class="level3" id="definition-7.4">
<h3 class="anchored" data-anchor-id="definition-7.4">Definition 7.4</h3>
<p>We say that <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> are independent if either of the following conditions hold:<br/>
(1) <span class="math inline">\(f(y, x) = f_Y(y) \cdot f_X(x)\)</span> for all <span class="math inline">\(y, x\)</span>;<br/>
(2) <span class="math inline">\(f_{Y|X}(y|x) = f_Y(y)\)</span> for all <span class="math inline">\(x\)</span>;<br/>
(3) <span class="math inline">\(f_{X|Y}(x|y) = f_X(x)\)</span> for all <span class="math inline">\(y\)</span>.<br/>
</p>
<p>Note that if <span class="math inline">\(Y\)</span> is independent of <span class="math inline">\(X\)</span>, then <span class="math inline">\(g(X)\)</span> is independent of <span class="math inline">\(h(Y)\)</span> for any measurable functions <span class="math inline">\(g\)</span> and <span class="math inline">\(h\)</span>.</p>
<p>Suppose that <span class="math inline">\(Y = g(X)\)</span> for some measurable transformation <span class="math inline">\(g\)</span>, then <span class="math inline">\(Y\)</span> is “perfectly dependent” on <span class="math inline">\(X\)</span>, in the sense that knowledge of <span class="math inline">\(X\)</span> determines <span class="math inline">\(Y\)</span>. Of course it may be that knowledge of <span class="math inline">\(Y\)</span> does not perfectly determine <span class="math inline">\(X\)</span>: for example, if <span class="math inline">\(g(x) = \cos(x)\)</span> or if <span class="math inline">\(g(x) = \mathbf{1}(x &gt; 0)\)</span>.</p>
<p>We now consider the relationship between two (or more) random variables when they are not independent. In this case, conditional densities <span class="math inline">\(f_{Y|X}\)</span> and c.d.f.’s <span class="math inline">\(F_{Y|X}\)</span> in general vary with the conditioning point <span class="math inline">\(x\)</span>. Likewise for the conditional mean <span class="math inline">\(E(Y|X)\)</span>, the conditional median <span class="math inline">\(M(Y|X)\)</span>, conditional variance <span class="math inline">\(\text{var}(Y|X)\)</span>, conditional characteristic function <span class="math inline">\(E(e^{itY}|X)\)</span>, and other functionals, all of which characterize the relationship between <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span>. We first consider the notion of covariance.</p>
</section>
</section>
<section class="level2" id="covariance">
<h2 class="anchored" data-anchor-id="covariance">7.3 COVARIANCE</h2>
<section class="level3" id="definition-7.5">
<h3 class="anchored" data-anchor-id="definition-7.5">Definition 7.5</h3>
<p>Let <span class="math inline">\(X, Y\)</span> be random variables with finite variance. Then</p>
<p><span class="math display">\[
\text{cov}(X, Y) = E[\{X - E(X)\}\{Y - E(Y)\}] = E(YX) - \mu_X \mu_Y.
\]</span></p>
<p>If <span class="math inline">\(X\)</span> or <span class="math inline">\(Y\)</span> are constant, then <span class="math inline">\(\text{cov}(X, Y) = 0\)</span>. Note that</p>
<p><span class="math display">\[
\text{cov}(aX + c, bY + d) = ab \text{cov}(X, Y),
\]</span></p>
<p>so that covariance is affected by <span class="math inline">\(a, b\)</span> but not by <span class="math inline">\(c, d\)</span>. The covariance measures how much <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> move together: <span class="math inline">\(\sigma_{XY} &gt; 0\)</span> corresponds to positive movement, <span class="math inline">\(\sigma_{XY} &lt; 0\)</span> corresponds to negative movement.</p>
<p>The covariance is related to variance in the following way. We have</p>
<p><span class="math display">\[
\begin{aligned}
\text{var}(X + Y) &amp;= E[(X - EX + Y - EY)^2] \\
&amp;= E[(X - EX)^2] + E[(Y - EY)^2] + 2E[(X - EX)(Y - EY)] \\
&amp;= \text{var}(X) + \text{var}(Y) + 2\text{cov}(X, Y). \qquad (7.10)
\end{aligned}
\]</span></p>
<p>If <span class="math inline">\(\text{cov}(X, Y) = 0\)</span>, then <span class="math inline">\(\text{var}(X + Y) = \text{var}(X) + \text{var}(Y)\)</span>. If <span class="math inline">\(\text{cov}(X, Y) &gt; 0\)</span>, then <span class="math inline">\(\text{var}(X + Y) &gt; \text{var}(X) + \text{var}(Y)\)</span>. If <span class="math inline">\(\text{cov}(X, Y) &lt; 0\)</span>, then <span class="math inline">\(\text{var}(X + Y) &lt; \text{var}(X) + \text{var}(Y)\)</span>. For example, suppose that <span class="math inline">\(X\)</span> represents stock returns today and <span class="math inline">\(Y\)</span> represents stock returns tomorrow. Then, the argument above says that the variance of two period returns is greater than, equal to, or lesser than the sum of the variances of the one period returns, depending on the covariance between the two random returns. Under the efficient markets hypothesis we would have the implication that <span class="math inline">\(\text{cov}(X, Y) = 0\)</span>, and the variance of the sum is the sum of the variances.</p>
<p>An alternative measure of association is given by the correlation coefficient.</p>
</section>
<section class="level3" id="definition-7.6">
<h3 class="anchored" data-anchor-id="definition-7.6">Definition 7.6</h3>
<p>Let <span class="math inline">\(X, Y\)</span> be random variables with finite variance. Then</p>
<p><span class="math display">\[
\rho_{XY} = \text{corr}(X, Y) = \frac{\text{cov}(X, Y)}{\sigma_X \sigma_Y}.
\]</span></p>
<p>This satisfies <span class="math inline">\(-1 \leq \rho_{XY} \leq 1\)</span>. The correlation coefficient is invariant, in magnitude, to the scale in which <span class="math inline">\(X, Y\)</span> are measured, so that</p>
<p><span class="math display">\[
\rho_{aX+b, cY+d} = \text{sign}(a) \text{sign}(c) \rho_{XY}
\]</span></p>
<p>for any <span class="math inline">\(a, b, c, d\)</span>. However, the correlation coefficient is not invariant to nonlinear transformations, i.e.,</p>
<p><span class="math display">\[
\rho_{g(X), h(Y)} \neq \rho_{XY} \quad \text{for arbitrary } X, Y.
\]</span></p>
</section>
<section class="level3" id="theorem-7.6">
<h3 class="anchored" data-anchor-id="theorem-7.6">Theorem 7.6</h3>
<p>We have <span class="math inline">\(\text{corr}(X, Y) = \pm 1\)</span> if and only if <span class="math inline">\(Y = aX + b\)</span> for some constants <span class="math inline">\(a, b\)</span> with <span class="math inline">\(a \neq 0\)</span>.</p>
<p><strong>Proof:</strong> If <span class="math inline">\(Y = aX + b\)</span>, then it is straightforward to show that <span class="math inline">\(\text{corr}(X, Y) = \pm 1\)</span> with the sign determined by the sign of <span class="math inline">\(a\)</span>. What about the converse? This will be a consequence of the Cauchy-Schwarz inequality, which we give below. <span class="math inline">\(\square\)</span></p>
<p>This says that nonlinear functions of <span class="math inline">\(X\)</span> can’t be perfectly correlated with <span class="math inline">\(X\)</span> itself. The intuition is that correlation is about linear relationships, so any nonlinear function must have an imperfect correlation with its argument. Covariance is a weaker property than independence, since independence is equivalent to the lack of covariance between any functions of the two random variables.</p>
</section>
<section class="level3" id="theorem-7.7">
<h3 class="anchored" data-anchor-id="theorem-7.7">Theorem 7.7</h3>
<p><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables if and only if</p>
<p><span class="math display">\[
\text{cov}(g(X), h(Y)) = 0,
\]</span></p>
<p>for all functions <span class="math inline">\(g, h\)</span> for which the covariance is defined. Specifically, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, then <span class="math inline">\(\text{cov}(X, Y) = 0\)</span>.</p>
<p><strong>Proof:</strong> The second claim follows because:</p>
<p><span class="math display">\[
\begin{aligned}
E(g(X)h(Y)) &amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x)h(y) f(y, x) dy dx \\
&amp;= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} g(x)h(y) f(y) f(x) dy dx \\
&amp;= \int_{-\infty}^{\infty} g(x) f_X(x) dx \cdot \int_{-\infty}^{\infty} h(y) f_Y(y) dy = E(g(X))E(h(Y)).
\end{aligned}
\]</span></p>
<p>The reverse direction is not proved here. <span class="math inline">\(\square\)</span></p>
<p>Here is an example that illustrates the fact that uncorrelated random variables need not be independent.</p>
</section>
<section class="level3" id="example-7.5">
<h3 class="anchored" data-anchor-id="example-7.5">Example 7.5</h3>
<p>Suppose that <span class="math inline">\(X = \cos \theta\)</span> and <span class="math inline">\(Y = \sin \theta\)</span>, where <span class="math inline">\(\theta\)</span> is uniform on <span class="math inline">\([0, 2\pi]\)</span>. Then <span class="math inline">\(Y^2 = 1 - X^2\)</span> so <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are functionally related and not independent. Since <span class="math inline">\(\theta\)</span> is uniform on <span class="math inline">\([0, 2\pi]\)</span>, its probability density function is <span class="math display">\[ f_\theta(\theta) = \begin{cases} \frac{1}{2\pi} &amp; 0 \le \theta \le 2\pi \\ 0 &amp; \text{otherwise} \end{cases} \]</span> and</p>
<p><span class="math display">\[
\text{cov}(X, Y) = \int_{0}^{2\pi} \cos(\theta) \sin(\theta) \frac{1}{2\pi} d\theta - \int_{0}^{2\pi} \cos(\theta) \frac{1}{2\pi} d\theta \times \int_{0}^{2\pi} \sin(\theta) \frac{1}{2\pi} d\theta = 0
\]</span> so that the two random variables are uncorrelated. We have <span class="math inline">\(X^2 + Y^2 = \cos^2 \theta + \sin^2 \theta = 1\)</span>, so the joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is concentrated on the unit circle.</p>
<p>For a given value of <span class="math inline">\(x\)</span>, where <span class="math inline">\(-1 &lt; x &lt; 1\)</span>, there are two possible values of <span class="math inline">\(\theta\)</span> in <span class="math inline">\([0, 2\pi]\)</span> such that <span class="math inline">\(\cos \theta = x\)</span>. Let these values be <span class="math inline">\(\theta_1\)</span> and <span class="math inline">\(\theta_2 = 2\pi - \theta_1\)</span>, where <span class="math inline">\(\theta_1 = \arccos x\)</span>. Then we have <span class="math inline">\(y_1 = \sin \theta_1 = \sqrt{1 - x^2}\)</span> and <span class="math inline">\(y_2 = \sin \theta_2 = \sin(2\pi - \theta_1) = -\sin \theta_1 = -\sqrt{1 - x^2}\)</span>.</p>
<p>For a fixed <span class="math inline">\(x \in (-1, 1)\)</span>, we have two possible values for <span class="math inline">\(y\)</span>, namely <span class="math inline">\(y = \pm \sqrt{1 - x^2}\)</span>. The conditional distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> is discrete, with each value having probability <span class="math inline">\(\frac{1}{2}\)</span>.</p>
<p>In this case, <span class="math inline">\(f_{Y|X}\)</span> is discrete, and we can write the conditional probability mass function as:</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = P(Y = y | X = x) = \begin{cases}
\frac{1}{2} &amp; \text{if } y = \sqrt{1 - x^2} \\
\frac{1}{2} &amp; \text{if } y = -\sqrt{1 - x^2} \\
0 &amp; \text{else}
\end{cases}
\]</span> Since <span class="math inline">\(Y\)</span> given <span class="math inline">\(X=x\)</span> takes only two values, there is no conditional density in the continuous sense.</p>
</section>
<section class="level3" id="example-7.6">
<h3 class="anchored" data-anchor-id="example-7.6">Example 7.6</h3>
<p>Correlation is a useful and widely used concept, but it is important to distinguish correlation from causation. There are many examples of spurious correlations. <a href="http://www.tylervigen.com/spurious-correlations">http://www.tylervigen.com/spurious-correlations</a> US spending on science, space, and technology correlates (99.8%) with Suicides by hanging, strangulation and suffocation.</p>
</section>
</section>
<section class="level2" id="conditional-expectation-and-the-regression-function">
<h2 class="anchored" data-anchor-id="conditional-expectation-and-the-regression-function">7.4 CONDITIONAL EXPECTATION AND THE REGRESSION FUNCTION</h2>
<p>The conditional expectation of the random variable <span class="math inline">\(Y\)</span> given the random variable <span class="math inline">\(X\)</span> is an important quantity in much applied statistics/econometrics. Later we will spend a lot of time on empirical regression. We are concerned here with the population quantity. We first give a very general definition of a conditional expectation of a random variable given a sigma field, and then specialize that.</p>
<section class="level3" id="definition-7.7">
<h3 class="anchored" data-anchor-id="definition-7.7">Definition 7.7</h3>
<p>A random variable <span class="math inline">\(Z\)</span> is called the conditional expectation of <span class="math inline">\(Y\)</span> given the sigma-algebra <span class="math inline">\(\mathcal{A}\)</span> (we write <span class="math inline">\(E(Y|\mathcal{A})\)</span>) if<br/>
(1) The sigma algebra generated by <span class="math inline">\(Z\)</span> is contained in <span class="math inline">\(\mathcal{A}\)</span>.<br/>
(2) <span class="math inline">\(Z\)</span> satisfies for all <span class="math inline">\(A \in \mathcal{A}\)</span><br/>
</p>
<p><span class="math display">\[
E(Y \mathbf{1}_A) = E(Z \mathbf{1}_A).
\]</span></p>
<p>One can show the existence and uniqueness of the random variable <span class="math inline">\(Z\)</span> using measure theoretic arguments. This is not a very constructive definition but it does apply very generally to any kind of random variable and any kind of sigma algebra. We are mostly concerned here with the case where the sigma field <span class="math inline">\(\mathcal{A}\)</span> is generated by some random variable <span class="math inline">\(X\)</span>, in which case we write <span class="math inline">\(E(Y|X)\)</span>. Note that unlike covariance, regression is not symmetric in <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, so that <span class="math inline">\(E(X|Y)\)</span> is potentially totally different from <span class="math inline">\(E(Y|X)\)</span>, just like <span class="math inline">\(P(A|B)\)</span> may not equal <span class="math inline">\(P(A|B)\)</span>.</p>
<p>We will next specialize the theory to the polar cases we have mostly considered so far. In general <span class="math inline">\(X\)</span> could be multivariate (<span class="math inline">\(\in \mathbb{R}^k\)</span>) but we just focus on the scalar <span class="math inline">\(k = 1\)</span> case. We can classify the problem into several cases: (1) <span class="math inline">\(Y, X\)</span> discrete; (2) <span class="math inline">\(Y\)</span> is discrete and <span class="math inline">\(X\)</span> is continuous; (3) <span class="math inline">\(Y\)</span> is continuous and <span class="math inline">\(X\)</span> is discrete; (4) <span class="math inline">\(Y, X\)</span> are both continuously distributed. We mostly focus on case (4) for simplicity of presentation. Case (1) contains nothing substantially different and can be analyzed by replacing the integrals by sums and the densities by probability mass functions in the sequel. The case (3) is mostly covered by our analysis below. The case (2) is important and is worth a short additional discussion at the end.</p>
<p>We can define expectations within the (continuous) conditional distribution as follows</p>
<p><span class="math display">\[
m(x) = E(Y|X = x) = \int_{-\infty}^{\infty} y f_{Y|X}(y|x) dy = \frac{\int_{-\infty}^{\infty} y f(y, x) dy}{\int_{-\infty}^{\infty} f(y, x) dy}, \qquad (7.11)
\]</span></p>
<p>provided the integrals are well defined. We let <span class="math inline">\(E(Y|X) = m(X)\)</span> denote the conditional expectation evaluated at the random conditioning variable <span class="math inline">\(X\)</span>, so that <span class="math inline">\(E(Y|X)\)</span> is a random variable, while <span class="math inline">\(E(Y|X = x)\)</span> is a particular realization of that random variable.</p>
<p>Conditional expectation obey similar monotonicity and linearity properties as expectation:</p>
</section>
<section class="level3" id="theorem-7.8">
<h3 class="anchored" data-anchor-id="theorem-7.8">Theorem 7.8</h3>
<p>For random variables <span class="math inline">\(Y_1, Y_2, X\)</span>:<br/>
(1) If <span class="math inline">\(Y_1 \leq Y_2 \Rightarrow E(Y_1|X = x) \leq E(Y_2|X = x)\)</span>;<br/>
(2) For constants <span class="math inline">\(\alpha_1, \alpha_2, \alpha_3\)</span><br/>
</p>
<p><span class="math display">\[
E(\alpha_1 Y_1 + \alpha_2 Y_2 + \alpha_3|X = x) = \alpha_1 E(Y_1|X = x) + \alpha_2 E(Y_2|X = x) + \alpha_3.
\]</span></p>
<p>Note however that <span class="math inline">\(E(X|\alpha_1 Y_1 + \alpha_2 Y_2)\)</span> may have no relation to <span class="math inline">\(\alpha_1 E(X|Y_1) + \alpha_2 E(X|Y_2)\)</span>, the linearity property is only on the left hand side.</p>
<p>When the expectation is defined, we can write for any such random variables <span class="math inline">\(Y, X\)</span></p>
<p><span class="math display">\[
\begin{aligned}
Y &amp;= \overbrace{E(Y|X)}^{\text{systematic part}} + \overbrace{(Y - E(Y|X))}^{\text{random part}} \\
&amp;= m(X) + \varepsilon, \qquad (7.12)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(m(x) = E(Y|X = x)\)</span> is called the <strong>regression function</strong>. By the linearity property of conditional expectations, the random variable <span class="math inline">\(\varepsilon\)</span> satisfies <span class="math inline">\(E(\varepsilon|X) = 0\)</span>, but <span class="math inline">\(\varepsilon\)</span> is not necessarily independent of <span class="math inline">\(X\)</span>. For example, <span class="math inline">\(\text{var}(\varepsilon|X) = \text{var}[Y - E(Y|X)] = \text{var}(Y|X) = \sigma^2(X)\)</span> can be expected to vary with <span class="math inline">\(X\)</span> as much as <span class="math inline">\(m(X) = E(Y|X)\)</span>. Note that the quantities <span class="math inline">\(E(Y|X), E(\varepsilon|X)\)</span>, and <span class="math inline">\(\text{var}(\varepsilon|X)\)</span> are random variables depending on the realization of the random variable <span class="math inline">\(X\)</span>, and so we should say that <span class="math inline">\(E(\varepsilon|X) = 0\)</span> with probability one or <span class="math inline">\(E(\varepsilon|X = x) = 0\)</span> for all <span class="math inline">\(x\)</span> in the support of <span class="math inline">\(X\)</span>, and we shall sometimes remember to do this. One could equally write <span class="math inline">\(Y = m(X) \eta\)</span>, where <span class="math inline">\(E(\eta|X) = 1\)</span>, except that we should require <span class="math inline">\(m(.) \neq 0\)</span> otherwise <span class="math inline">\(\eta = Y / m(X)\)</span> may not be well defined. We are taking a <strong>top down</strong> approach here working from the random variables <span class="math inline">\(Y, X\)</span> to define <span class="math inline">\(\varepsilon\)</span> or <span class="math inline">\(\eta\)</span>; often the reverse <strong>bottoms up</strong> approach is taken where (7.12) is assumed with say <span class="math inline">\(\varepsilon\)</span> being independent of <span class="math inline">\(X\)</span>, so that given <span class="math inline">\(X\)</span> and <span class="math inline">\(\varepsilon\)</span> one determines <span class="math inline">\(Y\)</span>. A lot of thinking about regression proceeds from this conceptualization. Indeed, a convenient and popular simplification is to assume that the regression function is linear and the error term is</p>
<p><span class="math display">\[
Y = \alpha + \beta X + \varepsilon, \qquad (7.13)
\]</span></p>
<p>where <span class="math inline">\(\varepsilon\)</span> is independent of <span class="math inline">\(X\)</span> with variance <span class="math inline">\(\sigma^2\)</span>. This follows from joint normality of <span class="math inline">\((Y, X)\)</span>, although normality is not necessary. We may have any kind of nonlinear function for example <span class="math inline">\(m(x) = \sin(x) \exp(-x)\)</span>.</p>
<p>We have the following results about conditional expectations.</p>
</section>
<section class="level3" id="theorem-7.9">
<h3 class="anchored" data-anchor-id="theorem-7.9">Theorem 7.9</h3>
<p><strong>Law of iterated expectations.</strong> Suppose that <span class="math inline">\(E(|Y|) &lt; \infty\)</span>. Then we have the following property:</p>
<p><span class="math display">\[
E(Y) = E(E(Y|X)).
\]</span></p>
<p><strong>Proof:</strong> We suppose continuous random variables. Write <span class="math inline">\(f_{Y,X}(y, x) = f_{Y|X}(y|x) f_X(x)\)</span>.</p>
<p><span class="math display">\[
\begin{aligned}
E(Y) &amp;= \int y f_Y(y) dy = \int y \left\{ \int f_{Y,X}(y, x) dx \right\} dy \\
&amp;= \iint y f_{Y,X}(y, x) dy dx = \int \left\{ \int y f_{Y|X}(y|x) dy \right\} f_X(x) dx \\
&amp;= \int \{ E(Y|X = x) \} f_X(x) dx = E(E(Y|X)). \qquad \square
\end{aligned}
\]</span></p>
</section>
<section class="level3" id="example-7.7">
<h3 class="anchored" data-anchor-id="example-7.7">Example 7.7</h3>
<p>For example <span class="math inline">\(Y\)</span> is temperature and <span class="math inline">\(X \in \{\text{day}, \text{night}\}\)</span>. This says the average temperature is the average of average nighttime temperature and average daytime temperature. Say <span class="math inline">\(E(Y|X = \text{day}) = 25\)</span> and <span class="math inline">\(E(Y|X = \text{night}) = 15\)</span>, then <span class="math inline">\(E(Y) = 20\)</span>.</p>
<p>More generally, we have</p>
<p><span class="math display">\[
E(Y|X) = E[E(Y|X, Z)|X] \qquad (7.14)
\]</span></p>
<p>for any <span class="math inline">\(Y, X, Z\)</span>. If I have more information, <span class="math inline">\((X, Z)\)</span>, than you, <span class="math inline">\(X\)</span>, then your forecast is the average of mine over my additional information.</p>
<p>Note that <span class="math inline">\(E(Y|X = x)\)</span> may exist for all <span class="math inline">\(x\)</span>, while <span class="math inline">\(E(Y)\)</span> does not exist. For example <span class="math inline">\(Y = X + \varepsilon\)</span>, where <span class="math inline">\(X\)</span> is Cauchy and <span class="math inline">\(\varepsilon\)</span> is normal. Then <span class="math inline">\(E(Y|X = x) = x\)</span> for all <span class="math inline">\(x\)</span> but <span class="math inline">\(E(Y)\)</span> is not well defined. However, if <span class="math inline">\(E(|Y|) &lt; \infty\)</span>, then we must have <span class="math inline">\(E(|Y||X = x) &lt; \infty\)</span> except at a countable number of points. For example, suppose that <span class="math inline">\(Y = 1/X + \varepsilon\)</span>, where <span class="math inline">\(X\)</span> has the density <span class="math inline">\(f_X(x) = |x| \mathbf{1}(|x| \leq 1)\)</span> and <span class="math inline">\(\varepsilon\)</span> is normal. In this case</p>
<p><span class="math display">\[
E(Y|X = x) = \frac{1}{x}
\]</span></p>
<p>for any <span class="math inline">\(x \neq 0\)</span>. Therefore,</p>
<p><span class="math display">\[
E(Y) = \lim_{\epsilon \rightarrow 0} \int_{-1}^{-\epsilon} \frac{1}{x} (-x) dx + \lim_{\epsilon \rightarrow 0} \int_{\epsilon}^{1} \frac{1}{x} x dx = \int_{-1}^{1} |x| dx = 2 \int_{0}^{1} dx = 2.
\]</span></p>
<p>However, <span class="math inline">\(E(Y|X = 0)\)</span> is not well defined since</p>
<p><span class="math display">\[
\infty = \lim_{n \rightarrow \infty} E \left( Y|X = \frac{1}{n} \right) \neq \lim_{n \rightarrow \infty} E \left( Y|X = -\frac{1}{n} \right) = -\infty.
\]</span></p>
</section>
<section class="level3" id="theorem-7.10">
<h3 class="anchored" data-anchor-id="theorem-7.10">Theorem 7.10</h3>
<p><strong>Analysis of Variance formula.</strong> Suppose that <span class="math inline">\(E(Y^2) &lt; \infty\)</span>. Then we have the following property</p>
<p><span class="math display">\[
\text{var}(Y) = \overbrace{\text{var}(E(Y|X))}^{\text{Explained "sum of squares"}} + \overbrace{E(\text{var}(Y|X))}^{\text{Residual "sum of squares"}}.
\]</span></p>
<p><strong>Proof:</strong> We have</p>
<p><span class="math display">\[
\begin{aligned}
\text{var}(Y) &amp;= E[\{Y - E(Y)\}^2] = E(\{Y - E(Y|X) + E(Y|X) - E(Y)\}^2) \\
&amp;= E[\{Y - E(Y|X)\}^2] + E[\{E(Y|X) - E(Y)\}^2] \\
&amp;+ 2 E[(Y - E(Y|X)) \{E(Y|X) - E(Y)\}] \\
&amp;= I + II + III.
\end{aligned}
\]</span></p>
<p>The first term is</p>
<p><span class="math display">\[
E[\{Y - E(Y|X)\}^2] = E[E\{(Y - E(Y|X))^2|X\}] = E \text{var}(Y|X).
\]</span></p>
<p>The second term is</p>
<p><span class="math display">\[
E[\{E(Y|X) - E(Y)\}^2] = \text{var} E(Y|X).
\]</span></p>
<p>The third term is zero, because <span class="math inline">\(\varepsilon = Y - E(Y|X)\)</span> is such that <span class="math inline">\(E(\varepsilon|X) = 0\)</span> and <span class="math inline">\(E(Y|X) - E(Y)\)</span> is measurable with respect to <span class="math inline">\(X\)</span>, i.e., is just a function of <span class="math inline">\(X\)</span>. So use earlier Theorem</p>
<p><span class="math display">\[
E(\varepsilon h(X)) = E[E(\varepsilon h(X)|X)] = E[h(X) E(\varepsilon|X)] = 0. \qquad \square
\]</span></p>
</section>
<section class="level3" id="example-7.8">
<h3 class="anchored" data-anchor-id="example-7.8">Example 7.8</h3>
<p>In the temperature example, say <span class="math inline">\(\text{var}(Y|X = \text{day}) = 5\)</span> and <span class="math inline">\(\text{var}(Y|X = \text{night}) = 10\)</span>, then</p>
<p><span class="math display">\[
\begin{aligned}
\text{var}(Y) &amp;= \frac{1}{2} (5 + 10) + \text{var}(E(Y|X)) = 7.5 + \text{var}(E(Y|X)) \\
\text{var}(E(Y|X)) &amp;= \frac{1}{2} (25 - 20)^2 + \frac{1}{2} (15 - 20)^2 = 25 \\
\text{var}(Y) &amp;= 32.5 \\
\end{aligned}
\]</span></p>
</section>
<section class="level3" id="example-7.9">
<h3 class="anchored" data-anchor-id="example-7.9">Example 7.9</h3>
<p>Consider the linear regression example</p>
<p><span class="math display">\[
Y = \alpha + \beta X + \varepsilon,
\]</span></p>
<p>where <span class="math inline">\(\varepsilon\)</span> is independent of <span class="math inline">\(X\)</span> with variance <span class="math inline">\(\sigma^2\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
\text{var}(Y) &amp;= E(\text{var}(Y|X)) + \text{var}(E(Y|X)) \\
&amp;= E(\sigma^2) + \text{var}(\alpha + \beta X) \\
&amp;= \sigma^2 + \beta^2 \text{var}(X).
\end{aligned}
\]</span></p>
<p>Only in the special case where <span class="math inline">\(E(Y|X)\)</span> is constant with respect to <span class="math inline">\(X\)</span> is it the case that <span class="math inline">\(\text{var}(Y) = E(\text{var}(Y|X))\)</span>.</p>
<p>We next introduce the concept of mean independence.</p>
</section>
<section class="level3" id="definition-7.8">
<h3 class="anchored" data-anchor-id="definition-7.8">Definition 7.8</h3>
<p>We say that <span class="math inline">\(Y\)</span> is <strong>mean independent</strong> of <span class="math inline">\(X\)</span> if <span class="math inline">\(E(Y|X) = \mu\)</span> for all <span class="math inline">\(x\)</span>.</p>
</section>
<section class="level3" id="theorem-7.11">
<h3 class="anchored" data-anchor-id="theorem-7.11">Theorem 7.11</h3>
<p>Suppose that <span class="math inline">\(E(Y|X) = \alpha = E(Y)\)</span> with probability one. Then <span class="math inline">\(\text{cov}(Y, X) = 0\)</span>, but not vice versa.</p>
<p><strong>Proof:</strong> We have</p>
<p><span class="math display">\[
\begin{aligned}
\text{cov}(Y, X) &amp;= E(XY) - \mu_X \mu_Y = E([X E(Y|X)]) - \mu_X \mu_Y \\
&amp;= E[X E(Y)] - \mu_X \mu_Y = \mu_X \mu_Y - \mu_X \mu_Y \\
&amp;= 0. \qquad \square
\end{aligned}
\]</span></p>
<p>We have a hierarchy of dependence concepts. Independence is strongest and implies mean independence, which in turn implies lack of correlation. In the normal distribution they are equivalent.</p>
<p>We next give a further interpretation of the regression function as being the best mean squared error predictor and compare it with the best linear such predictor.</p>
</section>
<section class="level3" id="theorem-7.12">
<h3 class="anchored" data-anchor-id="theorem-7.12">Theorem 7.12</h3>
<p>Suppose that <span class="math inline">\(EY^2 &lt; \infty\)</span> and <span class="math inline">\(EX^2 &lt; \infty\)</span>. Then we have the following properties:<br/>
(a) <span class="math inline">\(E(Y|X)\)</span> minimizes <span class="math inline">\(E[(Y - g(X))^2]\)</span> over all measurable functions <span class="math inline">\(g(\cdot)\)</span>;<br/>
(b) The linear function <span class="math inline">\(E_L(Y|X) = \alpha_0 + \beta_0 X\)</span>, where <span class="math inline">\(\alpha_0 = E(Y) - \beta_0 E(X)\)</span> and <span class="math inline">\(\beta_0 = \text{cov}(Y, X) / \text{var}(X)\)</span>, minimizes <span class="math inline">\(E[(Y - \alpha - \beta X)^2]\)</span>, and<br/>
</p>
<p><span class="math display">\[
E[(Y - E(Y|X))^2] \leq E[(Y - E_L(Y|X))^2].
\]</span></p>
<p><strong>Proof of (a):</strong> Using the Law of Iterated Expectations we write</p>
<p><span class="math display">\[
E[(Y - g(X))^2] = E[E[(Y - g(X))^2|X]].
\]</span></p>
<p>Then by completing the square we obtain</p>
<p><span class="math display">\[
\begin{aligned}
E[(Y - g(X))^2|X] &amp;= E[(Y^2 - 2g(X)Y + g(X)^2)|X] \\
&amp;= E(Y^2|X) - 2g(X)E(Y|X) + g(X)^2 + E(Y|X)^2 - E(Y|X)^2 \\
&amp;= g(X)^2 - 2g(X)E(Y|X) + E(Y|X)^2 + E(Y^2|X) - E(Y|X)^2 \\
&amp;= (g(X) - E(Y|X))^2 + \text{var}(Y|X) \geq \text{var}(Y|X)
\end{aligned}
\]</span></p>
<p>for all functions <span class="math inline">\(g\)</span> and all realizations <span class="math inline">\(X\)</span> with equality if and only if <span class="math inline">\(g(X) = E(Y|X)\)</span> with probability one, which establishes the result. <span class="math inline">\(\square\)</span></p>
<p><strong>Proof of (b):</strong> Let <span class="math inline">\(\tilde{Y} = Y - E(Y)\)</span> and <span class="math inline">\(\tilde{X} = X - E(X)\)</span>. Then we consider the problem to minimize with respect to <span class="math inline">\(\alpha, \beta\)</span> the objective function</p>
<p><span class="math display">\[
E[(\tilde{Y} - \alpha - \beta \tilde{X})^2] = E(\tilde{Y})^2 + \alpha^2 + \beta^2 E(\tilde{X}^2) - 2 \beta E(\tilde{X} \tilde{Y}).
\]</span></p>
<p>Clearly, <span class="math inline">\(\alpha = 0\)</span> is minimal, and since <span class="math inline">\(E(\tilde{Y})^2\)</span> does not depend on the parameters we minimize <span class="math inline">\(\beta^2 E(\tilde{X}^2) - 2 \beta E(\tilde{X} \tilde{Y})\)</span> with respect to <span class="math inline">\(\beta\)</span>. Then dividing through by <span class="math inline">\(E(\tilde{X}^2)\)</span> (assuming this is non-zero) this is equivalent to minimizing</p>
<p><span class="math display">\[
(\beta - \beta_0)^2 - \beta_0^2,
\]</span></p>
<p>which can be achieved only by setting <span class="math inline">\(\beta = \beta_0\)</span> as claimed. It follows that <span class="math inline">\(\alpha_0, \beta_0\)</span> minimize <span class="math inline">\(E[(Y - \alpha - \beta X)^2]\)</span>.</p>
<p>The second result follows from the property of conditional expectation, since <span class="math inline">\(E_L(Y|X)\)</span> is just another measurable function of <span class="math inline">\(X\)</span> to be compared with the regression function. Note that in general the regression function <span class="math inline">\(E(Y|X) \neq E_L(Y|X)\)</span>. <span class="math inline">\(\square\)</span></p>
<p>Note that we may define conditional expectation and best linear prediction without reference to the second moment of <span class="math inline">\(Y\)</span> although the above theorem uses that property.</p>
</section>
<section class="level3" id="example-7.10">
<h3 class="anchored" data-anchor-id="example-7.10">Example 7.10</h3>
<p>Suppose that</p>
<p><span class="math display">\[
Y = X + \varepsilon,
\]</span></p>
<p>where <span class="math inline">\(X \sim N(0, \sigma_X^2)\)</span> and <span class="math inline">\(\varepsilon \sim N(0, \sigma_\varepsilon^2)\)</span> with <span class="math inline">\(\varepsilon\)</span> independent of <span class="math inline">\(X\)</span>. Then</p>
<p><span class="math display">\[
E_L(Y|X) = \frac{\text{cov}(X, Y)}{\text{var}(X)} X = X = E(Y|X).
\]</span></p>
<p>But what about <span class="math inline">\(E(X|Y)\)</span>? We have</p>
<p><span class="math display">\[
E_L(X|Y) = \frac{\text{cov}(X, Y)}{\text{var}(Y)} Y = \frac{\sigma_X^2}{\sigma_X^2 + \sigma_\varepsilon^2} Y = E(X|Y),
\]</span></p>
<p>where the final equality follows from Bayes rule applied to the bivariate normal <span class="math inline">\((X, Y)\)</span>. Suppose instead that <span class="math inline">\(X \sim U[-\sqrt{3 \sigma_X}, \sqrt{3 \sigma_X}]\)</span> and <span class="math inline">\(\varepsilon \sim U[-\sqrt{3 \sigma_\varepsilon}, \sqrt{3 \sigma_\varepsilon}]\)</span>, so that <span class="math inline">\((X, Y)\)</span> are no longer normal. Then we have the same variances and covariances as before so that</p>
<p><span class="math display">\[
E_L(X|Y) = \frac{\text{cov}(X, Y)}{\text{var}(Y)} Y = \frac{\sigma_X^2}{\sigma_X^2 + \sigma_\varepsilon^2} Y,
\]</span></p>
<p>but <span class="math inline">\(E(X|Y) \neq E_L(X|Y)\)</span> by the following argument. Suppose that <span class="math inline">\(Y = \sqrt{3} (\sigma_X + \sigma_\varepsilon)\)</span>, which is the maximal value that <span class="math inline">\(Y\)</span> could achieve, then <span class="math inline">\(E(X|Y) = \sqrt{3} \sigma_X\)</span>, because in that case we know that <span class="math inline">\(X\)</span> has to be at its maximum. This does not coincide with the best linear predictor evaluated at the same <span class="math inline">\(Y\)</span>. In this case, the best predictor of <span class="math inline">\(X\)</span> by <span class="math inline">\(Y\)</span> has to be nonlinear.</p>
<p>We discuss here the relationship between the regression function and the best linear fit. Suppose that <span class="math inline">\(Y, X\)</span> are scalar random variables and that</p>
<p><span class="math display">\[
E(Y|X) = g(X)
\]</span></p>
<p>for some differentiable function <span class="math inline">\(g\)</span>. The effect of <span class="math inline">\(X\)</span> on <span class="math inline">\(Y\)</span> could be measured by the derivative of <span class="math inline">\(g, g'(x)\)</span>, which varies with the point of evaluation. The average marginal effect is</p>
<p><span class="math display">\[
\delta = E[g'(X)] = \int g'(x) f_X(x) dx = \int \left\{ \frac{\partial}{\partial x} \int y f_{Y|X=x}(y|x) dy \right\} f_X(x) dx. \qquad (7.15)
\]</span></p>
<p>This quantity is widely used in nonlinear models to summarize the effect of the <span class="math inline">\(X\)</span>’s on the <span class="math inline">\(y\)</span>. In the case where the regression function is linear, i.e., <span class="math inline">\(g(x) = \alpha + \beta x\)</span> for constants <span class="math inline">\(\alpha, \beta\)</span>, then <span class="math inline">\(\delta = \beta\)</span>. Consider the general nonlinear case. In general we do not expect <span class="math inline">\(\delta = \beta\)</span>.</p>
<p>Many authors argue heuristically, that the linear approximation to the regression function obtained by finding the <strong>Taylor Approximation</strong> to <span class="math inline">\(m\)</span> at its mean. So consider the second order Mean Value Theorem</p>
<p><span class="math display">\[
\begin{aligned}
m(X) &amp;= m(\mu) + (X - \mu) m'(\mu) + \frac{1}{2} (X - \mu)^2 m''(X^*) \\
&amp;\sim m(\mu) + (X - \mu) m'(\mu), \qquad (7.16)
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(X^*\)</span> is a value that lies between <span class="math inline">\(\mu = E(X)\)</span> and <span class="math inline">\(X\)</span>. In effect they argue that <span class="math inline">\(\beta = m'(\mu)\)</span> and <span class="math inline">\(\alpha = m(\mu) - \mu m'(\mu)\)</span>, i.e., the best linear fit is providing a linear approximation to <span class="math inline">\(m\)</span>. This is not true in general.</p>
</section>
<section class="level3" id="example-7.11">
<h3 class="anchored" data-anchor-id="example-7.11">Example 7.11</h3>
<p>Suppose that</p>
<p><span class="math display">\[
Y = 1 + X^3 + \varepsilon
\]</span></p>
<p>where <span class="math inline">\(X \sim U[0, 1]\)</span> and <span class="math inline">\(\varepsilon\)</span> standard normal. We have</p>
<p><span class="math display">\[
\begin{aligned}
E_L(Y|X) &amp;= \frac{16}{20} + \frac{9}{10} X \\
\delta &amp;= 3 E X^2 = 1 \quad ; \quad \beta = \frac{9}{10} \quad ; \quad m'(E(X)) = \frac{3}{4}. \qquad (7.17)
\end{aligned}
\]</span></p>
<p>In general, all three quantities in (7.17) are different, and the best linear approximation to <span class="math inline">\(Y\)</span> and hence to <span class="math inline">\(m(X)\)</span> is not given by the Taylor approximation. See Fig. 7.1.</p>
<p>When is the linearization (7.16) a good approximation? We can show that</p>
<p><span class="math display">\[
E[|(m(X) - m(\mu) - (X - \mu) m'(\mu))|] \leq \frac{1}{2} \text{var}(X) \sup_x |m''(x)|,
\]</span></p>
<p>so that if <span class="math inline">\(\text{var}(X)\)</span> or <span class="math inline">\(\sup_x |m''(x)|\)</span> is “small” and the other quantity not offsettingly large then the approximation error can be small enough such that the linear approximation is good and the <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are essentially as argued. Clearly, if <span class="math inline">\(m\)</span> is linear then <span class="math inline">\(m'' = 0\)</span> but otherwise there will be approximation error.</p>
<p>There is one special case where the slope of the best linear fit <span class="math inline">\(\beta\)</span> measures the marginal effect.</p>
</section>
<section class="level3" id="theorem-7.13">
<h3 class="anchored" data-anchor-id="theorem-7.13">Theorem 7.13</h3>
<p>Suppose that <span class="math inline">\(X \sim N(\mu_X, \sigma_X^2)\)</span> and that <span class="math inline">\(Y = g(X) + \varepsilon\)</span>, where <span class="math inline">\(E(\varepsilon|X) = 0\)</span>. Then <span class="math inline">\(\delta = \beta\)</span>.</p>
<p><strong>Proof:</strong> For any normally distributed random variables, <strong>Stein’s Lemma</strong> (essentially integration by parts) says that</p>
<p><span class="math display">\[
\text{cov}(X, g(X)) = \text{var}(X) E[g'(X)]. \qquad (7.18)
\]</span></p>
<p>Furthermore,</p>
<p><span class="math display">\[
\text{cov}(X, Y) = \text{cov}(X, g(X) + \varepsilon) = \text{cov}(X, g(X)) + \text{cov}(X, \varepsilon) = \text{cov}(X, g(X)).
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
E[g'(X)] = \frac{\text{cov}(X, Y)}{\text{var}(X)} = \beta, \qquad \square
\]</span></p>
<p>the slope of the best linear fit to <span class="math inline">\(Y\)</span>.</p>
<p>This says that for normally distributed random covariates the linear regression slope measures the average marginal effect even when the regression function is nonlinear.</p>
<p>We conclude with a discussion of the case where <span class="math inline">\(Y\)</span> is continuous and <span class="math inline">\(X\)</span> is discrete. This case is rather simple, and a lot of the above discussion is not needed because the regression function is very simple and possibly linear. Consider the case where <span class="math inline">\(X \in \{0, 1\}\)</span> and <span class="math inline">\(X = 1\)</span> with probability <span class="math inline">\(p\)</span>. Then we have two conditional densities <span class="math inline">\(f_{Y|X=0}\)</span> and <span class="math inline">\(f_{Y|X=1}\)</span> and the conditional expectation is either <span class="math inline">\(\int y f_{Y|X=0}(y) dy = \alpha_1\)</span> or <span class="math inline">\(\int y f_{Y|X=1}(y) dy = \alpha_2\)</span> and we can therefore equivalently write</p>
<p><span class="math display">\[
E(Y|X) = \alpha + \beta X
\]</span></p>
<p>with <span class="math inline">\(\alpha = \alpha_1\)</span> and <span class="math inline">\(\beta = \alpha_2 - \alpha_1\)</span>.</p>
<p>We next turn to a discussion of the conditional median and its interpretation. The conditional median can be defined similarly through the conditional distribution. Specifically, <span class="math inline">\(M(Y|X = x) = F_{Y|X}^{-1}(0.5|x)\)</span>. We may also write</p>
<p><span class="math display">\[
Y = M(Y|X) + \overbrace{Y - M(Y|X)}^{\eta},
\]</span></p>
<p>where <span class="math inline">\(M(\eta|X) = 0\)</span> because <span class="math inline">\(M(Y|X)\)</span> is a constant in the conditional distribution.</p>
</section>
<section class="level3" id="theorem-7.14">
<h3 class="anchored" data-anchor-id="theorem-7.14">Theorem 7.14</h3>
<p>Suppose that the distribution of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is continuous. Then <span class="math inline">\(M(Y|X)\)</span> minimizes the objective function <span class="math inline">\(E[|Y - g(X)|]\)</span> over all measurable functions <span class="math inline">\(g(\cdot)\)</span>.</p>
<p>This follows by similar arguments given for the unconditional case. Finally, does the following equality (<strong>Law of the Iterated Median</strong>) hold?</p>
<p><span class="math display">\[
M(Y) = M(M(Y|X)). \qquad (7.19)
\]</span></p>
<p>This statement is not always true, although for example in the bivariate normal case it is, see below.</p>
</section>
<section class="level3" id="example-7.12">
<h3 class="anchored" data-anchor-id="example-7.12">Example 7.12</h3>
<p>Suppose that</p>
<p><span class="math display">\[
Y = X + \eta,
\]</span></p>
<p>where <span class="math inline">\(\eta|X = x \sim U[-x, x]\)</span>, which has conditional median zero, and <span class="math inline">\(X \sim U[0, 1]\)</span>. Thus <span class="math inline">\(M(Y|X) = X\)</span> and so <span class="math inline">\(M(M(Y|X)) = M(X) = 1/2\)</span>. So now we</p>
<p>have to check whether</p>
<p><span class="math display">\[
\text{Pr}[Y \leq 1/2] = \text{Pr}[X + \eta \leq 1/2] = 1/2.
\]</span></p>
<p>The left hand side is equal to</p>
<p><span class="math display">\[
E[\mathbf{1}(X + \eta \leq 1/2)] = E(E[\mathbf{1}(X + \eta \leq 1/2)|X]) = E(\text{Pr}[X + \eta \leq 1/2|X])
\]</span></p>
<p>by an application of the law of iterated expectation. We have</p>
<p><span class="math display">\[
\text{Pr}[X + \eta \leq 1/2|X = x] = \text{Pr}[\eta \leq 1/2 - x|X] = \begin{cases}
1 &amp; x \leq 1/4 \\
\frac{1}{4x} &amp; x &gt; 1/4
\end{cases}.
\]</span></p>
<p>Taking expectations</p>
<p><span class="math display">\[
E(\text{Pr}[X + \eta \leq 1/2|X]) = \int_{1/4}^1 \frac{1}{4x} dx + \frac{1}{4} = \frac{1}{4} (1 + \log 4) \approx 0.6.
\]</span></p>
<p>In fact the median of <span class="math inline">\(Y\)</span> is about 0.37. To conclude we have shown that <span class="math inline">\(M(M(Y|X)) \neq M(Y)\)</span>.</p>
</section>
</section>
<section class="level2" id="examples">
<h2 class="anchored" data-anchor-id="examples">7.5 EXAMPLES</h2>
<section class="level3" id="example-7.13">
<h3 class="anchored" data-anchor-id="example-7.13">Example 7.13</h3>
<p><strong>MULTINOMIAL.</strong> We can think of this as a vector of <span class="math inline">\(k\)</span> binary outcomes, thus we can define random variables <span class="math inline">\(X_i\)</span> such that</p>
<p><span class="math display">\[
(X_1, ..., X_k) = \begin{cases}
(1, 0, ..., 0) &amp; \text{with probability } p_1 \\
(0, 1, ..., 0) &amp; \text{with probability } p_2 \\
\vdots \\
(0, 0, ..., 1) &amp; \text{with probability } p_k
\end{cases} \quad \sum_{j=1}^k p_j = 1.
\]</span></p>
<p>The random variables <span class="math inline">\(X_i\)</span> are themselves Bernoulli random variables with respective parameters <span class="math inline">\(p_i\)</span>. Can <span class="math inline">\(X_1, ..., X_k\)</span> be mutually independent? No, because <span class="math inline">\(\sum_{i=1}^k X_i = 1\)</span>. There is perfect dependence between the random variables, and the conditional distributions are degenerate. Now suppose we repeat the multinomial <span class="math inline">\(n\)</span> times. We get <span class="math inline">\((n_1, ..., n_k) = Y\)</span></p>
<p><span class="math display">\[
\text{Pr}(Y = (n_1, ..., n_k)) = \frac{n!}{n_1! \cdots n_k!} p_1^{n_1} \times \cdots \times p_k^{n_k} \quad \text{for any } \sum_{j=1}^k n_j = n.
\]</span></p>
<p>Letting <span class="math inline">\(Y = (Y_1, ..., Y_k)\)</span>, the marginals <span class="math inline">\(Y_l, l = 1, ..., k\)</span>, are Binomial<span class="math inline">\((n, p_l)\)</span>. They are no longer perfectly dependent. Knowing that <span class="math inline">\(Y_1 = n_1\)</span> does not pinpoint <span class="math inline">\(Y_2, ..., Y_n\)</span> for me although it does tell you <span class="math inline">\(\sum_{l=2}^n Y_l\)</span>.</p>
</section>
<section class="level3" id="example-7.14">
<h3 class="anchored" data-anchor-id="example-7.14">Example 7.14</h3>
<p><strong>EUROMILLIONS LOTTERY.</strong> The outcome is a set of 5 numbers between 1 and 50 chosen equally likely but without replacement (you can’t have the same number). That is</p>
<p><span class="math display">\[
X = (Y_1, Y_2, Y_3, Y_4, Y_5),
\]</span></p>
<p>where <span class="math inline">\(Y_1 \in I\)</span>, where <span class="math inline">\(I = \{1, 2, ..., 50\}\)</span> with <span class="math inline">\(\text{Pr}(Y_1 = i) = 1/50\)</span> for <span class="math inline">\(i = 1, 2, ..., 50\)</span>, while <span class="math inline">\(Y_2|Y_1 = i \in I \setminus \{i\}\)</span> with probability <span class="math inline">\(1/49\)</span> etc. The support of <span class="math inline">\(X\)</span> is the product of <span class="math inline">\((I \times I \times I \times I \times I) \setminus T\)</span>, where <span class="math inline">\(T\)</span> is the set of all ties such that <span class="math inline">\(Y_i = Y_j\)</span> for any <span class="math inline">\(i \neq j\)</span>. The probability of winning is</p>
<p><span class="math display">\[
\frac{1}{50} \times \frac{1}{49} \times \frac{1}{48} \times \frac{1}{47} \times \frac{1}{46}.
\]</span></p>
<p>How would you test whether the lottery is truly random?</p>
</section>
<section class="level3" id="example-7.15">
<h3 class="anchored" data-anchor-id="example-7.15">Example 7.15</h3>
<p>Let <span class="math inline">\(Y_j \sim \text{Po}(\lambda), j = 1, 2\)</span>, be independent Poisson random variables with parameter <span class="math inline">\(\lambda\)</span> and let</p>
<p><span class="math display">\[
X = \frac{Y_1}{Y_1 + Y_2}
\]</span></p>
<p>with <span class="math inline">\(0/0 = 0\)</span>. Verify that <span class="math inline">\(X\)</span> is a random variable whose support is the set of rational numbers on the interval <span class="math inline">\([0, 1]\)</span>, i.e.,</p>
<p><span class="math display">\[
\text{Pr}(X = x) &gt; 0 \quad \text{for all } x \in S = \mathbb{Q} \cap [0, 1].
\]</span></p>
</section>
<section class="level3" id="example-7.16">
<h3 class="anchored" data-anchor-id="example-7.16">Example 7.16</h3>
<p><strong>BIVARIATE NORMAL.</strong> We say that <span class="math inline">\(X, Y\)</span> is bivariate normal if</p>
<p><span class="math display">\[
f_{X,Y}(x, y|\mu_X, \mu_Y, \sigma_X^2, \sigma_Y^2, \rho) = \frac{1}{2 \pi \sigma_X \sigma_Y \sqrt{1 - \rho^2}} \exp \left( -\frac{u(x, y)}{2(1 - \rho^2)} \right),
\]</span></p>
<p><span class="math display">\[
u(x, y) = \left( \frac{x - \mu_X}{\sigma_X} \right)^2 + \left( \frac{y - \mu_Y}{\sigma_Y} \right)^2 - \left( \frac{2 \rho (x - \mu_X)(y - \mu_Y)}{\sigma_X \sigma_Y} \right),
\]</span></p>
<p>where <span class="math inline">\(\mu_X = E(X), \mu_Y = E(Y), \sigma_X^2 = \text{var}(X), \sigma_Y^2 = \text{var}(Y)\)</span>, and <span class="math inline">\(\rho = \text{corr}(X, Y)\)</span>.</p>
</section>
<section class="level3" id="theorem-7.15">
<h3 class="anchored" data-anchor-id="theorem-7.15">Theorem 7.15</h3>
<ol type="a">
<li>If <span class="math inline">\(X, Y\)</span> are bivariate normal as above, then <span class="math inline">\(X \sim N(\mu_X, \sigma_X^2)\)</span>, which is shown by integration of the joint density with respect to the other variables.</li>
<li>The conditional distributions of <span class="math inline">\(X\)</span> are Gaussian too</li>
</ol>
<p><span class="math display">\[
f_{Y|X}(y|x) \sim N(\mu_{Y|X}, \sigma_{Y|X}^2)
\]</span></p>
<p>where the conditional mean vector and conditional covariance matrix are given by</p>
<p><span class="math display">\[
\mu_{Y|X} = E(Y|X) = \mu_Y + \frac{\sigma_{XY}}{\sigma_X^2} (X - \mu_X)
\]</span></p>
<p><span class="math display">\[
\sigma_{Y|X}^2 = \sigma_Y^2 - \frac{\sigma_{XY}^2}{\sigma_X^2}.
\]</span></p>
<p>In fact, we may write for the bivariate normal case</p>
<p><span class="math display">\[
Y = \mu_Y + \frac{\sigma_{XY}}{\sigma_X^2} (X - \mu_X) + \varepsilon, \qquad (7.20)
\]</span></p>
<p>where <span class="math inline">\(\varepsilon\)</span> is independent of <span class="math inline">\(X\)</span> with mean zero and variance <span class="math inline">\(\sigma_{Y|X}^2\)</span>. Note that the conditional median is equal to the conditional mean. In fact, when <span class="math inline">\(\mu_X = \mu_Y = 0\)</span> and <span class="math inline">\(\sigma_X^2 = \sigma_Y^2 = 1\)</span> we have</p>
<p><span class="math display">\[
Y = \rho X + \sqrt{1 - \rho^2} Z, \qquad (7.21)
\]</span></p>
<p>where <span class="math inline">\(Z\)</span> is standard normal independent of <span class="math inline">\(X\)</span>. Note that the conditional median is equal to the conditional mean. If <span class="math inline">\((X, Y)\)</span> are jointly normally distributed, they are independent if and only <span class="math inline">\(\text{cov}(X, Y) = 0\)</span>.</p>
<p>Bayes rule combines very well with normal distributions to yield simple solutions to signal extraction problems.</p>
</section>
<section class="level3" id="theorem-7.16">
<h3 class="anchored" data-anchor-id="theorem-7.16">Theorem 7.16</h3>
<p>Suppose that <span class="math inline">\(X \sim N(\mu_X, \sigma_X^2)\)</span> and <span class="math inline">\(Y|X \sim N(X, \sigma_\varepsilon^2)\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
X|Y &amp;= y \sim N(m(y), v(y)) \\
m(y) &amp;= \frac{\sigma_\varepsilon^2}{\sigma_\varepsilon^2 + \sigma_X^2} \mu_X + \frac{\sigma_X^2}{\sigma_\varepsilon^2 + \sigma_X^2} y \\
v(y) &amp;= \frac{\sigma_\varepsilon^2 \sigma_X^2}{\sigma_\varepsilon^2 + \sigma_X^2}.
\end{aligned}
\]</span></p>
<p><strong>Proof:</strong> We have <span class="math inline">\(Y \sim N(\mu_X, \sigma_X^2 + \sigma_\varepsilon^2)\)</span>, so that</p>
<p><span class="math display">\[
\begin{aligned}
f_X(x) &amp;= \frac{1}{\sqrt{2 \pi \sigma_X^2}} \exp \left( -\frac{1}{2} \frac{(x - \mu_X)^2}{\sigma_X^2} \right) \\
f_{Y|X}(y|x) &amp;= \frac{1}{\sqrt{2 \pi \sigma_\varepsilon^2}} \exp \left( -\frac{1}{2} \frac{(y - x)^2}{\sigma_\varepsilon^2} \right) \\
f_Y(y) &amp;= \frac{1}{\sqrt{2 \pi (\sigma_X^2 + \sigma_\varepsilon^2)}} \exp \left( -\frac{1}{2} \frac{(y - \mu_X)^2}{\sigma_X^2 + \sigma_\varepsilon^2} \right).
\end{aligned}
\]</span></p>
<p>Bayes Theorem says that</p>
<p><span class="math display">\[
f_{X|Y}(x|y) = \frac{f_{Y|X}(y|x) f_X(x)}{f_Y(y)}.
\]</span></p>
<p>We have by combination of the terms that the exponent of <span class="math inline">\(f_{X|Y}(x|y)\)</span> is minus one half times</p>
<p><span class="math display">\[
\begin{aligned}
\frac{(y - \mu_X)^2}{\sigma_\varepsilon^2 + \sigma_X^2} + \frac{(y - x)^2}{\sigma_\varepsilon^2} + \frac{(x - \mu_X)^2}{\sigma_X^2} &amp;= \frac{1}{\sigma_\varepsilon^2 \sigma_X^2 (\sigma_\varepsilon^2 + \sigma_X^2)} \left( x^2 (\sigma_\varepsilon^2 + \sigma_X^2) - 2x (\sigma_X^2 y + \sigma_\varepsilon^2 \mu_X) \right) \\
&amp;= \frac{\sigma_\varepsilon^2 + \sigma_X^2}{\sigma_\varepsilon^2 \sigma_X^2} \left( x - \frac{\sigma_X^2 y + \sigma_\varepsilon^2 \mu_X}{\sigma_\varepsilon^2 + \sigma_X^2} \right)^2.
\end{aligned}
\]</span></p>
<p>Therefore, this is the density of a normal with the stated mean and variance. <span class="math inline">\(\square\)</span></p>
<p>We next consider the class of <strong>Mixture Models</strong>, which provide a simple way of generalizing the classical distributions, and allowing for <strong>heterogeneity</strong>. That is, individuals may have difference preference parameters for example and one might think of these <strong>hyperparameters</strong> as coming from some distribution across individuals and then given the preference parameter some other outcome is realized.</p>
</section>
<section class="level3" id="example-7.17">
<h3 class="anchored" data-anchor-id="example-7.17">Example 7.17</h3>
<p><strong>NORMAL MIXTURES.</strong> For example,</p>
<p><span class="math display">\[
Y|\mu, \sigma^2 \sim N(\mu, \sigma^2),
\]</span></p>
<p>where <span class="math inline">\((\mu, \sigma^2) \sim P_{\mu, \sigma^2}\)</span> for some distribution <span class="math inline">\(P_{\mu, \sigma^2}\)</span>. We can write <span class="math inline">\(Y = \mu + \sigma Z\)</span>, where <span class="math inline">\(Z\)</span> is standard normal and independent of <span class="math inline">\((\mu, \sigma^2)\)</span>. This says essentially that within a certain subgroup you have a normal distribution, but that the unconditional distribution is some average over subgroups indexed by <span class="math inline">\(\mu, \sigma^2\)</span>, with proportions determined by the distribution <span class="math inline">\(P_{\mu, \sigma^2}\)</span>. The unconditional distribution of <span class="math inline">\(Y\)</span> is not normally distributed and can be asymmetric. Consider a special case of this, a <strong>scale mixture of normals</strong>, in which:</p>
<p><span class="math display">\[
Y|\sigma^2 \sim N(0, \sigma^2),
\]</span></p>
<p>where <span class="math inline">\(\sigma^2\)</span> has some density <span class="math inline">\(p\)</span>. The density function <span class="math inline">\(f_Y\)</span> of a scale mixture <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
f_Y(y) = \int \phi_{\sigma^2}(y) p(\sigma^2) d \sigma^2.
\]</span></p>
<p>In this case, <span class="math inline">\(Y\)</span> is symmetric about zero, but can be leptokurtic. We can write <span class="math inline">\(Y = \sigma Z\)</span>, where <span class="math inline">\(Z \sim N(0, 1)\)</span> and <span class="math inline">\(\sigma\)</span> are mutually independent. It follows that <span class="math inline">\(\text{var}(Y) = E \sigma^2\)</span>, and</p>
<p><span class="math display">\[
E Y^4 = E(\sigma^4) E Z^4 = 3 E(\sigma^4) \geq 3 \text{var}(Y)^2 = 3 E^2 \sigma^2,
\]</span></p>
<p>because <span class="math inline">\(\text{var}(\sigma^2) = E \sigma^4 - E^2 \sigma^2 &gt; 0\)</span> with strict inequality unless <span class="math inline">\(\sigma^2\)</span> is constant with probability one.</p>
<p>We give an example for a discrete random variable.</p>
</section>
<section class="level3" id="example-7.18">
<h3 class="anchored" data-anchor-id="example-7.18">Example 7.18</h3>
<p><strong>POISSON MIXTURE.</strong> Suppose that <span class="math inline">\(Y|\lambda\)</span> is Poisson (<span class="math inline">\(\lambda\)</span>) and <span class="math inline">\(\lambda\)</span> is a continuous random variable on <span class="math inline">\((0, \infty)\)</span> with density <span class="math inline">\(f\)</span>. Then <span class="math inline">\(Y\)</span> is a discrete random variable with support the non-negative integers with</p>
<p><span class="math display">\[
\text{Pr}(Y = y) = E_\lambda[\text{Pr}(Y = y|\lambda)] = E_\lambda \left[ \frac{e^{-\lambda} \lambda^y}{y!} \right] = \int_0^\infty \frac{e^{-\lambda} \lambda^y}{y!} f(\lambda) d \lambda.
\]</span></p>
<p>This frees up the Poisson from the restriction that the mean is equal to the variance. This is important in practice, because many datasets report quite different values for these moments. We have</p>
<p><span class="math display">\[
\text{var}(Y) = E(\lambda) + \text{var}(\lambda) \geq E(\lambda),
\]</span></p>
<p>and we say that <span class="math inline">\(Y\)</span> has <strong>overdispersion</strong>.</p>
</section>
</section>
<section class="level2" id="multivariate-transformations">
<h2 class="anchored" data-anchor-id="multivariate-transformations">7.6 MULTIVARIATE TRANSFORMATIONS</h2>
<p>Suppose we have some transformation</p>
<p><span class="math display">\[
g : \mathbb{R}^p \rightarrow \mathbb{R}^q, \quad q \leq p
\]</span></p>
<p><span class="math display">\[
Y = g(u) = g(u_1, ..., u_p) = [g_1(u), ..., g_q(u)].
\]</span></p>
<p>Starting from the p.m.f. or p.d.f. of <span class="math inline">\(u_1, ..., u_p\)</span>, we would like to calculate the p.m.f. or p.d.f. of <span class="math inline">\(Y\)</span>. We state a general transformation result that under some conditions gives the joint density function of the transformed random variables. This generalizes the univariate result and is essentially the change of variables formula for multivariate calculus. We just consider the case <span class="math inline">\(p = q = 2\)</span> here because the full result requires matrix theory, which is treated later. What is <span class="math inline">\(q &lt; p\)</span>, then we can’t directly apply the transformation result. However, we may complete the system by adding some redundant transformations, apply the theorem, and then integrate out those components not of interest.</p>
<section class="level3" id="example-7.19">
<h3 class="anchored" data-anchor-id="example-7.19">Example 7.19</h3>
<p>The simultaneous equations model</p>
<p><span class="math display">\[
\begin{aligned}
B_{11} Y_1 + B_{12} Y_2 + C_{11} X_1 + C_{12} X_2 &amp;= u_1 \\
B_{21} Y_1 + B_{22} Y_2 + C_{21} X_1 + C_{22} X_2 &amp;= u_2,
\end{aligned}
\]</span></p>
<p>which often is used to represent the interaction of supply and demand. Often assume that <span class="math inline">\((u_1, u_2)\)</span> is bivariate normal with parameters <span class="math inline">\((0, 0, \sigma_1^2, \sigma_2^2, \rho_{12})\)</span> and that <span class="math inline">\(X\)</span> is fixed [or, more precisely, every calculation is made in the conditional distribution given <span class="math inline">\(X\)</span>]. We would like to calculate the distribution of <span class="math inline">\(Y_1, Y_2\)</span> conditional on <span class="math inline">\(X_1, X_2\)</span>.</p>
</section>
<section class="level3" id="example-7.20">
<h3 class="anchored" data-anchor-id="example-7.20">Example 7.20</h3>
<p>A more general class of models allows nonlinear transformations. For example,</p>
<p><span class="math display">\[
\begin{bmatrix}
\frac{y_1^{\lambda_1} - 1}{\lambda_1} \\
\frac{y_2^{\lambda_2} - 1}{\lambda_2}
\end{bmatrix} =
\begin{bmatrix}
\mu_1 \\
\mu_2
\end{bmatrix} +
\begin{bmatrix}
u_1 \\
u_2
\end{bmatrix}
\]</span></p>
<p>is an example of a nonlinear simultaneous equation. What is the distribution of <span class="math inline">\(y = (y_1, y_2)\)</span> given the distribution of <span class="math inline">\(u = (u_1, u_2)\)</span>?</p>
<p>We give following result for transformations from <span class="math inline">\(\mathbb{R}^2\)</span> to <span class="math inline">\(\mathbb{R}^2\)</span>. A more general result can be expressed in terms of matrix algebra, which we treat later.</p>
</section>
<section class="level3" id="theorem-7.17">
<h3 class="anchored" data-anchor-id="theorem-7.17">Theorem 7.17</h3>
<p>Suppose that <span class="math inline">\(p = q = 2\)</span> and that <span class="math inline">\(X \in \mathbb{R}^2\)</span> is continuously distributed with density function <span class="math inline">\(f_X\)</span>. Suppose that</p>
<p><span class="math display">\[
S = \{x : f_X(x) &gt; 0\} = \bigcup_{j=1}^m S_j,
\]</span></p>
<p>where <span class="math inline">\(g_j = (g_{j1}, g_{j2}) : \mathbb{R}^2 \rightarrow \mathbb{R}^2\)</span> with</p>
<p><span class="math display">\[
y_1 = g_{j1}(x_1, x_2), \quad y_2 = g_{j2}(x_1, x_2)
\]</span></p>
<p>is a one-to-one transformation of <span class="math inline">\(S_j\)</span> into <span class="math inline">\(g_j(S_j) = V_j \subset \mathbb{R}^2\)</span> for <span class="math inline">\(j = 1, ..., m\)</span>. Let</p>
<p><span class="math display">\[
x_1 = g_{1j}^{-1}(y), \quad x_2 = g_{2j}^{-1}(y)
\]</span></p>
<p>denote the inverse transformation of <span class="math inline">\(V_j \rightarrow S_j\)</span>. Suppose that all the partial derivatives are continuous on <span class="math inline">\(V_j\)</span> and the quantities</p>
<p><span class="math display">\[
J_j(y) = \begin{vmatrix}
\frac{\partial g_{j1}^{-1}}{\partial y_1} &amp; \frac{\partial g_{j2}^{-1}}{\partial y_1} \\
\frac{\partial g_{j1}^{-1}}{\partial y_2} &amp; \frac{\partial g_{j2}^{-1}}{\partial y_2}
\end{vmatrix} \qquad (7.22)
\]</span></p>
<p>are non-zero. Then</p>
<p><span class="math display">\[
f_Y(y) = \sum_{j=1}^m |J_j(y)| f_X(g_{j1}^{-1}(y), g_{j2}^{-1}(y))
\]</span></p>
<p>for any <span class="math inline">\(y \in Y = \bigcup_{j=1}^m V_j\)</span>.</p>
</section>
<section class="level3" id="example-7.21">
<h3 class="anchored" data-anchor-id="example-7.21">Example 7.21</h3>
<p>In the simultaneous equations case with no <span class="math inline">\(X\)</span>, the transformation is one-to-one for all <span class="math inline">\(u_1, u_2\)</span> provided</p>
<p><span class="math display">\[
J = B_{11} B_{22} - B_{12} B_{21} \neq 0.
\]</span></p>
<p>We have</p>
<p><span class="math display">\[
\frac{\partial u_i}{\partial Y_j} = B_{ij},
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
f_Y(y) = |B_{11} B_{22} - B_{12} B_{21}| f_u(B_{11} Y_1 + B_{12} Y_2, B_{21} Y_1 + B_{22} Y_2).
\]</span></p>
</section>
<section class="level3" id="example-7.22">
<h3 class="anchored" data-anchor-id="example-7.22">Example 7.22</h3>
<p>In the <strong>Box-Cox</strong> case</p>
<p><span class="math display">\[
J = \begin{vmatrix}
\frac{\partial \theta_1(y_1)}{\partial \lambda_1} &amp; \frac{\partial \theta_2(y_2)}{\partial y_2}
\end{vmatrix},
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
f_Y(y) = \frac{\partial \theta_1}{\partial \lambda_1}(y_1) \frac{\partial \theta_2}{\partial \lambda_2}(y_2) f_u(\theta_1(y_1), \theta_2(y_2)).
\]</span></p>
</section>
<section class="level3" id="some-special-cases-where-q-1-and-p-2">
<h3 class="anchored" data-anchor-id="some-special-cases-where-q-1-and-p-2">7.6.1 Some Special Cases Where <span class="math inline">\(q = 1\)</span> and <span class="math inline">\(p = 2\)</span></h3>
<p>We can solve some questions without the general transformation theorem.</p>
</section>
<section class="level3" id="example-7.23">
<h3 class="anchored" data-anchor-id="example-7.23">Example 7.23</h3>
<p>Suppose that <span class="math inline">\(Y = X_1 + X_2\)</span>, where <span class="math inline">\(X_1, X_2\)</span> are continuously distributed with joint density <span class="math inline">\(f_{X_1 X_2}\)</span>. We have</p>
<p><span class="math display">\[
\begin{aligned}
\text{Pr}[Y \leq y] &amp;= \text{Pr}[X_1 + X_2 \leq y] \\
&amp;= \iint_{x_1 + x_2 \leq y} f_{X_1 X_2}(x_1, x_2) dx_1 dx_2 \\
&amp;= \int_{-\infty}^\infty \left[ \int_{-\infty}^{y - x_2} f_{X_1 X_2}(x_1, x_2) dx_1 \right] dx_2 \\
&amp;= \int_{-\infty}^\infty \left[ \int_{-\infty}^y f_{X_1 X_2}(u - x_2, x_2) du \right] dx_2,
\end{aligned}
\]</span></p>
<p>which we denote by <span class="math inline">\(F_Y(y)\)</span>. Here, we use the change of variables <span class="math inline">\(x_1 \rightarrow u = x_1 + x_2\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
f_Y(y) &amp;= \frac{d}{dy} F_Y(y) = \frac{d}{dy} \int_{-\infty}^\infty \left[ \int_{-\infty}^y f_{X_1 X_2}(u - x_2, x_2) du \right] dx_2 \\
&amp;= \int_{-\infty}^\infty f_{X_1 X_2}(y - x_2, x_2) dx_2,
\end{aligned}
\]</span></p>
<p>by symmetry. When <span class="math inline">\(X_1 \perp X_2\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
F_Y(y) &amp;= \int_{-\infty}^\infty F_{X_1}(y - x_1) f_{X_2}(x_1) dx_1 \\
f_Y(y) &amp;= \int_{-\infty}^\infty f_{X_1}(y - x_1) f_{X_2}(x_1) dx_1,
\end{aligned}
\]</span></p>
<p>which is called the <strong>convolution</strong> of <span class="math inline">\(f_{X_1}\)</span> with <span class="math inline">\(f_{X_2}\)</span>. We denote <span class="math inline">\(f_Y(y)\)</span> by <span class="math inline">\((f_{X_1} * f_{X_2})(y)\)</span>. This is quite a difficult expression to work with in general. The characteristic function is easier to work with, at least in the independent case. Recall that</p>
<p><span class="math display">\[
\phi_Y(t) = E e^{itY_1} = E e^{it(X_1 + X_2)} \overset{\text{independence}}{=} E e^{itX_1} E e^{itX_2} = \phi_{X_1}(t) \cdot \phi_{X_2}(t),
\]</span></p>
<p>where <span class="math inline">\(\phi_{X_j}(t) = E e^{itX_j}, j = 1, 2\)</span>. In other words, convolution is just multiplication in the Fourier domain.</p>
<p>In the special case that <span class="math inline">\(X_2\)</span> has zero mean, <span class="math inline">\(Y\)</span> is a <strong>mean preserving spread</strong> of <span class="math inline">\(X_1\)</span>, and one can see that <span class="math inline">\(\text{var}(Y) \geq \text{var}(X_1)\)</span>. In fact, one can show that <span class="math inline">\(\int_{-\infty}^y F_Y(y') dy' \leq \int_{-\infty}^y F_{X_1}(y') dy'\)</span> for all <span class="math inline">\(y\)</span>.</p>
<p>An alternative method is to apply the transformation theorem. In this case we add a redundant transformation. Specifically, let</p>
<p><span class="math display">\[
Y_1 = X_1 + X_2; \quad Y_2 = X_1 - X_2,
\]</span></p>
<p>which has well defined inverse</p>
<p><span class="math display">\[
X_1 = \frac{1}{2} (Y_1 + Y_2); \quad X_2 = \frac{1}{2} (Y_1 - Y_2).
\]</span></p>
<p>We obtain the joint distribution of <span class="math inline">\(Y_1, Y_2\)</span> by applying the formula, with <span class="math inline">\(|J| = 1/2\)</span>, and then obtain the marginal distribution of <span class="math inline">\(Y_1\)</span> by integrating out over <span class="math inline">\(Y_2\)</span>. The choice of redundant transformation is arbitrary. Instead one could take</p>
<p><span class="math display">\[
Y_1 = X_1 + X_2; \quad Y_2 = X_1 - 2X_2,
\]</span></p>
<p>which has well defined inverse</p>
<p><span class="math display">\[
X_1 = \frac{1}{3} (2Y_1 + Y_2); \quad X_2 = \frac{1}{3} (Y_1 - Y_2).
\]</span></p>
<p>Although the joint distribution of <span class="math inline">\(Y_1, Y_2\)</span> is different in this case, the marginal distribution of <span class="math inline">\(Y_1\)</span> is the same.</p>
</section>
<section class="level3" id="example-7.24">
<h3 class="anchored" data-anchor-id="example-7.24">Example 7.24</h3>
<p>Let <span class="math inline">\(Y = X_1 X_2\)</span>, where <span class="math inline">\(X_i \geq 0\)</span>. Then we have</p>
<p><span class="math display">\[
\begin{aligned}
\text{Pr}[Y \leq y] &amp;= \iint_{x_1 x_2 \leq y} f_{X_1 X_2}(x_1, x_2) dx_1 dx_2 \\
&amp;= \int_0^\infty \left[ \int_0^{y/x_2} f_{X_1 X_2}(x_1, x_2) dx_1 \right] dx_2.
\end{aligned}
\]</span></p>
<p>Substitute <span class="math inline">\(u = x_1 x_2 \Rightarrow du = x_2 dx_1\)</span>, i.e., <span class="math inline">\(dx_1 = \frac{du}{x_2}\)</span>. Then</p>
<p><span class="math display">\[
F_Y(y) = \int_0^\infty \int_0^y f_{X_1 X_2} \left( \frac{u}{x_2}, x_2 \right) \frac{1}{x_2} du dx_2,
\]</span></p>
<p>so that</p>
<p><span class="math display">\[
f_Y(y) = \int_0^\infty \frac{1}{x_2} f_{X_1 X_2} \left( \frac{y}{x_2}, x_2 \right) dx_2.
\]</span></p>
<p>In this case, applying the transformation theorem, we might add the redundant transformation</p>
<p><span class="math display">\[
Y_1 = X_1 X_2; \quad Y_2 = X_1 / X_2,
\]</span></p>
<p>which has well defined inverse</p>
<p><span class="math display">\[
X_1 = \sqrt{Y_1 Y_2}; \quad X_2 = \sqrt{Y_1 / Y_2}.
\]</span></p>
</section>
<section class="level3" id="example-7.25">
<h3 class="anchored" data-anchor-id="example-7.25">Example 7.25</h3>
<p>Suppose that <span class="math inline">\(Y = \max\{X_1, X_2\}\)</span>. This quantity is of interest in many applications from competing risks models to value at extreme risk models. We have</p>
<p><span class="math display">\[
F_Y(y) = \text{Pr}[Y \leq y] = \text{Pr}[X_1 \leq y \text{ and } X_2 \leq y] = \int_{-\infty}^y \int_{-\infty}^y f_{X_1 X_2}(x_1, x_2) dx_1 dx_2.
\]</span></p>
<p>The density <span class="math inline">\(f_Y(y) = \frac{d}{dy} F_Y(y)\)</span>, and we take the total derivative of the integral to obtain that</p>
<p><span class="math display">\[
f_Y(y) = \int_{-\infty}^y f_{X_1 X_2}(y, x_2) dx_2 + \int_{-\infty}^y f_{X_1 X_2}(x_1, y) dx_1. \qquad (7.23)
\]</span></p>
<p>In the special case that <span class="math inline">\(X_1 \perp X_2\)</span>,</p>
<p><span class="math display">\[
\begin{aligned}
F_Y(y) &amp;= \int_{-\infty}^y f_{X_1}(x) dx \int_{-\infty}^y f_{X_2}(x) dx = F_{X_1}(y) \cdot F_{X_2}(y) \\
f_Y(y) &amp;= F_{X_1}(y) f_{X_2}(y) + F_{X_2}(y) f_{X_1}(y).
\end{aligned}
\]</span></p>
<p>In this case, applying the transformation theorem is a little tricky. Perhaps we might complete the system by</p>
<p><span class="math display">\[
Y_1 = \max\{X_1, X_2\}; \quad Y_2 = \min\{X_1, X_2\}.
\]</span></p>
<p>We then divide <span class="math inline">\(\mathbb{R}\)</span> into two regions <span class="math inline">\(S_1 = \{(x_1, x_2) : x_1 &gt; x_2\}\)</span> and <span class="math inline">\(S_2 = \{(x_1, x_2) : x_1 &lt; x_2\}\)</span>; we may ignore the case that <span class="math inline">\(x_1 = x_2\)</span>, since this is a set of measure zero on the plain. On the set <span class="math inline">\(S_1, Y_1 = X_1\)</span> and <span class="math inline">\(Y_2 = X_2\)</span>, and the inverse transformation is <span class="math inline">\(X_1 = Y_1\)</span> and <span class="math inline">\(X_2 = Y_2\)</span> with <span class="math inline">\(|J| = 1\)</span>, while on the set <span class="math inline">\(S_2, Y_1 = X_2\)</span> and <span class="math inline">\(Y_2 = X_1\)</span> etc. It follows that</p>
<p><span class="math display">\[
f_{Y_1, Y_2}(y_1, y_2) = \begin{cases}
f_{X_1, X_2}(y_1, y_2) + f_{X_1, X_2}(y_2, y_1) &amp; \text{if } y_1 \geq y_2 \\
0 &amp; \text{else}
\end{cases}.
\]</span></p>
<p>The marginal density is obtained by integrating <span class="math inline">\(y_2\)</span> out from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(y_1\)</span> to obtain (7.23).</p>
</section>
<section class="level3" id="copula">
<h3 class="anchored" data-anchor-id="copula">7.6.2 Copula</h3>
<p>Suppose that <span class="math inline">\(Y_1 = F_{X_1}(X_1)\)</span> and <span class="math inline">\(Y_2 = F_{X_2}(X_2)\)</span>, then <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are uniformly distributed. The function</p>
<p><span class="math display">\[
C(u_1, u_2) = \text{Pr}(Y_1 \leq u_1, Y_2 \leq u_2)
\]</span></p>
<p>is called the <strong>Copula</strong> of <span class="math inline">\(X_1, X_2\)</span>; it is a bivariate distribution function on <span class="math inline">\([0, 1] \times [0, 1]\)</span>. The joint distribution of <span class="math inline">\(X_1, X_2\)</span> is equivalently described by the copula <span class="math inline">\(C(u_1, u_2)\)</span> and the two marginal distribution functions <span class="math inline">\(F_{X_1}\)</span> and <span class="math inline">\(F_{X_2}\)</span>. This allows separate modelling of the dependence (by modelling <span class="math inline">\(C\)</span>) from the marginal distributions.</p>
</section>
<section class="level3" id="theorem-7.18">
<h3 class="anchored" data-anchor-id="theorem-7.18">Theorem 7.18</h3>
<p><strong>(Sklar)</strong> Suppose that <span class="math inline">\(X_1, X_2\)</span> are continuously distributed. Then the joint distribution of <span class="math inline">\(X_1, X_2\)</span> can be written uniquely as</p>
<p><span class="math display">\[
\text{Pr}(X_1 \leq x, X_2 \leq x_2) = C(F_{X_1}(x_1), F_{X_1}(x_2))
\]</span></p>
<p>for some distribution function <span class="math inline">\(C : [0, 1]^2 \rightarrow [0, 1]\)</span>.</p>
<p>This approach converts marginal distributions into a standard scale, which allows modelling of the dependence in a common framework. If <span class="math inline">\(C(u_1, u_2) = u_1 u_2\)</span>, then <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent. A leading example is the so-called <strong>Gaussian copula</strong></p>
<p><span class="math display">\[
C(u_1, u_2) = \Phi_2(\Phi^{-1}(u_1), \Phi^{-1}(u_2); \rho) \qquad (7.24)
\]</span></p>
<p>where <span class="math inline">\(\Phi\)</span> is the standard univariate normal c.d.f., while <span class="math inline">\(\Phi_2(s, t; \rho)\)</span> is the c.d.f. of the standard bivariate normal distribution (with mean vector zero and variances equal to one) with correlation parameter <span class="math inline">\(\rho\)</span>. This implies the model for the bivariate c.d.f. and density function</p>
<p><span class="math display">\[
\begin{aligned}
F(x_1, x_2) &amp;= \frac{1}{2 \pi \sqrt{1 - \rho^2}} \int_{-\infty}^{\Phi^{-1}(F_1(x_1))} \int_{-\infty}^{\Phi^{-1}(F_1(x_2))} \exp \left( -\frac{s^2 + t^2 - 2 \rho s t}{2(1 - \rho^2)} \right) ds dt \qquad (7.25) \\
f(x_1, x_2) &amp;= \frac{1}{\sqrt{1 - \rho^2}} \exp \left( -\frac{\rho \Phi^{-1}(F_1(x_1)) \Phi^{-1}(F_2(x_2))}{2(1 - \rho^2)} \right) \times f_1(x_1) f_2(x_2). \qquad (7.26)
\end{aligned}
\]</span></p>
<p>This has been called <strong>The formula that killed Wall Street</strong>, and not because it is too complicated; it is because it was widely used in credit risk modelling, because it is very flexible with regard to the marginal distributions not being Gaussian and so had the veneer of respectable generality. The weakness was that not only are extreme events likely but when they happen for one risk they tend to happen to all risks.</p>
</section>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch07exercise1">
<h3 class="anchored" data-anchor-id="sec-ch07exercise1">Exercise 1</h3>
<p><a href="#sec-ch07solution1">Solution 1</a></p>
<p>Let <span class="math inline">\(X = (X_1, X_2)\)</span> be a bivariate random variable with joint probability density function (pdf) given by</p>
<p><span class="math display">\[
f_X(x_1, x_2) = \begin{cases}
c(x_1 + 2x_2) &amp; \text{if } 0 &lt; x_1 &lt; 1, 0 &lt; x_2 &lt; 1 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Find the value of the constant <span class="math inline">\(c\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise2">
<h3 class="anchored" data-anchor-id="sec-ch07exercise2">Exercise 2</h3>
<p><a href="#sec-ch07solution2">Solution 2</a></p>
<p>Using the joint pdf from Exercise 1, find the marginal probability density functions <span class="math inline">\(f_{X_1}(x_1)\)</span> and <span class="math inline">\(f_{X_2}(x_2)\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise3">
<h3 class="anchored" data-anchor-id="sec-ch07exercise3">Exercise 3</h3>
<p><a href="#sec-ch07solution3">Solution 3</a></p>
<p>Using the joint pdf from Exercise 1, find the conditional probability density functions <span class="math inline">\(f_{X_1|X_2}(x_1|x_2)\)</span> and <span class="math inline">\(f_{X_2|X_1}(x_2|x_1)\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise4">
<h3 class="anchored" data-anchor-id="sec-ch07exercise4">Exercise 4</h3>
<p><a href="#sec-ch07solution4">Solution 4</a></p>
<p>Let <span class="math inline">\(X = (X_1, X_2)\)</span> be a bivariate random variable with joint cumulative distribution function (cdf) given by</p>
<p><span class="math display">\[
F_X(x_1, x_2) = \begin{cases}
1 - e^{-x_1} - e^{-x_2} + e^{-(x_1 + x_2)} &amp; \text{if } x_1 &gt; 0, x_2 &gt; 0 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Determine if <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent.</p>
</section>
<section class="level3" id="sec-ch07exercise5">
<h3 class="anchored" data-anchor-id="sec-ch07exercise5">Exercise 5</h3>
<p><a href="#sec-ch07solution5">Solution 5</a></p>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be independent random variables with <span class="math inline">\(X_1 \sim \text{Exp}(1)\)</span> and <span class="math inline">\(X_2 \sim \text{Exp}(2)\)</span>. Find the probability that <span class="math inline">\(X_1 &lt; X_2\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise6">
<h3 class="anchored" data-anchor-id="sec-ch07exercise6">Exercise 6</h3>
<p><a href="#sec-ch07solution6">Solution 6</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two random variables with joint pdf</p>
<p><span class="math display">\[
f_{X,Y}(x, y) = \begin{cases}
x + y &amp; \text{if } 0 &lt; x &lt; 1, 0 &lt; y &lt; 1 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Find the conditional expectation <span class="math inline">\(E(Y|X = x)\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise7">
<h3 class="anchored" data-anchor-id="sec-ch07exercise7">Exercise 7</h3>
<p><a href="#sec-ch07solution7">Solution 7</a></p>
<p>For the random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> in Exercise 6, find the best linear predictor <span class="math inline">\(E_L(Y|X)\)</span> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise8">
<h3 class="anchored" data-anchor-id="sec-ch07exercise8">Exercise 8</h3>
<p><a href="#sec-ch07solution8">Solution 8</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables with <span class="math inline">\(\text{cov}(X, Y) = 3\)</span> and <span class="math inline">\(\text{var}(X) = 4, \text{var}(Y) = 9\)</span>. Find the correlation coefficient <span class="math inline">\(\rho_{XY}\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise9">
<h3 class="anchored" data-anchor-id="sec-ch07exercise9">Exercise 9</h3>
<p><a href="#sec-ch07solution9">Solution 9</a></p>
<p>Suppose <span class="math inline">\(X \sim N(0, 1)\)</span> and <span class="math inline">\(Y|X \sim N(X^2, 1)\)</span>. Find <span class="math inline">\(E(Y)\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise10">
<h3 class="anchored" data-anchor-id="sec-ch07exercise10">Exercise 10</h3>
<p><a href="#sec-ch07solution10">Solution 10</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables such that <span class="math inline">\(E(Y|X) = X\)</span>. If <span class="math inline">\(X\)</span> follows an exponential distribution with parameter <span class="math inline">\(\lambda = 1\)</span>, what is <span class="math inline">\(E(Y)\)</span>?</p>
</section>
<section class="level3" id="sec-ch07exercise11">
<h3 class="anchored" data-anchor-id="sec-ch07exercise11">Exercise 11</h3>
<p><a href="#sec-ch07solution11">Solution 11</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables with joint pdf <span class="math inline">\(f_{X,Y}(x, y)\)</span>. Show that <span class="math inline">\(\text{var}(Y) = E[\text{var}(Y|X)] + \text{var}[E(Y|X)]\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise12">
<h3 class="anchored" data-anchor-id="sec-ch07exercise12">Exercise 12</h3>
<p><a href="#sec-ch07solution12">Solution 12</a></p>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Find <span class="math inline">\(\text{cov}(X_1 + X_2, X_1 - X_2)\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise13">
<h3 class="anchored" data-anchor-id="sec-ch07exercise13">Exercise 13</h3>
<p><a href="#sec-ch07solution13">Solution 13</a></p>
<p>Suppose <span class="math inline">\(Y = X_1 + X_2\)</span>, where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent random variables with <span class="math inline">\(X_1 \sim \text{Poisson}(\lambda_1)\)</span> and <span class="math inline">\(X_2 \sim \text{Poisson}(\lambda_2)\)</span>. Use the concept of convolution to find the probability mass function (pmf) of <span class="math inline">\(Y\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise14">
<h3 class="anchored" data-anchor-id="sec-ch07exercise14">Exercise 14</h3>
<p><a href="#sec-ch07solution14">Solution 14</a></p>
<p>Let <span class="math inline">\(Y = \max\{X_1, X_2\}\)</span>, where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent and identically distributed exponential random variables with parameter <span class="math inline">\(\lambda\)</span>. Find the cumulative distribution function <span class="math inline">\(F_Y(y)\)</span> and the probability density function <span class="math inline">\(f_Y(y)\)</span> of <span class="math inline">\(Y\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise15">
<h3 class="anchored" data-anchor-id="sec-ch07exercise15">Exercise 15</h3>
<p><a href="#sec-ch07solution15">Solution 15</a></p>
<p>Given the joint pdf of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> as</p>
<p><span class="math display">\[
f_{X,Y}(x, y) = \begin{cases}
\frac{6}{7}(x^2 + \frac{xy}{2}) &amp; \text{if } 0 &lt; x &lt; 1, 0 &lt; y &lt; 2 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Find <span class="math inline">\(P(Y &gt; 1 | X = x)\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise16">
<h3 class="anchored" data-anchor-id="sec-ch07exercise16">Exercise 16</h3>
<p><a href="#sec-ch07solution16">Solution 16</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be continuous random variables with joint pdf <span class="math inline">\(f_{X,Y}(x, y)\)</span>. Derive the formula for the conditional pdf <span class="math inline">\(f_{Y|X}(y|x)\)</span> in terms of <span class="math inline">\(f_{X,Y}(x, y)\)</span> and the marginal pdf <span class="math inline">\(f_X(x)\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise17">
<h3 class="anchored" data-anchor-id="sec-ch07exercise17">Exercise 17</h3>
<p><a href="#sec-ch07solution17">Solution 17</a></p>
<p>Let <span class="math inline">\(X_1, X_2, ..., X_n\)</span> be independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i\)</span>. Find <span class="math inline">\(\text{cov}(\bar{X}, X_i)\)</span> for any <span class="math inline">\(i = 1, 2, ..., n\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise18">
<h3 class="anchored" data-anchor-id="sec-ch07exercise18">Exercise 18</h3>
<p><a href="#sec-ch07solution18">Solution 18</a></p>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be independent standard normal random variables. Find the pdf of <span class="math inline">\(Y = X_1 / X_2\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise19">
<h3 class="anchored" data-anchor-id="sec-ch07exercise19">Exercise 19</h3>
<p><a href="#sec-ch07solution19">Solution 19</a></p>
<p>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are jointly normally distributed with <span class="math inline">\(E(X) = \mu_X\)</span>, <span class="math inline">\(E(Y) = \mu_Y\)</span>, <span class="math inline">\(\text{var}(X) = \sigma_X^2\)</span>, <span class="math inline">\(\text{var}(Y) = \sigma_Y^2\)</span>, and <span class="math inline">\(\text{corr}(X, Y) = \rho\)</span>. Show that <span class="math inline">\(E(Y|X = x)\)</span> is a linear function of <span class="math inline">\(x\)</span>.</p>
</section>
<section class="level3" id="sec-ch07exercise20">
<h3 class="anchored" data-anchor-id="sec-ch07exercise20">Exercise 20</h3>
<p><a href="#sec-ch07solution20">Solution 20</a></p>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be independent random variables with <span class="math inline">\(X_1 \sim U(0, 1)\)</span> and <span class="math inline">\(X_2 \sim U(0, 1)\)</span>. Find the pdf of <span class="math inline">\(Y = X_1 + X_2\)</span> using the convolution formula.</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch07solution1">
<h3 class="anchored" data-anchor-id="sec-ch07solution1">Solution 1</h3>
<p><a href="#sec-ch07exercise1">Exercise 1</a></p>
<p>Since <span class="math inline">\(f_X(x_1, x_2)\)</span> is a joint pdf, it must integrate to 1 over the support of <span class="math inline">\(X\)</span>. Thus, we have</p>
<p><span class="math display">\[
\begin{aligned}
\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} f_X(x_1, x_2) dx_1 dx_2 &amp;= 1 \\
\int_0^1 \int_0^1 c(x_1 + 2x_2) dx_1 dx_2 &amp;= 1 \\
c \int_0^1 \left[ \frac{1}{2} x_1^2 + 2x_1 x_2 \right]_0^1 dx_2 &amp;= 1 \\
c \int_0^1 \left( \frac{1}{2} + 2x_2 \right) dx_2 &amp;= 1 \\
c \left[ \frac{1}{2} x_2 + x_2^2 \right]_0^1 &amp;= 1 \\
c \left( \frac{1}{2} + 1 \right) &amp;= 1 \\
c &amp;= \frac{2}{3}.
\end{aligned}
\]</span></p>
<p><strong>Intuitive explanation:</strong> The constant <span class="math inline">\(c\)</span> is a normalizing factor that ensures the total probability over the entire support of the random variable is equal to 1. The integration is performed over the region where the joint pdf is non-zero, and the properties of integrals and basic calculus are used to arrive at the value of <span class="math inline">\(c\)</span>. The key concept used here is the definition of a <strong>joint probability density function</strong>, which requires the integral over its support to be 1.</p>
</section>
<section class="level3" id="sec-ch07solution2">
<h3 class="anchored" data-anchor-id="sec-ch07solution2">Solution 2</h3>
<p><a href="#sec-ch07exercise2">Exercise 2</a></p>
<p>The marginal pdf <span class="math inline">\(f_{X_1}(x_1)\)</span> is found by integrating the joint pdf over all possible values of <span class="math inline">\(x_2\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X_1}(x_1) &amp;= \int_{-\infty}^{\infty} f_X(x_1, x_2) dx_2 \\
&amp;= \int_0^1 \frac{2}{3} (x_1 + 2x_2) dx_2 \\
&amp;= \frac{2}{3} \left[ x_1 x_2 + x_2^2 \right]_0^1 \\
&amp;= \frac{2}{3} (x_1 + 1), \quad 0 &lt; x_1 &lt; 1.
\end{aligned}
\]</span></p>
<p>Similarly, for <span class="math inline">\(f_{X_2}(x_2)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
f_{X_2}(x_2) &amp;= \int_{-\infty}^{\infty} f_X(x_1, x_2) dx_1 \\
&amp;= \int_0^1 \frac{2}{3} (x_1 + 2x_2) dx_1 \\
&amp;= \frac{2}{3} \left[ \frac{1}{2} x_1^2 + 2x_2 x_1 \right]_0^1 \\
&amp;= \frac{2}{3} (2x_2 + \frac{1}{2}), \quad 0 &lt; x_2 &lt; 1.
\end{aligned}
\]</span></p>
<p><strong>Intuitive explanation:</strong> To find the marginal pdf of one variable, we integrate the joint pdf over the entire range of the other variable. This effectively “removes” the other variable from the joint distribution, leaving only the distribution of the variable of interest. This uses the concept of <strong>marginalization</strong> introduced in the text.</p>
</section>
<section class="level3" id="sec-ch07solution3">
<h3 class="anchored" data-anchor-id="sec-ch07solution3">Solution 3</h3>
<p><a href="#sec-ch07exercise3">Exercise 3</a></p>
<p>The conditional pdf <span class="math inline">\(f_{X_1|X_2}(x_1|x_2)\)</span> is given by</p>
<p><span class="math display">\[
f_{X_1|X_2}(x_1|x_2) = \frac{f_X(x_1, x_2)}{f_{X_2}(x_2)}.
\]</span></p>
<p>Using the results from Exercises 1 and 2,</p>
<p><span class="math display">\[
f_{X_1|X_2}(x_1|x_2) = \frac{\frac{2}{3}(x_1 + 2x_2)}{\frac{2}{3}(2x_2 + \frac{1}{2})} = \frac{x_1 + 2x_2}{2x_2 + \frac{1}{2}}, \quad 0 &lt; x_1 &lt; 1.
\]</span></p>
<p>Similarly,</p>
<p><span class="math display">\[
f_{X_2|X_1}(x_2|x_1) = \frac{f_X(x_1, x_2)}{f_{X_1}(x_1)} = \frac{\frac{2}{3}(x_1 + 2x_2)}{\frac{2}{3}(x_1 + 1)} = \frac{x_1 + 2x_2}{x_1 + 1}, \quad 0 &lt; x_2 &lt; 1.
\]</span></p>
<p><strong>Intuitive explanation:</strong> The conditional pdf describes the distribution of one variable given a specific value of another variable. It is found by dividing the joint pdf by the marginal pdf of the conditioning variable. The key concept here is the definition of <strong>conditional probability density function</strong> as introduced in the text.</p>
</section>
<section class="level3" id="sec-ch07solution4">
<h3 class="anchored" data-anchor-id="sec-ch07solution4">Solution 4</h3>
<p><a href="#sec-ch07exercise4">Exercise 4</a></p>
<p><span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent if and only if <span class="math inline">\(F_X(x_1, x_2) = F_{X_1}(x_1) F_{X_2}(x_2)\)</span> for all <span class="math inline">\(x_1, x_2\)</span>. First, we find the marginal cdfs:</p>
<p><span class="math display">\[
F_{X_1}(x_1) = \lim_{x_2 \rightarrow \infty} F_X(x_1, x_2) = 1 - e^{-x_1}, \quad x_1 &gt; 0
\]</span></p>
<p><span class="math display">\[
F_{X_2}(x_2) = \lim_{x_1 \rightarrow \infty} F_X(x_1, x_2) = 1 - e^{-x_2}, \quad x_2 &gt; 0
\]</span></p>
<p>Now we check if the joint cdf is the product of the marginal cdfs:</p>
<p><span class="math display">\[
F_{X_1}(x_1) F_{X_2}(x_2) = (1 - e^{-x_1})(1 - e^{-x_2}) = 1 - e^{-x_1} - e^{-x_2} + e^{-(x_1 + x_2)} = F_X(x_1, x_2).
\]</span></p>
<p>Since the joint cdf is equal to the product of the marginal cdfs, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent.</p>
<p><strong>Intuitive explanation:</strong> Independence between two random variables means that the occurrence of one does not affect the probability of the other. In terms of cdfs, this translates to the joint cdf being the product of the individual marginal cdfs. This uses the concept of <strong>independence</strong> as defined in the text.</p>
</section>
<section class="level3" id="sec-ch07solution5">
<h3 class="anchored" data-anchor-id="sec-ch07solution5">Solution 5</h3>
<p><a href="#sec-ch07exercise5">Exercise 5</a></p>
<p>Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent, their joint pdf is the product of their marginal pdfs:</p>
<p><span class="math display">\[
f_{X_1, X_2}(x_1, x_2) = f_{X_1}(x_1) f_{X_2}(x_2) = e^{-x_1} (2e^{-2x_2}) = 2e^{-x_1 - 2x_2}, \quad x_1 &gt; 0, x_2 &gt; 0.
\]</span></p>
<p>We want to find <span class="math inline">\(P(X_1 &lt; X_2)\)</span>. This is given by</p>
<p><span class="math display">\[
\begin{aligned}
P(X_1 &lt; X_2) &amp;= \int_0^\infty \int_0^{x_2} 2e^{-x_1 - 2x_2} dx_1 dx_2 \\
&amp;= \int_0^\infty 2e^{-2x_2} \left[ -e^{-x_1} \right]_0^{x_2} dx_2 \\
&amp;= \int_0^\infty 2e^{-2x_2} (1 - e^{-x_2}) dx_2 \\
&amp;= \int_0^\infty (2e^{-2x_2} - 2e^{-3x_2}) dx_2 \\
&amp;= \left[ -e^{-2x_2} + \frac{2}{3} e^{-3x_2} \right]_0^\infty \\
&amp;= 1 - \frac{2}{3} = \frac{1}{3}.
\end{aligned}
\]</span></p>
<p><strong>Intuitive explanation:</strong> To find the probability of an event involving two independent random variables, we integrate their joint pdf over the region defined by the event. The joint pdf of independent variables is the product of their marginal pdfs. This solution uses the concepts of <strong>independence</strong> and <strong>joint probability density function</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution6">
<h3 class="anchored" data-anchor-id="sec-ch07solution6">Solution 6</h3>
<p><a href="#sec-ch07exercise6">Exercise 6</a></p>
<p>The conditional expectation <span class="math inline">\(E(Y|X = x)\)</span> is given by</p>
<p><span class="math display">\[
E(Y|X = x) = \int_{-\infty}^{\infty} y f_{Y|X}(y|x) dy.
\]</span></p>
<p>First, we need to find the conditional pdf <span class="math inline">\(f_{Y|X}(y|x)\)</span>. We know that</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_X(x)}.
\]</span></p>
<p>We first find the marginal pdf <span class="math inline">\(f_X(x)\)</span>:</p>
<p><span class="math display">\[
f_X(x) = \int_0^1 (x + y) dy = \left[ xy + \frac{1}{2} y^2 \right]_0^1 = x + \frac{1}{2}, \quad 0 &lt; x &lt; 1.
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = \frac{x + y}{x + \frac{1}{2}}, \quad 0 &lt; y &lt; 1.
\]</span></p>
<p>Now we can find the conditional expectation:</p>
<p><span class="math display">\[
\begin{aligned}
E(Y|X = x) &amp;= \int_0^1 y \frac{x + y}{x + \frac{1}{2}} dy \\
&amp;= \frac{1}{x + \frac{1}{2}} \int_0^1 (xy + y^2) dy \\
&amp;= \frac{1}{x + \frac{1}{2}} \left[ \frac{1}{2} xy^2 + \frac{1}{3} y^3 \right]_0^1 \\
&amp;= \frac{\frac{1}{2}x + \frac{1}{3}}{x + \frac{1}{2}} = \frac{3x + 2}{6x + 3}.
\end{aligned}
\]</span></p>
<p><strong>Intuitive explanation:</strong> The conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span> is the expected value of <span class="math inline">\(Y\)</span> when <span class="math inline">\(X\)</span> is fixed at <span class="math inline">\(x\)</span>. It is calculated by integrating the product of <span class="math inline">\(y\)</span> and the conditional pdf of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span> over all possible values of <span class="math inline">\(y\)</span>. The key concept here is the definition of <strong>conditional expectation</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution7">
<h3 class="anchored" data-anchor-id="sec-ch07solution7">Solution 7</h3>
<p><a href="#sec-ch07exercise7">Exercise 7</a></p>
<p>The best linear predictor <span class="math inline">\(E_L(Y|X)\)</span> is given by</p>
<p><span class="math display">\[
E_L(Y|X) = \alpha_0 + \beta_0 X,
\]</span></p>
<p>where <span class="math inline">\(\alpha_0 = E(Y) - \beta_0 E(X)\)</span> and <span class="math inline">\(\beta_0 = \frac{\text{cov}(X, Y)}{\text{var}(X)}\)</span>.</p>
<p>First, we find <span class="math inline">\(E(X)\)</span> and <span class="math inline">\(E(Y)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
E(X) &amp;= \int_0^1 x f_X(x) dx = \int_0^1 x(x + \frac{1}{2}) dx = \left[ \frac{1}{3} x^3 + \frac{1}{4} x^2 \right]_0^1 = \frac{7}{12} \\
E(Y) &amp;= \int_0^1 y f_Y(y) dy = \int_0^1 y(y + \frac{1}{2}) dy = \left[ \frac{1}{3} y^3 + \frac{1}{4} y^2 \right]_0^1 = \frac{7}{12}.
\end{aligned}
\]</span></p>
<p>Next, we find <span class="math inline">\(\text{var}(X)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
E(X^2) &amp;= \int_0^1 x^2 (x + \frac{1}{2}) dx = \left[ \frac{1}{4} x^4 + \frac{1}{6} x^3 \right]_0^1 = \frac{5}{12} \\
\text{var}(X) &amp;= E(X^2) - [E(X)]^2 = \frac{5}{12} - \left( \frac{7}{12} \right)^2 = \frac{11}{144}.
\end{aligned}
\]</span></p>
<p>Now, we find <span class="math inline">\(\text{cov}(X, Y)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
E(XY) &amp;= \int_0^1 \int_0^1 xy(x + y) dx dy = \int_0^1 \int_0^1 (x^2 y + xy^2) dx dy \\
&amp;= \int_0^1 \left[ \frac{1}{3} x^3 y + \frac{1}{2} x^2 y^2 \right]_0^1 dy = \int_0^1 (\frac{1}{3} y + \frac{1}{2} y^2) dy \\
&amp;= \left[ \frac{1}{6} y^2 + \frac{1}{6} y^3 \right]_0^1 = \frac{1}{3} \\
\text{cov}(X, Y) &amp;= E(XY) - E(X)E(Y) = \frac{1}{3} - \frac{7}{12} \cdot \frac{7}{12} = -\frac{1}{144}.
\end{aligned}
\]</span></p>
<p>Then, <span class="math inline">\(\beta_0 = \frac{-1/144}{11/144} = -\frac{1}{11}\)</span> and <span class="math inline">\(\alpha_0 = \frac{7}{12} - (-\frac{1}{11}) \frac{7}{12} = \frac{7}{12} + \frac{7}{132} = \frac{84}{132} = \frac{7}{11}\)</span>.</p>
<p>Thus, <span class="math inline">\(E_L(Y|X) = \frac{7}{11} - \frac{1}{11} X\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The best linear predictor is the linear function of <span class="math inline">\(X\)</span> that is closest to <span class="math inline">\(Y\)</span> in terms of mean squared error. The coefficients of this linear function are determined by the means, variances, and covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. This solution uses the concepts of <strong>best linear predictor</strong> and <strong>covariance</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution8">
<h3 class="anchored" data-anchor-id="sec-ch07solution8">Solution 8</h3>
<p><a href="#sec-ch07exercise8">Exercise 8</a></p>
<p>The correlation coefficient <span class="math inline">\(\rho_{XY}\)</span> is given by</p>
<p><span class="math display">\[
\rho_{XY} = \frac{\text{cov}(X, Y)}{\sqrt{\text{var}(X) \text{var}(Y)}}.
\]</span></p>
<p>Plugging in the given values, we have</p>
<p><span class="math display">\[
\rho_{XY} = \frac{3}{\sqrt{4 \cdot 9}} = \frac{3}{6} = \frac{1}{2}.
\]</span></p>
<p><strong>Intuitive explanation:</strong> The correlation coefficient measures the strength and direction of the linear relationship between two random variables. It is calculated as the covariance of the two variables divided by the product of their standard deviations. This solution uses the concept of <strong>correlation coefficient</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution9">
<h3 class="anchored" data-anchor-id="sec-ch07solution9">Solution 9</h3>
<p><a href="#sec-ch07exercise9">Exercise 9</a></p>
<p>We use the law of iterated expectations: <span class="math inline">\(E(Y) = E[E(Y|X)]\)</span>. We are given that <span class="math inline">\(E(Y|X) = X^2\)</span>. So, we need to find <span class="math inline">\(E(X^2)\)</span>. Since <span class="math inline">\(X \sim N(0, 1)\)</span>, we know that <span class="math inline">\(E(X) = 0\)</span> and <span class="math inline">\(\text{var}(X) = 1\)</span>. Also, <span class="math inline">\(\text{var}(X) = E(X^2) - [E(X)]^2\)</span>, so <span class="math inline">\(E(X^2) = \text{var}(X) + [E(X)]^2 = 1 + 0^2 = 1\)</span>.</p>
<p>Therefore, <span class="math inline">\(E(Y) = E(X^2) = 1\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The law of iterated expectations states that the expected value of a random variable can be found by taking the expected value of its conditional expectation. In this case, we find the expected value of <span class="math inline">\(Y\)</span> by first finding the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> and then taking the expectation of that conditional expectation. This solution uses the concept of the <strong>law of iterated expectations</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution10">
<h3 class="anchored" data-anchor-id="sec-ch07solution10">Solution 10</h3>
<p><a href="#sec-ch07exercise10">Exercise 10</a></p>
<p>We are given that <span class="math inline">\(E(Y|X) = X\)</span>. Using the law of iterated expectations, we have <span class="math inline">\(E(Y) = E[E(Y|X)] = E(X)\)</span>. Since <span class="math inline">\(X\)</span> follows an exponential distribution with parameter <span class="math inline">\(\lambda = 1\)</span>, we have <span class="math inline">\(E(X) = \frac{1}{\lambda} = 1\)</span>. Therefore, <span class="math inline">\(E(Y) = 1\)</span>.</p>
<p><strong>Intuitive explanation:</strong> This problem is another application of the law of iterated expectations. Since the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> is equal to <span class="math inline">\(X\)</span>, the expected value of <span class="math inline">\(Y\)</span> is simply the expected value of <span class="math inline">\(X\)</span>. This solution uses the concept of the <strong>law of iterated expectations</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution11">
<h3 class="anchored" data-anchor-id="sec-ch07solution11">Solution 11</h3>
<p><a href="#sec-ch07exercise11">Exercise 11</a></p>
<p>We start with the definition of variance:</p>
<p><span class="math display">\[
\text{var}(Y) = E(Y^2) - [E(Y)]^2.
\]</span></p>
<p>Using the law of iterated expectations, we have <span class="math inline">\(E(Y) = E[E(Y|X)]\)</span>. Also, <span class="math inline">\(E(Y^2) = E[E(Y^2|X)]\)</span>. Now, we can write</p>
<p><span class="math display">\[
\begin{aligned}
\text{var}(Y) &amp;= E[E(Y^2|X)] - \{E[E(Y|X)]\}^2 \\
&amp;= E[E(Y^2|X) - \{E(Y|X)\}^2 + \{E(Y|X)\}^2] - \{E[E(Y|X)]\}^2 \\
&amp;= E[E(Y^2|X) - \{E(Y|X)\}^2] + E[\{E(Y|X)\}^2] - \{E[E(Y|X)]\}^2 \\
&amp;= E[\text{var}(Y|X)] + \text{var}[E(Y|X)].
\end{aligned}
\]</span></p>
<p><strong>Intuitive explanation:</strong> This is a derivation of the <strong>analysis of variance formula</strong>. It shows that the total variance of a random variable can be decomposed into the sum of the expected value of its conditional variance and the variance of its conditional expectation. This solution uses the concepts of <strong>variance</strong> and the <strong>law of iterated expectations</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution12">
<h3 class="anchored" data-anchor-id="sec-ch07solution12">Solution 12</h3>
<p><a href="#sec-ch07exercise12">Exercise 12</a></p>
<p>We have</p>
<p><span class="math display">\[
\begin{aligned}
\text{cov}(X_1 + X_2, X_1 - X_2) &amp;= \text{cov}(X_1, X_1) - \text{cov}(X_1, X_2) + \text{cov}(X_2, X_1) - \text{cov}(X_2, X_2) \\
&amp;= \text{var}(X_1) - \text{cov}(X_1, X_2) + \text{cov}(X_1, X_2) - \text{var}(X_2).
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent and identically distributed, <span class="math inline">\(\text{cov}(X_1, X_2) = 0\)</span> and <span class="math inline">\(\text{var}(X_1) = \text{var}(X_2) = \sigma^2\)</span>. Thus,</p>
<p><span class="math display">\[
\text{cov}(X_1 + X_2, X_1 - X_2) = \sigma^2 - 0 + 0 - \sigma^2 = 0.
\]</span></p>
<p><strong>Intuitive explanation:</strong> This problem uses the properties of covariance. The covariance of a sum (or difference) of random variables can be expanded linearly. Also, the covariance of independent variables is zero, and the covariance of a variable with itself is its variance. This solution uses the concept of <strong>covariance</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution13">
<h3 class="anchored" data-anchor-id="sec-ch07solution13">Solution 13</h3>
<p><a href="#sec-ch07exercise13">Exercise 13</a></p>
<p>The convolution of two independent random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> is given by</p>
<p><span class="math display">\[
f_Y(y) = \sum_{x_1} f_{X_1}(x_1) f_{X_2}(y - x_1).
\]</span></p>
<p>Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are Poisson random variables, we have</p>
<p><span class="math display">\[
f_{X_1}(x_1) = \frac{e^{-\lambda_1} \lambda_1^{x_1}}{x_1!}, \quad f_{X_2}(x_2) = \frac{e^{-\lambda_2} \lambda_2^{x_2}}{x_2!}.
\]</span></p>
<p>Thus, the pmf of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
f_Y(y) &amp;= \sum_{x_1 = 0}^y \frac{e^{-\lambda_1} \lambda_1^{x_1}}{x_1!} \frac{e^{-\lambda_2} \lambda_2^{y - x_1}}{(y - x_1)!} \\
&amp;= e^{-(\lambda_1 + \lambda_2)} \sum_{x_1 = 0}^y \frac{\lambda_1^{x_1} \lambda_2^{y - x_1}}{x_1! (y - x_1)!} \\
&amp;= \frac{e^{-(\lambda_1 + \lambda_2)}}{y!} \sum_{x_1 = 0}^y \binom{y}{x_1} \lambda_1^{x_1} \lambda_2^{y - x_1} \\
&amp;= \frac{e^{-(\lambda_1 + \lambda_2)}}{y!} (\lambda_1 + \lambda_2)^y, \quad y = 0, 1, 2, ...
\end{aligned}
\]</span></p>
<p>This is the pmf of a Poisson distribution with parameter <span class="math inline">\(\lambda_1 + \lambda_2\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The convolution formula gives the distribution of the sum of two independent random variables. In this case, the sum of two independent Poisson random variables is also a Poisson random variable with a parameter equal to the sum of the parameters of the original variables. This solution uses the concepts of <strong>convolution</strong> and <strong>Poisson distribution</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution14">
<h3 class="anchored" data-anchor-id="sec-ch07solution14">Solution 14</h3>
<p><a href="#sec-ch07exercise14">Exercise 14</a></p>
<p>Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent and identically distributed exponential random variables with parameter <span class="math inline">\(\lambda\)</span>, their cdfs are given by <span class="math inline">\(F_{X_i}(x) = 1 - e^{-\lambda x}\)</span> for <span class="math inline">\(x \geq 0\)</span> and <span class="math inline">\(i = 1, 2\)</span>. The cdf of <span class="math inline">\(Y = \max\{X_1, X_2\}\)</span> is</p>
<p><span class="math display">\[
\begin{aligned}
F_Y(y) &amp;= P(Y \leq y) = P(\max\{X_1, X_2\} \leq y) = P(X_1 \leq y, X_2 \leq y) \\
&amp;= P(X_1 \leq y) P(X_2 \leq y) \quad \text{(since } X_1 \text{ and } X_2 \text{ are independent)} \\
&amp;= (1 - e^{-\lambda y})^2, \quad y \geq 0.
\end{aligned}
\]</span></p>
<p>To find the pdf <span class="math inline">\(f_Y(y)\)</span>, we differentiate <span class="math inline">\(F_Y(y)\)</span> with respect to <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
f_Y(y) = \frac{d}{dy} F_Y(y) = 2(1 - e^{-\lambda y})(\lambda e^{-\lambda y}) = 2\lambda e^{-\lambda y}(1 - e^{-\lambda y}), \quad y \geq 0.
\]</span></p>
<p><strong>Intuitive explanation:</strong> The maximum of two random variables is less than or equal to a value <span class="math inline">\(y\)</span> if and only if both variables are less than or equal to <span class="math inline">\(y\)</span>. For independent variables, the probability of this joint event is the product of the individual probabilities. The pdf is then found by differentiating the cdf. This solution uses the concepts of <strong>independence</strong> and the <strong>distribution of the maximum of random variables</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution15">
<h3 class="anchored" data-anchor-id="sec-ch07solution15">Solution 15</h3>
<p><a href="#sec-ch07exercise15">Exercise 15</a></p>
<p>We want to find <span class="math inline">\(P(Y &gt; 1 | X = x)\)</span>. First, we need to find the conditional pdf <span class="math inline">\(f_{Y|X}(y|x)\)</span>:</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_X(x)}.
\]</span></p>
<p>We first find the marginal pdf <span class="math inline">\(f_X(x)\)</span>:</p>
<p><span class="math display">\[
f_X(x) = \int_0^2 \frac{6}{7} \left( x^2 + \frac{xy}{2} \right) dy = \frac{6}{7} \left[ x^2 y + \frac{xy^2}{4} \right]_0^2 = \frac{6}{7} (2x^2 + x), \quad 0 &lt; x &lt; 1.
\]</span></p>
<p>Then,</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = \frac{\frac{6}{7}(x^2 + \frac{xy}{2})}{\frac{6}{7}(2x^2 + x)} = \frac{x^2 + \frac{xy}{2}}{2x^2 + x} = \frac{2x + y}{2(2x + 1)}, \quad 0 &lt; y &lt; 2.
\]</span></p>
<p>Now we can find <span class="math inline">\(P(Y &gt; 1 | X = x)\)</span>:</p>
<p><span class="math display">\[
P(Y &gt; 1 | X = x) = \int_1^2 f_{Y|X}(y|x) dy = \int_1^2 \frac{2x + y}{2(2x + 1)} dy = \frac{1}{2(2x + 1)} \left[ 2xy + \frac{1}{2} y^2 \right]_1^2 = \frac{2x + \frac{3}{2}}{2(2x + 1)} = \frac{4x + 3}{4(2x + 1)}.
\]</span></p>
<p><strong>Intuitive explanation:</strong> This problem involves finding a conditional probability given a specific value of another variable. We first find the conditional pdf using the formula, and then integrate it over the specified range of <span class="math inline">\(Y\)</span> to find the desired probability. This solution uses the concept of <strong>conditional probability</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution16">
<h3 class="anchored" data-anchor-id="sec-ch07solution16">Solution 16</h3>
<p><a href="#sec-ch07exercise16">Exercise 16</a></p>
<p>The conditional pdf <span class="math inline">\(f_{Y|X}(y|x)\)</span> is defined as the joint pdf <span class="math inline">\(f_{X,Y}(x, y)\)</span> divided by the marginal pdf <span class="math inline">\(f_X(x)\)</span>, provided that <span class="math inline">\(f_X(x) &gt; 0\)</span>. That is,</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_X(x)}, \quad \text{where } f_X(x) = \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy.
\]</span></p>
<p>This formula arises from the definition of conditional probability. For continuous random variables, we replace probabilities with probability density functions. The conditional pdf represents the probability density of <span class="math inline">\(Y\)</span> taking the value <span class="math inline">\(y\)</span> given that <span class="math inline">\(X\)</span> takes the value <span class="math inline">\(x\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The conditional pdf describes the distribution of one variable given a specific value of another variable. It is defined as the joint pdf divided by the marginal pdf of the conditioning variable. This is analogous to the formula for conditional probability of events. This solution uses the concept of <strong>conditional probability density function</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution17">
<h3 class="anchored" data-anchor-id="sec-ch07solution17">Solution 17</h3>
<p><a href="#sec-ch07exercise17">Exercise 17</a></p>
<p>We have <span class="math inline">\(\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i\)</span>. We want to find <span class="math inline">\(\text{cov}(\bar{X}, X_i)\)</span>. Using the properties of covariance, we have</p>
<p><span class="math display">\[
\begin{aligned}
\text{cov}(\bar{X}, X_i) &amp;= \text{cov} \left( \frac{1}{n} \sum_{j=1}^n X_j, X_i \right) \\
&amp;= \frac{1}{n} \text{cov} \left( \sum_{j=1}^n X_j, X_i \right) \\
&amp;= \frac{1}{n} \sum_{j=1}^n \text{cov}(X_j, X_i).
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(X_1, X_2, ..., X_n\)</span> are independent, <span class="math inline">\(\text{cov}(X_j, X_i) = 0\)</span> for <span class="math inline">\(j \neq i\)</span>. Also, <span class="math inline">\(\text{cov}(X_i, X_i) = \text{var}(X_i) = \sigma^2\)</span>. Therefore,</p>
<p><span class="math display">\[
\text{cov}(\bar{X}, X_i) = \frac{1}{n} \text{cov}(X_i, X_i) = \frac{1}{n} \sigma^2.
\]</span></p>
<p><strong>Intuitive explanation:</strong> This problem utilizes the properties of covariance, particularly how it behaves with linear combinations of random variables and how independence affects it. The covariance between the sample mean and any individual observation is equal to the variance of the individual observation divided by the sample size. This solution uses the concepts of <strong>covariance</strong> and <strong>independence</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution18">
<h3 class="anchored" data-anchor-id="sec-ch07solution18">Solution 18</h3>
<p><a href="#sec-ch07exercise18">Exercise 18</a></p>
<p>Let <span class="math inline">\(Y = X_1 / X_2\)</span>. We will use the transformation technique. Let <span class="math inline">\(Z = X_2\)</span>. Then <span class="math inline">\(X_1 = YZ\)</span> and <span class="math inline">\(X_2 = Z\)</span>. The Jacobian of this transformation is</p>
<p><span class="math display">\[
J = \begin{vmatrix}
\frac{\partial x_1}{\partial y} &amp; \frac{\partial x_1}{\partial z} \\
\frac{\partial x_2}{\partial y} &amp; \frac{\partial x_2}{\partial z}
\end{vmatrix} =
\begin{vmatrix}
z &amp; y \\
0 &amp; 1
\end{vmatrix} = z.
\]</span></p>
<p>Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent standard normal random variables, their joint pdf is</p>
<p><span class="math display">\[
f_{X_1, X_2}(x_1, x_2) = \frac{1}{2\pi} e^{-\frac{1}{2}(x_1^2 + x_2^2)}.
\]</span></p>
<p>The joint pdf of <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> is</p>
<p><span class="math display">\[
f_{Y,Z}(y, z) = f_{X_1, X_2}(yz, z) |J| = \frac{1}{2\pi} e^{-\frac{1}{2}(y^2 z^2 + z^2)} |z| = \frac{|z|}{2\pi} e^{-\frac{z^2}{2}(1 + y^2)}.
\]</span></p>
<p>To find the marginal pdf of <span class="math inline">\(Y\)</span>, we integrate with respect to <span class="math inline">\(z\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
f_Y(y) &amp;= \int_{-\infty}^{\infty} f_{Y,Z}(y, z) dz = \int_{-\infty}^{\infty} \frac{|z|}{2\pi} e^{-\frac{z^2}{2}(1 + y^2)} dz \\
&amp;= \frac{1}{\pi} \int_0^{\infty} z e^{-\frac{z^2}{2}(1 + y^2)} dz.
\end{aligned}
\]</span></p>
<p>Let <span class="math inline">\(u = \frac{z^2}{2}(1 + y^2)\)</span>, then <span class="math inline">\(du = z(1 + y^2) dz\)</span>. So,</p>
<p><span class="math display">\[
f_Y(y) = \frac{1}{\pi} \int_0^{\infty} \frac{e^{-u}}{1 + y^2} du = \frac{1}{\pi(1 + y^2)} \left[ -e^{-u} \right]_0^\infty = \frac{1}{\pi(1 + y^2)}, \quad -\infty &lt; y &lt; \infty.
\]</span></p>
<p>This is the pdf of a Cauchy distribution with location parameter 0 and scale parameter 1.</p>
<p><strong>Intuitive explanation:</strong> This problem involves finding the distribution of a ratio of two independent standard normal random variables using the transformation technique. The resulting distribution is a Cauchy distribution. This solution uses the concepts of <strong>transformation of random variables</strong> and <strong>Jacobian</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution19">
<h3 class="anchored" data-anchor-id="sec-ch07solution19">Solution 19</h3>
<p><a href="#sec-ch07exercise19">Exercise 19</a></p>
<p>For jointly normally distributed random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the conditional expectation is given by</p>
<p><span class="math display">\[
E(Y|X = x) = \mu_Y + \rho \frac{\sigma_Y}{\sigma_X} (x - \mu_X).
\]</span></p>
<p>This is a linear function of <span class="math inline">\(x\)</span>, where the slope is <span class="math inline">\(\rho \frac{\sigma_Y}{\sigma_X}\)</span> and the intercept is <span class="math inline">\(\mu_Y - \rho \frac{\sigma_Y}{\sigma_X} \mu_X\)</span>.</p>
<p><strong>Intuitive explanation:</strong> When two variables are jointly normally distributed, the conditional expectation of one given the other is a linear function. This is a special property of the bivariate normal distribution. This solution uses the concept of <strong>conditional expectation in the context of bivariate normal distribution</strong>.</p>
</section>
<section class="level3" id="sec-ch07solution20">
<h3 class="anchored" data-anchor-id="sec-ch07solution20">Solution 20</h3>
<p><a href="#sec-ch07exercise20">Exercise 20</a></p>
<p>Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent <span class="math inline">\(U(0, 1)\)</span> random variables, their pdfs are <span class="math inline">\(f_{X_1}(x_1) = 1\)</span> for <span class="math inline">\(0 &lt; x_1 &lt; 1\)</span> and <span class="math inline">\(f_{X_2}(x_2) = 1\)</span> for <span class="math inline">\(0 &lt; x_2 &lt; 1\)</span>. The convolution formula for the pdf of <span class="math inline">\(Y = X_1 + X_2\)</span> is</p>
<p><span class="math display">\[
f_Y(y) = \int_{-\infty}^{\infty} f_{X_1}(x_1) f_{X_2}(y - x_1) dx_1.
\]</span></p>
<p>We have <span class="math inline">\(f_{X_2}(y - x_1) = 1\)</span> if <span class="math inline">\(0 &lt; y - x_1 &lt; 1\)</span>, which is equivalent to <span class="math inline">\(y - 1 &lt; x_1 &lt; y\)</span>. Also, <span class="math inline">\(0 &lt; x_1 &lt; 1\)</span>. Thus, we have two cases:</p>
<ol type="1">
<li>If <span class="math inline">\(0 &lt; y &lt; 1\)</span>, then <span class="math inline">\(f_Y(y) = \int_0^y dx_1 = y\)</span>.</li>
<li>If <span class="math inline">\(1 &lt; y &lt; 2\)</span>, then <span class="math inline">\(f_Y(y) = \int_{y-1}^1 dx_1 = 1 - (y - 1) = 2 - y\)</span>.</li>
</ol>
<p>Therefore, the pdf of <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
f_Y(y) = \begin{cases}
y &amp; \text{if } 0 &lt; y &lt; 1 \\
2 - y &amp; \text{if } 1 &lt; y &lt; 2 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>This is the pdf of a triangular distribution on the interval <span class="math inline">\((0, 2)\)</span>.</p>
<p><strong>Intuitive explanation:</strong> This problem involves finding the distribution of the sum of two independent uniform random variables using the convolution formula. The resulting distribution is a triangular distribution. This solution uses the concept of <strong>convolution</strong>.</p>
</section>
</section>
<section class="level2" id="r-scripts">
<h2 class="anchored" data-anchor-id="r-scripts">R Scripts</h2>
<section class="level3" id="r-script-1-joint-marginal-and-conditional-distributions">
<h3 class="anchored" data-anchor-id="r-script-1-joint-marginal-and-conditional-distributions">R Script 1: Joint, Marginal, and Conditional Distributions</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb3-3"><a aria-hidden="true" href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a aria-hidden="true" href="#cb3-4" tabindex="-1"></a><span class="co"># Simulate data from a bivariate normal distribution</span></span>
<span id="cb3-5"><a aria-hidden="true" href="#cb3-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb3-6"><a aria-hidden="true" href="#cb3-6" tabindex="-1"></a>mu_x <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb3-7"><a aria-hidden="true" href="#cb3-7" tabindex="-1"></a>mu_y <span class="ot">&lt;-</span> <span class="dv">0</span></span>
<span id="cb3-8"><a aria-hidden="true" href="#cb3-8" tabindex="-1"></a>sigma_x <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb3-9"><a aria-hidden="true" href="#cb3-9" tabindex="-1"></a>sigma_y <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb3-10"><a aria-hidden="true" href="#cb3-10" tabindex="-1"></a>rho <span class="ot">&lt;-</span> <span class="fl">0.6</span></span>
<span id="cb3-11"><a aria-hidden="true" href="#cb3-11" tabindex="-1"></a>sigma_xy <span class="ot">&lt;-</span> rho <span class="sc">*</span> sigma_x <span class="sc">*</span> sigma_y</span>
<span id="cb3-12"><a aria-hidden="true" href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a aria-hidden="true" href="#cb3-13" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, mu_x, sigma_x)</span>
<span id="cb3-14"><a aria-hidden="true" href="#cb3-14" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, <span class="fu">sqrt</span>(sigma_y<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> sigma_xy<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> sigma_x<span class="sc">^</span><span class="dv">2</span>))</span>
<span id="cb3-15"><a aria-hidden="true" href="#cb3-15" tabindex="-1"></a>y <span class="ot">&lt;-</span> mu_y <span class="sc">+</span> sigma_xy <span class="sc">/</span> sigma_x<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> (x <span class="sc">-</span> mu_x) <span class="sc">+</span> epsilon</span>
<span id="cb3-16"><a aria-hidden="true" href="#cb3-16" tabindex="-1"></a></span>
<span id="cb3-17"><a aria-hidden="true" href="#cb3-17" tabindex="-1"></a><span class="co"># Create a data frame</span></span>
<span id="cb3-18"><a aria-hidden="true" href="#cb3-18" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y)</span>
<span id="cb3-19"><a aria-hidden="true" href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a aria-hidden="true" href="#cb3-20" tabindex="-1"></a><span class="co"># Visualize the joint distribution</span></span>
<span id="cb3-21"><a aria-hidden="true" href="#cb3-21" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb3-22"><a aria-hidden="true" href="#cb3-22" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb3-23"><a aria-hidden="true" href="#cb3-23" tabindex="-1"></a>  <span class="fu">geom_density_2d</span>() <span class="sc">+</span></span>
<span id="cb3-24"><a aria-hidden="true" href="#cb3-24" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Joint Distribution of X and Y"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-1-1.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb4-1"><a aria-hidden="true" href="#cb4-1" tabindex="-1"></a><span class="co"># Calculate and visualize marginal distributions</span></span>
<span id="cb4-2"><a aria-hidden="true" href="#cb4-2" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb4-3"><a aria-hidden="true" href="#cb4-3" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">bins =</span> <span class="dv">30</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">fill =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb4-4"><a aria-hidden="true" href="#cb4-4" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">"blue"</span>) <span class="sc">+</span></span>
<span id="cb4-5"><a aria-hidden="true" href="#cb4-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Marginal Distribution of X"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The dot-dot notation (`..density..`) was deprecated in ggplot2 3.4.0.
ℹ Please use `after_stat(density)` instead.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-1-2.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a aria-hidden="true" href="#cb6-1" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> y)) <span class="sc">+</span></span>
<span id="cb6-2"><a aria-hidden="true" href="#cb6-2" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> ..density..), <span class="at">bins =</span> <span class="dv">30</span>, <span class="at">alpha =</span> <span class="fl">0.5</span>, <span class="at">fill =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb6-3"><a aria-hidden="true" href="#cb6-3" tabindex="-1"></a>  <span class="fu">geom_density</span>(<span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb6-4"><a aria-hidden="true" href="#cb6-4" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Marginal Distribution of Y"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-1-3.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a aria-hidden="true" href="#cb7-1" tabindex="-1"></a><span class="co"># Estimate conditional distribution of Y given X</span></span>
<span id="cb7-2"><a aria-hidden="true" href="#cb7-2" tabindex="-1"></a>x_grid <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fu">min</span>(x), <span class="fu">max</span>(x), <span class="at">length.out =</span> <span class="dv">100</span>)</span>
<span id="cb7-3"><a aria-hidden="true" href="#cb7-3" tabindex="-1"></a>conditional_means <span class="ot">&lt;-</span> mu_y <span class="sc">+</span> sigma_xy <span class="sc">/</span> sigma_x<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> (x_grid <span class="sc">-</span> mu_x)</span>
<span id="cb7-4"><a aria-hidden="true" href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a aria-hidden="true" href="#cb7-5" tabindex="-1"></a>conditional_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_grid, <span class="at">y =</span> conditional_means)</span>
<span id="cb7-6"><a aria-hidden="true" href="#cb7-6" tabindex="-1"></a></span>
<span id="cb7-7"><a aria-hidden="true" href="#cb7-7" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb7-8"><a aria-hidden="true" href="#cb7-8" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb7-9"><a aria-hidden="true" href="#cb7-9" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data =</span> conditional_df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb7-10"><a aria-hidden="true" href="#cb7-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Conditional Expectation of Y given X"</span>, <span class="at">y =</span> <span class="st">"E(Y|X)"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
ℹ Please use `linewidth` instead.</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-1-4.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<ol type="1">
<li><strong>Load Libraries and Simulate Data:</strong> The script starts by loading the <code>tidyverse</code> package for data manipulation and visualization. It then simulates data from a <strong>bivariate normal distribution</strong>, which is a specific example of a <strong>multivariate distribution</strong> discussed in the text. The parameters <code>mu_x</code>, <code>mu_y</code>, <code>sigma_x</code>, <code>sigma_y</code>, and <code>rho</code> define the means, standard deviations, and correlation of the two variables, respectively.</li>
<li><strong>Create Data Frame:</strong> The simulated data points <code>x</code> and <code>y</code> are combined into a data frame <code>df</code>.</li>
<li><strong>Visualize Joint Distribution:</strong> The <code>ggplot2</code> functions are used to create a scatter plot of <code>x</code> and <code>y</code>, visualizing their <strong>joint distribution</strong>. The <code>geom_density_2d()</code> function adds contour lines to represent the density of the joint distribution.</li>
<li><strong>Calculate and Visualize Marginal Distributions:</strong> The script then calculates and visualizes the <strong>marginal distributions</strong> of <code>x</code> and <code>y</code> using histograms and density plots. The marginal distribution of a variable is obtained by “integrating out” the other variable from the joint distribution, as explained in the text (Theorem 7.3).</li>
<li><strong>Estimate and Visualize Conditional Distribution:</strong> Finally, the script estimates the <strong>conditional expectation</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span> using the formula <span class="math inline">\(E(Y|X) = \mu_Y + \rho \frac{\sigma_Y}{\sigma_X}(X - \mu_X)\)</span>, which is specific to the bivariate normal distribution (Example 7.16). The <code>x_grid</code> variable creates a sequence of <span class="math inline">\(x\)</span> values over which the conditional mean is calculated. The conditional mean is then plotted as a red line on top of the scatter plot, illustrating how the expected value of <span class="math inline">\(Y\)</span> changes with <span class="math inline">\(X\)</span>.</li>
</ol>
</section>
<section class="level3" id="r-script-2-law-of-iterated-expectations">
<h3 class="anchored" data-anchor-id="r-script-2-law-of-iterated-expectations">R Script 2: Law of Iterated Expectations</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a aria-hidden="true" href="#cb9-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb9-2"><a aria-hidden="true" href="#cb9-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb9-3"><a aria-hidden="true" href="#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a aria-hidden="true" href="#cb9-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb9-5"><a aria-hidden="true" href="#cb9-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb9-6"><a aria-hidden="true" href="#cb9-6" tabindex="-1"></a></span>
<span id="cb9-7"><a aria-hidden="true" href="#cb9-7" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb9-8"><a aria-hidden="true" href="#cb9-8" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb9-9"><a aria-hidden="true" href="#cb9-9" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb9-10"><a aria-hidden="true" href="#cb9-10" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="sc">*</span> x <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb9-11"><a aria-hidden="true" href="#cb9-11" tabindex="-1"></a></span>
<span id="cb9-12"><a aria-hidden="true" href="#cb9-12" tabindex="-1"></a><span class="co"># Calculate E(Y|X)</span></span>
<span id="cb9-13"><a aria-hidden="true" href="#cb9-13" tabindex="-1"></a><span class="co"># We'll use a simple linear model to estimate E(Y|X)</span></span>
<span id="cb9-14"><a aria-hidden="true" href="#cb9-14" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb9-15"><a aria-hidden="true" href="#cb9-15" tabindex="-1"></a>e_y_given_x <span class="ot">&lt;-</span> <span class="fu">fitted</span>(model)</span>
<span id="cb9-16"><a aria-hidden="true" href="#cb9-16" tabindex="-1"></a></span>
<span id="cb9-17"><a aria-hidden="true" href="#cb9-17" tabindex="-1"></a><span class="co"># Calculate E(Y)</span></span>
<span id="cb9-18"><a aria-hidden="true" href="#cb9-18" tabindex="-1"></a>e_y <span class="ot">&lt;-</span> <span class="fu">mean</span>(y)</span>
<span id="cb9-19"><a aria-hidden="true" href="#cb9-19" tabindex="-1"></a></span>
<span id="cb9-20"><a aria-hidden="true" href="#cb9-20" tabindex="-1"></a><span class="co"># Calculate E(E(Y|X))</span></span>
<span id="cb9-21"><a aria-hidden="true" href="#cb9-21" tabindex="-1"></a>e_e_y_given_x <span class="ot">&lt;-</span> <span class="fu">mean</span>(e_y_given_x)</span>
<span id="cb9-22"><a aria-hidden="true" href="#cb9-22" tabindex="-1"></a></span>
<span id="cb9-23"><a aria-hidden="true" href="#cb9-23" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb9-24"><a aria-hidden="true" href="#cb9-24" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"E(Y):"</span>, e_y))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "E(Y): 6.2952098880291"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a aria-hidden="true" href="#cb11-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"E(E(Y|X)):"</span>, e_e_y_given_x))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "E(E(Y|X)): 6.2952098880291"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a aria-hidden="true" href="#cb13-1" tabindex="-1"></a><span class="co"># Visualize the relationship</span></span>
<span id="cb13-2"><a aria-hidden="true" href="#cb13-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y, e_y_given_x)</span>
<span id="cb13-3"><a aria-hidden="true" href="#cb13-3" tabindex="-1"></a></span>
<span id="cb13-4"><a aria-hidden="true" href="#cb13-4" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb13-5"><a aria-hidden="true" href="#cb13-5" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> y), <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb13-6"><a aria-hidden="true" href="#cb13-6" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> e_y_given_x, <span class="at">color =</span> <span class="st">"E(Y|X)"</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb13-7"><a aria-hidden="true" href="#cb13-7" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> e_y, <span class="at">color =</span> <span class="st">"E(Y)"</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb13-8"><a aria-hidden="true" href="#cb13-8" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="fu">aes</span>(<span class="at">yintercept =</span> e_e_y_given_x, <span class="at">color =</span> <span class="st">"E(E(Y|X))"</span>), <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb13-9"><a aria-hidden="true" href="#cb13-9" tabindex="-1"></a>  <span class="fu">scale_color_manual</span>(</span>
<span id="cb13-10"><a aria-hidden="true" href="#cb13-10" tabindex="-1"></a>    <span class="at">values =</span> <span class="fu">c</span>(<span class="st">"E(Y|X)"</span> <span class="ot">=</span> <span class="st">"red"</span>, <span class="st">"E(Y)"</span> <span class="ot">=</span> <span class="st">"blue"</span>, <span class="st">"E(E(Y|X))"</span> <span class="ot">=</span> <span class="st">"green"</span>)</span>
<span id="cb13-11"><a aria-hidden="true" href="#cb13-11" tabindex="-1"></a>  ) <span class="sc">+</span></span>
<span id="cb13-12"><a aria-hidden="true" href="#cb13-12" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb13-13"><a aria-hidden="true" href="#cb13-13" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Illustration of the Law of Iterated Expectations"</span>,</span>
<span id="cb13-14"><a aria-hidden="true" href="#cb13-14" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"X"</span>,</span>
<span id="cb13-15"><a aria-hidden="true" href="#cb13-15" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Y"</span></span>
<span id="cb13-16"><a aria-hidden="true" href="#cb13-16" tabindex="-1"></a>  )</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-2-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<ol type="1">
<li><strong>Simulate Data:</strong> The script simulates two variables, <code>x</code> and <code>y$, where</code>y<code>is linearly related to</code>x` plus some random noise.</li>
<li><strong>Calculate E(Y|X):</strong> A simple linear regression model (<code>lm(y ~ x)</code>) is used to estimate the <strong>conditional expectation</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, denoted as <span class="math inline">\(E(Y|X)\)</span>. The fitted values from this model are used as an estimate of <span class="math inline">\(E(Y|X)\)</span>.</li>
<li><strong>Calculate E(Y):</strong> The script calculates the sample mean of <code>y</code>, which serves as an estimate of the unconditional expectation of <span class="math inline">\(Y\)</span>, <span class="math inline">\(E(Y)\)</span>.</li>
<li><strong>Calculate E(E(Y|X)):</strong> The script then calculates the mean of the estimated conditional expectations, which serves as an estimate of <span class="math inline">\(E(E(Y|X))\)</span>.</li>
<li><strong>Print and Visualize:</strong> The script prints the estimated values of <span class="math inline">\(E(Y)\)</span> and <span class="math inline">\(E(E(Y|X))\)</span>. It also creates a plot showing the simulated data, the estimated <span class="math inline">\(E(Y|X)\)</span> (red line), <span class="math inline">\(E(Y)\)</span> (blue line), and <span class="math inline">\(E(E(Y|X))\)</span> (green dashed line). The <strong>law of iterated expectations</strong> (Theorem 7.9) states that <span class="math inline">\(E(Y) = E(E(Y|X))\)</span>. The plot visually demonstrates this, as the blue and green lines should be close to each other.</li>
</ol>
</section>
<section class="level3" id="r-script-3-covariance-and-correlation">
<h3 class="anchored" data-anchor-id="r-script-3-covariance-and-correlation">R Script 3: Covariance and Correlation</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a aria-hidden="true" href="#cb14-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb14-2"><a aria-hidden="true" href="#cb14-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb14-3"><a aria-hidden="true" href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a aria-hidden="true" href="#cb14-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb14-5"><a aria-hidden="true" href="#cb14-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb14-6"><a aria-hidden="true" href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a aria-hidden="true" href="#cb14-7" tabindex="-1"></a><span class="co"># Simulate data with positive covariance</span></span>
<span id="cb14-8"><a aria-hidden="true" href="#cb14-8" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb14-9"><a aria-hidden="true" href="#cb14-9" tabindex="-1"></a>x_pos <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb14-10"><a aria-hidden="true" href="#cb14-10" tabindex="-1"></a>y_pos <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> x_pos <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb14-11"><a aria-hidden="true" href="#cb14-11" tabindex="-1"></a></span>
<span id="cb14-12"><a aria-hidden="true" href="#cb14-12" tabindex="-1"></a><span class="co"># Simulate data with negative covariance</span></span>
<span id="cb14-13"><a aria-hidden="true" href="#cb14-13" tabindex="-1"></a>x_neg <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb14-14"><a aria-hidden="true" href="#cb14-14" tabindex="-1"></a>y_neg <span class="ot">&lt;-</span> <span class="sc">-</span><span class="dv">2</span> <span class="sc">*</span> x_neg <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb14-15"><a aria-hidden="true" href="#cb14-15" tabindex="-1"></a></span>
<span id="cb14-16"><a aria-hidden="true" href="#cb14-16" tabindex="-1"></a><span class="co"># Calculate covariance and correlation</span></span>
<span id="cb14-17"><a aria-hidden="true" href="#cb14-17" tabindex="-1"></a>cov_pos <span class="ot">&lt;-</span> <span class="fu">cov</span>(x_pos, y_pos)</span>
<span id="cb14-18"><a aria-hidden="true" href="#cb14-18" tabindex="-1"></a>cor_pos <span class="ot">&lt;-</span> <span class="fu">cor</span>(x_pos, y_pos)</span>
<span id="cb14-19"><a aria-hidden="true" href="#cb14-19" tabindex="-1"></a></span>
<span id="cb14-20"><a aria-hidden="true" href="#cb14-20" tabindex="-1"></a>cov_neg <span class="ot">&lt;-</span> <span class="fu">cov</span>(x_neg, y_neg)</span>
<span id="cb14-21"><a aria-hidden="true" href="#cb14-21" tabindex="-1"></a>cor_neg <span class="ot">&lt;-</span> <span class="fu">cor</span>(x_neg, y_neg)</span>
<span id="cb14-22"><a aria-hidden="true" href="#cb14-22" tabindex="-1"></a></span>
<span id="cb14-23"><a aria-hidden="true" href="#cb14-23" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb14-24"><a aria-hidden="true" href="#cb14-24" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Positive Covariance:"</span>, cov_pos))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Positive Covariance: 2.02719235078027"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a aria-hidden="true" href="#cb16-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Positive Correlation:"</span>, cor_pos))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Positive Correlation: 0.723505834055404"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a aria-hidden="true" href="#cb18-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Negative Covariance:"</span>, cov_neg))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Negative Covariance: -1.82049919786636"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a aria-hidden="true" href="#cb20-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Negative Correlation:"</span>, cor_neg))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Negative Correlation: -0.672282220130653"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a aria-hidden="true" href="#cb22-1" tabindex="-1"></a><span class="co"># Visualize the data</span></span>
<span id="cb22-2"><a aria-hidden="true" href="#cb22-2" tabindex="-1"></a>df_pos <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_pos, <span class="at">y =</span> y_pos, <span class="at">type =</span> <span class="st">"Positive"</span>)</span>
<span id="cb22-3"><a aria-hidden="true" href="#cb22-3" tabindex="-1"></a>df_neg <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x_neg, <span class="at">y =</span> y_neg, <span class="at">type =</span> <span class="st">"Negative"</span>)</span>
<span id="cb22-4"><a aria-hidden="true" href="#cb22-4" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(df_pos, df_neg)</span>
<span id="cb22-5"><a aria-hidden="true" href="#cb22-5" tabindex="-1"></a></span>
<span id="cb22-6"><a aria-hidden="true" href="#cb22-6" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> type)) <span class="sc">+</span></span>
<span id="cb22-7"><a aria-hidden="true" href="#cb22-7" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb22-8"><a aria-hidden="true" href="#cb22-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Scatter Plots with Positive and Negative Covariance"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-3-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<ol type="1">
<li><strong>Simulate Data:</strong> The script simulates two sets of data. In the first set (<code>x_pos</code>, <code>y_pos</code>), <code>y_pos</code> is positively related to <code>x_pos</code>, meaning that when <code>x_pos</code> increases, <code>y_pos</code> tends to increase as well. In the second set (<code>x_neg</code>, <code>y_neg</code>), <code>y_neg</code> is negatively related to <code>x_neg</code>, meaning that when <code>x_neg</code> increases, <code>y_neg</code> tends to decrease.</li>
<li><strong>Calculate Covariance and Correlation:</strong> The script calculates the <strong>covariance</strong> and <strong>correlation</strong> (Definition 7.5 and Definition 7.6) between <code>x_pos</code> and <code>y_pos</code>, and between <code>x_neg</code> and <code>y_neg</code>, using the <code>cov()</code> and <code>cor()</code> functions, respectively.</li>
<li><strong>Print and Visualize:</strong> The script prints the calculated covariance and correlation values. It also creates a scatter plot showing the two sets of data in different colors. The plot visually demonstrates the concept of covariance. When the covariance is positive, the points tend to slope upwards. When the covariance is negative, the points tend to slope downwards. The correlation coefficient standardizes the covariance, making it easier to compare the strength of the linear relationship across different datasets.</li>
</ol>
</section>
<section class="level3" id="r-script-4-analysis-of-variance">
<h3 class="anchored" data-anchor-id="r-script-4-analysis-of-variance">R Script 4: Analysis of Variance</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a aria-hidden="true" href="#cb23-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb23-2"><a aria-hidden="true" href="#cb23-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb23-3"><a aria-hidden="true" href="#cb23-3" tabindex="-1"></a></span>
<span id="cb23-4"><a aria-hidden="true" href="#cb23-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb23-5"><a aria-hidden="true" href="#cb23-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb23-6"><a aria-hidden="true" href="#cb23-6" tabindex="-1"></a></span>
<span id="cb23-7"><a aria-hidden="true" href="#cb23-7" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb23-8"><a aria-hidden="true" href="#cb23-8" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb23-9"><a aria-hidden="true" href="#cb23-9" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb23-10"><a aria-hidden="true" href="#cb23-10" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> <span class="fu">sin</span>(x) <span class="sc">+</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb23-11"><a aria-hidden="true" href="#cb23-11" tabindex="-1"></a></span>
<span id="cb23-12"><a aria-hidden="true" href="#cb23-12" tabindex="-1"></a><span class="co"># Estimate E(Y|X) using a smoothing spline</span></span>
<span id="cb23-13"><a aria-hidden="true" href="#cb23-13" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">smooth.spline</span>(x, y)</span>
<span id="cb23-14"><a aria-hidden="true" href="#cb23-14" tabindex="-1"></a>e_y_given_x <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, x)<span class="sc">$</span>y</span>
<span id="cb23-15"><a aria-hidden="true" href="#cb23-15" tabindex="-1"></a></span>
<span id="cb23-16"><a aria-hidden="true" href="#cb23-16" tabindex="-1"></a><span class="co"># Calculate Var(Y)</span></span>
<span id="cb23-17"><a aria-hidden="true" href="#cb23-17" tabindex="-1"></a>var_y <span class="ot">&lt;-</span> <span class="fu">var</span>(y)</span>
<span id="cb23-18"><a aria-hidden="true" href="#cb23-18" tabindex="-1"></a></span>
<span id="cb23-19"><a aria-hidden="true" href="#cb23-19" tabindex="-1"></a><span class="co"># Calculate Var(E(Y|X))</span></span>
<span id="cb23-20"><a aria-hidden="true" href="#cb23-20" tabindex="-1"></a>var_e_y_given_x <span class="ot">&lt;-</span> <span class="fu">var</span>(e_y_given_x)</span>
<span id="cb23-21"><a aria-hidden="true" href="#cb23-21" tabindex="-1"></a></span>
<span id="cb23-22"><a aria-hidden="true" href="#cb23-22" tabindex="-1"></a><span class="co"># Calculate E(Var(Y|X))</span></span>
<span id="cb23-23"><a aria-hidden="true" href="#cb23-23" tabindex="-1"></a>residuals <span class="ot">&lt;-</span> y <span class="sc">-</span> e_y_given_x</span>
<span id="cb23-24"><a aria-hidden="true" href="#cb23-24" tabindex="-1"></a>e_var_y_given_x <span class="ot">&lt;-</span> <span class="fu">mean</span>(residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb23-25"><a aria-hidden="true" href="#cb23-25" tabindex="-1"></a></span>
<span id="cb23-26"><a aria-hidden="true" href="#cb23-26" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb23-27"><a aria-hidden="true" href="#cb23-27" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Var(Y):"</span>, var_y))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Var(Y): 2.78866518293261"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a aria-hidden="true" href="#cb25-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Var(E(Y|X)):"</span>, var_e_y_given_x))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Var(E(Y|X)): 1.71071639953854"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a aria-hidden="true" href="#cb27-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"E(Var(Y|X)):"</span>, e_var_y_given_x))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "E(Var(Y|X)): 1.06172397438605"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a aria-hidden="true" href="#cb29-1" tabindex="-1"></a><span class="fu">print</span>(</span>
<span id="cb29-2"><a aria-hidden="true" href="#cb29-2" tabindex="-1"></a>  <span class="fu">paste</span>(</span>
<span id="cb29-3"><a aria-hidden="true" href="#cb29-3" tabindex="-1"></a>    <span class="st">"Var(E(Y|X)) + E(Var(Y|X)):"</span>,</span>
<span id="cb29-4"><a aria-hidden="true" href="#cb29-4" tabindex="-1"></a>    var_e_y_given_x <span class="sc">+</span> e_var_y_given_x</span>
<span id="cb29-5"><a aria-hidden="true" href="#cb29-5" tabindex="-1"></a>  )</span>
<span id="cb29-6"><a aria-hidden="true" href="#cb29-6" tabindex="-1"></a>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Var(E(Y|X)) + E(Var(Y|X)): 2.77244037392459"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a aria-hidden="true" href="#cb31-1" tabindex="-1"></a><span class="co"># Visualize the relationship</span></span>
<span id="cb31-2"><a aria-hidden="true" href="#cb31-2" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(x, y, e_y_given_x)</span>
<span id="cb31-3"><a aria-hidden="true" href="#cb31-3" tabindex="-1"></a></span>
<span id="cb31-4"><a aria-hidden="true" href="#cb31-4" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb31-5"><a aria-hidden="true" href="#cb31-5" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> y), <span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb31-6"><a aria-hidden="true" href="#cb31-6" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="fu">aes</span>(<span class="at">y =</span> e_y_given_x, <span class="at">color =</span> <span class="st">"E(Y|X)"</span>), <span class="at">size =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb31-7"><a aria-hidden="true" href="#cb31-7" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb31-8"><a aria-hidden="true" href="#cb31-8" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Illustration of Analysis of Variance"</span>,</span>
<span id="cb31-9"><a aria-hidden="true" href="#cb31-9" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"X"</span>,</span>
<span id="cb31-10"><a aria-hidden="true" href="#cb31-10" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Y"</span></span>
<span id="cb31-11"><a aria-hidden="true" href="#cb31-11" tabindex="-1"></a>  )</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-4-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<ol type="1">
<li><strong>Simulate Data:</strong> The script simulates data where <code>y</code> is a nonlinear function of <code>x</code> (specifically, a sine function) plus some random noise.</li>
<li><strong>Estimate E(Y|X):</strong> It uses a smoothing spline (<code>smooth.spline()</code>) to estimate the <strong>conditional expectation</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, <span class="math inline">\(E(Y|X)\)</span>. Smoothing splines are a nonparametric method for estimating the relationship between two variables.</li>
<li><strong>Calculate Variances:</strong> The script calculates the <strong>variance</strong> of <code>y</code>, <span class="math inline">\(\text{var}(Y)\)</span>, the variance of the estimated conditional expectation, <span class="math inline">\(\text{var}(E(Y|X))\)</span>, and the average squared residuals, which serves as an estimate of <span class="math inline">\(E[\text{var}(Y|X)]\)</span>.</li>
<li><strong>Print Results:</strong> The script prints the calculated variances and their sum. The <strong>analysis of variance formula</strong> (Theorem 7.10) states that <span class="math inline">\(\text{var}(Y) = \text{var}(E(Y|X)) + E[\text{var}(Y|X)]\)</span>. The printed results should demonstrate this equality, with some approximation error due to the estimation of <span class="math inline">\(E(Y|X)\)</span>.</li>
<li><strong>Visualize:</strong> The plot shows the simulated data and the estimated <span class="math inline">\(E(Y|X)\)</span> (red line). This visually illustrates how the conditional expectation captures the average trend of <code>y</code> as <code>x</code> changes.</li>
</ol>
</section>
<section class="level3" id="r-script-5-multivariate-transformation">
<h3 class="anchored" data-anchor-id="r-script-5-multivariate-transformation">R Script 5: Multivariate Transformation</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a aria-hidden="true" href="#cb32-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb32-2"><a aria-hidden="true" href="#cb32-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb32-3"><a aria-hidden="true" href="#cb32-3" tabindex="-1"></a></span>
<span id="cb32-4"><a aria-hidden="true" href="#cb32-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb32-5"><a aria-hidden="true" href="#cb32-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">202</span>)</span>
<span id="cb32-6"><a aria-hidden="true" href="#cb32-6" tabindex="-1"></a></span>
<span id="cb32-7"><a aria-hidden="true" href="#cb32-7" tabindex="-1"></a><span class="co"># Define the transformation function</span></span>
<span id="cb32-8"><a aria-hidden="true" href="#cb32-8" tabindex="-1"></a>transform_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(x1, x2) {</span>
<span id="cb32-9"><a aria-hidden="true" href="#cb32-9" tabindex="-1"></a>  y1 <span class="ot">&lt;-</span> x1 <span class="sc">+</span> x2</span>
<span id="cb32-10"><a aria-hidden="true" href="#cb32-10" tabindex="-1"></a>  y2 <span class="ot">&lt;-</span> x1 <span class="sc">-</span> x2</span>
<span id="cb32-11"><a aria-hidden="true" href="#cb32-11" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">data.frame</span>(y1, y2))</span>
<span id="cb32-12"><a aria-hidden="true" href="#cb32-12" tabindex="-1"></a>}</span>
<span id="cb32-13"><a aria-hidden="true" href="#cb32-13" tabindex="-1"></a></span>
<span id="cb32-14"><a aria-hidden="true" href="#cb32-14" tabindex="-1"></a><span class="co"># Simulate independent standard normal random variables</span></span>
<span id="cb32-15"><a aria-hidden="true" href="#cb32-15" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb32-16"><a aria-hidden="true" href="#cb32-16" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb32-17"><a aria-hidden="true" href="#cb32-17" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb32-18"><a aria-hidden="true" href="#cb32-18" tabindex="-1"></a></span>
<span id="cb32-19"><a aria-hidden="true" href="#cb32-19" tabindex="-1"></a><span class="co"># Apply the transformation</span></span>
<span id="cb32-20"><a aria-hidden="true" href="#cb32-20" tabindex="-1"></a>transformed_data <span class="ot">&lt;-</span> <span class="fu">transform_fun</span>(x1, x2)</span>
<span id="cb32-21"><a aria-hidden="true" href="#cb32-21" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> transformed_data<span class="sc">$</span>y1</span>
<span id="cb32-22"><a aria-hidden="true" href="#cb32-22" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> transformed_data<span class="sc">$</span>y2</span>
<span id="cb32-23"><a aria-hidden="true" href="#cb32-23" tabindex="-1"></a></span>
<span id="cb32-24"><a aria-hidden="true" href="#cb32-24" tabindex="-1"></a><span class="co"># Calculate the Jacobian (absolute value)</span></span>
<span id="cb32-25"><a aria-hidden="true" href="#cb32-25" tabindex="-1"></a><span class="co"># In this case, the Jacobian is a constant</span></span>
<span id="cb32-26"><a aria-hidden="true" href="#cb32-26" tabindex="-1"></a>jacobian <span class="ot">&lt;-</span> <span class="fu">abs</span>(<span class="fu">det</span>(<span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="sc">-</span><span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">2</span>)))</span>
<span id="cb32-27"><a aria-hidden="true" href="#cb32-27" tabindex="-1"></a></span>
<span id="cb32-28"><a aria-hidden="true" href="#cb32-28" tabindex="-1"></a><span class="co"># Visualize the original and transformed data</span></span>
<span id="cb32-29"><a aria-hidden="true" href="#cb32-29" tabindex="-1"></a>df_original <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> x1, <span class="at">y =</span> x2, <span class="at">type =</span> <span class="st">"Original"</span>)</span>
<span id="cb32-30"><a aria-hidden="true" href="#cb32-30" tabindex="-1"></a>df_transformed <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb32-31"><a aria-hidden="true" href="#cb32-31" tabindex="-1"></a>  <span class="at">x =</span> y1,</span>
<span id="cb32-32"><a aria-hidden="true" href="#cb32-32" tabindex="-1"></a>  <span class="at">y =</span> y2,</span>
<span id="cb32-33"><a aria-hidden="true" href="#cb32-33" tabindex="-1"></a>  <span class="at">type =</span> <span class="st">"Transformed"</span></span>
<span id="cb32-34"><a aria-hidden="true" href="#cb32-34" tabindex="-1"></a>)</span>
<span id="cb32-35"><a aria-hidden="true" href="#cb32-35" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">rbind</span>(df_original, df_transformed)</span>
<span id="cb32-36"><a aria-hidden="true" href="#cb32-36" tabindex="-1"></a></span>
<span id="cb32-37"><a aria-hidden="true" href="#cb32-37" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y, <span class="at">color =</span> type)) <span class="sc">+</span></span>
<span id="cb32-38"><a aria-hidden="true" href="#cb32-38" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>) <span class="sc">+</span></span>
<span id="cb32-39"><a aria-hidden="true" href="#cb32-39" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb32-40"><a aria-hidden="true" href="#cb32-40" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Original vs. Transformed Data"</span>,</span>
<span id="cb32-41"><a aria-hidden="true" href="#cb32-41" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Variable 1"</span>,</span>
<span id="cb32-42"><a aria-hidden="true" href="#cb32-42" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Variable 2"</span></span>
<span id="cb32-43"><a aria-hidden="true" href="#cb32-43" tabindex="-1"></a>  )</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-5-1.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a aria-hidden="true" href="#cb33-1" tabindex="-1"></a><span class="co"># Estimate the joint density of the transformed variables</span></span>
<span id="cb33-2"><a aria-hidden="true" href="#cb33-2" tabindex="-1"></a><span class="co"># We'll use kernel density estimation for visualization</span></span>
<span id="cb33-3"><a aria-hidden="true" href="#cb33-3" tabindex="-1"></a>kde <span class="ot">&lt;-</span> MASS<span class="sc">::</span><span class="fu">kde2d</span>(y1, y2, <span class="at">n =</span> <span class="dv">100</span>)</span>
<span id="cb33-4"><a aria-hidden="true" href="#cb33-4" tabindex="-1"></a></span>
<span id="cb33-5"><a aria-hidden="true" href="#cb33-5" tabindex="-1"></a><span class="co"># Create a data frame for the density</span></span>
<span id="cb33-6"><a aria-hidden="true" href="#cb33-6" tabindex="-1"></a>kde_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb33-7"><a aria-hidden="true" href="#cb33-7" tabindex="-1"></a>  <span class="fu">expand.grid</span>(<span class="at">y1 =</span> kde<span class="sc">$</span>x, <span class="at">y2 =</span> kde<span class="sc">$</span>y),</span>
<span id="cb33-8"><a aria-hidden="true" href="#cb33-8" tabindex="-1"></a>  <span class="at">z =</span> <span class="fu">as.vector</span>(kde<span class="sc">$</span>z)</span>
<span id="cb33-9"><a aria-hidden="true" href="#cb33-9" tabindex="-1"></a>)</span>
<span id="cb33-10"><a aria-hidden="true" href="#cb33-10" tabindex="-1"></a></span>
<span id="cb33-11"><a aria-hidden="true" href="#cb33-11" tabindex="-1"></a><span class="co"># Visualize the joint density of the transformed variables</span></span>
<span id="cb33-12"><a aria-hidden="true" href="#cb33-12" tabindex="-1"></a><span class="fu">ggplot</span>(kde_df, <span class="fu">aes</span>(<span class="at">x =</span> y1, <span class="at">y =</span> y2, <span class="at">z =</span> z)) <span class="sc">+</span></span>
<span id="cb33-13"><a aria-hidden="true" href="#cb33-13" tabindex="-1"></a>  <span class="fu">geom_contour</span>(<span class="fu">aes</span>(<span class="at">color =</span> ..level..)) <span class="sc">+</span></span>
<span id="cb33-14"><a aria-hidden="true" href="#cb33-14" tabindex="-1"></a>  <span class="fu">labs</span>(</span>
<span id="cb33-15"><a aria-hidden="true" href="#cb33-15" tabindex="-1"></a>    <span class="at">title =</span> <span class="st">"Estimated Joint Density of Transformed Variables"</span>,</span>
<span id="cb33-16"><a aria-hidden="true" href="#cb33-16" tabindex="-1"></a>    <span class="at">x =</span> <span class="st">"Y1"</span>,</span>
<span id="cb33-17"><a aria-hidden="true" href="#cb33-17" tabindex="-1"></a>    <span class="at">y =</span> <span class="st">"Y2"</span></span>
<span id="cb33-18"><a aria-hidden="true" href="#cb33-18" tabindex="-1"></a>  )</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap07_files/figure-html/unnamed-chunk-5-2.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<ol type="1">
<li><strong>Define Transformation:</strong> The script defines a function <code>transform_fun</code> that takes two variables, <code>x1</code> and <code>x2</code>, and returns two new variables, <code>y1 = x1 + x2</code> and <code>y2 = x1 - x2</code>. This is a linear <strong>multivariate transformation</strong>.</li>
<li><strong>Simulate Data:</strong> It simulates two independent standard normal random variables, <code>x1</code> and <code>x2</code>.</li>
<li><strong>Apply Transformation:</strong> The <code>transform_fun</code> function is applied to <code>x1</code> and <code>x2</code> to obtain the transformed variables <code>y1</code> and <code>y2</code>.</li>
<li><strong>Calculate Jacobian:</strong> The script calculates the <strong>Jacobian</strong> of the transformation (Theorem 7.17), which is a determinant of the matrix of partial derivatives of the transformation. In this case, the Jacobian is a constant, 2.</li>
<li><strong>Visualize Data:</strong> The script creates a scatter plot showing both the original data (<code>x1</code>, <code>x2</code>) and the transformed data (<code>y1</code>, <code>y2</code>) in different colors. This visually demonstrates how the transformation changes the relationship between the variables.</li>
<li><strong>Estimate and Visualize Joint Density:</strong> The script uses kernel density estimation (<code>MASS::kde2d</code>) to estimate the <strong>joint density</strong> of the transformed variables <code>y1</code> and <code>y2</code>. It then creates a contour plot of the estimated density, providing a visualization of the distribution of the transformed variables.</li>
</ol>
<p>These R scripts demonstrate various concepts from the text, including joint, marginal, and conditional distributions, the law of iterated expectations, covariance and correlation, analysis of variance, and multivariate transformations. The visualizations help to build an intuitive understanding of these concepts. The scripts also illustrate how to use R functions to simulate data, estimate statistical quantities, and create informative plots.</p>
</section>
</section>
<section class="level2" id="youtube-videos">
<h2 class="anchored" data-anchor-id="youtube-videos">YouTube Videos</h2>
<p>Here are some YouTube videos that explain concepts mentioned in the attached text, along with explanations of how they relate to the text:</p>
<section class="level3" id="multivariate-random-variables-and-distributions">
<h3 class="anchored" data-anchor-id="multivariate-random-variables-and-distributions">1. Multivariate Random Variables and Distributions</h3>
<ul>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=h4jvu8PW8YE">Multivariate Normal Distribution - by Jack Baker</a></p>
<ul>
<li><strong>Relation to Text:</strong> This series of videos provides a comprehensive introduction to the <strong>multivariate normal distribution</strong>, which is a key example of a <strong>multivariate distribution</strong> discussed in Section 7.1 and Example 7.16 of the text. It covers topics such as the joint probability density function, marginal distributions, and conditional distributions, aligning with the concepts introduced in the text.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=CQS4xxz-2s4">Joint Probability Explained | by Intelligent Systems Lab</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video explains the concepts of <strong>joint probability</strong>, <strong>marginal probability</strong>, and <strong>conditional probability</strong> using simple examples. It is directly related to Section 7.1, where the joint c.d.f., marginal c.d.f., and conditional distributions are defined.</li>
</ul></li>
</ul>
</section>
<section class="level3" id="conditional-distributions-and-independence-1">
<h3 class="anchored" data-anchor-id="conditional-distributions-and-independence-1">2. Conditional Distributions and Independence</h3>
<ul>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=HZGCoVF3YvM">Bayes theorem, the geometry of changing beliefs - by 3Blue1Brown</a></p>
<ul>
<li><strong>Relation to Text:</strong> While not specifically focused on continuous distributions, this video provides an excellent intuitive explanation of <strong>conditional probability</strong>, which is fundamental to understanding <strong>conditional distributions</strong> as defined in Section 7.2. The visuals and examples can help grasp the concept before moving on to the more formal definitions in the text.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=SrEmzdOT65s">Joint, marginal and conditional probability | Independence | by zedstatistics</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video focuses on discrete random variables, it explains the concept of <strong>independence</strong>, relating to Definition 7.3 and 7.4 in the text.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=EqqUfIQ8948">Joint, Marginal, and Conditional Distributions | by MIT OpenCourseWare</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video discusses <strong>conditional distributions</strong> for both discrete and continuous random variables, it is a good introductory material related to Section 7.2, where conditional distributions are defined for discrete and continuous cases.</li>
</ul></li>
</ul>
</section>
<section class="level3" id="covariance-and-correlation">
<h3 class="anchored" data-anchor-id="covariance-and-correlation">3. Covariance and Correlation</h3>
<ul>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=mG__Wpp9dns">What is COVARIANCE? What is CORRELATION? - by zedstatistics</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video provides a clear explanation of <strong>covariance</strong> and <strong>correlation</strong>, which are defined and discussed in Section 7.3. It explains how these measures quantify the linear relationship between two random variables.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=dsyTQNUvqH0">Introduction to Correlation | by Cody Baldwin</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video discusses the concept of <strong>dependence</strong> and <strong>correlation</strong>, <strong>covariance</strong>, and their differences. It aligns with Section 7.3 and Theorem 7.7.</li>
</ul></li>
</ul>
</section>
<section class="level3" id="conditional-expectation-and-regression">
<h3 class="anchored" data-anchor-id="conditional-expectation-and-regression">4. Conditional Expectation and Regression</h3>
<ul>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=_gRQ67i7yL8">Understanding Conditional Expectation | by Harvard Online</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video introduces the concept of <strong>conditional expectation</strong>, which is a central topic in Section 7.4. It provides examples and explains how to calculate conditional expectation in different scenarios.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=nk2CQITm_eo">StatQuest: Linear Regression | by StatQuest with Josh Starmer</a></p>
<ul>
<li><strong>Relation to Text:</strong> While this video is about linear regression in general, it provides a good intuitive understanding of how the <strong>best linear predictor</strong> (discussed in Theorem 7.12) relates to the concept of minimizing the mean squared error.</li>
</ul></li>
</ul>
</section>
<section class="level3" id="law-of-iterated-expectations">
<h3 class="anchored" data-anchor-id="law-of-iterated-expectations">5. Law of Iterated Expectations</h3>
<ul>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=Ki2HpTCPwhM">Law of Iterated Expectations | by Ben Lambert</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video explains the <strong>law of iterated expectations</strong> (Theorem 7.9) with clear examples and intuitive explanations. It demonstrates how <span class="math inline">\(E(Y) = E[E(Y|X)]\)</span>.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=GnEyIawrWBg">Law of Total Expectation | by MIT OpenCourseWare</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video presents the <strong>law of iterated expectations</strong> in a broader context. It provides various examples and problems to understand the concept related to Theorem 7.9.</li>
</ul></li>
</ul>
</section>
<section class="level3" id="analysis-of-variance">
<h3 class="anchored" data-anchor-id="analysis-of-variance">6. Analysis of Variance</h3>
<ul>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=-nF8ayyYLkU">Conditional Variance | by Mike, the Mathematician</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video explains how to calculate <strong>conditional variance</strong> and then introduces the <strong>analysis of variance formula</strong> (Theorem 7.10), showing how total variance can be decomposed into the sum of the expectation of conditional variance and the variance of conditional expectation.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=awkMnxvTGrk">Law of total variance | by Mike, the Mathematician</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video explains the <strong>analysis of variance formula</strong> in more detail (Theorem 7.10).</li>
</ul></li>
</ul>
</section>
<section class="level3" id="multivariate-transformations-1">
<h3 class="anchored" data-anchor-id="multivariate-transformations-1">7. Multivariate Transformations</h3>
<ul>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=rvQdrKgG3kI">Multivariable Change of Variable Theorem: Example 1 | by Mike, the Mathematician</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video provides another example of <strong>multivariable change of variables</strong> using Jacobian determinant, which helps in understanding <strong>multivariate transformations</strong> discussed in Section 7.6 and Theorem 7.17.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=wUF-lyyWpUc">Change of Variables &amp; The Jacobian | Multi-variable Integration | by Dr. Trefor Bazett</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video explains how to find the Jacobian determinant in general, which is a fundamental concept in <strong>multivariate transformations</strong> discussed in Section 7.6 and Theorem 7.17.</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/watch?v=IaSGqQa5O-M">Convolutions | Why X+Y in probability is a beautiful mess | by 3Blue1Brown</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video explains how to add random variables, with connections to the central limit theorem. Convolution was discussed in Theorem 4.25 (BH), Exercise 4.14 (BH) and Example 7.23 (OL).</li>
</ul></li>
<li><p><strong>Video:</strong> <a href="https://www.youtube.com/playlist?list=PLJYjjnnccKYDppALiJlHskU8md904FXgd">Structural Reliability 10h - Copulas | by Jack Baker</a></p>
<ul>
<li><strong>Relation to Text:</strong> This video explores the concept of copulas—a technique used in Monte Carlo simulations to simulate random variables from a joint distribution. It delves into the process of generating samples using marginal CDFs and the copula function. This high-level overview provides insight into how copulas introduce dependency structures. Copulas are covered in Section 7.6.2 Copula (OL) and Theorem 7.18 (OL).</li>
</ul></li>
</ul>
<p>These videos provide a good starting point for understanding the concepts presented in the text. They offer visual explanations, examples, and intuitive interpretations that can complement the more formal mathematical treatment in the text. Remember to watch the videos in conjunction with reading the text to get a comprehensive understanding of the topics.</p>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch07mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch07mcsolution1">MC Solution 1</a></p>
<p>The joint probability density function of two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is given by:</p>
<p><span class="math display">\[
f_{X,Y}(x, y) = \begin{cases}
c(2x + y) &amp; \text{if } 0 &lt; x &lt; 1, 0 &lt; y &lt; 2 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>What is the value of the constant <span class="math inline">\(c\)</span>?</p>
<ol type="a">
<li><span class="math inline">\(\frac{1}{4}\)</span></li>
<li><span class="math inline">\(\frac{1}{6}\)</span></li>
<li><span class="math inline">\(\frac{1}{8}\)</span></li>
<li><span class="math inline">\(\frac{1}{12}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch07mcsolution2">MC Solution 2</a></p>
<p>Given the joint probability density function:</p>
<p><span class="math display">\[
f_{X,Y}(x, y) = \begin{cases}
\frac{1}{6}(2x + y) &amp; \text{if } 0 &lt; x &lt; 1, 0 &lt; y &lt; 2 \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>What is the marginal probability density function of <span class="math inline">\(X\)</span>, <span class="math inline">\(f_X(x)\)</span>?</p>
<ol type="a">
<li><span class="math inline">\(\frac{1}{3}(2x + 1)\)</span></li>
<li><span class="math inline">\(\frac{1}{6}(4x + 1)\)</span></li>
<li><span class="math inline">\(\frac{1}{3}(x + 2)\)</span></li>
<li><span class="math inline">\(\frac{1}{6}(2x + 2)\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch07mcsolution3">MC Solution 3</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be independent random variables with <span class="math inline">\(X \sim \text{Exp}(1)\)</span> and <span class="math inline">\(Y \sim \text{Exp}(2)\)</span>. What is <span class="math inline">\(P(X &lt; Y)\)</span>?</p>
<ol type="a">
<li><span class="math inline">\(\frac{1}{3}\)</span></li>
<li><span class="math inline">\(\frac{1}{2}\)</span></li>
<li><span class="math inline">\(\frac{2}{3}\)</span></li>
<li><span class="math inline">\(\frac{3}{4}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch07mcsolution4">MC Solution 4</a></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables with <span class="math inline">\(\text{cov}(X, Y) = 0\)</span>, which of the following statements is always true?</p>
<ol type="a">
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</li>
<li><span class="math inline">\(E(XY) = 0\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = E(Y)\)</span>.</li>
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are normally distributed.</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch07mcsolution5">MC Solution 5</a></p>
<p>The <strong>correlation coefficient</strong> between two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is <span class="math inline">\(\rho_{XY} = 0.8\)</span>. If <span class="math inline">\(Z = 2X + 3\)</span> and <span class="math inline">\(W = 4Y - 1\)</span>, what is the correlation coefficient <span class="math inline">\(\rho_{ZW}\)</span>?</p>
<ol type="a">
<li>0.4</li>
<li>0.6</li>
<li>0.8</li>
<li>1.0</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch07mcsolution6">MC Solution 6</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables with <span class="math inline">\(E(Y|X) = 2X + 1\)</span>. If <span class="math inline">\(E(X) = 3\)</span>, what is <span class="math inline">\(E(Y)\)</span> according to the <strong>law of iterated expectations</strong>?</p>
<ol type="a">
<li>3</li>
<li>5</li>
<li>7</li>
<li>9</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch07mcsolution7">MC Solution 7</a></p>
<p>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are jointly normally distributed with <span class="math inline">\(\text{var}(X) = 4\)</span>, <span class="math inline">\(\text{var}(Y) = 9\)</span>, and <span class="math inline">\(\text{cov}(X, Y) = 3\)</span>. What is the <strong>conditional variance</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>, <span class="math inline">\(\text{var}(Y|X)\)</span>?</p>
<ol type="a">
<li>2.25</li>
<li>6.75</li>
<li>8.25</li>
<li>9.00</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch07mcsolution8">MC Solution 8</a></p>
<p>Which of the following statements is true about the <strong>best linear predictor</strong> <span class="math inline">\(E_L(Y|X)\)</span> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>?</p>
<ol type="a">
<li><span class="math inline">\(E_L(Y|X)\)</span> is always equal to <span class="math inline">\(E(Y|X)\)</span>.</li>
<li><span class="math inline">\(E_L(Y|X)\)</span> minimizes the mean squared error among all linear functions of <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(E_L(Y|X)\)</span> is always a better predictor than <span class="math inline">\(E(Y|X)\)</span> in terms of mean squared error.</li>
<li><span class="math inline">\(E_L(Y|X)\)</span> requires <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> to be jointly normally distributed.</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch07mcsolution9">MC Solution 9</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables with joint pdf <span class="math inline">\(f_{X,Y}(x, y)\)</span>. The <strong>conditional pdf</strong> of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span>, denoted by <span class="math inline">\(f_{Y|X}(y|x)\)</span>, is given by:</p>
<ol type="a">
<li><span class="math inline">\(\frac{f_{X,Y}(x, y)}{f_Y(y)}\)</span></li>
<li><span class="math inline">\(\frac{f_{X,Y}(x, y)}{f_X(x)}\)</span></li>
<li><span class="math inline">\(f_{X,Y}(x, y) \cdot f_X(x)\)</span></li>
<li><span class="math inline">\(f_{X,Y}(x, y) \cdot f_Y(y)\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch07mcsolution10">MC Solution 10</a></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent random variables, what is the value of <span class="math inline">\(\text{cov}(X, Y)\)</span>?</p>
<ol type="a">
<li>-1</li>
<li>0</li>
<li>1</li>
<li>Cannot be determined without further information.</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch07mcsolution11">MC Solution 11</a></p>
<p>Let <span class="math inline">\(X \sim N(0, 1)\)</span> and <span class="math inline">\(Y|X \sim N(X, 1)\)</span>. What is the value of <span class="math inline">\(E(Y)\)</span>?</p>
<ol type="a">
<li>-1</li>
<li>0</li>
<li>1</li>
<li>2</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch07mcsolution12">MC Solution 12</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be random variables such that <span class="math inline">\(Y = 2X + 3\)</span>. If <span class="math inline">\(\text{var}(X) = 4\)</span>, what is <span class="math inline">\(\text{var}(Y)\)</span>?</p>
<ol type="a">
<li>4</li>
<li>8</li>
<li>11</li>
<li>16</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch07mcsolution13">MC Solution 13</a></p>
<p>The <strong>analysis of variance (ANOVA)</strong> formula states that for random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>:</p>
<ol type="a">
<li><span class="math inline">\(\text{var}(Y) = E[\text{var}(Y|X)] + \text{var}[E(Y|X)]\)</span></li>
<li><span class="math inline">\(\text{var}(Y) = E[\text{var}(Y|X)] - \text{var}[E(Y|X)]\)</span></li>
<li><span class="math inline">\(\text{var}(Y) = \text{var}(Y|X) + E(Y|X)\)</span></li>
<li><span class="math inline">\(\text{var}(Y) = \text{var}(Y|X) - E(Y|X)\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch07mcsolution14">MC Solution 14</a></p>
<p>Let <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> be independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. What is <span class="math inline">\(\text{cov}(X_1 + X_2, X_1 - X_2)\)</span>?</p>
<ol type="a">
<li><span class="math inline">\(- \sigma^2\)</span></li>
<li><span class="math inline">\(0\)</span></li>
<li><span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(2\sigma^2\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch07mcsolution15">MC Solution 15</a></p>
<p>Suppose <span class="math inline">\(Y = X_1 X_2\)</span>, where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent random variables. If <span class="math inline">\(f_{X_1}(x_1)\)</span> and <span class="math inline">\(f_{X_2}(x_2)\)</span> are the probability density functions of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, respectively, which of the following expressions represents the probability density function of <span class="math inline">\(Y\)</span>, <span class="math inline">\(f_Y(y)\)</span>, according to the <strong>convolution formula</strong>?</p>
<ol type="a">
<li><span class="math inline">\(\int_{-\infty}^{\infty} f_{X_1}(x_1) f_{X_2}(y - x_1) dx_1\)</span></li>
<li><span class="math inline">\(\int_{-\infty}^{\infty} f_{X_1}(x_1) f_{X_2}(\frac{y}{x_1}) dx_1\)</span></li>
<li><span class="math inline">\(\int_{-\infty}^{\infty} \frac{1}{|x_1|} f_{X_1}(x_1) f_{X_2}(\frac{y}{x_1}) dx_1\)</span></li>
<li><span class="math inline">\(\int_{-\infty}^{\infty} \frac{1}{|x_2|} f_{X_1}(\frac{y}{x_2}) f_{X_2}(x_2) dx_2\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch07mcsolution16">MC Solution 16</a></p>
<p>Let <span class="math inline">\(Y = \max\{X_1, X_2\}\)</span>, where <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent random variables with cumulative distribution functions <span class="math inline">\(F_{X_1}(x)\)</span> and <span class="math inline">\(F_{X_2}(x)\)</span>, respectively. What is the cumulative distribution function of <span class="math inline">\(Y\)</span>, <span class="math inline">\(F_Y(y)\)</span>?</p>
<ol type="a">
<li><span class="math inline">\(F_{X_1}(y) + F_{X_2}(y)\)</span></li>
<li><span class="math inline">\(F_{X_1}(y) \cdot F_{X_2}(y)\)</span></li>
<li><span class="math inline">\(1 - F_{X_1}(y) \cdot F_{X_2}(y)\)</span></li>
<li><span class="math inline">\(\max\{F_{X_1}(y), F_{X_2}(y)\}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch07mcsolution17">MC Solution 17</a></p>
<p>Suppose <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are independent and identically distributed random variables with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>. Let <span class="math inline">\(\bar{X} = \frac{1}{n}\sum_{i=1}^n X_i\)</span>. What is <span class="math inline">\(\text{cov}(\bar{X}, X_1)\)</span>?</p>
<ol type="a">
<li><span class="math inline">\(\frac{\sigma^2}{n}\)</span></li>
<li><span class="math inline">\(\frac{\sigma^2}{n^2}\)</span></li>
<li><span class="math inline">\(\sigma^2\)</span></li>
<li><span class="math inline">\(n\sigma^2\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch07mcsolution18">MC Solution 18</a></p>
<p>Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be jointly normally distributed. Which of the following statements is always true?</p>
<ol type="a">
<li><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent.</li>
<li><span class="math inline">\(E(Y|X)\)</span> is a linear function of <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(E(Y|X) = E(Y)\)</span>.</li>
<li><span class="math inline">\(\text{var}(Y|X) = \text{var}(Y)\)</span>.</li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch07mcsolution19">MC Solution 19</a></p>
<p>Suppose we have a transformation <span class="math inline">\(Y = g(X)\)</span> where <span class="math inline">\(X\)</span> is a two-dimensional vector with joint pdf <span class="math inline">\(f_X(x)\)</span> and <span class="math inline">\(g\)</span> is a one-to-one function. Let <span class="math inline">\(J\)</span> be the Jacobian of the inverse transformation. According to the <strong>multivariate transformation theorem</strong>, the joint pdf of <span class="math inline">\(Y\)</span>, <span class="math inline">\(f_Y(y)\)</span>, is given by:</p>
<ol type="a">
<li><span class="math inline">\(f_X(g^{-1}(y))|J|\)</span></li>
<li><span class="math inline">\(\frac{f_X(g^{-1}(y))}{|J|}\)</span></li>
<li><span class="math inline">\(f_X(g(y))|J|\)</span></li>
<li><span class="math inline">\(\frac{f_X(g(y))}{|J|}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch07mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch07mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch07mcsolution20">MC Solution 20</a></p>
<p>The <strong>copula</strong> function <span class="math inline">\(C(u_1, u_2)\)</span> for two random variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> with marginal cdfs <span class="math inline">\(F_{X_1}(x_1)\)</span> and <span class="math inline">\(F_{X_2}(x_2)\)</span> is defined as:</p>
<ol type="a">
<li><span class="math inline">\(C(u_1, u_2) = P(X_1 \leq F_{X_1}^{-1}(u_1), X_2 \leq F_{X_2}^{-1}(u_2))\)</span></li>
<li><span class="math inline">\(C(u_1, u_2) = P(X_1 \leq u_1, X_2 \leq u_2)\)</span></li>
<li><span class="math inline">\(C(u_1, u_2) = P(F_{X_1}(X_1) \leq u_1, F_{X_2}(X_2) \leq u_2)\)</span></li>
<li><span class="math inline">\(C(u_1, u_2) = P(F_{X_1}(X_1) \leq x_1, F_{X_2}(X_2) \leq x_2)\)</span></li>
</ol>
</section>
</section>
<section class="level2" id="multiple-choice-solutions">
<h2 class="anchored" data-anchor-id="multiple-choice-solutions">Multiple Choice Solutions</h2>
<section class="level3" id="sec-ch07mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch07mcexercise1">MC Exercise 1</a></p>
<p>The joint pdf must integrate to 1 over the support of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. Thus, we have</p>
<p><span class="math display">\[
\begin{aligned}
\int_0^1 \int_0^2 c(2x + y) dy dx &amp;= 1 \\
c \int_0^1 \left[ 2xy + \frac{1}{2} y^2 \right]_0^2 dx &amp;= 1 \\
c \int_0^1 (4x + 2) dx &amp;= 1 \\
c \left[ 2x^2 + 2x \right]_0^1 &amp;= 1 \\
c(2 + 2) &amp;= 1 \\
c &amp;= \frac{1}{4}.
\end{aligned}
\]</span></p>
<p>Therefore, the correct answer is <strong>a. <span class="math inline">\(\frac{1}{4}\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> The constant <span class="math inline">\(c\)</span> in the joint pdf is a normalizing constant that ensures the total probability over the entire support is equal to 1. We find <span class="math inline">\(c\)</span> by integrating the joint pdf over its support and setting the result equal to 1. This utilizes the fundamental property of <strong>probability density functions</strong>.</p>
</section>
<section class="level3" id="sec-ch07mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch07mcexercise2">MC Exercise 2</a></p>
<p>The marginal pdf of <span class="math inline">\(X\)</span> is found by integrating the joint pdf with respect to <span class="math inline">\(y\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
f_X(x) &amp;= \int_{-\infty}^{\infty} f_{X,Y}(x, y) dy \\
&amp;= \int_0^2 \frac{1}{6}(2x + y) dy \\
&amp;= \frac{1}{6} \left[ 2xy + \frac{1}{2} y^2 \right]_0^2 \\
&amp;= \frac{1}{6} (4x + 2) \\
&amp;= \frac{1}{3} (2x + 1), \quad 0 &lt; x &lt; 1.
\end{aligned}
\]</span></p>
<p>Therefore, the correct answer is <strong>a. <span class="math inline">\(\frac{1}{3}(2x + 1)\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> To find the <strong>marginal distribution</strong> of one variable, we integrate the joint pdf over all possible values of the other variable, as described in Section 7.1 (Theorem 7.3) of the text.</p>
</section>
<section class="level3" id="sec-ch07mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch07mcexercise3">MC Exercise 3</a></p>
<p>Since <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, their joint pdf is the product of their marginal pdfs: <span class="math inline">\(f_{X,Y}(x, y) = f_X(x) f_Y(y) = e^{-x} \cdot 2e^{-2y} = 2e^{-x - 2y}\)</span> for <span class="math inline">\(x &gt; 0, y &gt; 0\)</span>. We want to find <span class="math inline">\(P(X &lt; Y)\)</span>:</p>
<p><span class="math display">\[
\begin{aligned}
P(X &lt; Y) &amp;= \int_0^\infty \int_0^y 2e^{-x - 2y} dx dy \\
&amp;= \int_0^\infty 2e^{-2y} \left[ -e^{-x} \right]_0^y dy \\
&amp;= \int_0^\infty 2e^{-2y} (1 - e^{-y}) dy \\
&amp;= \int_0^\infty (2e^{-2y} - 2e^{-3y}) dy \\
&amp;= \left[ -e^{-2y} + \frac{2}{3} e^{-3y} \right]_0^\infty \\
&amp;= 1 - \frac{2}{3} = \frac{1}{3}.
\end{aligned}
\]</span></p>
<p>Therefore, the correct answer is <strong>a. <span class="math inline">\(\frac{1}{3}\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> We use the definition of <strong>independence</strong> (Definition 7.3 and 7.4) to find the joint pdf as the product of the marginal pdfs. Then, we integrate the joint pdf over the region where <span class="math inline">\(X &lt; Y\)</span> to find the desired probability.</p>
</section>
<section class="level3" id="sec-ch07mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch07mcexercise4">MC Exercise 4</a></p>
<p>If <span class="math inline">\(\text{cov}(X, Y) = 0\)</span>, it means that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are <strong>uncorrelated</strong>. While independence implies uncorrelatedness, the converse is not necessarily true. However, if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated, then <span class="math inline">\(E(XY) = E(X)E(Y)\)</span>, and <span class="math inline">\(E(Y|X) = E(Y)\)</span> when <span class="math inline">\(E(Y|X)\)</span> exists. Therefore, the statement that is always true is <strong>c. <span class="math inline">\(E(Y|X) = E(Y)\)</span></strong>. The other statements are not always true.</p>
<p><strong>Intuitive explanation:</strong> Zero covariance indicates a lack of linear relationship between the variables. While independence implies zero covariance (Theorem 7.7), the reverse is not always true. Option c is a consequence of Theorem 7.11 in the text.</p>
</section>
<section class="level3" id="sec-ch07mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch07mcexercise5">MC Exercise 5</a></p>
<p>The correlation coefficient is invariant to scale changes, but the sign can change. Since <span class="math inline">\(Z = 2X + 3\)</span> and <span class="math inline">\(W = 4Y - 1\)</span>, the correlation coefficient <span class="math inline">\(\rho_{ZW}\)</span> will have the same sign as <span class="math inline">\(\rho_{XY}\)</span>, and the magnitude will remain the same because the scaling factors are positive. Therefore, <span class="math inline">\(\rho_{ZW} = \rho_{XY} = 0.8\)</span>. The correct answer is <strong>c. 0.8</strong>.</p>
<p><strong>Intuitive explanation:</strong> The correlation coefficient is invariant to linear transformations with positive coefficients, as stated in the text following Definition 7.6.</p>
</section>
<section class="level3" id="sec-ch07mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch07mcexercise6">MC Exercise 6</a></p>
<p>By the <strong>law of iterated expectations</strong> (Theorem 7.9), <span class="math inline">\(E(Y) = E[E(Y|X)]\)</span>. We are given that <span class="math inline">\(E(Y|X) = 2X + 1\)</span>. Therefore, <span class="math inline">\(E(Y) = E(2X + 1) = 2E(X) + 1 = 2(3) + 1 = 7\)</span>. The correct answer is <strong>c. 7</strong>.</p>
<p><strong>Intuitive explanation:</strong> The law of iterated expectations allows us to compute the expectation of <span class="math inline">\(Y\)</span> by taking the expectation of the conditional expectation of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="sec-ch07mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch07mcexercise7">MC Exercise 7</a></p>
<p>For jointly normally distributed random variables, the conditional variance is given by <span class="math inline">\(\text{var}(Y|X) = \sigma_Y^2 (1 - \rho^2)\)</span>, where <span class="math inline">\(\rho\)</span> is the correlation coefficient between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We have <span class="math inline">\(\rho = \frac{\text{cov}(X, Y)}{\sigma_X \sigma_Y} = \frac{3}{\sqrt{4 \cdot 9}} = \frac{1}{2}\)</span>. Therefore, <span class="math inline">\(\text{var}(Y|X) = 9 (1 - (\frac{1}{2})^2) = 9 (1 - \frac{1}{4}) = 9 \cdot \frac{3}{4} = 6.75\)</span>. The correct answer is <strong>b. 6.75</strong>.</p>
<p><strong>Intuitive explanation:</strong> This utilizes the formula for the conditional variance in the case of a <strong>bivariate normal distribution</strong> (Example 7.16).</p>
</section>
<section class="level3" id="sec-ch07mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch07mcexercise8">MC Exercise 8</a></p>
<p>The <strong>best linear predictor</strong> <span class="math inline">\(E_L(Y|X)\)</span> is defined as the linear function of <span class="math inline">\(X\)</span> that minimizes the mean squared error. Therefore, statement <strong>b. <span class="math inline">\(E_L(Y|X)\)</span> minimizes the mean squared error among all linear functions of <span class="math inline">\(X\)</span></strong> is always true. Statement a is not always true, as <span class="math inline">\(E_L(Y|X)\)</span> is an approximation of <span class="math inline">\(E(Y|X)\)</span>. Statement c is incorrect because <span class="math inline">\(E(Y|X)\)</span> minimizes the mean squared error among all functions of <span class="math inline">\(X\)</span>, not just linear functions. Statement d is not a requirement for the existence of <span class="math inline">\(E_L(Y|X)\)</span>.</p>
<p><strong>Intuitive explanation:</strong> This question tests the understanding of the definition and properties of the <strong>best linear predictor</strong> as presented in Theorem 7.12.</p>
</section>
<section class="level3" id="sec-ch07mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch07mcexercise9">MC Exercise 9</a></p>
<p>The conditional pdf of <span class="math inline">\(Y\)</span> given <span class="math inline">\(X = x\)</span> is defined as the joint pdf divided by the marginal pdf of <span class="math inline">\(X\)</span>: <span class="math inline">\(f_{Y|X}(y|x) = \frac{f_{X,Y}(x, y)}{f_X(x)}\)</span>. Therefore, the correct answer is <strong>b. <span class="math inline">\(\frac{f_{X,Y}(x, y)}{f_X(x)}\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> This is the definition of the <strong>conditional probability density function</strong> as given in equation (7.5) of the text.</p>
</section>
<section class="level3" id="sec-ch07mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch07mcexercise10">MC Exercise 10</a></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, their covariance is 0 (Theorem 7.7). Therefore, the correct answer is <strong>b. 0</strong>.</p>
<p><strong>Intuitive explanation:</strong> Independence implies zero covariance, as stated in the text.</p>
</section>
<section class="level3" id="sec-ch07mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch07mcexercise11">MC Exercise 11</a></p>
<p>Using the law of iterated expectations, <span class="math inline">\(E(Y) = E[E(Y|X)]\)</span>. Since <span class="math inline">\(Y|X \sim N(X, 1)\)</span>, we have <span class="math inline">\(E(Y|X) = X\)</span>. Therefore, <span class="math inline">\(E(Y) = E(X)\)</span>. Given that <span class="math inline">\(X \sim N(0, 1)\)</span>, we have <span class="math inline">\(E(X) = 0\)</span>. Thus, <span class="math inline">\(E(Y) = 0\)</span>. The correct answer is <strong>b. 0</strong>.</p>
<p><strong>Intuitive explanation:</strong> This problem applies the <strong>law of iterated expectations</strong> (Theorem 7.9) and the properties of the normal distribution.</p>
</section>
<section class="level3" id="sec-ch07mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch07mcexercise12">MC Exercise 12</a></p>
<p>Using the properties of variance, we have <span class="math inline">\(\text{var}(Y) = \text{var}(2X + 3) = 2^2 \text{var}(X) = 4 \cdot 4 = 16\)</span>. The correct answer is <strong>d. 16</strong>.</p>
<p><strong>Intuitive explanation:</strong> This utilizes the property that <span class="math inline">\(\text{var}(aX + b) = a^2 \text{var}(X)\)</span> for constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
</section>
<section class="level3" id="sec-ch07mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch07mcexercise13">MC Exercise 13</a></p>
<p>The <strong>analysis of variance (ANOVA)</strong> formula, as stated in Theorem 7.10, is <span class="math inline">\(\text{var}(Y) = E[\text{var}(Y|X)] + \text{var}[E(Y|X)]\)</span>. Therefore, the correct answer is <strong>a. <span class="math inline">\(\text{var}(Y) = E[\text{var}(Y|X)] + \text{var}[E(Y|X)]\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> This question directly tests the knowledge of the <strong>ANOVA formula</strong> presented in the text.</p>
</section>
<section class="level3" id="sec-ch07mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch07mcexercise14">MC Exercise 14</a></p>
<p>We have</p>
<p><span class="math display">\[
\begin{aligned}
\text{cov}(X_1 + X_2, X_1 - X_2) &amp;= \text{cov}(X_1, X_1) - \text{cov}(X_1, X_2) + \text{cov}(X_2, X_1) - \text{cov}(X_2, X_2) \\
&amp;= \text{var}(X_1) - \text{cov}(X_1, X_2) + \text{cov}(X_2, X_1) - \text{var}(X_2).
\end{aligned}
\]</span></p>
<p>Since <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are independent and identically distributed, <span class="math inline">\(\text{cov}(X_1, X_2) = \text{cov}(X_2, X_1) = 0\)</span> and <span class="math inline">\(\text{var}(X_1) = \text{var}(X_2) = \sigma^2\)</span>. Thus, <span class="math inline">\(\text{cov}(X_1 + X_2, X_1 - X_2) = \sigma^2 - 0 + 0 - \sigma^2 = 0\)</span>. The correct answer is <strong>b. 0</strong>.</p>
<p><strong>Intuitive explanation:</strong> This problem uses the properties of <strong>covariance</strong> for independent variables and the fact that the covariance of a variable with itself is its variance.</p>
</section>
<section class="level3" id="sec-ch07mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch07mcexercise15">MC Exercise 15</a></p>
<p>For <span class="math inline">\(Y = X_1 X_2\)</span>, the <strong>convolution formula</strong> should involve the transformation of variables where we divide by the absolute value of one of the variables. The correct formula is given by</p>
<p><span class="math display">\[
f_Y(y) = \int_{-\infty}^{\infty} \frac{1}{|x_2|} f_{X_1}(\frac{y}{x_2}) f_{X_2}(x_2) dx_2.
\]</span></p>
<p>Therefore, the correct answer is <strong>d. <span class="math inline">\(\int_{-\infty}^{\infty} \frac{1}{|x_2|} f_{X_1}(\frac{y}{x_2}) f_{X_2}(x_2) dx_2\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> This question tests the understanding of the <strong>convolution formula</strong> for the product of two independent random variables, as discussed in the text in the context of <strong>multivariate transformations</strong>. The division by <span class="math inline">\(|x_2|\)</span> comes from the Jacobian of the transformation.</p>
</section>
<section class="level3" id="sec-ch07mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch07mcexercise16">MC Exercise 16</a></p>
<p>The cdf of <span class="math inline">\(Y = \max\{X_1, X_2\}\)</span> is given by</p>
<p><span class="math display">\[
\begin{aligned}
F_Y(y) &amp;= P(Y \leq y) = P(\max\{X_1, X_2\} \leq y) = P(X_1 \leq y, X_2 \leq y) \\
&amp;= P(X_1 \leq y) P(X_2 \leq y) \quad \text{(since } X_1 \text{ and } X_2 \text{ are independent)} \\
&amp;= F_{X_1}(y) F_{X_2}(y).
\end{aligned}
\]</span></p>
<p>Therefore, the correct answer is <strong>b. <span class="math inline">\(F_{X_1}(y) \cdot F_{X_2}(y)\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> This problem utilizes the property that the maximum of two independent random variables is less than or equal to a value <span class="math inline">\(y\)</span> if and only if both variables are less than or equal to <span class="math inline">\(y\)</span>.</p>
</section>
<section class="level3" id="sec-ch07mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch07mcexercise17">MC Exercise 17</a></p>
<p>We have</p>
<p><span class="math display">\[
\text{cov}(\bar{X}, X_1) = \text{cov} \left( \frac{1}{n}\sum_{i=1}^n X_i, X_1 \right) = \frac{1}{n} \sum_{i=1}^n \text{cov}(X_i, X_1).
\]</span></p>
<p>Since the <span class="math inline">\(X_i\)</span> are independent, <span class="math inline">\(\text{cov}(X_i, X_1) = 0\)</span> for <span class="math inline">\(i \neq 1\)</span>. Also, <span class="math inline">\(\text{cov}(X_1, X_1) = \text{var}(X_1) = \sigma^2\)</span>. Therefore, <span class="math inline">\(\text{cov}(\bar{X}, X_1) = \frac{1}{n} \sigma^2\)</span>. The correct answer is <strong>a. <span class="math inline">\(\frac{\sigma^2}{n}\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> This problem uses the properties of <strong>covariance</strong> and the fact that the variables are independent and identically distributed.</p>
</section>
<section class="level3" id="sec-ch07mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch07mcexercise18">MC Exercise 18</a></p>
<p>For jointly normally distributed random variables, the conditional expectation <span class="math inline">\(E(Y|X)\)</span> is a linear function of <span class="math inline">\(X\)</span>. Therefore, the correct answer is <strong>b. <span class="math inline">\(E(Y|X)\)</span> is a linear function of <span class="math inline">\(X\)</span></strong>. The other statements are not always true for jointly normally distributed variables.</p>
<p><strong>Intuitive explanation:</strong> This question tests the knowledge of a key property of the <strong>bivariate normal distribution</strong>, as mentioned in Example 7.16.</p>
</section>
<section class="level3" id="sec-ch07mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch07mcexercise19">MC Exercise 19</a></p>
<p>According to the <strong>multivariate transformation theorem</strong> (Theorem 7.17), the joint pdf of <span class="math inline">\(Y\)</span> is given by <span class="math inline">\(f_Y(y) = f_X(g^{-1}(y)) |J|\)</span>, where <span class="math inline">\(|J|\)</span> is the absolute value of the Jacobian determinant. Therefore, the correct answer is <strong>a. <span class="math inline">\(f_X(g^{-1}(y))|J|\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> This question tests the understanding of the <strong>multivariate transformation theorem</strong> as presented in the text.</p>
</section>
<section class="level3" id="sec-ch07mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch07mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch07mcexercise20">MC Exercise 20</a></p>
<p>The <strong>copula</strong> function is defined as the joint cumulative distribution function of the transformed random variables <span class="math inline">\(U_1 = F_{X_1}(X_1)\)</span> and <span class="math inline">\(U_2 = F_{X_2}(X_2)\)</span>. Therefore, <span class="math inline">\(C(u_1, u_2) = P(F_{X_1}(X_1) \leq u_1, F_{X_2}(X_2) \leq u_2)\)</span>. The correct answer is <strong>c. <span class="math inline">\(C(u_1, u_2) = P(F_{X_1}(X_1) \leq u_1, F_{X_2}(X_2) \leq u_2)\)</span></strong>.</p>
<p><strong>Intuitive explanation:</strong> This question tests the understanding of the definition of a <strong>copula</strong> as presented in Section 7.6.2 of the text.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>