<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 11: Estimation Theory – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap12.html" rel="next"/>
<link href="../chapters/chap10.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 11: Estimation Theory</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level2" id="estimation-methods">
<h2 class="anchored" data-anchor-id="estimation-methods">11.1 ESTIMATION METHODS</h2>
<p><strong>Estimation</strong> is a formal process of using sample information to calculate population quantities, or parameters. We discuss two general strategies for obtaining estimators of population parameters of interest from samples of data:</p>
<ol type="1">
<li><strong>The Analogy principle or Method of Moments</strong></li>
<li><strong>Maximum Likelihood</strong></li>
</ol>
<p>There are other ways of organizing the material, but this approach gets to the main issues.</p>
<section class="level3" id="the-original-method-of-moments-or-analogy-principle">
<h3 class="anchored" data-anchor-id="the-original-method-of-moments-or-analogy-principle">11.1.1 The Original Method of Moments or Analogy Principle</h3>
<p>In many cases, parameters of interest can be expressed as functionals of the probability distribution or c.d.f., that is, we can write <span class="math inline">\(\theta(F_X)\)</span>, formally a mapping from the set of c.d.f.s to the real line. For example, moments, quantiles, and density functions can be expressed in this way:</p>
<p><span class="math display">\[
\mu = E(X) = \int x dF_X ; \quad \sigma^2 = \text{var}(X) = E(X^2) - E^2(X)
\]</span> <span class="math display">\[
\kappa_3 = \frac{E[(X - E(X))^3]}{\text{var}(X)^{3/2}}
\]</span></p>
<p>where <span class="math inline">\(F_X\)</span> is the c.d.f. of <span class="math inline">\(X\)</span>. Suppose that we have a sample <span class="math inline">\(X_1, \dots, X_n\)</span> from the population. The <strong>analogy principle</strong> suggests substituting population moments by sample moments. For example, <span class="math inline">\(E(X)\)</span> by <span class="math inline">\(\bar{X}\)</span>, <span class="math inline">\(\text{var}(X)\)</span> by <span class="math inline">\(s^2\)</span>, and <span class="math inline">\(\kappa_3\)</span> by</p>
<p><span class="math display">\[
\hat{\kappa}_3 = \frac{\frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^3}{\left[ \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 \right]^{3/2}} = g \left( \frac{1}{n} \sum_{i=1}^n X_i, \frac{1}{n} \sum_{i=1}^n X_i^2, \frac{1}{n} \sum_{i=1}^n X_i^3 \right)
\]</span></p>
<p>This function <span class="math inline">\(g\)</span> is quite complicated but it is continuous and differentiable in its three arguments (at almost all points of <span class="math inline">\(\mathbb{R}^3\)</span>). The analogy principle can be made explicit by noting that we may, for example, write <span class="math inline">\(\bar{X} = \int x dF_n\)</span>, where <span class="math inline">\(F_n\)</span> is the empirical distribution, which is the natural analogue of the population distribution function. Likewise for any other moment or cumulant.</p>
<p>We can also apply the analogy principle to estimate quantiles. For example, to estimate the median <span class="math inline">\(M\)</span> in the population, we can use the sample median. We order the sample information and take the middle value. Define the <strong>order statistics</strong></p>
<p><span class="math display">\[
X_{(1)} \leq \dots \leq X_{(n)}
\]</span></p>
<p>and then define the sample median as <span class="math inline">\(\hat{M} = X_{([n/2])}\)</span>, where <span class="math inline">\([n/2]\)</span> denotes the largest integer less than or equal to <span class="math inline">\(n/2\)</span>. An alternative way of looking at this is to recall that the population median is the unique value that solves the equation <span class="math inline">\(F_X(M) = 0.5\)</span> when <span class="math inline">\(F_X\)</span> is continuous and strictly increasing. Therefore, look for the value <span class="math inline">\(\hat{M}\)</span> that solves the equation</p>
<p><span class="math display">\[
F_n(\hat{M}) = \frac{1}{2}
\]</span></p>
<p>However, the empirical distribution function is not strictly increasing everywhere, and so it may not be possible to uniquely define the median from this equation. Therefore, we have to use the generalized inverse. That is, we define the <span class="math inline">\(\alpha\)</span> sample quantile</p>
<p><span class="math display">\[
\hat{Q}_X(\alpha) = \inf \{ x : F_n(x) \geq \alpha \}
\qquad (11.1)
\]</span></p>
<p>with the sample median being the case with <span class="math inline">\(\alpha = 1/2\)</span>. This is an implicit equation for the sample quantile.</p>
<p>We may generalize the analogue notion further. Another population relation we may make use of is the argument that <span class="math inline">\(E(X)\)</span> is the unique minimizer of <span class="math inline">\(E((X-\theta)^2) = \int (x-\theta)^2 dF_X\)</span> with respect to <span class="math inline">\(\theta\)</span>. Therefore, we can define an estimator of <span class="math inline">\(E(X)\)</span> as the minimizer of the sample objective function</p>
<p><span class="math display">\[
\int (x - \theta)^2 dF_n = \frac{1}{n} \sum_{i=1}^n (X_i - \theta)^2
\]</span></p>
<p>It is easy to see that <span class="math inline">\(\bar{X}\)</span> is the unique minimizer of this objective function. This principle also carries over to the median case. Recall that <span class="math inline">\(M\)</span> is the unique minimizer of <span class="math inline">\(E(|X-\theta|) = \int |x-\theta| dF_X\)</span> with respect to <span class="math inline">\(\theta\)</span>. The analogy principle suggests that we find <span class="math inline">\(\hat{M}\)</span> as the minimizer of</p>
<p><span class="math display">\[
\int |x-\theta| dF_n = \frac{1}{n} \sum_{i=1}^n |X_i - \theta|
\qquad (11.2)
\]</span></p>
<p>This objective function will not generally have a unique minimizer, and we should choose one of the minimizers. However, the solution is essentially the sample median, modulo the way we impose uniqueness. Finally, we may recall that the population median <span class="math inline">\(M\)</span> satisfies <span class="math inline">\(E(\text{sign}(X-M)) = 0\)</span>, and define <span class="math inline">\(\hat{M}\)</span> as the solution to the sample equation</p>
<p><span class="math display">\[
\int \text{sign}(x-\theta) dF_n = \frac{1}{n} \sum_{i=1}^n \text{sign}(X_i - \theta) = 0
\qquad (11.3)
\]</span></p>
<p>In this approach the empirical distribution function is of central importance; knowing its behavior allows one to derive the behavior of the analogue defined estimators. The special case of the analogue principle that is to do with moments is called the <strong>Method of Moments (MoM)</strong>, and goes back to the early 20th century. In this case, we use the moments to define estimators of parameters in a parametric model <span class="math inline">\(\{ P_\theta, \theta \in \Theta \}\)</span>.</p>
</section>
<section class="level3" id="example-11.1">
<h3 class="anchored" data-anchor-id="example-11.1">Example 11.1</h3>
<p>The standard t-distribution <span class="math inline">\(f(x|v) \propto (1+x^2/v)^{-(v+1)/2}\)</span>, where <span class="math inline">\(v\)</span> is the degrees of freedom. In this case, we have</p>
<p><span class="math display">\[
\text{var}(X) = \frac{v}{v-2} \implies v = \frac{2 \text{var}(X)}{\text{var}(X) - 1}
\]</span></p>
<p>whenever <span class="math inline">\(v &gt; 2\)</span>. We can thus estimate the parameter <span class="math inline">\(v\)</span> by</p>
<p><span class="math display">\[
\hat{v} = \frac{2s^2}{s^2 - 1}
\]</span></p>
<p>This is a very simple to compute estimator, whereas, the Maximum Likelihood estimator below is more complicated (although it also works for <span class="math inline">\(v \leq 2\)</span>).</p>
</section>
<section class="level3" id="example-11.2">
<h3 class="anchored" data-anchor-id="example-11.2">Example 11.2</h3>
<p>The unit Pareto distribution <span class="math inline">\(f(x|\theta) = \theta x^{-(\theta+1)}, x \geq 1\)</span>. We have, provided <span class="math inline">\(\theta &gt; 1\)</span>,</p>
<p><span class="math display">\[
E(X) = \frac{\theta}{\theta - 1}
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
\theta = \frac{E(X)}{E(X) - 1}
\]</span></p>
<p>Therefore, we may let</p>
<p><span class="math display">\[
\hat{\theta} = \frac{\bar{X}}{\bar{X} - 1}
\]</span></p>
<p>Alternatively, we have</p>
<p><span class="math display">\[
Q_X(\alpha | \theta) = \left( \frac{1}{1-\alpha} \right)^{1/\theta}
\]</span></p>
<p>Therefore, we may write</p>
<p><span class="math display">\[
\hat{\alpha} = \frac{\log(2)}{\log(\hat{M})}
\]</span></p>
<p>where <span class="math inline">\(\hat{M}\)</span> is the sample median. This estimator is defined for all <span class="math inline">\(\theta &gt; 0\)</span>.</p>
<p>We can define this principle more generally, i.e., where there are multiple unknown parameters of interest. Suppose that <span class="math inline">\(\theta_1, \dots, \theta_k\)</span> are the parameters of interest, and suppose that</p>
<p><span class="math display">\[
\mu_1 = g_1(\theta_1, \dots, \theta_k), \dots, \mu_k = g_k(\theta_1, \dots, \theta_k)
\qquad (11.4)
\]</span></p>
<p>for some functions <span class="math inline">\(g_1, \dots, g_k\)</span>, where <span class="math inline">\(\mu_j\)</span> are some population quantities, for example <span class="math inline">\(\mu_j = E(X^j), j=1, \dots, k\)</span>. Now suppose that we can solve the system of equations (11.4) to write</p>
<p><span class="math display">\[
\theta_1 = f_1(\mu_1, \dots, \mu_k), \dots, \theta_k = f_k(\mu_1, \dots, \mu_k)
\]</span></p>
<p>for some functions <span class="math inline">\(f_1, \dots, f_k\)</span>. This may not be explicit, i.e., we may be able to prove the existence of unique functions <span class="math inline">\(f_j\)</span> but may not be able to write them down. We will deal with this issue more concretely in a later chapter. Then we estimate <span class="math inline">\(\theta_j\)</span> by</p>
<p><span class="math display">\[
\hat{\theta}_j = f_j(\hat{m}_1, \dots, \hat{m}_k)
\]</span></p>
<p>where <span class="math inline">\(\hat{m}_j\)</span> are sample analogues or estimators of <span class="math inline">\(\mu_j\)</span>.</p>
</section>
<section class="level3" id="example-11.3">
<h3 class="anchored" data-anchor-id="example-11.3">Example 11.3</h3>
<p>The Gamma<span class="math inline">\((\alpha, \beta)\)</span> distribution <span class="math inline">\(f(x|\alpha, \beta) \propto x^{\alpha-1} \exp(-x/\beta)\)</span>. In this case, we have</p>
<p><span class="math display">\[
E(X) = \alpha \beta, \quad \text{var}(X) = \alpha \beta^2
\]</span></p>
<p>which leads to the exact solution:</p>
<p><span class="math display">\[
\frac{E^2(X)}{\text{var}(X)} = \alpha; \quad \frac{E(X)}{\alpha} = \beta
\]</span></p>
<p>Therefore, let <span class="math inline">\(\hat{\alpha} = \bar{X}^2/s^2\)</span> and <span class="math inline">\(\hat{\beta} = \bar{X}/\hat{\alpha}\)</span>.</p>
</section>
<section class="level3" id="example-11.4">
<h3 class="anchored" data-anchor-id="example-11.4">Example 11.4</h3>
<p>The Beta<span class="math inline">\((\alpha, \beta)\)</span> distribution <span class="math inline">\(f(x|\alpha, \beta) \propto x^{\alpha-1}(1-x)^{\beta-1}\)</span>. In this case, we have</p>
<p><span class="math display">\[
E(X) = \frac{\alpha}{\alpha+\beta}; \quad \text{var}(X) = \frac{\alpha \beta}{(\alpha+\beta)^2(\alpha+\beta+1)}
\]</span></p>
<p>In this case there are no explicit solutions. Then, define <span class="math inline">\(\hat{\alpha}, \hat{\beta}\)</span> to solve the equations</p>
<p><span class="math display">\[
\bar{X} = \frac{\hat{\alpha}}{\hat{\alpha}+\hat{\beta}}; \quad s^2 = \frac{\hat{\alpha}\hat{\beta}}{(\hat{\alpha}+\hat{\beta})^2(\hat{\alpha}+\hat{\beta}+1)}
\]</span></p>
</section>
<section class="level3" id="example-11.5">
<h3 class="anchored" data-anchor-id="example-11.5">Example 11.5</h3>
<p>Suppose that you want to measure the sides of a table, i.e., length (<span class="math inline">\(L\)</span>) and width (<span class="math inline">\(W\)</span>). However, your research assistant reports to you only the area (<span class="math inline">\(A\)</span>). Luckily, she was trained at Oxford and so makes an error in each measurement. Specifically,</p>
<p><span class="math display">\[
L_i = L + \epsilon_i; \quad W_i = W + \eta_i
\]</span></p>
<p>where <span class="math inline">\(\epsilon_i, \eta_i\)</span> are mutually independent standard normal random variables. The RA reports <span class="math inline">\(\{ A_i \}_{i=1}^n\)</span>, where <span class="math inline">\(A_i = L_i W_i\)</span>. In this case</p>
<p><span class="math display">\[
E(A_i) = L \times W
\]</span> <span class="math display">\[
E(A_i^2) = L^2 W^2 + L^2 + W^2 + 1
\]</span></p>
<p>These two equations can be solved explicitly for the unknown quantities <span class="math inline">\(L, W\)</span> in terms of <span class="math inline">\(E(A_i)\)</span> and <span class="math inline">\(E(A_i^2)\)</span>.</p>
</section>
</section>
<section class="level2" id="maximum-likelihood">
<h2 class="anchored" data-anchor-id="maximum-likelihood">11.1.2 Maximum Likelihood</h2>
<p>The method of maximum likelihood was apparently invented by R.A. Fisher in 1912 when he was 22 years old and an undergraduate at Cambridge, Aldrich (1997). It is widely used in many fields of science. For fully specified parametric models with a finite number of parameters and certain other regularity conditions, it is the best procedure according to the widely accepted criterion of asymptotic variance. There is a school of thought that argues that in any problem all one has to do is to specify a parametric model, derive the likelihood, and compute the <strong>Maximum Likelihood Estimator (MLE)</strong> and other related quantities. The context is a parametric model <span class="math inline">\(\{ P_\theta, \theta \in \Theta \}\)</span>, where <span class="math inline">\(\Theta\)</span> contains all the possible values of <span class="math inline">\(\theta\)</span>.</p>
<p>As we have seen, the likelihood function for an i.i.d. sample <span class="math inline">\(X^n = \{ X_1, \dots, X_n \}\)</span> is</p>
<p><span class="math display">\[
L(\theta|X^n) = \prod_{i=1}^n f(X_i|\theta)
\]</span></p>
<p>The MLE <span class="math inline">\(\hat{\theta} = \hat{\theta}(X^n)\)</span> maximizes <span class="math inline">\(L(\theta|X^n)\)</span>, or equivalently <span class="math inline">\(l(\theta|X^n) = \log L(\theta|X^n)\)</span>, with respect to <span class="math inline">\(\theta \in \Theta \subset \mathbb{R}^k\)</span>. There are two issues.</p>
<p><strong>Existence</strong>: Does there exist a maximum? If <span class="math inline">\(\theta \mapsto l(\theta|X^n)\)</span> is continuous and the parameter set <span class="math inline">\(\Theta\)</span> is a <em>compact</em>, i.e., a closed and bounded, subset of <span class="math inline">\(\mathbb{R}^k\)</span>, then there exists such a maximum. More generally, if <span class="math inline">\(\hat{\theta}\)</span> comes ‘close’ [this can happen in certain microeconometric models that have discontinuities in <span class="math inline">\(\theta\)</span>] to maximizing <span class="math inline">\(l\)</span> over <span class="math inline">\(\Theta\)</span>, then this is usually close enough for theoretical and practical purposes. There are some cases where the maximum does not exist because the likelihood function is unbounded in <span class="math inline">\(\theta\)</span>, such as mixture models.</p>
<p><strong>Uniqueness</strong>: Is the set of maximizing values: a singleton, a finite set, or an uncountable set, e.g., an interval? If <span class="math inline">\(l(\theta|X^n)\)</span> is globally concave, then <span class="math inline">\(\hat{\theta}\)</span> is unique. Otherwise, we can select some element of the set of maximizers so this is usually no problem. However, it may point to a more fundamental problem, which is whether the model is identified. Let us consider the concept of in sample identification.</p>
<section class="level3" id="definition-11.1">
<h3 class="anchored" data-anchor-id="definition-11.1">Definition 11.1</h3>
<p>A parameter point <span class="math inline">\(\theta\)</span> in <span class="math inline">\(\Theta\)</span> is said to be <strong>not uniquely identified</strong> in sample if for the sample <span class="math inline">\(X^n\)</span>, there exists <span class="math inline">\(\theta' \in \Theta\)</span> such that</p>
<p><span class="math display">\[
L(\theta|X^n) = L(\theta'|X^n)
\]</span></p>
<p>According to this we are unable to distinguish between <span class="math inline">\(\theta\)</span> and <span class="math inline">\(\theta'\)</span> given the data available to us.</p>
</section>
<section class="level3" id="example-11.6">
<h3 class="anchored" data-anchor-id="example-11.6">Example 11.6</h3>
<p>Suppose that <span class="math inline">\(X_i\)</span> are i.i.d. with Laplace density <span class="math inline">\(\exp(-|x-\theta|)\)</span>. Then</p>
<p><span class="math display">\[
l(\theta|X^n) = - \sum_{i=1}^n |X_i - \theta|
\]</span></p>
<p>We show in Fig. 11.1 a case (from some particular <span class="math inline">\(X^n\)</span>) where <span class="math inline">\(n=2,4,6,8,10\)</span>, and <span class="math inline">\(100\)</span> with <span class="math inline">\(\theta\)</span> varying over <span class="math inline">\([0,1]\)</span>.</p>
<p>Note that if a maximum exists it could be on the boundary of the parameter space or it could be in the interior. If the maximum is in the interior of the parameter space, then we can usually find it by calculus. Provided the log likelihood is twice differentiable at <span class="math inline">\(\hat{\theta}\)</span> we can define the <strong>score function</strong> and <strong>Hessian</strong>:</p>
<p><span class="math display">\[
s(\theta|X^n) = \frac{\partial l}{\partial \theta} (\theta|X^n)
\qquad (11.5)
\]</span></p>
<p><span class="math display">\[
h(\theta|X^n) = \frac{\partial^2 l}{\partial \theta^2} (\theta|X^n)
\qquad (11.6)
\]</span></p>
<p>It then follows that the maximizing value should satisfy</p>
<p><span class="math display">\[
s(\hat{\theta}|X^n) = 0
\qquad (11.7)
\]</span></p>
<p>In the multivariate case this is a vector of <span class="math inline">\(k\)</span> (possibly nonlinear) equations in <span class="math inline">\(k\)</span> unknowns. This is a necessary condition for an interior maximum of the likelihood function being reached. A sufficient condition in the scalar case for an interior local maximum is that the second derivative at <span class="math inline">\(\hat{\theta}\)</span> is negative, i.e.,</p>
<p><span class="math display">\[
h(\hat{\theta}|X^n) &lt; 0
\]</span></p>
<p>For a global maximum it suffices to have the second derivative be negative for all <span class="math inline">\(\theta\)</span>. In the multivariate case, this condition has a natural extension using the language of matrices.</p>
</section>
<section class="level3" id="example-11.7">
<h3 class="anchored" data-anchor-id="example-11.7">Example 11.7</h3>
<p>The Binomial case where <span class="math inline">\(X_i = 1\)</span> with probability <span class="math inline">\(p\)</span> and <span class="math inline">\(X_i = 0\)</span> with probability <span class="math inline">\(1-p\)</span>. In this case the natural parameter set is <span class="math inline">\(\Theta = [0,1]\)</span> and the likelihood, score, and Hessian are:</p>
<p><span class="math display">\[
l(p|X^n) = \sum_{i=1}^n X_i \log p + (1-X_i) \log (1-p)
\]</span> <span class="math display">\[
s(p|X^n) = \sum_{i=1}^n \left( \frac{X_i}{p} - \frac{1-X_i}{1-p} \right) = \sum_{i=1}^n \frac{X_i - p}{p(1-p)}
\]</span> <span class="math display">\[
h(p|X^n) = \sum_{i=1}^n \left( -\frac{X_i}{p^2} - \frac{1-X_i}{(1-p)^2} \right) = - \sum_{i=1}^n \frac{(X_i-p)^2}{p^2(1-p)^2} + \sum_{i=1}^n \frac{(X_i-p)}{p(1-p)}
\]</span></p>
<p>From this we see that</p>
<p><span class="math display">\[
\hat{p}_{MLE} = \frac{1}{n} \sum_{i=1}^n X_i = \frac{k}{n}
\]</span></p>
<p>where <span class="math inline">\(k\)</span> is the number of ones in the sample. Then</p>
<p><span class="math display">\[
h(\hat{p}_{MLE}|X^n) = - \sum_{i=1}^n \frac{(X_i - \hat{p}_{MLE})^2}{\hat{p}_{MLE}(1-\hat{p}_{MLE})^2} &lt; 0
\]</span></p>
<p>as required. If <span class="math inline">\(X_i = 1\)</span> for all <span class="math inline">\(i\)</span>, then <span class="math inline">\(l(p|X^n) = n \log p\)</span> so that <span class="math inline">\(\hat{p}_{MLE} = 1\)</span>. In this case, the first order condition method is not useful since the likelihood is not differentiable at the boundary (not defined for <span class="math inline">\(p&gt;1\)</span> or <span class="math inline">\(p&lt;0\)</span>).</p>
</section>
<section class="level3" id="example-11.8">
<h3 class="anchored" data-anchor-id="example-11.8">Example 11.8</h3>
<p>The Poisson case with unknown parameter <span class="math inline">\(\lambda\)</span> has natural parameter space <span class="math inline">\(\Theta = \mathbb{R}_+\)</span> and the likelihood, score, and Hessian are:</p>
<p><span class="math display">\[
l(\lambda|X^n) = \sum_{i=1}^n X_i \log \lambda - n\lambda - \sum_{i=1}^n \log X_i
\]</span> <span class="math display">\[
s(\lambda|X^n) = \sum_{i=1}^n \frac{X_i}{\lambda} - n; \quad h(\lambda|X^n) = - \sum_{i=1}^n \frac{X_i}{\lambda^2}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\hat{\lambda}_{MLE} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
</section>
<section class="level3" id="example-11.9">
<h3 class="anchored" data-anchor-id="example-11.9">Example 11.9</h3>
<p>The Uniform distribution <span class="math inline">\(U(0, \theta)\)</span> has likelihood</p>
<p><span class="math display">\[
L(\theta|X^n) = \prod_{i=1}^n \frac{1}{\theta} \mathbb{1}(0 \leq X_i \leq \theta) = \begin{cases}
\frac{1}{\theta^n} &amp; \text{if } \max X_i \leq \theta \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>The natural parameter space here is <span class="math inline">\(\Theta = \mathbb{R}_+\)</span>. In this case, <span class="math inline">\(L\)</span> is differentiable with respect to <span class="math inline">\(\theta\)</span> for all <span class="math inline">\(\theta &gt; \max X_i\)</span> but discontinuous at exactly this point. The maximum of <span class="math inline">\(L\)</span> is at <span class="math inline">\(\hat{\theta} = \max X_i\)</span>, and so does not satisfy a first order condition.</p>
<p>Fig. 11.2 shows the likelihood function for a sample of uniform data with true parameter <span class="math inline">\(\theta = 1\)</span> and sample size <span class="math inline">\(n=1000\)</span>.</p>
</section>
<section class="level3" id="example-11.10">
<h3 class="anchored" data-anchor-id="example-11.10">Example 11.10</h3>
<p>The normal distribution <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>. Here, there are two parameters <span class="math inline">\(\theta = (\mu, \sigma^2)\)</span> with natural parameter set <span class="math inline">\(\Theta = \mathbb{R} \times \mathbb{R}_+\)</span>. We have</p>
<p><span class="math display">\[
L(\theta|X^n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{(X_i-\mu)^2}{2\sigma^2} \right)
\]</span></p>
<p><span class="math display">\[
l(\theta|X^n) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i - \mu)^2
\]</span></p>
<p>The first order conditions are:</p>
<p><span class="math display">\[
s_\mu(\theta|X^n) = \frac{\partial l}{\partial \mu} (\theta|X^n) = \frac{1}{\sigma^2} \sum_{i=1}^n (X_i - \mu)
\]</span> <span class="math display">\[
s_{\sigma^2}(\theta|X^n) = \frac{\partial l}{\partial \sigma^2} (\theta|X^n) = -\frac{n}{2\sigma^2} + \frac{1}{2\sigma^4} \sum_{i=1}^n (X_i - \mu)^2
\]</span></p>
<p>Setting these equations to zero we have two equations in two unknowns. In fact the first equation can be solved directly by itself</p>
<p><span class="math display">\[
\sum_{i=1}^n (X_i - \hat{\mu}_{MLE}) = 0 \iff \hat{\mu}_{MLE} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>The second equation can be solved exactly conditional on <span class="math inline">\(\hat{\mu}_{MLE}\)</span> to give</p>
<p><span class="math display">\[
\hat{\sigma}^2_{MLE} = \frac{1}{n} \sum_{i=1}^n (X_i - \hat{\mu}_{MLE})^2
\]</span></p>
<p>To check the second order conditions here we need to derive the second derivatives.</p>
<p>These are all examples where the solutions are in closed form, i.e., one can explicitly write</p>
<p><span class="math display">\[
\hat{\theta}_{MLE} = g(X^n)
\]</span></p>
<p>for some known function <span class="math inline">\(g\)</span>. More generally, the MLE is not explicitly defined. For example, consider the Cauchy distribution:</p>
<p><span class="math display">\[
f(x|\theta) = \frac{1}{\pi} \frac{1}{1+(x-\theta)^2}
\]</span></p>
<p>for which the log likelihood function and its score function are:</p>
<p><span class="math display">\[
l(\theta|X^n) = -n \log(\pi) - \sum_{i=1}^n \log[1+(X_i-\theta)^2]
\]</span> <span class="math display">\[
s(\theta|X^n) = 2 \sum_{i=1}^n \frac{X_i - \theta}{1+(X_i-\theta)^2}
\]</span></p>
<p>This gives a nonlinear equation in <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[
g(\hat{\theta}, X^n) = 0
\]</span></p>
<p>How do we solve this? By numerical methods.</p>
</section>
<section class="level3" id="example-11.11">
<h3 class="anchored" data-anchor-id="example-11.11">Example 11.11</h3>
<p>Cauchy data with <span class="math inline">\(\theta = 0\)</span> and sample size <span class="math inline">\(n=1000\)</span>. See Fig. 11.3.</p>
</section>
</section>
<section class="level2" id="computation">
<h2 class="anchored" data-anchor-id="computation">11.1.3 Computation</h2>
<p>Many software languages such as MATLAB and R have modules that will maximize any given function in a number of different ways, all you need to do is to program in the criterion function and deal with input and output. Many programs will compute MLE’s for standard models such as probit, logit, censored regression, for example STATA, SAS, BMDP, and SPSS.</p>
<p>Methods can be divided according to whether they use derivatives (and then whether first or second) or not. <strong>Grid search</strong> (works well for scalar parameters) involves computing <span class="math inline">\(l(\theta|X^n)\)</span> at a fine grid of points and then choosing the best value in terms of the height of the likelihood. In multiparameter problems, this can perform poorly because a very large grid is required to achieve good accuracy. In this case, derivative based methods are widely applied. These are typically variants of the classical <strong>Newton Raphson method</strong> for finding the zero of a function that you learnt in high school. Perhaps the variable step algorithm is the most commonly used derivative method. Given starting values <span class="math inline">\(\theta_1\)</span> one iteratively computes (<span class="math inline">\(r=2,3,\dots\)</span>)</p>
<p><span class="math display">\[
\theta_{r+1} = \theta_r + \lambda_{opt} H_r^{-1} s_r
\qquad (11.8)
\]</span></p>
<p>where <span class="math inline">\(s_r = \sum_{i=1}^n \frac{\partial l_i}{\partial \theta} (\theta_r | X_i)\)</span>, and <span class="math inline">\(l_i\)</span> is the contribution of the <span class="math inline">\(i\)</span>-th observation to the log-likelihood, while <span class="math inline">\(H_r = \sum_{i=1}^n \left( \frac{\partial l_i}{\partial \theta} (\theta_r | X_i) \right)^2\)</span> or <span class="math inline">\(H_r = - \frac{\partial^2 l}{\partial \theta^2} (\theta_r | X^n)\)</span>. The step length <span class="math inline">\(\lambda_{opt}\)</span> is chosen to maximize the criterion <span class="math inline">\(l(\theta_{r+1}(\lambda)|X^n)\)</span> at each iteration. This can be done by a simple grid search since <span class="math inline">\(\lambda\)</span> is a scalar (this method is usually applied in the context where <span class="math inline">\(\theta\)</span> is multivariate, but we have written this down just in the univariate case). The iterations are terminated when the outcome doesn’t change much as measured by either <span class="math inline">\(|\theta_{r+1} - \theta_r|\)</span> or <span class="math inline">\(|l(\theta_{r+1}|X^n) - l(\theta_r|X^n)|\)</span>. In some cases, computation of derivatives is very difficult and one may substitute analytic derivatives by <strong>numerical derivatives</strong>, which for example replace the functions <span class="math inline">\(f'(x)\)</span> and <span class="math inline">\(f''(x)\)</span> by respectively</p>
<p><span class="math display">\[
f'_\epsilon(x) = \frac{f(x+\epsilon)-f(x-\epsilon)}{2\epsilon}; \quad f''_\epsilon(x) = \frac{f(x+\epsilon) - 2f(x) + f(x-\epsilon)}{\epsilon^2}
\qquad (11.9)
\]</span></p>
<p>where <span class="math inline">\(\epsilon\)</span> is a small number, say <span class="math inline">\(\epsilon = 10^{-8}\)</span>. This only requires the evaluation of the function <span class="math inline">\(f\)</span> and the choice of <span class="math inline">\(\epsilon\)</span>.</p>
<p>In some problems, one can divide the parameters into groups that can be treated differently. For example, there may be a simple way of obtaining good estimators for a subset of the parameters.</p>
<p><strong>Two-step Estimator.</strong> Suppose that <span class="math inline">\(\theta = (\theta_1, \theta_2)\)</span> and that <span class="math inline">\(\hat{\theta}_2\)</span> is a preliminary estimate. Let</p>
<p><span class="math display">\[
\hat{\theta}_1 = \arg \max_{\theta_1 \in \Theta_1} l(\theta_1, \hat{\theta}_2 | X^n)
\]</span></p>
<p>A leading example here is <strong>Heckman’s two-step estimator</strong> in sample selection models, and more generally the generated regressor problem. An important question here is whether the two-step estimator is fully efficient; this will depend on the quality of the preliminary estimate and the structure of the likelihood.</p>
<p>A second approach is called <strong>Concentrated/Profiled Likelihood</strong>. Suppose that <span class="math inline">\(\hat{\theta}_1(\theta_2)\)</span> is the MLE of <span class="math inline">\(\theta_1\)</span> when <span class="math inline">\(\theta_2\)</span> is fixed. Then define the profile or concentrated likelihood</p>
<p><span class="math display">\[
l^P(\theta_2|X^n) = l(\hat{\theta}_1(\theta_2), \theta_2|X^n)
\]</span></p>
<p>which is a function of <span class="math inline">\(\theta_2\)</span> only. In many respects one can treat <span class="math inline">\(l^P(\theta_2|X^n)\)</span> as a proper likelihood. Maximizing <span class="math inline">\(l^P(\theta_2|X^n)\)</span> with respect to <span class="math inline">\(\theta_2\)</span> to obtain <span class="math inline">\(\hat{\theta}_2\)</span> and <span class="math inline">\(\hat{\theta}_1(\hat{\theta}_2)\)</span> gives the same answer as jointly maximizing <span class="math inline">\(l(\theta|X^n)\)</span> with respect to <span class="math inline">\(\theta\)</span>. This construction is useful when <span class="math inline">\(\theta_2\)</span> is one-dimensional and <span class="math inline">\(\hat{\theta}_1(\theta_2)\)</span> is high-dimensional but easy to obtain. For example in the transformation model, if the scalar <span class="math inline">\(\lambda\)</span> were known, then the MLE of <span class="math inline">\(\beta\)</span> is obtained from the least squares regression of <span class="math inline">\(t(y_i;\lambda)\)</span> on <span class="math inline">\(X_i\)</span>. One can then plot out the profile likelihood to aid in the computation of the more difficult maximization problem. There are many other varieties of likelihood including marginal likelihood, conditional likelihood, partial likelihood, and pseudo likelihood.</p>
<p><strong>Invariance Property of MLE</strong></p>
<p>Consider the function <span class="math inline">\(\tau(\theta): \mathbb{R} \to \mathbb{R}\)</span>. What is the maximum likelihood estimator of <span class="math inline">\(\tau\)</span>? For example, if <span class="math inline">\(\theta = \sigma^2\)</span> is the variance parameter, we may be interested in the standard deviation <span class="math inline">\(\sigma = \tau_1(\sigma^2) = \sqrt{\sigma^2}\)</span> or the logarithm of <span class="math inline">\(\sigma^2\)</span>, i.e., <span class="math inline">\(\tau_2(\sigma^2) = \log \sigma^2\)</span>, or “the precision” <span class="math inline">\(\tau_3(\sigma^2) = 1/\sigma^2\)</span>. Let</p>
<p><span class="math display">\[
L^*(\tau|X^n) = \sup_{\{\theta:\tau(\theta)=\tau\}} L(\theta|X^n)
\]</span></p>
<p>be the induced likelihood function. When <span class="math inline">\(\tau\)</span> is one to one, this is <span class="math inline">\(L^*(\tau|X^n) = L(\tau^{-1}(\theta)|X^n)\)</span>. We have</p>
<p><span class="math display">\[
L^*(\hat{\tau}|X^n) = \sup_{\{\theta:\tau(\theta)=\hat{\tau}\}} L(\theta|X^n) \leq \sup_{\theta \in \Theta} L(\theta|X^n) = L(\hat{\theta}|X^n)
\]</span> <span class="math display">\[
= \sup_{\{\theta:\tau(\theta)=\tau(\hat{\theta})\}} L(\theta|X^n) = L^*(\tau(\hat{\theta})|X^n)
\]</span></p>
<p>Therefore, we can say that <span class="math inline">\(\hat{\tau}_{MLE} = \tau(\hat{\theta}_{MLE})\)</span>. We can view this as a reparameterization from <span class="math inline">\(\theta \mapsto \tau\)</span>.</p>
<section class="level3" id="example-11.12">
<h3 class="anchored" data-anchor-id="example-11.12">Example 11.12</h3>
<p>The Pareto distribution <span class="math inline">\(f(x|\alpha) = \alpha x^{-(\alpha+1)}, x \geq 1\)</span>. We have, provided <span class="math inline">\(\alpha &gt; 1\)</span>, that <span class="math inline">\(\alpha = \mu/(1+\mu)\)</span>, where <span class="math inline">\(\mu = E(X)\)</span>, and so we can write the log likelihood in terms of <span class="math inline">\(\mu\)</span> as</p>
<p><span class="math display">\[
l(\mu|X^n) = n \log \mu - n \log (1+\mu) - \frac{1+2\mu}{1+\mu} \sum_{i=1}^n \log X_i
\]</span></p>
<p>from which we can directly compute the MLE for <span class="math inline">\(\mu\)</span>.</p>
<p>What if the function <span class="math inline">\(\tau\)</span> is not one to one, e.g., <span class="math inline">\(\tau(\theta) = \text{sign}(\theta)\)</span>?</p>
</section>
</section>
<section class="level2" id="comparison-of-estimators-and-optimality">
<h2 class="anchored" data-anchor-id="comparison-of-estimators-and-optimality">11.2 COMPARISON OF ESTIMATORS AND OPTIMALITY</h2>
<p>We now discuss the statistical properties of estimators <span class="math inline">\(\hat{\theta}\)</span> of a scalar parameter <span class="math inline">\(\theta\)</span>. These can be divided into: finite sample properties and large sample (asymptotic) properties. The estimator <span class="math inline">\(\hat{\theta}\)</span> is a random variable with a distribution, say <span class="math inline">\(P_{n,\theta_0}\)</span>, that depends on the true value of <span class="math inline">\(\theta\)</span>, sometimes distinguished by the zero subscript, and on the sample size <span class="math inline">\(n\)</span>. It may have a finite mean and variance. Supposing that these quantities exist, we define</p>
<p><span class="math display">\[
E(\hat{\theta}) = \mathbb{E}(\hat{\theta}); \quad \text{var}(\hat{\theta}) = \mathbb{V}(\hat{\theta})
\]</span></p>
<p>Both quantities depend on <span class="math inline">\(\theta\)</span>, which we do not know. We say that</p>
<p><span class="math display">\[
b(\theta) = \mathbb{E}(\hat{\theta}) - \theta
\]</span></p>
<p>is the bias of <span class="math inline">\(\hat{\theta}\)</span>. An ideal estimator would have</p>
<p><span class="math display">\[
b(\theta) = 0 \text{ and } \mathbb{V}(\theta) = 0 \text{ for all } \theta \in \Theta
\]</span></p>
<p>Unfortunately, this is impossible unless <span class="math inline">\(\theta\)</span> is trivial. Both bias and variance can be considered to be undesirable properties. A more general notion of estimator performance is the <strong>mean squared error</strong>.</p>
<section class="level3" id="definition-11.2">
<h3 class="anchored" data-anchor-id="definition-11.2">Definition 11.2</h3>
<p><strong>Mean squared error (MSE)</strong>. Suppose that the parameter estimator <span class="math inline">\(\hat{\theta}\)</span> has finite variance. Then define the MSE</p>
<p><span class="math display">\[
\text{MSE}(\hat{\theta}, \theta_0) = \mathbb{E}[(\hat{\theta}-\theta_0)^2]
\]</span></p>
<p>where the expectation is taken using the distribution <span class="math inline">\(P_{n,\theta_0}\)</span>. For convenience sometimes we replace <span class="math inline">\(\theta_0\)</span> by <span class="math inline">\(\theta\)</span>. Note that <span class="math inline">\(\text{MSE}(\hat{\theta},\theta_0)\)</span> may depend on other unknown quantities.</p>
<p>This is a widely agreed on criterion for measuring estimator performance. An alternative measure the <strong>Mean Absolute Error</strong> is defined as <span class="math inline">\(\mathbb{E}[|\hat{\theta}-\theta_0|]\)</span>; this is harder to work with analytically and we shall most just work with the MSE. In general the MSE depends on <span class="math inline">\(\theta, \theta_0\)</span>, and on the sample size <span class="math inline">\(n\)</span>. An ideal estimator would have zero MSE whatever the truth is. Unfortunately, this is impossible unless the problem is trivial. One can always construct an estimator with zero MSE at a single point. For example, consider the estimator <span class="math inline">\(\hat{\theta} = 0\)</span>. This estimator has</p>
<p><span class="math display">\[
\text{MSE}(\hat{\theta}, \theta) = \theta^2 \geq 0
\]</span></p>
<p>which is equal to zero if and only if <span class="math inline">\(\theta = 0\)</span>. We want an estimator that does well over a range of <span class="math inline">\(\theta\)</span>, since we do not know what the truth is.</p>
</section>
<section class="level3" id="theorem-11.1">
<h3 class="anchored" data-anchor-id="theorem-11.1">Theorem 11.1</h3>
<p>Let <span class="math inline">\(\hat{\theta}\)</span> be an estimator of a parameter <span class="math inline">\(\theta\)</span> whose true value is <span class="math inline">\(\theta_0\)</span>. Then we have</p>
<p><span class="math display">\[
\text{MSE}(\hat{\theta}, \theta_0) = \mathbb{E}[(\hat{\theta}-\mathbb{E}(\hat{\theta}))^2] + (\mathbb{E}(\hat{\theta})-\theta)^2 = \text{var}(\hat{\theta}) + \text{bias}^2(\theta)
\]</span></p>
<p>The mean squared error allows a trade-off between bias and variance. Big bias could be offset by small variance and vice versa.</p>
</section>
<section class="level3" id="example-11.13">
<h3 class="anchored" data-anchor-id="example-11.13">Example 11.13</h3>
<p>Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, where <span class="math inline">\(\theta = (\mu, \sigma^2)\)</span>. Then, <span class="math inline">\(\mathbb{E}(\bar{X}) = \mu\)</span> and <span class="math inline">\(\text{var}(\bar{X}) = \sigma^2/n\)</span>, so that bias = 0, and</p>
<p><span class="math display">\[
\text{MSE}(\bar{X}, \mu) = \sigma^2/n
\]</span></p>
<p>for all <span class="math inline">\(\mu\)</span>.</p>
<p>Now consider estimators of <span class="math inline">\(\sigma^2\)</span> in the same setup</p>
<p><span class="math display">\[
s_*^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2; \quad s^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
\]</span></p>
</section>
<section class="level3" id="theorem-11.2">
<h3 class="anchored" data-anchor-id="theorem-11.2">Theorem 11.2</h3>
<p>Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>. Then we have:</p>
<p><span class="math display">\[
\mathbb{E}(s_*^2) = \sigma^2; \quad \text{var}(s_*^2) = \frac{2\sigma^4}{n-1}
\]</span> <span class="math display">\[
\mathbb{E}(s^2) = \frac{n-1}{n} \sigma^2; \quad \text{var}(s^2) = \frac{2\sigma^4(n-1)}{n^2}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\text{MSE}(s^2, \sigma^2) = \frac{\sigma^4}{n^2} + \frac{2\sigma^4(n-1)}{n^2} = \frac{(2n-1)\sigma^4}{n^2} &lt; \frac{2\sigma^4}{n-1} = \text{MSE}(s_*^2, \sigma^2)
\qquad (11.10)
\]</span></p>
<p>i.e., the maximum likelihood estimator is better according to mean squared error no matter what <span class="math inline">\(\sigma^2\)</span> is.</p>
<p><strong>Proof</strong>. Write</p>
<p><span class="math display">\[
s_*^2 = \frac{1}{n-1} \left[ \sum_{i=1}^n (X_i - \mu)^2 - n(\bar{X}-\mu)^2 \right]
\]</span></p>
<p>Then, by the properties of variance</p>
<p><span class="math display">\[
\text{var}[(n-1)s_*^2] = \text{var} \left[ \sum_{i=1}^n (X_i - \mu)^2 \right] + n^2 \text{var}[(\bar{X}-\mu)^2] - 2n \times \text{cov} \left[ \sum_{i=1}^n (X_i - \mu)^2, (\bar{X}-\mu)^2 \right]
\]</span></p>
<p>We have under the normality assumption</p>
<p><span class="math display">\[
\text{var} \left[ \sum_{i=1}^n (X_i - \mu)^2 \right] = n \text{var}[(X_i-\mu)^2] = n \mathbb{E}[(X_i-\mu)^4] - n \mathbb{E}^2[(X_i-\mu)^2] = 2n\sigma^4
\]</span></p>
<p><span class="math display">\[
(\bar{X}-\mu)^2 = \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \mu) \right]^2 = \frac{1}{n^2} \sum_{i=1}^n (X_i - \mu)^2 + \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1, j \neq i}^n (X_i - \mu)(X_j - \mu)
\]</span></p>
<p>and so it follows that</p>
<p><span class="math display">\[
\text{cov} \left( \sum_{i=1}^n (X_i - \mu)^2, (\bar{X}-\mu)^2 \right) = \sum_{i=1}^n \text{cov} \left( (X_i - \mu)^2, (\bar{X}-\mu)^2 \right)
\qquad (11.11)
\]</span></p>
<p><span class="math display">\[
= \sum_{i=1}^n \text{cov} \left( (X_i - \mu)^2, \frac{1}{n^2} \sum_{j=1}^n (X_j - \mu)^2 \right)
\qquad (11.12)
\]</span></p>
<p><span class="math display">\[
= \sum_{i=1}^n \frac{1}{n^2} \text{cov} \left( (X_i - \mu)^2, (X_i - \mu)^2 \right) = \frac{1}{n^2} \text{var} \left[ \sum_{i=1}^n (X_i - \mu)^2 \right] = \frac{2\sigma^4}{n}
\qquad (11.13)
\]</span></p>
<p>where in (11.11) and (11.13) we use the fact that <span class="math inline">\(\text{cov}(A+B,C) = \text{cov}(A,C) + \text{cov}(B,C)\)</span> for any random variables <span class="math inline">\(A, B, C\)</span> for which the moments are defined and in (11.12) we use the fact that <span class="math inline">\(\text{cov}((X_i-\mu)^2, (X_j-\mu)(X_k-\mu)) = 0\)</span> for <span class="math inline">\(j \neq k\)</span>.</p>
<p>Furthermore, <span class="math inline">\(\mathbb{E}[(\bar{X}-\mu)^2] = \text{var}(\bar{X}) = \sigma^2/n\)</span>, while</p>
<p><span class="math display">\[
\mathbb{E}[(\bar{X}-\mu)^4] = \frac{1}{n^4} \sum_{i=1}^n \mathbb{E}[(X_i-\mu)^4] + \frac{3}{n^4} \sum_{i \neq j} \mathbb{E}[(X_i-\mu)^2] \mathbb{E}[(X_j-\mu)^2] = \frac{3\sigma^4}{n^3} + \frac{3(n-1)\sigma^4}{n^3}
\]</span></p>
<p>Therefore, <span class="math inline">\(\text{var}[(\bar{X}-\mu)^2] = \frac{2\sigma^4}{n^2}\)</span>, so that</p>
<p><span class="math display">\[
\text{MSE}(s_*^2, \sigma^2) = \frac{1}{(n-1)^2} \left[ 2n\sigma^4 + 2\sigma^4 - 4\sigma^4 \right] = \frac{2\sigma^4}{n-1}
\]</span></p>
<p>Now consider the MLE <span class="math inline">\(s^2 = (n-1)s_*^2/n\)</span>. We have</p>
<p><span class="math display">\[
\mathbb{E}(s^2) = \frac{n-1}{n} \sigma^2
\]</span> <span class="math display">\[
\text{var}(s^2) = \frac{2\sigma^4}{n-1} \left( \frac{n-1}{n} \right)^2 = \frac{2\sigma^4(n-1)}{n^2}
\]</span></p>
<p>Therefore,</p>
<p><span class="math display">\[
\text{MSE}(s^2, \sigma^2) = \frac{\sigma^4}{n^2} + \frac{2\sigma^4(n-1)}{n^2} = \frac{(2n-1)\sigma^4}{n^2}
\]</span></p>
<p>The result follows.</p>
<p>Mean squared error provides an unambiguous way to rank estimators for a given parameter value, at least in the scalar case. In general, there are many different estimators of a parameter <span class="math inline">\(\theta\)</span>. In particular cases we can calculate MSE and rank estimators. We would like a general result that describes the best procedure to use for whatever parameter is true. Unfortunately, requiring a best MSE estimator uniformly over <span class="math inline">\(\theta\)</span> is not possible, or rather not useful, because at any given point <span class="math inline">\(\theta = \theta_0\)</span> the estimator <span class="math inline">\(\hat{\theta}_0 = \theta_0\)</span> is best since it has <span class="math inline">\(\text{MSE} = 0\)</span> there. We can resolve this in a number of ways.</p>
<ol type="1">
<li>Take some representative values of the parameter space. For example, the <strong>minimax</strong> approach considers the following criterion</li>
</ol>
<p><span class="math display">\[
\max_{\theta \in \Theta} \text{MSE}(\hat{\theta};\theta)
\]</span></p>
<p>which amounts to the worst case scenario. One should find the estimator that minimizes this worst case MSE. You then get a logically consistent theory; but hard to work out in general. Also, it is rather pessimistic. In most cases, we don’t expect nature to be playing against us.</p>
<ol start="2" type="1">
<li>Restrict the class of estimators. For example suppose that <span class="math inline">\(\hat{\theta}\)</span> is unbiased, i.e.,</li>
</ol>
<p><span class="math display">\[
b(\theta) = 0, \text{ for all } \theta \in \Theta
\]</span></p>
<p>In that case, the <strong>Gauss-Markov Theorem</strong> says that the sample mean is the <strong>BLUE (Best Linear Unbiased)</strong>.</p>
<ol start="3" type="1">
<li>If we have large sample sizes, then many estimators are asymptotically normal with mean zero and variance <span class="math inline">\(V\)</span>. In this case, we can rank estimators that are asymptotically normal unambiguously by their asymptotic variance.</li>
</ol>
<p>An estimator is said to be <strong>UMVUE [Uniformly Minimum Variance Unbiased Estimator]</strong> if <span class="math inline">\(\hat{\theta}\)</span> has the property claimed in its title. An alternative acronym here is <strong>BUE [Best Unbiased Estimator]</strong>. We have an extensive theory of optimality about Unbiased Estimators. Specifically, the <strong>Cramér-Rao Theorem</strong>.</p>
</section>
<section class="level3" id="theorem-11.3">
<h3 class="anchored" data-anchor-id="theorem-11.3">Theorem 11.3</h3>
<p><strong>Cramér-Rao</strong>. Let <span class="math inline">\(\hat{\theta}\)</span> be any unbiased estimator of <span class="math inline">\(\theta \in \mathbb{R}\)</span>. Then, under regularity conditions [We assume that the support of <span class="math inline">\(X\)</span> does not depend on <span class="math inline">\(\theta\)</span> and that all “limit” operations can be interchanged], we have</p>
<p><span class="math display">\[
\text{var}(\hat{\theta}) \geq \left[ \mathbb{E} \left( s(\theta_0|X^n)^2 \right) \right]^{-1} = \left[ \text{var}(s(\theta|X^n)) \right]^{-1} = \mathcal{I}^{-1}(\theta)
\qquad (11.14)
\]</span></p>
<p>which is known as the <strong>information inequality</strong>.</p>
<p><strong>Proof</strong>. For any unbiased estimator <span class="math inline">\(\hat{\theta}\)</span></p>
<p><span class="math display">\[
\mathbb{E}(\hat{\theta}) = \int \hat{\theta}(X^n) L(\theta|X^n) dX_1 \dots dX_n = \theta
\]</span></p>
<p>Differentiating both sides of this equation with respect to <span class="math inline">\(\theta\)</span>, we obtain</p>
<p><span class="math display">\[
1 = \int \hat{\theta}(X^n) \frac{\partial L}{\partial \theta} (\theta|X^n) dX_1 \dots dX_n
\]</span> <span class="math display">\[
= \int \hat{\theta}(X^n) \frac{\partial l}{\partial \theta} (\theta|X^n) \frac{1}{L(\theta|X^n)} L(\theta|X^n) dX_1 \dots dX_n
\]</span> <span class="math display">\[
= \int \hat{\theta}(X^n) \frac{\partial l}{\partial \theta} (\theta|X^n) L(\theta|X^n) dX_1 \dots dX_n = \mathbb{E} \left[ \hat{\theta}(X^n) s(\theta|X^n) \right]
\qquad (11.15)
\]</span></p>
<p>Furthermore, for all <span class="math inline">\(\theta\)</span>,</p>
<p><span class="math display">\[
\int L(\theta|X^n) dX_1 \dots dX_n = 1
\]</span></p>
<p>which implies that</p>
<p><span class="math display">\[
0 = \int \frac{\partial l}{\partial \theta} (\theta|X^n) dX_1 \dots dX_n = \int \frac{\partial l}{\partial \theta} (\theta|X^n) L(\theta|X^n) dX_1 \dots dX_n = \mathbb{E}[s(\theta|X^n)]
\qquad (11.16)
\]</span></p>
<p>Combining these two results we have obtained</p>
<p><span class="math display">\[
\int (\hat{\theta}(X^n) - \theta) s(\theta|X^n) L(\theta|X^n) dX_1 \dots dX_n = \text{cov}(\hat{\theta}, s(\theta|X^n)) = 1
\]</span></p>
<p>We now use the <strong>Cauchy-Schwarz inequality</strong> [taking <span class="math inline">\(f = (\hat{\theta}(X^n)-\theta) \times L^{1/2}(\theta|X^n)\)</span> and <span class="math inline">\(g = \frac{\partial l}{\partial \theta}(\theta|X^n) L^{1/2}(\theta|X^n)\)</span>] to obtain that</p>
<p><span class="math display">\[
\left\{ \int (\hat{\theta}(X^n)-\theta)^2 L(\theta|X^n) dX_1 \dots dX_n \right\} \times \left\{ \int (s(\theta|X^n))^2 L(\theta|X^n) dX_1 \dots dX_n \right\} \geq 1
\]</span></p>
<p>i.e.,</p>
<p><span class="math display">\[
\int (\hat{\theta}(X^n) - \theta)^2 L(\theta|X^n) dX_1 \dots dX_n \geq \frac{1}{\int (s(\theta|X^n))^2 L(\theta|X^n) dX_1 \dots dX_n}
\]</span></p>
<p>as required.</p>
<p><strong>Remarks</strong>.</p>
<ol type="1">
<li>Differentiating (11.16) with respect to <span class="math inline">\(\theta\)</span> yields the <strong>information equality</strong></li>
</ol>
<p><span class="math display">\[
\mathcal{I}(\theta) = \mathbb{E} \left[ (s(\theta|X^n))^2 \right] = -\mathbb{E}[h(\theta|X^n)]
\qquad (11.17)
\]</span></p>
<p>which relates the variance of the score function to the mean of the Hessian. This says that the information <span class="math inline">\(\mathcal{I}(\theta)\)</span> can be expressed as minus the expected value of the Hessian. As shown in Rothenberg (1971), a parameter <span class="math inline">\(\theta\)</span> is locally identifiable at <span class="math inline">\(\theta_0\)</span> if and only if <span class="math inline">\(\mathcal{I}(\theta_0)\)</span> is nonzero in a neighbourhood of <span class="math inline">\(\theta_0\)</span>. This is because heuristically in a neighbourhood of the true parameter value the expected log likelihood is a quadratic function of <span class="math inline">\(\theta\)</span></p>
<p><span class="math display">\[
\mathbb{E}l(\theta|X^n) \approx \mathbb{E}l(\theta_0|X^n) + \frac{\partial}{\partial \theta} \mathbb{E} l(\theta_0|X^n) (\theta - \theta_0) + \frac{1}{2} \frac{\partial^2}{\partial \theta^2} \mathbb{E} l(\theta_0|X^n) (\theta-\theta_0)^2
\]</span> <span class="math display">\[
\approx \mathbb{E}l(\theta_0|X^n) - \frac{1}{2} \mathcal{I}(\theta_0) (\theta - \theta_0)^2
\]</span></p>
<p>Provided <span class="math inline">\(\mathcal{I}(\theta_0) &gt; 0\)</span>, this shows that <span class="math inline">\(\mathbb{E}l(\theta|X^n)\)</span> is uniquely maximized at <span class="math inline">\(\theta = \theta_0\)</span>.</p>
<ol start="2" type="1">
<li><p>The CR result does not apply to distributions like the uniform whose support depends on <span class="math inline">\(\theta\)</span>. Does not apply to non-differentiable densities (for example <span class="math inline">\(f(x) = \exp(-|x-\theta|)\)</span>) either.</p></li>
<li><p>This result only outlines the possible, in the sense of the best achievable variance. It does not say what happens in a particular case. In finite samples, the information bound may not be achieved by any estimator. But when the MLE is the sample mean, it typically is achieved, because the MLE is unbiased, specifically</p></li>
</ol>
<p><span class="math display">\[
X \sim N(\mu, 1) \quad \hat{\mu}_{MLE} = \bar{X} \quad \mathcal{I}_n^{-1} = \frac{1}{n}
\]</span> <span class="math display">\[
X \sim B(n,p) \quad \hat{p}_{MLE} = \bar{X} \quad \mathcal{I}_n^{-1} = \frac{p(1-p)}{n}
\]</span> <span class="math display">\[
X \sim P(\lambda) \quad \hat{\lambda}_{MLE} = \bar{X} \quad \mathcal{I}_n^{-1} = \frac{\lambda}{n}
\]</span></p>
<p>In this case, the MLE is BUE.</p>
<ol start="4" type="1">
<li>The matrix extension of (11.14) is straightforward, but requires matrix inversion, which we will treat later.</li>
</ol>
</section>
<section class="level3" id="example-11.14">
<h3 class="anchored" data-anchor-id="example-11.14">Example 11.14</h3>
<p>Let <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>. Then,</p>
<p><span class="math display">\[
-\mathbb{E} \left( \frac{\partial^2 l}{\partial \mu^2} (\mu, \sigma^2) \right) = \frac{n}{\sigma^2}
\]</span> <span class="math display">\[
-\mathbb{E} \left( \frac{\partial^2 l}{\partial (\sigma^2)^2} (\mu, \sigma^2) \right) = \frac{n}{2\sigma^4}
\]</span> <span class="math display">\[
\mathbb{E} \left( \frac{\partial^2 l}{\partial \mu \partial \sigma^2} (\mu, \sigma^2) \right) = 0
\]</span></p>
<p>Suppose that <span class="math inline">\(\sigma^2\)</span> is known. Then for any unbiased estimators of <span class="math inline">\(\mu\)</span>, we have</p>
<p><span class="math display">\[
\text{var}(\hat{\mu}) \geq \frac{\sigma^2}{n}
\]</span></p>
<p>In this case, the MLE <span class="math inline">\(\bar{X}\)</span> has variance <span class="math inline">\(\sigma^2/n\)</span>, so it is UMVUE in this case. In fact, it remains the UMVUE even when <span class="math inline">\(\sigma^2\)</span> is unknown since its variance is unaffected by this issue.</p>
</section>
<section class="level3" id="example-11.15">
<h3 class="anchored" data-anchor-id="example-11.15">Example 11.15</h3>
<p>Suppose that <span class="math inline">\(\mu\)</span> is known, then for any unbiased estimator of <span class="math inline">\(\sigma^2\)</span> we have</p>
<p><span class="math display">\[
\text{var}(\hat{\sigma}^2) \geq \frac{2\sigma^4}{n}
\]</span></p>
<p>In this case, the MLE is</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2
\]</span></p>
<p>and is unbiased with variance <span class="math inline">\(2\sigma^4/n\)</span>, so it is UMVUE in this case.</p>
</section>
<section class="level3" id="example-11.16">
<h3 class="anchored" data-anchor-id="example-11.16">Example 11.16</h3>
<p>Suppose that both <span class="math inline">\(\mu, \sigma^2\)</span> are unknown. Then, the MLE <span class="math inline">\(s^2\)</span> is biased, and the unbiased estimator <span class="math inline">\(s_*^2\)</span> has variance</p>
<p><span class="math display">\[
\frac{2\sigma^4}{n-1} &gt; \frac{2\sigma^4}{n}
\]</span></p>
<p>In this case, the Cramér-Rao lower bound for <span class="math inline">\(\sigma^2\)</span> is unattainable in any finite sample.</p>
</section>
</section>
<section class="level2" id="asymptotic-properties">
<h2 class="anchored" data-anchor-id="asymptotic-properties">11.2.1 Asymptotic Properties</h2>
<p>In many cases calculating exactly the MSE is difficult and we may just work with an approximation to it called <strong>Asymptotic Mean Squared Error</strong>, which arises by taking <span class="math inline">\(n \to \infty\)</span>; this will be a valid approximation provided the sample size is large.</p>
<section class="level3" id="definition-11.3">
<h3 class="anchored" data-anchor-id="definition-11.3">Definition 11.3</h3>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> is <strong>consistent</strong> if for all <span class="math inline">\(\theta \in \Theta\)</span></p>
<p><span class="math display">\[
\hat{\theta} \overset{P}{\to} \theta
\]</span></p>
<p>It is sufficient (but not necessary) for consistency that: (1) <span class="math inline">\(\hat{\theta}\)</span> is asymptotically unbiased, i.e., <span class="math inline">\(\mathbb{E} \hat{\theta} \to \theta\)</span>, and (2) <span class="math inline">\(\text{var}(\hat{\theta}) \to 0\)</span>.</p>
</section>
<section class="level3" id="definition-11.4">
<h3 class="anchored" data-anchor-id="definition-11.4">Definition 11.4</h3>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> is <strong>asymptotically normal</strong> if for all <span class="math inline">\(\theta\)</span>, there is a positive, finite quantity <span class="math inline">\(V(\theta)\)</span> such that</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\theta}-\theta) \overset{D}{\to} N(0, V(\theta))
\qquad (11.18)
\]</span></p>
<p>This says essentially that the asymptotic MSE of <span class="math inline">\(\hat{\theta}\)</span> is equal to <span class="math inline">\(V(\theta)/n\)</span>, where <span class="math inline">\(V\)</span> depends on <span class="math inline">\(\theta\)</span> and on <span class="math inline">\(\hat{\theta}\)</span>. It follows that we may rank estimators according to the <span class="math inline">\(V\)</span> in their limiting distribution.</p>
</section>
<section class="level3" id="definition-11.5">
<h3 class="anchored" data-anchor-id="definition-11.5">Definition 11.5</h3>
<p>Suppose that <span class="math inline">\(\hat{\theta}_1\)</span> and <span class="math inline">\(\hat{\theta}_2\)</span> are two estimators of <span class="math inline">\(\theta\)</span> that both satisfy (11.18) with asymptotic variance <span class="math inline">\(V_1(\theta)\)</span> and <span class="math inline">\(V_2(\theta)\)</span>, respectively. Suppose that</p>
<p><span class="math display">\[
V_1(\theta) \leq V_2(\theta)
\qquad (11.19)
\]</span></p>
<p>for all <span class="math inline">\(\theta \in \Theta\)</span>. Then we say that <span class="math inline">\(\hat{\theta}_1\)</span> is more <strong>asymptotically efficient</strong> than <span class="math inline">\(\hat{\theta}_2\)</span>.</p>
</section>
<section class="level3" id="example-11.17">
<h3 class="anchored" data-anchor-id="example-11.17">Example 11.17</h3>
<p>Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> and let</p>
<p><span class="math display">\[
\hat{\mu} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Then, <span class="math inline">\(\mathbb{E}(\hat{\mu}) = \mu\)</span> and <span class="math inline">\(\text{var}(\hat{\mu}) = \sigma^2/n\)</span>, so that bias = 0, and <span class="math inline">\(\text{MSE} = \sigma^2/n\)</span>, for all <span class="math inline">\(\mu\)</span>. In addition</p>
<p><span class="math display">\[
\hat{\mu} \overset{P}{\to} \mu; \quad \sqrt{n}(\hat{\mu}-\mu) \overset{D}{\to} N(0, \sigma^2)
\]</span></p>
</section>
<section class="level3" id="example-11.18">
<h3 class="anchored" data-anchor-id="example-11.18">Example 11.18</h3>
<p>An alternative estimator would be based on the average of every even observation</p>
<p><span class="math display">\[
\hat{\mu}_E = \frac{2}{n} \sum_{i=1}^{n/2} X_{2i}
\]</span></p>
<p>This is unbiased but has variance (and hence MSE) equal to <span class="math inline">\(2\sigma^2/n\)</span>, which is inferior to that of <span class="math inline">\(\hat{\mu}\)</span>. Furthermore,</p>
<p><span class="math display">\[
\hat{\mu}_E \overset{P}{\to} \mu; \quad \sqrt{n}(\hat{\mu}_E - \mu) \overset{D}{\to} N(0, 2\sigma^2)
\]</span></p>
<p>It is asymptotically inefficient.</p>
</section>
<section class="level3" id="example-11.19">
<h3 class="anchored" data-anchor-id="example-11.19">Example 11.19</h3>
<p>Suppose that <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span>, and consider the following estimators of <span class="math inline">\(\sigma^2\)</span>:</p>
<p><span class="math display">\[
s_*^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar{X})^2; \quad s^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2
\]</span></p>
<p>We showed above that in finite samples <span class="math inline">\(\text{MSE}(s^2, \sigma^2) &lt; \text{MSE}(s_*^2, \sigma^2)\)</span> for all parameter values. Note that as <span class="math inline">\(n \to \infty\)</span>, the difference between <span class="math inline">\(s_*^2\)</span> and <span class="math inline">\(s^2\)</span> disappears, i.e.,</p>
<p><span class="math display">\[
\sqrt{n}(s_*^2 - \sigma^2) \overset{D}{\to} N(0, 2\sigma^4)
\]</span> <span class="math display">\[
\sqrt{n}(s^2 - \sigma^2) \overset{D}{\to} N(0, 2\sigma^4)
\]</span></p>
<p>so they are equally asymptotically efficient. In fact, if <span class="math inline">\(\mu\)</span> were known, the estimator <span class="math inline">\(s_n^2 = \sum_{i=1}^n (X_i - \mu)^2/n\)</span> also satisfies <span class="math inline">\(\sqrt{n}(s_n^2 - \sigma^2) \overset{D}{\to} N(0, 2\sigma^4)\)</span>.</p>
</section>
<section class="level3" id="theorem-11.4">
<h3 class="anchored" data-anchor-id="theorem-11.4">Theorem 11.4</h3>
<p><strong>Asymptotic CR bound</strong>. Under quite general conditions, it can be shown that as <span class="math inline">\(n \to \infty\)</span></p>
<p><span class="math display">\[
\sqrt{n}(\hat{\theta}_{MLE} - \theta) \overset{D}{\to} N(0, \mathcal{I}_\infty^{-1}(\theta))
\]</span> <span class="math display">\[
\mathcal{I}_\infty(\theta) = \lim_{n \to \infty} \frac{1}{n} \mathcal{I}_n(\theta)
\]</span></p>
<p>Let <span class="math inline">\(\hat{\theta}\)</span> be any other estimator with</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\theta}-\theta) \overset{D}{\to} N(0, V(\theta))
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
V(\theta) \geq \mathcal{I}_\infty^{-1}(\theta)
\]</span></p>
<p>This implies that in large samples the MLE is efficient, because it is asymptotically unbiased.</p>
</section>
</section>
<section class="level2" id="robustness-and-other-issues-with-the-mle">
<h2 class="anchored" data-anchor-id="robustness-and-other-issues-with-the-mle">11.3 ROBUSTNESS AND OTHER ISSUES WITH THE MLE</h2>
<p>We have shown how the MLE is the most efficient method when the sample size is large. This was predicated on a complete and correct specification of the data distribution, that is, the true <span class="math inline">\(P\)</span> satisfies <span class="math inline">\(P \in \{ P_\theta, \theta \in \Theta \}\)</span>. An alternative question to ask about any given statistical method is: how well it works when not all the assumptions made are correct? Frequently, misspecification causes inconsistency, although in some cases only an increase in variance. A leading example here is estimation of the population mean by the sample mean. The sample mean can be interpreted as the MLE for normally distributed data. If the data are heavy tailed such as a <span class="math inline">\(t\)</span> distribution, then the sample mean suffers loss of precision relative to the best attainable method (which is then the <span class="math inline">\(t\)</span>-MLE). In fact, the normal based MLE is often highly susceptible to fat tailed distributions suffering great losses in variance (and even inconsistency when for example the distribution is Cauchy). By contrast, the sample median estimator does close to optimally in a much bigger range of circumstances (and is consistent even for the Cauchy distribution). We compare three different procedures for estimating the centre of location (mean or median) in terms of their performances in three different scenarios. The estimators we consider are: the sample median (the MLE for double exponential distribution); the sample mean = (the MLE for Gaussian distribution); and the Cauchy MLE. We consider three (symmetric) distributions for <span class="math inline">\(P\)</span>: Normal <span class="math inline">\(N(0,1)\)</span>; Double exponential <span class="math inline">\(f(x) = \frac{1}{2}e^{-|x|}\)</span>; and Cauchy <span class="math inline">\(f(x) = \frac{1}{\pi(1+x^2)}\)</span>. Below we give the asymptotic variances of the three estimation procedures under the three different data distributions</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>Mean</th>
<th>Median</th>
<th>Cauchy MLE</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Norm</td>
<td>1.00</td>
<td>1.57</td>
<td>1.32</td>
</tr>
<tr class="even">
<td>Exp</td>
<td>1.40</td>
<td>1.00</td>
<td>1.18</td>
</tr>
<tr class="odd">
<td>Cauchy</td>
<td>∞</td>
<td>2.47</td>
<td>2.00</td>
</tr>
</tbody>
</table>
<p>This shows the range of outcomes that are obtained depending on the estimation procedure and the true distribution.</p>
<p>One form of misspecification that has received a lot of attention is the <strong>gross error model</strong>, where the typist occasionally hits the wrong key and you get some extra zeros on the end. In this case, the normal-based likelihood procedures can be wildly inaccurate. They have what is known as <strong>unbounded influence functions</strong>, and can be arbitrarily messed up by such errors as under the Cauchy distribution. The median has a bounded influence function as does the trimmed mean (i.e., throw away the 5% largest and smallest observations). As estimators of a population centre of symmetry these are robust. In conclusion, sometimes the MLE is robust, but sometimes it is not. In practice, one has to recognize the frequent arbitrariness of the model and accept that it will only provide an approximation to the data distribution. A literature has developed that treats the consequences of misspecified likelihood estimation, see White (1982). Some authors use the adjectives “quasi” or “pseudo” before likelihood in order to emphasize their belief that the model is only an approximation. As we have argued, in some cases this misspecification still permits consistent (but inefficient) estimation of quantities of interest, whereas in other cases it does not.</p>
<p>There are some further areas of criticism of maximum likelihood.</p>
<p>Firstly, the MLE can be computationally intensive and in practice may not be worth the additional cost involved in their computation relative to simpler methods like the Method of Moments.</p>
<p>Secondly, there are even some models for which the MLE is not best. These situations violate the regularity conditions that go along with the distribution and optimality theory. For example, the fixed effect panel data regression model has parameters increasing in number with sample size.</p>
<p>Finally, the Bayesian school argues that one should treat the parameters as random variables about which there is some knowledge expressed through a prior probability distribution <span class="math inline">\(\pi(\theta)\)</span>. See Leamer (1978) for further discussion. These methods are enjoying a revival due in part to advances in computational methodology that has made it possible to actually carry out the Bayesian procedures for reasonable models.</p>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch11exercise1">
<h3 class="anchored" data-anchor-id="sec-ch11exercise1">Exercise 1</h3>
<p><a href="#sec-ch11solution1">Solution 1</a></p>
<p>Consider a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a population with the probability density function (pdf) given by <span class="math inline">\(f(x|\theta) = \theta x^{\theta - 1}\)</span> for <span class="math inline">\(0 &lt; x &lt; 1\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. Using the <strong>method of moments</strong>, find an estimator for <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise2">
<h3 class="anchored" data-anchor-id="sec-ch11exercise2">Exercise 2</h3>
<p><a href="#sec-ch11solution2">Solution 2</a></p>
<p>Suppose you have a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a distribution with the probability density function <span class="math inline">\(f(x|\theta) = \frac{1}{\theta} e^{-x/\theta}\)</span> for <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. Derive the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise3">
<h3 class="anchored" data-anchor-id="sec-ch11exercise3">Exercise 3</h3>
<p><a href="#sec-ch11solution3">Solution 3</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a Bernoulli distribution with parameter <span class="math inline">\(p\)</span>, where <span class="math inline">\(P(X_i = 1) = p\)</span> and <span class="math inline">\(P(X_i = 0) = 1-p\)</span>. Find the <strong>method of moments estimator</strong> for <span class="math inline">\(p\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise4">
<h3 class="anchored" data-anchor-id="sec-ch11exercise4">Exercise 4</h3>
<p><a href="#sec-ch11solution4">Solution 4</a></p>
<p>Given a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a normal distribution with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, find the <strong>maximum likelihood estimators (MLE)</strong> for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise5">
<h3 class="anchored" data-anchor-id="sec-ch11exercise5">Exercise 5</h3>
<p><a href="#sec-ch11solution5">Solution 5</a></p>
<p>Consider a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a uniform distribution on the interval <span class="math inline">\([0, \theta]\)</span>. Find the <strong>method of moments estimator</strong> for <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise6">
<h3 class="anchored" data-anchor-id="sec-ch11exercise6">Exercise 6</h3>
<p><a href="#sec-ch11solution6">Solution 6</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. Derive the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(\lambda\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise7">
<h3 class="anchored" data-anchor-id="sec-ch11exercise7">Exercise 7</h3>
<p><a href="#sec-ch11solution7">Solution 7</a></p>
<p>Suppose <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are i.i.d. random variables with the cumulative distribution function (CDF) <span class="math inline">\(F(x|\theta) = 1 - e^{-x/\theta}\)</span> for <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. Find the <strong>method of moments estimator</strong> for <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise8">
<h3 class="anchored" data-anchor-id="sec-ch11exercise8">Exercise 8</h3>
<p><a href="#sec-ch11solution8">Solution 8</a></p>
<p>Consider a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a geometric distribution with parameter <span class="math inline">\(p\)</span>, where <span class="math inline">\(P(X_i = k) = (1-p)^{k-1}p\)</span> for <span class="math inline">\(k = 1, 2, \dots\)</span>. Derive the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(p\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise9">
<h3 class="anchored" data-anchor-id="sec-ch11exercise9">Exercise 9</h3>
<p><a href="#sec-ch11solution9">Solution 9</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a distribution with the probability density function (pdf) <span class="math inline">\(f(x|\theta) = \frac{2x}{\theta^2}\)</span> for <span class="math inline">\(0 &lt; x &lt; \theta\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. Find the <strong>method of moments estimator</strong> for <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise10">
<h3 class="anchored" data-anchor-id="sec-ch11exercise10">Exercise 10</h3>
<p><a href="#sec-ch11solution10">Solution 10</a></p>
<p>Given a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from an exponential distribution with parameter <span class="math inline">\(\lambda\)</span>, where the probability density function (pdf) is given by <span class="math inline">\(f(x|\lambda) = \lambda e^{-\lambda x}\)</span> for <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(\lambda &gt; 0\)</span>. Find the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(\lambda\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise11">
<h3 class="anchored" data-anchor-id="sec-ch11exercise11">Exercise 11</h3>
<p><a href="#sec-ch11solution11">Solution 11</a></p>
<p>Consider a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a Pareto distribution with probability density function (pdf) <span class="math inline">\(f(x|\alpha) = \alpha x^{-(\alpha+1)}\)</span> for <span class="math inline">\(x \geq 1\)</span> and <span class="math inline">\(\alpha &gt; 0\)</span>. Find the <strong>method of moments estimator</strong> for <span class="math inline">\(\alpha\)</span> when <span class="math inline">\(\alpha &gt; 1\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise12">
<h3 class="anchored" data-anchor-id="sec-ch11exercise12">Exercise 12</h3>
<p><a href="#sec-ch11solution12">Solution 12</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a normal distribution with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>. Derive the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(\sigma^2\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise13">
<h3 class="anchored" data-anchor-id="sec-ch11exercise13">Exercise 13</h3>
<p><a href="#sec-ch11solution13">Solution 13</a></p>
<p>Suppose <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are i.i.d. random variables with the probability density function (pdf) <span class="math inline">\(f(x|\theta) = \frac{1}{2\theta}\)</span> for <span class="math inline">\(-\theta &lt; x &lt; \theta\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. Find the <strong>method of moments estimator</strong> for <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise14">
<h3 class="anchored" data-anchor-id="sec-ch11exercise14">Exercise 14</h3>
<p><a href="#sec-ch11solution14">Solution 14</a></p>
<p>Given a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a beta distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, where the probability density function (pdf) is given by <span class="math inline">\(f(x|\alpha, \beta) = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{\alpha-1}(1-x)^{\beta-1}\)</span> for <span class="math inline">\(0 &lt; x &lt; 1\)</span> and <span class="math inline">\(\alpha, \beta &gt; 0\)</span>. Derive the <strong>method of moments estimators</strong> for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise15">
<h3 class="anchored" data-anchor-id="sec-ch11exercise15">Exercise 15</h3>
<p><a href="#sec-ch11solution15">Solution 15</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a gamma distribution with shape parameter <span class="math inline">\(k\)</span> and scale parameter <span class="math inline">\(\theta\)</span>, where the probability density function (pdf) is given by <span class="math inline">\(f(x|k, \theta) = \frac{1}{\Gamma(k)\theta^k} x^{k-1} e^{-x/\theta}\)</span> for <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(k, \theta &gt; 0\)</span>. Find the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(\theta\)</span> when <span class="math inline">\(k\)</span> is known.</p>
</section>
<section class="level3" id="sec-ch11exercise16">
<h3 class="anchored" data-anchor-id="sec-ch11exercise16">Exercise 16</h3>
<p><a href="#sec-ch11solution16">Solution 16</a></p>
<p>Consider a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a distribution with the cumulative distribution function (CDF) <span class="math inline">\(F(x|\theta) = (x/\theta)^\alpha\)</span> for <span class="math inline">\(0 &lt; x &lt; \theta\)</span> and <span class="math inline">\(\alpha, \theta &gt; 0\)</span>. Find the <strong>method of moments estimator</strong> for <span class="math inline">\(\theta\)</span> when <span class="math inline">\(\alpha\)</span> is known.</p>
</section>
<section class="level3" id="sec-ch11exercise17">
<h3 class="anchored" data-anchor-id="sec-ch11exercise17">Exercise 17</h3>
<p><a href="#sec-ch11solution17">Solution 17</a></p>
<p>Suppose <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> are i.i.d. random variables with the probability density function (pdf) <span class="math inline">\(f(x|\theta) = \frac{1}{\theta} x^{(1-\theta)/\theta}\)</span> for <span class="math inline">\(0 &lt; x &lt; 1\)</span> and <span class="math inline">\(\theta &gt; 0\)</span>. Derive the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch11exercise18">
<h3 class="anchored" data-anchor-id="sec-ch11exercise18">Exercise 18</h3>
<p><a href="#sec-ch11solution18">Solution 18</a></p>
<p>Let <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> be a random sample from a Laplace distribution with location parameter <span class="math inline">\(\mu\)</span> and scale parameter <span class="math inline">\(b\)</span>, where the probability density function (pdf) is given by <span class="math inline">\(f(x|\mu, b) = \frac{1}{2b} e^{-|x-\mu|/b}\)</span> for <span class="math inline">\(-\infty &lt; x &lt; \infty\)</span> and <span class="math inline">\(b &gt; 0\)</span>. Find the <strong>method of moments estimator</strong> for <span class="math inline">\(b\)</span> when <span class="math inline">\(\mu\)</span> is known.</p>
</section>
<section class="level3" id="sec-ch11exercise19">
<h3 class="anchored" data-anchor-id="sec-ch11exercise19">Exercise 19</h3>
<p><a href="#sec-ch11solution19">Solution 19</a></p>
<p>Given a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a Weibull distribution with shape parameter <span class="math inline">\(k\)</span> and scale parameter <span class="math inline">\(\lambda\)</span>, where the probability density function (pdf) is given by <span class="math inline">\(f(x|k, \lambda) = \frac{k}{\lambda} \left(\frac{x}{\lambda}\right)^{k-1} e^{-(x/\lambda)^k}\)</span> for <span class="math inline">\(x &gt; 0\)</span> and <span class="math inline">\(k, \lambda &gt; 0\)</span>. Derive the <strong>maximum likelihood estimator (MLE)</strong> for <span class="math inline">\(\lambda\)</span> when <span class="math inline">\(k\)</span> is known.</p>
</section>
<section class="level3" id="sec-ch11exercise20">
<h3 class="anchored" data-anchor-id="sec-ch11exercise20">Exercise 20</h3>
<p><a href="#sec-ch11solution20">Solution 20</a></p>
<p>Consider a random sample <span class="math inline">\(X_1, X_2, \dots, X_n\)</span> from a Cauchy distribution with location parameter <span class="math inline">\(\theta\)</span>, where the probability density function (pdf) is given by <span class="math inline">\(f(x|\theta) = \frac{1}{\pi(1+(x-\theta)^2)}\)</span> for <span class="math inline">\(-\infty &lt; x &lt; \infty\)</span>. Show that the sample mean is <strong>not a consistent estimator</strong> for <span class="math inline">\(\theta\)</span>.</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch11solution1">
<h3 class="anchored" data-anchor-id="sec-ch11solution1">Solution 1</h3>
<p><a href="#sec-ch11exercise1">Exercise 1</a></p>
<p>The method of moments estimator is found by equating the sample moments to the population moments.</p>
<p>The first population moment (mean) is given by:</p>
<p><span class="math display">\[
E(X) = \int_0^1 x f(x|\theta) dx = \int_0^1 x \cdot \theta x^{\theta - 1} dx = \theta \int_0^1 x^\theta dx = \frac{\theta}{\theta + 1}
\]</span></p>
<p>The first sample moment is the sample mean:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Equating the population moment to the sample moment, we have:</p>
<p><span class="math display">\[
\frac{\theta}{\theta + 1} = \bar{X}
\]</span></p>
<p>Solving for <span class="math inline">\(\theta\)</span>, we get:</p>
<p><span class="math display">\[
\theta = \bar{X}(\theta + 1)
\]</span> <span class="math display">\[
\theta(1 - \bar{X}) = \bar{X}
\]</span> <span class="math display">\[
\hat{\theta} = \frac{\bar{X}}{1 - \bar{X}}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \frac{\bar{X}}{1 - \bar{X}}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator is based on the idea of equating the sample moments (like the sample mean) to the corresponding population moments (like the population mean) and then solving for the unknown parameter. In this case, we equate the first population moment to the first sample moment to find an estimator for <span class="math inline">\(\theta\)</span>. This is a direct application of the method of moments as defined in the text in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution2">
<h3 class="anchored" data-anchor-id="sec-ch11solution2">Solution 2</h3>
<p><a href="#sec-ch11exercise2">Exercise 2</a></p>
<p>The likelihood function is given by the product of the pdfs for each observation:</p>
<p><span class="math display">\[
L(\theta|X^n) = \prod_{i=1}^n f(X_i|\theta) = \prod_{i=1}^n \frac{1}{\theta} e^{-X_i/\theta} = \frac{1}{\theta^n} e^{-\sum_{i=1}^n X_i/\theta}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(\theta|X^n) = \log L(\theta|X^n) = -n \log \theta - \frac{1}{\theta} \sum_{i=1}^n X_i
\]</span></p>
<p>To find the maximum likelihood estimator, we take the derivative of the log-likelihood with respect to <span class="math inline">\(\theta\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \theta} = -\frac{n}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^n X_i = 0
\]</span></p>
<p>Solving for <span class="math inline">\(\theta\)</span>, we get:</p>
<p><span class="math display">\[
\frac{n}{\theta} = \frac{1}{\theta^2} \sum_{i=1}^n X_i
\]</span> <span class="math display">\[
\hat{\theta} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}
\]</span></p>
<p>Thus, the maximum likelihood estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \bar{X}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimator is the value of the parameter that maximizes the likelihood function, which represents the probability of observing the given sample. By taking the logarithm of the likelihood function and finding the value of <span class="math inline">\(\theta\)</span> that maximizes it, we are finding the parameter value that makes the observed data most probable. This process directly follows the definition and procedure for maximum likelihood estimation as described in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution3">
<h3 class="anchored" data-anchor-id="sec-ch11solution3">Solution 3</h3>
<p><a href="#sec-ch11exercise3">Exercise 3</a></p>
<p>The method of moments estimator is found by equating the sample moments to the population moments. For a Bernoulli distribution, the first population moment (mean) is:</p>
<p><span class="math display">\[
E(X) = p
\]</span></p>
<p>The first sample moment is the sample mean:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Equating the population moment to the sample moment, we have:</p>
<p><span class="math display">\[
p = \bar{X}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(p\)</span> is <span class="math inline">\(\hat{p} = \bar{X}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator for the Bernoulli distribution is straightforward. We equate the population mean, which is <span class="math inline">\(p\)</span>, to the sample mean <span class="math inline">\(\bar{X}\)</span>. This directly applies the principle of the method of moments as outlined in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution4">
<h3 class="anchored" data-anchor-id="sec-ch11solution4">Solution 4</h3>
<p><a href="#sec-ch11exercise4">Exercise 4</a></p>
<p>The likelihood function for a normal distribution is given by:</p>
<p><span class="math display">\[
L(\mu, \sigma^2|X^n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(X_i-\mu)^2}{2\sigma^2}} = (2\pi\sigma^2)^{-n/2} e^{-\frac{1}{2\sigma^2} \sum_{i=1}^n (X_i-\mu)^2}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(\mu, \sigma^2|X^n) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n (X_i-\mu)^2
\]</span></p>
<p>To find the MLE for <span class="math inline">\(\mu\)</span>, we take the derivative with respect to <span class="math inline">\(\mu\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \mu} = \frac{1}{\sigma^2} \sum_{i=1}^n (X_i - \mu) = 0
\]</span> <span class="math display">\[
\sum_{i=1}^n X_i - n\mu = 0
\]</span> <span class="math display">\[
\hat{\mu} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}
\]</span></p>
<p>To find the MLE for <span class="math inline">\(\sigma^2\)</span>, we take the derivative with respect to <span class="math inline">\(\sigma^2\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (X_i-\mu)^2 = 0
\]</span> <span class="math display">\[
\frac{n}{2\sigma^2} = \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n (X_i-\mu)^2
\]</span> <span class="math display">\[
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i-\hat{\mu})^2
\]</span></p>
<p>Thus, the MLE for <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\hat{\mu} = \bar{X}\)</span> and for <span class="math inline">\(\sigma^2\)</span> is <span class="math inline">\(\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i-\bar{X})^2\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimators for the normal distribution are found by maximizing the likelihood function with respect to the parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>. We take the derivatives of the log-likelihood function and set them to zero to find the values that maximize the likelihood. This process is consistent with the principles of maximum likelihood estimation as defined in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution5">
<h3 class="anchored" data-anchor-id="sec-ch11solution5">Solution 5</h3>
<p><a href="#sec-ch11exercise5">Exercise 5</a></p>
<p>For a uniform distribution on <span class="math inline">\([0, \theta]\)</span>, the first population moment (mean) is:</p>
<p><span class="math display">\[
E(X) = \frac{\theta}{2}
\]</span></p>
<p>The first sample moment is the sample mean:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Equating the population moment to the sample moment, we have:</p>
<p><span class="math display">\[
\frac{\theta}{2} = \bar{X}
\]</span></p>
<p>Solving for <span class="math inline">\(\theta\)</span>, we get:</p>
<p><span class="math display">\[
\hat{\theta} = 2\bar{X}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = 2\bar{X}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator for the uniform distribution is found by equating the population mean to the sample mean. We solve for <span class="math inline">\(\theta\)</span> to find the estimator. This follows the definition of the method of moments from Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution6">
<h3 class="anchored" data-anchor-id="sec-ch11solution6">Solution 6</h3>
<p><a href="#sec-ch11exercise6">Exercise 6</a></p>
<p>The likelihood function for a Poisson distribution is given by:</p>
<p><span class="math display">\[
L(\lambda|X^n) = \prod_{i=1}^n \frac{e^{-\lambda}\lambda^{X_i}}{X_i!} = e^{-n\lambda} \lambda^{\sum_{i=1}^n X_i} \prod_{i=1}^n \frac{1}{X_i!}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(\lambda|X^n) = -n\lambda + \left(\sum_{i=1}^n X_i\right) \log \lambda - \sum_{i=1}^n \log(X_i!)
\]</span></p>
<p>To find the MLE for <span class="math inline">\(\lambda\)</span>, we take the derivative with respect to <span class="math inline">\(\lambda\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \lambda} = -n + \frac{1}{\lambda} \sum_{i=1}^n X_i = 0
\]</span> <span class="math display">\[
n = \frac{1}{\lambda} \sum_{i=1}^n X_i
\]</span> <span class="math display">\[
\hat{\lambda} = \frac{1}{n} \sum_{i=1}^n X_i = \bar{X}
\]</span></p>
<p>Thus, the maximum likelihood estimator for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\hat{\lambda} = \bar{X}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimator for the Poisson distribution is found by maximizing the likelihood function with respect to <span class="math inline">\(\lambda\)</span>. We take the derivative of the log-likelihood function and set it to zero to find the value of <span class="math inline">\(\lambda\)</span> that maximizes the likelihood. This is in line with the principles of maximum likelihood estimation as described in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution7">
<h3 class="anchored" data-anchor-id="sec-ch11solution7">Solution 7</h3>
<p><a href="#sec-ch11exercise7">Exercise 7</a></p>
<p>The first population moment (mean) for the given CDF is:</p>
<p><span class="math display">\[
E(X) = \int_0^\infty x \frac{1}{\theta} e^{-x/\theta} dx = \theta
\]</span></p>
<p>The first sample moment is the sample mean:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Equating the population moment to the sample moment, we have:</p>
<p><span class="math display">\[
\theta = \bar{X}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \bar{X}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator is found by equating the population mean to the sample mean. Since the population mean is <span class="math inline">\(\theta\)</span>, the estimator is simply the sample mean <span class="math inline">\(\bar{X}\)</span>. This follows the definition and principles of the method of moments as described in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution8">
<h3 class="anchored" data-anchor-id="sec-ch11solution8">Solution 8</h3>
<p><a href="#sec-ch11exercise8">Exercise 8</a></p>
<p>The likelihood function for a geometric distribution is given by:</p>
<p><span class="math display">\[
L(p|X^n) = \prod_{i=1}^n (1-p)^{X_i-1} p = p^n (1-p)^{\sum_{i=1}^n (X_i-1)}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(p|X^n) = n \log p + \left(\sum_{i=1}^n (X_i-1)\right) \log(1-p)
\]</span></p>
<p>To find the MLE for <span class="math inline">\(p\)</span>, we take the derivative with respect to <span class="math inline">\(p\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial p} = \frac{n}{p} - \frac{\sum_{i=1}^n (X_i-1)}{1-p} = 0
\]</span> <span class="math display">\[
\frac{n}{p} = \frac{\sum_{i=1}^n X_i - n}{1-p}
\]</span> <span class="math display">\[
n(1-p) = p\left(\sum_{i=1}^n X_i - n\right)
\]</span> <span class="math display">\[
n - np = p\sum_{i=1}^n X_i - np
\]</span> <span class="math display">\[
n = p\sum_{i=1}^n X_i
\]</span> <span class="math display">\[
\hat{p} = \frac{n}{\sum_{i=1}^n X_i} = \frac{1}{\bar{X}}
\]</span></p>
<p>Thus, the maximum likelihood estimator for <span class="math inline">\(p\)</span> is <span class="math inline">\(\hat{p} = \frac{1}{\bar{X}}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimator for the geometric distribution is found by maximizing the likelihood function with respect to <span class="math inline">\(p\)</span>. We take the derivative of the log-likelihood function and set it to zero to find the value of <span class="math inline">\(p\)</span> that maximizes the likelihood. This follows the principles of maximum likelihood estimation as defined in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution9">
<h3 class="anchored" data-anchor-id="sec-ch11solution9">Solution 9</h3>
<p><a href="#sec-ch11exercise9">Exercise 9</a></p>
<p>The first population moment (mean) is:</p>
<p><span class="math display">\[
E(X) = \int_0^\theta x \frac{2x}{\theta^2} dx = \frac{2}{\theta^2} \int_0^\theta x^2 dx = \frac{2}{\theta^2} \cdot \frac{\theta^3}{3} = \frac{2\theta}{3}
\]</span></p>
<p>The first sample moment is the sample mean:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Equating the population moment to the sample moment, we have:</p>
<p><span class="math display">\[
\frac{2\theta}{3} = \bar{X}
\]</span></p>
<p>Solving for <span class="math inline">\(\theta\)</span>, we get:</p>
<p><span class="math display">\[
\hat{\theta} = \frac{3}{2}\bar{X}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \frac{3}{2}\bar{X}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator is found by equating the population mean to the sample mean. We solve for <span class="math inline">\(\theta\)</span> to find the estimator. This is a direct application of the method of moments as defined in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution10">
<h3 class="anchored" data-anchor-id="sec-ch11solution10">Solution 10</h3>
<p><a href="#sec-ch11exercise10">Exercise 10</a></p>
<p>The likelihood function for an exponential distribution is given by:</p>
<p><span class="math display">\[
L(\lambda|X^n) = \prod_{i=1}^n \lambda e^{-\lambda X_i} = \lambda^n e^{-\lambda \sum_{i=1}^n X_i}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(\lambda|X^n) = n \log \lambda - \lambda \sum_{i=1}^n X_i
\]</span></p>
<p>To find the MLE for <span class="math inline">\(\lambda\)</span>, we take the derivative with respect to <span class="math inline">\(\lambda\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \lambda} = \frac{n}{\lambda} - \sum_{i=1}^n X_i = 0
\]</span> <span class="math display">\[
\frac{n}{\lambda} = \sum_{i=1}^n X_i
\]</span> <span class="math display">\[
\hat{\lambda} = \frac{n}{\sum_{i=1}^n X_i} = \frac{1}{\bar{X}}
\]</span></p>
<p>Thus, the maximum likelihood estimator for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\hat{\lambda} = \frac{1}{\bar{X}}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimator for the exponential distribution is found by maximizing the likelihood function with respect to <span class="math inline">\(\lambda\)</span>. We take the derivative of the log-likelihood function and set it to zero to find the value of <span class="math inline">\(\lambda\)</span> that maximizes the likelihood. This is in line with the principles of maximum likelihood estimation as described in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution11">
<h3 class="anchored" data-anchor-id="sec-ch11solution11">Solution 11</h3>
<p><a href="#sec-ch11exercise11">Exercise 11</a></p>
<p>The first population moment (mean) for the Pareto distribution is:</p>
<p><span class="math display">\[
E(X) = \int_1^\infty x \alpha x^{-(\alpha+1)} dx = \frac{\alpha}{\alpha - 1}, \quad \text{for } \alpha &gt; 1
\]</span></p>
<p>The first sample moment is the sample mean:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Equating the population moment to the sample moment, we have:</p>
<p><span class="math display">\[
\frac{\alpha}{\alpha - 1} = \bar{X}
\]</span></p>
<p>Solving for <span class="math inline">\(\alpha\)</span>, we get:</p>
<p><span class="math display">\[
\alpha = \bar{X}(\alpha - 1)
\]</span> <span class="math display">\[
\alpha(1 - \bar{X}) = -\bar{X}
\]</span> <span class="math display">\[
\hat{\alpha} = \frac{\bar{X}}{\bar{X} - 1}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(\alpha\)</span> is <span class="math inline">\(\hat{\alpha} = \frac{\bar{X}}{\bar{X} - 1}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator for the Pareto distribution is found by equating the population mean to the sample mean. We solve for <span class="math inline">\(\alpha\)</span> to find the estimator. This follows the definition and principles of the method of moments as described in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution12">
<h3 class="anchored" data-anchor-id="sec-ch11solution12">Solution 12</h3>
<p><a href="#sec-ch11exercise12">Exercise 12</a></p>
<p>The likelihood function for a normal distribution with mean 0 and variance <span class="math inline">\(\sigma^2\)</span> is given by:</p>
<p><span class="math display">\[
L(\sigma^2|X^n) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{X_i^2}{2\sigma^2}} = (2\pi\sigma^2)^{-n/2} e^{-\frac{1}{2\sigma^2} \sum_{i=1}^n X_i^2}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(\sigma^2|X^n) = -\frac{n}{2} \log(2\pi) - \frac{n}{2} \log(\sigma^2) - \frac{1}{2\sigma^2} \sum_{i=1}^n X_i^2
\]</span></p>
<p>To find the MLE for <span class="math inline">\(\sigma^2\)</span>, we take the derivative with respect to <span class="math inline">\(\sigma^2\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \sigma^2} = -\frac{n}{2\sigma^2} + \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n X_i^2 = 0
\]</span> <span class="math display">\[
\frac{n}{2\sigma^2} = \frac{1}{2(\sigma^2)^2} \sum_{i=1}^n X_i^2
\]</span> <span class="math display">\[
\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n X_i^2
\]</span></p>
<p>Thus, the maximum likelihood estimator for <span class="math inline">\(\sigma^2\)</span> is <span class="math inline">\(\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n X_i^2\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimator for the variance <span class="math inline">\(\sigma^2\)</span> in a normal distribution with mean 0 is found by maximizing the likelihood function with respect to <span class="math inline">\(\sigma^2\)</span>. We take the derivative of the log-likelihood function and set it to zero to find the value of <span class="math inline">\(\sigma^2\)</span> that maximizes the likelihood. This process aligns with the principles of maximum likelihood estimation as outlined in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution13">
<h3 class="anchored" data-anchor-id="sec-ch11solution13">Solution 13</h3>
<p><a href="#sec-ch11exercise13">Exercise 13</a></p>
<p>The second population moment is:</p>
<p><span class="math display">\[
E(X^2) = \int_{-\theta}^\theta x^2 \frac{1}{2\theta} dx = \frac{1}{2\theta} \int_{-\theta}^\theta x^2 dx = \frac{1}{2\theta} \cdot \frac{2\theta^3}{3} = \frac{\theta^2}{3}
\]</span></p>
<p>The second sample moment is:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^n X_i^2
\]</span></p>
<p>Equating the second population moment to the second sample moment, we have:</p>
<p><span class="math display">\[
\frac{\theta^2}{3} = \frac{1}{n} \sum_{i=1}^n X_i^2
\]</span></p>
<p>Solving for <span class="math inline">\(\theta\)</span>, we get:</p>
<p><span class="math display">\[
\hat{\theta} = \sqrt{\frac{3}{n} \sum_{i=1}^n X_i^2}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \sqrt{\frac{3}{n} \sum_{i=1}^n X_i^2}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator in this case is found by equating the second population moment to the second sample moment. We solve for <span class="math inline">\(\theta\)</span> to find the estimator. This follows the definition and principles of the method of moments as described in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution14">
<h3 class="anchored" data-anchor-id="sec-ch11solution14">Solution 14</h3>
<p><a href="#sec-ch11exercise14">Exercise 14</a></p>
<p>For a beta distribution, the first two population moments are:</p>
<p><span class="math display">\[
E(X) = \frac{\alpha}{\alpha + \beta}
\]</span> <span class="math display">\[
E(X^2) = \frac{\alpha(\alpha+1)}{(\alpha+\beta)(\alpha+\beta+1)}
\]</span></p>
<p>The first two sample moments are:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span> <span class="math display">\[
\overline{X^2} = \frac{1}{n} \sum_{i=1}^n X_i^2
\]</span></p>
<p>Equating the population moments to the sample moments, we have:</p>
<p><span class="math display">\[
\frac{\alpha}{\alpha + \beta} = \bar{X} \qquad (1)
\]</span> <span class="math display">\[
\frac{\alpha(\alpha+1)}{(\alpha+\beta)(\alpha+\beta+1)} = \overline{X^2} \qquad (2)
\]</span></p>
<p>From (1), we get:</p>
<p><span class="math display">\[
\alpha = \bar{X}(\alpha + \beta)
\]</span> <span class="math display">\[
\beta = \frac{\alpha(1-\bar{X})}{\bar{X}}
\]</span></p>
<p>Substituting <span class="math inline">\(\beta\)</span> in (2), we get:</p>
<p><span class="math display">\[
\frac{\alpha(\alpha+1)}{(\alpha+\frac{\alpha(1-\bar{X})}{\bar{X}})(\alpha+\frac{\alpha(1-\bar{X})}{\bar{X}}+1)} = \overline{X^2}
\]</span> <span class="math display">\[
\frac{\alpha(\alpha+1)}{\frac{\alpha}{\bar{X}}(\frac{\alpha}{\bar{X}}+1)} = \overline{X^2}
\]</span> <span class="math display">\[
\frac{\bar{X}^2(\alpha+1)}{\alpha+\bar{X}} = \overline{X^2}
\]</span> <span class="math display">\[
\bar{X}^2(\alpha+1) = \overline{X^2}(\alpha+\bar{X})
\]</span> <span class="math display">\[
\alpha(\bar{X}^2-\overline{X^2}) = \overline{X^2}\bar{X}-\bar{X}^2
\]</span> <span class="math display">\[
\hat{\alpha} = \frac{\bar{X}(\overline{X^2}-\bar{X})}{\bar{X}^2-\overline{X^2}}
\]</span></p>
<p>Substituting <span class="math inline">\(\hat{\alpha}\)</span> back into the equation for <span class="math inline">\(\beta\)</span>, we get:</p>
<p><span class="math display">\[
\hat{\beta} = \frac{\hat{\alpha}(1-\bar{X})}{\bar{X}}
\]</span></p>
<p>Thus, the method of moments estimators for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> are <span class="math inline">\(\hat{\alpha} = \frac{\bar{X}(\overline{X^2}-\bar{X})}{\bar{X}^2-\overline{X^2}}\)</span> and <span class="math inline">\(\hat{\beta} = \frac{\hat{\alpha}(1-\bar{X})}{\bar{X}}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimators for the beta distribution are found by equating the first two population moments to the corresponding sample moments. We solve the resulting system of equations to find the estimators for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. This is a direct application of the method of moments as defined in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution15">
<h3 class="anchored" data-anchor-id="sec-ch11solution15">Solution 15</h3>
<p><a href="#sec-ch11exercise15">Exercise 15</a></p>
<p>The likelihood function for a gamma distribution with known <span class="math inline">\(k\)</span> is given by:</p>
<p><span class="math display">\[
L(\theta|X^n) = \prod_{i=1}^n \frac{1}{\Gamma(k)\theta^k} X_i^{k-1} e^{-X_i/\theta} = \frac{1}{(\Gamma(k))^n \theta^{nk}} \left(\prod_{i=1}^n X_i\right)^{k-1} e^{-\sum_{i=1}^n X_i/\theta}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(\theta|X^n) = -n \log(\Gamma(k)) - nk \log(\theta) + (k-1) \sum_{i=1}^n \log(X_i) - \frac{1}{\theta} \sum_{i=1}^n X_i
\]</span></p>
<p>To find the MLE for <span class="math inline">\(\theta\)</span>, we take the derivative with respect to <span class="math inline">\(\theta\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \theta} = -\frac{nk}{\theta} + \frac{1}{\theta^2} \sum_{i=1}^n X_i = 0
\]</span> <span class="math display">\[
\frac{nk}{\theta} = \frac{1}{\theta^2} \sum_{i=1}^n X_i
\]</span> <span class="math display">\[
\hat{\theta} = \frac{1}{nk} \sum_{i=1}^n X_i = \frac{\bar{X}}{k}
\]</span></p>
<p>Thus, the maximum likelihood estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \frac{\bar{X}}{k}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimator for the scale parameter <span class="math inline">\(\theta\)</span> in a gamma distribution with known shape parameter <span class="math inline">\(k\)</span> is found by maximizing the likelihood function with respect to <span class="math inline">\(\theta\)</span>. We take the derivative of the log-likelihood function and set it to zero to find the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood. This process aligns with the principles of maximum likelihood estimation as outlined in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution16">
<h3 class="anchored" data-anchor-id="sec-ch11solution16">Solution 16</h3>
<p><a href="#sec-ch11exercise16">Exercise 16</a></p>
<p>The first population moment (mean) is:</p>
<p><span class="math display">\[
E(X) = \int_0^\theta x \frac{\alpha}{\theta} \left(\frac{x}{\theta}\right)^{\alpha-1} dx = \frac{\alpha}{\theta^\alpha} \int_0^\theta x^\alpha dx = \frac{\alpha}{\theta^\alpha} \cdot \frac{\theta^{\alpha+1}}{\alpha+1} = \frac{\alpha \theta}{\alpha+1}
\]</span></p>
<p>The first sample moment is the sample mean:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Equating the population moment to the sample moment, we have:</p>
<p><span class="math display">\[
\frac{\alpha \theta}{\alpha+1} = \bar{X}
\]</span></p>
<p>Solving for <span class="math inline">\(\theta\)</span>, we get:</p>
<p><span class="math display">\[
\hat{\theta} = \frac{\alpha+1}{\alpha} \bar{X}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = \frac{\alpha+1}{\alpha} \bar{X}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator is found by equating the population mean to the sample mean. We solve for <span class="math inline">\(\theta\)</span> to find the estimator, given that <span class="math inline">\(\alpha\)</span> is known. This follows the definition and principles of the method of moments as described in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution17">
<h3 class="anchored" data-anchor-id="sec-ch11solution17">Solution 17</h3>
<p><a href="#sec-ch11exercise17">Exercise 17</a></p>
<p>The likelihood function is given by:</p>
<p><span class="math display">\[
L(\theta|X^n) = \prod_{i=1}^n \frac{1}{\theta} X_i^{(1-\theta)/\theta} = \frac{1}{\theta^n} \left(\prod_{i=1}^n X_i\right)^{(1-\theta)/\theta}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(\theta|X^n) = -n \log \theta + \frac{1-\theta}{\theta} \sum_{i=1}^n \log X_i
\]</span></p>
<p>To find the MLE for <span class="math inline">\(\theta\)</span>, we take the derivative with respect to <span class="math inline">\(\theta\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \theta} = -\frac{n}{\theta} + \frac{-\theta - (1-\theta)}{\theta^2} \sum_{i=1}^n \log X_i = -\frac{n}{\theta} - \frac{1}{\theta^2} \sum_{i=1}^n \log X_i = 0
\]</span> <span class="math display">\[
\frac{n}{\theta} = -\frac{1}{\theta^2} \sum_{i=1}^n \log X_i
\]</span> <span class="math display">\[
\hat{\theta} = -\frac{1}{n} \sum_{i=1}^n \log X_i
\]</span></p>
<p>Thus, the maximum likelihood estimator for <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat{\theta} = -\frac{1}{n} \sum_{i=1}^n \log X_i\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimator for <span class="math inline">\(\theta\)</span> is found by maximizing the likelihood function with respect to <span class="math inline">\(\theta\)</span>. We take the derivative of the log-likelihood function and set it to zero to find the value of <span class="math inline">\(\theta\)</span> that maximizes the likelihood. This is in line with the principles of maximum likelihood estimation as described in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution18">
<h3 class="anchored" data-anchor-id="sec-ch11solution18">Solution 18</h3>
<p><a href="#sec-ch11exercise18">Exercise 18</a></p>
<p>The second central moment (variance) for the Laplace distribution is:</p>
<p><span class="math display">\[
\text{Var}(X) = 2b^2
\]</span></p>
<p>When <span class="math inline">\(\mu\)</span> is known, the second sample central moment is:</p>
<p><span class="math display">\[
\frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2
\]</span></p>
<p>Equating the population moment to the sample moment, we have:</p>
<p><span class="math display">\[
2b^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2
\]</span></p>
<p>Solving for <span class="math inline">\(b\)</span>, we get:</p>
<p><span class="math display">\[
\hat{b} = \sqrt{\frac{1}{2n} \sum_{i=1}^n (X_i - \mu)^2}
\]</span></p>
<p>Thus, the method of moments estimator for <span class="math inline">\(b\)</span> is <span class="math inline">\(\hat{b} = \sqrt{\frac{1}{2n} \sum_{i=1}^n (X_i - \mu)^2}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The method of moments estimator in this case is found by equating the second population central moment (variance) to the second sample central moment. We solve for <span class="math inline">\(b\)</span> to find the estimator. This follows the definition and principles of the method of moments as described in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11solution19">
<h3 class="anchored" data-anchor-id="sec-ch11solution19">Solution 19</h3>
<p><a href="#sec-ch11exercise19">Exercise 19</a></p>
<p>The likelihood function for a Weibull distribution with known <span class="math inline">\(k\)</span> is given by:</p>
<p><span class="math display">\[
L(\lambda|X^n) = \prod_{i=1}^n \frac{k}{\lambda} \left(\frac{X_i}{\lambda}\right)^{k-1} e^{-(X_i/\lambda)^k} = \frac{k^n}{\lambda^{nk}} \left(\prod_{i=1}^n X_i\right)^{k-1} e^{-\sum_{i=1}^n (X_i/\lambda)^k}
\]</span></p>
<p>The log-likelihood function is:</p>
<p><span class="math display">\[
l(\lambda|X^n) = n \log k - nk \log \lambda + (k-1) \sum_{i=1}^n \log X_i - \frac{1}{\lambda^k} \sum_{i=1}^n X_i^k
\]</span></p>
<p>To find the MLE for <span class="math inline">\(\lambda\)</span>, we take the derivative with respect to <span class="math inline">\(\lambda\)</span> and set it to zero:</p>
<p><span class="math display">\[
\frac{\partial l}{\partial \lambda} = -\frac{nk}{\lambda} + \frac{k}{\lambda^{k+1}} \sum_{i=1}^n X_i^k = 0
\]</span> <span class="math display">\[
\frac{nk}{\lambda} = \frac{k}{\lambda^{k+1}} \sum_{i=1}^n X_i^k
\]</span> <span class="math display">\[
\hat{\lambda}^k = \frac{1}{n} \sum_{i=1}^n X_i^k
\]</span> <span class="math display">\[
\hat{\lambda} = \left(\frac{1}{n} \sum_{i=1}^n X_i^k\right)^{1/k}
\]</span></p>
<p>Thus, the maximum likelihood estimator for <span class="math inline">\(\lambda\)</span> is <span class="math inline">\(\hat{\lambda} = \left(\frac{1}{n} \sum_{i=1}^n X_i^k\right)^{1/k}\)</span>.</p>
<p><strong>Intuitive explanation:</strong> The maximum likelihood estimator for the scale parameter <span class="math inline">\(\lambda\)</span> in a Weibull distribution with known shape parameter <span class="math inline">\(k\)</span> is found by maximizing the likelihood function with respect to <span class="math inline">\(\lambda\)</span>. We take the derivative of the log-likelihood function and set it to zero to find the value of <span class="math inline">\(\lambda\)</span> that maximizes the likelihood. This process aligns with the principles of maximum likelihood estimation as outlined in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11solution20">
<h3 class="anchored" data-anchor-id="sec-ch11solution20">Solution 20</h3>
<p><a href="#sec-ch11exercise20">Exercise 20</a></p>
<p>For a Cauchy distribution with location parameter <span class="math inline">\(\theta\)</span>, the probability density function (pdf) is given by:</p>
<p><span class="math display">\[
f(x|\theta) = \frac{1}{\pi(1+(x-\theta)^2)}
\]</span></p>
<p>The Cauchy distribution has undefined mean and variance. The sample mean is given by:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i
\]</span></p>
<p>Since the Cauchy distribution does not have a defined mean, the sample mean does not converge to a specific value as the sample size increases. In other words, the sample mean is not a consistent estimator for the location parameter <span class="math inline">\(\theta\)</span> of the Cauchy distribution. This is because the sample mean does not satisfy the conditions for consistency, as it does not converge in probability to the true parameter value. The distribution of the sample mean from a Cauchy distribution is also a Cauchy distribution with the same location parameter. Thus, the sample mean does not provide any additional information about the location parameter than any single observation.</p>
<p><strong>Intuitive explanation:</strong> The Cauchy distribution is a special case where the sample mean does not serve as a consistent estimator for the location parameter. This is due to the undefined mean and variance of the Cauchy distribution. The sample mean does not converge to the true parameter value as the sample size increases, violating the conditions for consistency as described in Section 11.2.1. The text explains in Section 11.3 that the sample mean can be interpreted as the MLE for normally distributed data. However, the normal-based MLE is highly susceptible to fat-tailed distributions like the Cauchy distribution and may produce inconsistent estimators, as illustrated by this exercise.</p>
</section>
</section>
<section class="level2" id="r-scripts">
<h2 class="anchored" data-anchor-id="r-scripts">R Scripts</h2>
<section class="level3" id="r-script-1-method-of-moments-estimation-for-gamma-distribution">
<h3 class="anchored" data-anchor-id="r-script-1-method-of-moments-estimation-for-gamma-distribution">R Script 1: Method of Moments Estimation for Gamma Distribution</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-3"><a aria-hidden="true" href="#cb1-3" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:stats':

    filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a aria-hidden="true" href="#cb5-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb5-2"><a aria-hidden="true" href="#cb5-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb5-3"><a aria-hidden="true" href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a aria-hidden="true" href="#cb5-4" tabindex="-1"></a><span class="co"># Generate sample data from a Gamma distribution</span></span>
<span id="cb5-5"><a aria-hidden="true" href="#cb5-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb5-6"><a aria-hidden="true" href="#cb5-6" tabindex="-1"></a>alpha <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb5-7"><a aria-hidden="true" href="#cb5-7" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="dv">3</span></span>
<span id="cb5-8"><a aria-hidden="true" href="#cb5-8" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="fu">rgamma</span>(n, <span class="at">shape =</span> alpha, <span class="at">rate =</span> beta)</span>
<span id="cb5-9"><a aria-hidden="true" href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a aria-hidden="true" href="#cb5-10" tabindex="-1"></a><span class="co"># Calculate sample mean and sample variance</span></span>
<span id="cb5-11"><a aria-hidden="true" href="#cb5-11" tabindex="-1"></a>sample_mean <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample_data)</span>
<span id="cb5-12"><a aria-hidden="true" href="#cb5-12" tabindex="-1"></a>sample_var <span class="ot">&lt;-</span> <span class="fu">var</span>(sample_data)</span>
<span id="cb5-13"><a aria-hidden="true" href="#cb5-13" tabindex="-1"></a></span>
<span id="cb5-14"><a aria-hidden="true" href="#cb5-14" tabindex="-1"></a><span class="co"># Estimate parameters using the method of moments</span></span>
<span id="cb5-15"><a aria-hidden="true" href="#cb5-15" tabindex="-1"></a>alpha_hat <span class="ot">&lt;-</span> sample_mean<span class="sc">^</span><span class="dv">2</span> <span class="sc">/</span> sample_var</span>
<span id="cb5-16"><a aria-hidden="true" href="#cb5-16" tabindex="-1"></a>beta_hat <span class="ot">&lt;-</span> sample_mean <span class="sc">/</span> sample_var</span>
<span id="cb5-17"><a aria-hidden="true" href="#cb5-17" tabindex="-1"></a></span>
<span id="cb5-18"><a aria-hidden="true" href="#cb5-18" tabindex="-1"></a><span class="co"># Print estimated parameters</span></span>
<span id="cb5-19"><a aria-hidden="true" href="#cb5-19" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Estimated alpha:"</span>, alpha_hat, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated alpha: 2.113471 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a aria-hidden="true" href="#cb7-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Estimated beta:"</span>, beta_hat, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated beta: 3.372622 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a aria-hidden="true" href="#cb9-1" tabindex="-1"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb9-2"><a aria-hidden="true" href="#cb9-2" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> sample_data)</span>
<span id="cb9-3"><a aria-hidden="true" href="#cb9-3" tabindex="-1"></a></span>
<span id="cb9-4"><a aria-hidden="true" href="#cb9-4" tabindex="-1"></a><span class="co"># Plot the histogram of the sample data</span></span>
<span id="cb9-5"><a aria-hidden="true" href="#cb9-5" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb9-6"><a aria-hidden="true" href="#cb9-6" tabindex="-1"></a><span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">bins =</span> <span class="dv">30</span>, <span class="at">fill =</span> <span class="st">"blue"</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb9-7"><a aria-hidden="true" href="#cb9-7" tabindex="-1"></a><span class="fu">stat_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fu">dgamma</span>(x, <span class="at">shape =</span> alpha_hat, <span class="at">rate =</span> beta_hat), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb9-8"><a aria-hidden="true" href="#cb9-8" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Gamma Distribution and MoM Estimates"</span>,</span>
<span id="cb9-9"><a aria-hidden="true" href="#cb9-9" tabindex="-1"></a><span class="at">x =</span> <span class="st">"X"</span>,</span>
<span id="cb9-10"><a aria-hidden="true" href="#cb9-10" tabindex="-1"></a><span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb9-11"><a aria-hidden="true" href="#cb9-11" tabindex="-1"></a><span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap11_files/figure-html/unnamed-chunk-1-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<p>This script demonstrates the <strong>method of moments</strong> estimation for a Gamma distribution.</p>
<ol type="1">
<li><strong>Load Libraries and Set Seed</strong>: We load the necessary libraries (<code>ggplot2</code> for plotting and <code>dplyr</code> for data manipulation) and set a seed for reproducibility of the random sample.</li>
<li><strong>Generate Sample Data</strong>: We generate a random sample of size <span class="math inline">\(n = 1000\)</span> from a Gamma distribution with true parameters <span class="math inline">\(\alpha = 2\)</span> and <span class="math inline">\(\beta = 3\)</span>. The <code>rgamma</code> function is used for this purpose. The text mentions the Gamma distribution in Example 11.3.</li>
<li><strong>Calculate Sample Moments</strong>: We calculate the sample mean and sample variance using the <code>mean</code> and <code>var</code> functions, respectively. The text defines the sample mean <span class="math inline">\(\bar{X}\)</span> and sample variance <span class="math inline">\(s^2\)</span> as estimators of the population mean <span class="math inline">\(E(X)\)</span> and population variance <span class="math inline">\(\text{var}(X)\)</span>.</li>
<li><strong>Estimate Parameters</strong>: We use the method of moments to estimate the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. From Example 11.3 in the text, we have the following relationships:</li>
</ol>
<p><span class="math display">\[
E(X) = \alpha \beta
\]</span> <span class="math display">\[
\text{var}(X) = \alpha \beta^2
\]</span></p>
<p>Solving these for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>, we get</p>
<p><span class="math display">\[
\alpha = \frac{E(X)^2}{\text{var}(X)}; \quad \beta = \frac{E(X)}{\alpha}
\]</span> We replace the population mean and variance with the sample mean and sample variance to obtain the estimators:</p>
<p><span class="math display">\[
\hat{\alpha} = \frac{\bar{X}^2}{s^2}; \quad \hat{\beta} = \frac{\bar{X}}{\hat{\alpha}}
\]</span> 5. <strong>Plot Data and Estimated Distribution</strong>: We create a data frame with the sample data and use <code>ggplot2</code> to plot a histogram of the sample data. We overlay the probability density function (pdf) of the Gamma distribution with the estimated parameters <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> using <code>stat_function</code>. This visualizes how well the estimated distribution fits the sample data.</p>
</section>
<section class="level3" id="r-script-2-maximum-likelihood-estimation-for-exponential-distribution">
<h3 class="anchored" data-anchor-id="r-script-2-maximum-likelihood-estimation-for-exponential-distribution">R Script 2: Maximum Likelihood Estimation for Exponential Distribution</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a aria-hidden="true" href="#cb10-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb10-2"><a aria-hidden="true" href="#cb10-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb10-3"><a aria-hidden="true" href="#cb10-3" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb10-4"><a aria-hidden="true" href="#cb10-4" tabindex="-1"></a></span>
<span id="cb10-5"><a aria-hidden="true" href="#cb10-5" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb10-6"><a aria-hidden="true" href="#cb10-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb10-7"><a aria-hidden="true" href="#cb10-7" tabindex="-1"></a></span>
<span id="cb10-8"><a aria-hidden="true" href="#cb10-8" tabindex="-1"></a><span class="co"># Generate sample data from an exponential distribution</span></span>
<span id="cb10-9"><a aria-hidden="true" href="#cb10-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb10-10"><a aria-hidden="true" href="#cb10-10" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb10-11"><a aria-hidden="true" href="#cb10-11" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="fu">rexp</span>(n, <span class="at">rate =</span> lambda)</span>
<span id="cb10-12"><a aria-hidden="true" href="#cb10-12" tabindex="-1"></a></span>
<span id="cb10-13"><a aria-hidden="true" href="#cb10-13" tabindex="-1"></a><span class="co"># Calculate the maximum likelihood estimator for lambda</span></span>
<span id="cb10-14"><a aria-hidden="true" href="#cb10-14" tabindex="-1"></a>lambda_hat <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> <span class="fu">mean</span>(sample_data)</span>
<span id="cb10-15"><a aria-hidden="true" href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a aria-hidden="true" href="#cb10-16" tabindex="-1"></a><span class="co"># Print estimated parameter</span></span>
<span id="cb10-17"><a aria-hidden="true" href="#cb10-17" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Estimated lambda:"</span>, lambda_hat, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Estimated lambda: 2.095246 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a aria-hidden="true" href="#cb12-1" tabindex="-1"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb12-2"><a aria-hidden="true" href="#cb12-2" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x =</span> sample_data)</span>
<span id="cb12-3"><a aria-hidden="true" href="#cb12-3" tabindex="-1"></a></span>
<span id="cb12-4"><a aria-hidden="true" href="#cb12-4" tabindex="-1"></a><span class="co"># Plot the histogram of the sample data</span></span>
<span id="cb12-5"><a aria-hidden="true" href="#cb12-5" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb12-6"><a aria-hidden="true" href="#cb12-6" tabindex="-1"></a><span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">bins =</span> <span class="dv">30</span>, <span class="at">fill =</span> <span class="st">"green"</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>) <span class="sc">+</span></span>
<span id="cb12-7"><a aria-hidden="true" href="#cb12-7" tabindex="-1"></a><span class="fu">stat_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fu">dexp</span>(x, <span class="at">rate =</span> lambda_hat), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb12-8"><a aria-hidden="true" href="#cb12-8" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Exponential Distribution and MLE"</span>,</span>
<span id="cb12-9"><a aria-hidden="true" href="#cb12-9" tabindex="-1"></a><span class="at">x =</span> <span class="st">"X"</span>,</span>
<span id="cb12-10"><a aria-hidden="true" href="#cb12-10" tabindex="-1"></a><span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb12-11"><a aria-hidden="true" href="#cb12-11" tabindex="-1"></a><span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap11_files/figure-html/unnamed-chunk-2-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<p>This script demonstrates <strong>maximum likelihood estimation (MLE)</strong> for an exponential distribution.</p>
<ol type="1">
<li><strong>Load Libraries and Set Seed</strong>: We load <code>ggplot2</code> and <code>dplyr</code> and set a seed for reproducibility.</li>
<li><strong>Generate Sample Data</strong>: We generate a random sample of size <span class="math inline">\(n = 1000\)</span> from an exponential distribution with true parameter <span class="math inline">\(\lambda = 2\)</span> using the <code>rexp</code> function. The text mentions the Exponential distribution in Example 11.10.</li>
<li><strong>Calculate MLE</strong>: We calculate the maximum likelihood estimator for <span class="math inline">\(\lambda\)</span> as <span class="math inline">\(\hat{\lambda} = 1 / \bar{X}\)</span>, as derived in Example 11.10 in the text.</li>
<li><strong>Plot Data and Estimated Distribution</strong>: We create a data frame and use <code>ggplot2</code> to plot a histogram of the sample data. We overlay the probability density function (pdf) of the exponential distribution with the estimated parameter <span class="math inline">\(\hat{\lambda}\)</span> using <code>stat_function</code>. This helps visualize how well the estimated distribution fits the sample data.</li>
</ol>
</section>
<section class="level3" id="r-script-3-comparison-of-sample-mean-and-median-as-estimators-of-location">
<h3 class="anchored" data-anchor-id="r-script-3-comparison-of-sample-mean-and-median-as-estimators-of-location">R Script 3: Comparison of Sample Mean and Median as Estimators of Location</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a aria-hidden="true" href="#cb13-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb13-2"><a aria-hidden="true" href="#cb13-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb13-3"><a aria-hidden="true" href="#cb13-3" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb13-4"><a aria-hidden="true" href="#cb13-4" tabindex="-1"></a></span>
<span id="cb13-5"><a aria-hidden="true" href="#cb13-5" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb13-6"><a aria-hidden="true" href="#cb13-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb13-7"><a aria-hidden="true" href="#cb13-7" tabindex="-1"></a></span>
<span id="cb13-8"><a aria-hidden="true" href="#cb13-8" tabindex="-1"></a><span class="co"># Generate sample data from a standard normal distribution</span></span>
<span id="cb13-9"><a aria-hidden="true" href="#cb13-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb13-10"><a aria-hidden="true" href="#cb13-10" tabindex="-1"></a>sample_data_normal <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb13-11"><a aria-hidden="true" href="#cb13-11" tabindex="-1"></a></span>
<span id="cb13-12"><a aria-hidden="true" href="#cb13-12" tabindex="-1"></a><span class="co"># Generate sample data from a t-distribution with 3 degrees of freedom</span></span>
<span id="cb13-13"><a aria-hidden="true" href="#cb13-13" tabindex="-1"></a>sample_data_t <span class="ot">&lt;-</span> <span class="fu">rt</span>(n, <span class="at">df =</span> <span class="dv">3</span>)</span>
<span id="cb13-14"><a aria-hidden="true" href="#cb13-14" tabindex="-1"></a></span>
<span id="cb13-15"><a aria-hidden="true" href="#cb13-15" tabindex="-1"></a><span class="co"># Calculate sample mean and sample median for both distributions</span></span>
<span id="cb13-16"><a aria-hidden="true" href="#cb13-16" tabindex="-1"></a>mean_normal <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample_data_normal)</span>
<span id="cb13-17"><a aria-hidden="true" href="#cb13-17" tabindex="-1"></a>median_normal <span class="ot">&lt;-</span> <span class="fu">median</span>(sample_data_normal)</span>
<span id="cb13-18"><a aria-hidden="true" href="#cb13-18" tabindex="-1"></a>mean_t <span class="ot">&lt;-</span> <span class="fu">mean</span>(sample_data_t)</span>
<span id="cb13-19"><a aria-hidden="true" href="#cb13-19" tabindex="-1"></a>median_t <span class="ot">&lt;-</span> <span class="fu">median</span>(sample_data_t)</span>
<span id="cb13-20"><a aria-hidden="true" href="#cb13-20" tabindex="-1"></a></span>
<span id="cb13-21"><a aria-hidden="true" href="#cb13-21" tabindex="-1"></a><span class="co"># Print results</span></span>
<span id="cb13-22"><a aria-hidden="true" href="#cb13-22" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Normal Distribution:</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Normal Distribution:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a aria-hidden="true" href="#cb15-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Sample Mean:"</span>, mean_normal, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sample Mean: -0.009969534 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a aria-hidden="true" href="#cb17-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Sample Median:"</span>, median_normal, <span class="st">"</span><span class="sc">\n\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sample Median: 0.02722263 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a aria-hidden="true" href="#cb19-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"t-Distribution:</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>t-Distribution:</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a aria-hidden="true" href="#cb21-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Sample Mean:"</span>, mean_t, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sample Mean: -0.1018703 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a aria-hidden="true" href="#cb23-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Sample Median:"</span>, median_t, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Sample Median: 0.01514069 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a aria-hidden="true" href="#cb25-1" tabindex="-1"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb25-2"><a aria-hidden="true" href="#cb25-2" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb25-3"><a aria-hidden="true" href="#cb25-3" tabindex="-1"></a><span class="at">x =</span> <span class="fu">c</span>(sample_data_normal, sample_data_t),</span>
<span id="cb25-4"><a aria-hidden="true" href="#cb25-4" tabindex="-1"></a><span class="at">Distribution =</span> <span class="fu">factor</span>(<span class="fu">rep</span>(<span class="fu">c</span>(<span class="st">"Normal"</span>, <span class="st">"t"</span>), <span class="at">each =</span> n))</span>
<span id="cb25-5"><a aria-hidden="true" href="#cb25-5" tabindex="-1"></a>)</span>
<span id="cb25-6"><a aria-hidden="true" href="#cb25-6" tabindex="-1"></a></span>
<span id="cb25-7"><a aria-hidden="true" href="#cb25-7" tabindex="-1"></a><span class="co"># Plot the histograms of the sample data</span></span>
<span id="cb25-8"><a aria-hidden="true" href="#cb25-8" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> Distribution)) <span class="sc">+</span></span>
<span id="cb25-9"><a aria-hidden="true" href="#cb25-9" tabindex="-1"></a><span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">bins =</span> <span class="dv">30</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">position =</span> <span class="st">"identity"</span>) <span class="sc">+</span></span>
<span id="cb25-10"><a aria-hidden="true" href="#cb25-10" tabindex="-1"></a><span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> mean_normal), <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb25-11"><a aria-hidden="true" href="#cb25-11" tabindex="-1"></a><span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> median_normal), <span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb25-12"><a aria-hidden="true" href="#cb25-12" tabindex="-1"></a><span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> mean_t), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb25-13"><a aria-hidden="true" href="#cb25-13" tabindex="-1"></a><span class="fu">geom_vline</span>(<span class="fu">aes</span>(<span class="at">xintercept =</span> median_t), <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb25-14"><a aria-hidden="true" href="#cb25-14" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Comparison of Sample Mean and Median"</span>,</span>
<span id="cb25-15"><a aria-hidden="true" href="#cb25-15" tabindex="-1"></a><span class="at">x =</span> <span class="st">"X"</span>,</span>
<span id="cb25-16"><a aria-hidden="true" href="#cb25-16" tabindex="-1"></a><span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb25-17"><a aria-hidden="true" href="#cb25-17" tabindex="-1"></a><span class="fu">facet_wrap</span>(<span class="sc">~</span> Distribution, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb25-18"><a aria-hidden="true" href="#cb25-18" tabindex="-1"></a><span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap11_files/figure-html/unnamed-chunk-3-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<p>This script compares the <strong>sample mean</strong> and <strong>sample median</strong> as estimators of location for normal and t-distributed data.</p>
<ol type="1">
<li><strong>Load Libraries and Set Seed</strong>: We load <code>ggplot2</code> and <code>dplyr</code> and set a seed for reproducibility.</li>
<li><strong>Generate Sample Data</strong>: We generate two random samples of size <span class="math inline">\(n = 1000\)</span>: one from a standard normal distribution using <code>rnorm</code> and another from a t-distribution with 3 degrees of freedom using <code>rt</code>. This relates to the discussion in Section 11.3 about the robustness of estimators to heavy-tailed distributions.</li>
<li><strong>Calculate Sample Mean and Median</strong>: We calculate the sample mean and sample median for both distributions using the <code>mean</code> and <code>median</code> functions, respectively.</li>
<li><strong>Print Results</strong>: We print the calculated sample means and medians for both distributions.</li>
<li><strong>Plot Data</strong>: We create a data frame combining both samples and use <code>ggplot2</code> to plot histograms for each distribution. We add vertical lines for the sample mean (dashed) and sample median (solid) for each distribution using <code>geom_vline</code>. We use <code>facet_wrap</code> to create separate plots for each distribution. This allows us to visually compare the performance of the sample mean and median for different distributions.</li>
</ol>
</section>
<section class="level3" id="r-script-4-demonstrating-the-central-limit-theorem">
<h3 class="anchored" data-anchor-id="r-script-4-demonstrating-the-central-limit-theorem">R Script 4: Demonstrating the Central Limit Theorem</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a aria-hidden="true" href="#cb26-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb26-2"><a aria-hidden="true" href="#cb26-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb26-3"><a aria-hidden="true" href="#cb26-3" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb26-4"><a aria-hidden="true" href="#cb26-4" tabindex="-1"></a></span>
<span id="cb26-5"><a aria-hidden="true" href="#cb26-5" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb26-6"><a aria-hidden="true" href="#cb26-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb26-7"><a aria-hidden="true" href="#cb26-7" tabindex="-1"></a></span>
<span id="cb26-8"><a aria-hidden="true" href="#cb26-8" tabindex="-1"></a><span class="co"># Define a function to generate sample means from an exponential distribution</span></span>
<span id="cb26-9"><a aria-hidden="true" href="#cb26-9" tabindex="-1"></a>generate_sample_means <span class="ot">&lt;-</span> <span class="cf">function</span>(n, lambda, num_samples) {</span>
<span id="cb26-10"><a aria-hidden="true" href="#cb26-10" tabindex="-1"></a>sample_means <span class="ot">&lt;-</span> <span class="fu">replicate</span>(num_samples, <span class="fu">mean</span>(<span class="fu">rexp</span>(n, <span class="at">rate =</span> lambda)))</span>
<span id="cb26-11"><a aria-hidden="true" href="#cb26-11" tabindex="-1"></a><span class="fu">return</span>(sample_means)</span>
<span id="cb26-12"><a aria-hidden="true" href="#cb26-12" tabindex="-1"></a>}</span>
<span id="cb26-13"><a aria-hidden="true" href="#cb26-13" tabindex="-1"></a></span>
<span id="cb26-14"><a aria-hidden="true" href="#cb26-14" tabindex="-1"></a><span class="co"># Set parameters</span></span>
<span id="cb26-15"><a aria-hidden="true" href="#cb26-15" tabindex="-1"></a>n_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">30</span>, <span class="dv">100</span>)  <span class="co"># Different sample sizes</span></span>
<span id="cb26-16"><a aria-hidden="true" href="#cb26-16" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb26-17"><a aria-hidden="true" href="#cb26-17" tabindex="-1"></a>num_samples <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb26-18"><a aria-hidden="true" href="#cb26-18" tabindex="-1"></a></span>
<span id="cb26-19"><a aria-hidden="true" href="#cb26-19" tabindex="-1"></a><span class="co"># Generate sample means for each sample size</span></span>
<span id="cb26-20"><a aria-hidden="true" href="#cb26-20" tabindex="-1"></a>sample_means_data <span class="ot">&lt;-</span> <span class="fu">lapply</span>(n_values, <span class="cf">function</span>(n) {</span>
<span id="cb26-21"><a aria-hidden="true" href="#cb26-21" tabindex="-1"></a><span class="fu">data.frame</span>(</span>
<span id="cb26-22"><a aria-hidden="true" href="#cb26-22" tabindex="-1"></a><span class="at">x =</span> <span class="fu">generate_sample_means</span>(n, lambda, num_samples),</span>
<span id="cb26-23"><a aria-hidden="true" href="#cb26-23" tabindex="-1"></a><span class="at">n =</span> <span class="fu">factor</span>(n)</span>
<span id="cb26-24"><a aria-hidden="true" href="#cb26-24" tabindex="-1"></a>)</span>
<span id="cb26-25"><a aria-hidden="true" href="#cb26-25" tabindex="-1"></a>}) <span class="sc">%&gt;%</span> <span class="fu">bind_rows</span>()</span>
<span id="cb26-26"><a aria-hidden="true" href="#cb26-26" tabindex="-1"></a></span>
<span id="cb26-27"><a aria-hidden="true" href="#cb26-27" tabindex="-1"></a><span class="co"># Calculate theoretical mean and standard deviation</span></span>
<span id="cb26-28"><a aria-hidden="true" href="#cb26-28" tabindex="-1"></a>theoretical_mean <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">/</span> lambda</span>
<span id="cb26-29"><a aria-hidden="true" href="#cb26-29" tabindex="-1"></a>theoretical_sd <span class="ot">&lt;-</span> <span class="cf">function</span>(n) { <span class="dv">1</span> <span class="sc">/</span> (lambda <span class="sc">*</span> <span class="fu">sqrt</span>(n)) }</span>
<span id="cb26-30"><a aria-hidden="true" href="#cb26-30" tabindex="-1"></a></span>
<span id="cb26-31"><a aria-hidden="true" href="#cb26-31" tabindex="-1"></a><span class="co"># Plot the distributions of sample means</span></span>
<span id="cb26-32"><a aria-hidden="true" href="#cb26-32" tabindex="-1"></a><span class="fu">ggplot</span>(sample_means_data, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">fill =</span> n)) <span class="sc">+</span></span>
<span id="cb26-33"><a aria-hidden="true" href="#cb26-33" tabindex="-1"></a><span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">bins =</span> <span class="dv">30</span>, <span class="at">color =</span> <span class="st">"black"</span>, <span class="at">alpha =</span> <span class="fl">0.7</span>, <span class="at">position =</span> <span class="st">"identity"</span>) <span class="sc">+</span></span>
<span id="cb26-34"><a aria-hidden="true" href="#cb26-34" tabindex="-1"></a><span class="fu">stat_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fu">dnorm</span>(x, <span class="at">mean =</span> theoretical_mean, <span class="at">sd =</span> <span class="fu">theoretical_sd</span>(<span class="dv">5</span>)),</span>
<span id="cb26-35"><a aria-hidden="true" href="#cb26-35" tabindex="-1"></a><span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">n =</span> <span class="dv">5</span>)) <span class="sc">+</span></span>
<span id="cb26-36"><a aria-hidden="true" href="#cb26-36" tabindex="-1"></a><span class="fu">stat_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fu">dnorm</span>(x, <span class="at">mean =</span> theoretical_mean, <span class="at">sd =</span> <span class="fu">theoretical_sd</span>(<span class="dv">30</span>)),</span>
<span id="cb26-37"><a aria-hidden="true" href="#cb26-37" tabindex="-1"></a><span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">n =</span> <span class="dv">30</span>)) <span class="sc">+</span></span>
<span id="cb26-38"><a aria-hidden="true" href="#cb26-38" tabindex="-1"></a><span class="fu">stat_function</span>(<span class="at">fun =</span> <span class="cf">function</span>(x) <span class="fu">dnorm</span>(x, <span class="at">mean =</span> theoretical_mean, <span class="at">sd =</span> <span class="fu">theoretical_sd</span>(<span class="dv">100</span>)),</span>
<span id="cb26-39"><a aria-hidden="true" href="#cb26-39" tabindex="-1"></a><span class="at">color =</span> <span class="st">"green"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">n =</span> <span class="dv">100</span>)) <span class="sc">+</span></span>
<span id="cb26-40"><a aria-hidden="true" href="#cb26-40" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Central Limit Theorem Demonstration"</span>,</span>
<span id="cb26-41"><a aria-hidden="true" href="#cb26-41" tabindex="-1"></a><span class="at">x =</span> <span class="st">"Sample Mean"</span>,</span>
<span id="cb26-42"><a aria-hidden="true" href="#cb26-42" tabindex="-1"></a><span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb26-43"><a aria-hidden="true" href="#cb26-43" tabindex="-1"></a><span class="fu">facet_wrap</span>(<span class="sc">~</span> n, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb26-44"><a aria-hidden="true" href="#cb26-44" tabindex="-1"></a><span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Computation failed in `stat_function()`.
Computation failed in `stat_function()`.
Computation failed in `stat_function()`.
Caused by error in `fun()`:
! unused argument (n = 5)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Computation failed in `stat_function()`.
Computation failed in `stat_function()`.
Computation failed in `stat_function()`.
Caused by error in `fun()`:
! unused argument (n = 30)</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Computation failed in `stat_function()`.
Computation failed in `stat_function()`.
Computation failed in `stat_function()`.
Caused by error in `fun()`:
! unused argument (n = 100)</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap11_files/figure-html/unnamed-chunk-4-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<p>This script demonstrates the <strong>Central Limit Theorem (CLT)</strong> using sample means from an exponential distribution.</p>
<ol type="1">
<li><strong>Load Libraries and Set Seed</strong>: We load <code>ggplot2</code> and <code>dplyr</code> and set a seed for reproducibility.</li>
<li><strong>Define Function to Generate Sample Means</strong>: We define a function <code>generate_sample_means</code> that takes a sample size <span class="math inline">\(n\)</span>, a rate parameter <span class="math inline">\(\lambda\)</span>, and the number of samples <code>num_samples</code> as input. It generates <code>num_samples</code> samples of size <span class="math inline">\(n\)</span> from an exponential distribution and returns a vector of their sample means. This relates to the concept of sampling distributions and the properties of estimators as discussed in Section 11.2.</li>
<li><strong>Set Parameters</strong>: We define different sample sizes <span class="math inline">\(n\)</span> (5, 30, 100), the rate parameter <span class="math inline">\(\lambda = 2\)</span>, and the number of samples <code>num_samples</code> (1000).</li>
<li><strong>Generate Sample Means</strong>: We use <code>lapply</code> to apply the <code>generate_sample_means</code> function to each sample size, creating a list of data frames, each containing sample means for a specific sample size. Then, we combine the list of data frames into a single data frame using <code>bind_rows</code>.</li>
<li><strong>Calculate Theoretical Mean and Standard Deviation</strong>: We calculate the theoretical mean (<span class="math inline">\(1/\lambda\)</span>) and a function <code>theoretical_sd</code> to compute the theoretical standard deviation (<span class="math inline">\(1/(\lambda\sqrt{n})\)</span>) of the sample means based on the CLT.</li>
<li><strong>Plot Distributions of Sample Means</strong>: We use <code>ggplot2</code> to plot histograms of the sample means for each sample size using <code>facet_wrap</code>. We overlay the normal distribution with the corresponding theoretical mean and standard deviation using <code>stat_function</code>. This visually demonstrates the CLT, showing that as the sample size increases, the distribution of the sample means approaches a normal distribution, regardless of the original distribution (which is exponential in this case). The dashed lines represent the theoretical normal distributions predicted by the CLT, and as <span class="math inline">\(n\)</span> increases, the histograms of sample means become closer to these theoretical distributions.</li>
</ol>
</section>
<section class="level3" id="r-script-5-calculating-and-plotting-the-log-likelihood-for-a-binomial-distribution">
<h3 class="anchored" data-anchor-id="r-script-5-calculating-and-plotting-the-log-likelihood-for-a-binomial-distribution">R Script 5: Calculating and Plotting the Log-Likelihood for a Binomial Distribution</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a aria-hidden="true" href="#cb30-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb30-2"><a aria-hidden="true" href="#cb30-2" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb30-3"><a aria-hidden="true" href="#cb30-3" tabindex="-1"></a><span class="fu">library</span>(dplyr)</span>
<span id="cb30-4"><a aria-hidden="true" href="#cb30-4" tabindex="-1"></a></span>
<span id="cb30-5"><a aria-hidden="true" href="#cb30-5" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb30-6"><a aria-hidden="true" href="#cb30-6" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2023</span>)</span>
<span id="cb30-7"><a aria-hidden="true" href="#cb30-7" tabindex="-1"></a></span>
<span id="cb30-8"><a aria-hidden="true" href="#cb30-8" tabindex="-1"></a><span class="co"># Generate sample data from a binomial distribution</span></span>
<span id="cb30-9"><a aria-hidden="true" href="#cb30-9" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb30-10"><a aria-hidden="true" href="#cb30-10" tabindex="-1"></a>p <span class="ot">&lt;-</span> <span class="fl">0.6</span></span>
<span id="cb30-11"><a aria-hidden="true" href="#cb30-11" tabindex="-1"></a>sample_data <span class="ot">&lt;-</span> <span class="fu">rbinom</span>(n, <span class="at">size =</span> <span class="dv">1</span>, <span class="at">prob =</span> p)</span>
<span id="cb30-12"><a aria-hidden="true" href="#cb30-12" tabindex="-1"></a></span>
<span id="cb30-13"><a aria-hidden="true" href="#cb30-13" tabindex="-1"></a><span class="co"># Define the log-likelihood function for a binomial distribution</span></span>
<span id="cb30-14"><a aria-hidden="true" href="#cb30-14" tabindex="-1"></a>log_likelihood <span class="ot">&lt;-</span> <span class="cf">function</span>(p, data) {</span>
<span id="cb30-15"><a aria-hidden="true" href="#cb30-15" tabindex="-1"></a><span class="fu">sum</span>(data <span class="sc">*</span> <span class="fu">log</span>(p) <span class="sc">+</span> (<span class="dv">1</span> <span class="sc">-</span> data) <span class="sc">*</span> <span class="fu">log</span>(<span class="dv">1</span> <span class="sc">-</span> p))</span>
<span id="cb30-16"><a aria-hidden="true" href="#cb30-16" tabindex="-1"></a>}</span>
<span id="cb30-17"><a aria-hidden="true" href="#cb30-17" tabindex="-1"></a></span>
<span id="cb30-18"><a aria-hidden="true" href="#cb30-18" tabindex="-1"></a><span class="co"># Create a sequence of p values</span></span>
<span id="cb30-19"><a aria-hidden="true" href="#cb30-19" tabindex="-1"></a>p_values <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="fl">0.01</span>, <span class="fl">0.99</span>, <span class="at">by =</span> <span class="fl">0.01</span>)</span>
<span id="cb30-20"><a aria-hidden="true" href="#cb30-20" tabindex="-1"></a></span>
<span id="cb30-21"><a aria-hidden="true" href="#cb30-21" tabindex="-1"></a><span class="co"># Calculate the log-likelihood for each p value</span></span>
<span id="cb30-22"><a aria-hidden="true" href="#cb30-22" tabindex="-1"></a>log_likelihood_values <span class="ot">&lt;-</span> <span class="fu">sapply</span>(p_values, <span class="cf">function</span>(p) <span class="fu">log_likelihood</span>(p, sample_data))</span>
<span id="cb30-23"><a aria-hidden="true" href="#cb30-23" tabindex="-1"></a></span>
<span id="cb30-24"><a aria-hidden="true" href="#cb30-24" tabindex="-1"></a><span class="co"># Find the MLE (maximum likelihood estimate)</span></span>
<span id="cb30-25"><a aria-hidden="true" href="#cb30-25" tabindex="-1"></a>mle_p <span class="ot">&lt;-</span> p_values[<span class="fu">which.max</span>(log_likelihood_values)]</span>
<span id="cb30-26"><a aria-hidden="true" href="#cb30-26" tabindex="-1"></a></span>
<span id="cb30-27"><a aria-hidden="true" href="#cb30-27" tabindex="-1"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb30-28"><a aria-hidden="true" href="#cb30-28" tabindex="-1"></a>plot_data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">p =</span> p_values, <span class="at">log_likelihood =</span> log_likelihood_values)</span>
<span id="cb30-29"><a aria-hidden="true" href="#cb30-29" tabindex="-1"></a></span>
<span id="cb30-30"><a aria-hidden="true" href="#cb30-30" tabindex="-1"></a><span class="co"># Plot the log-likelihood function</span></span>
<span id="cb30-31"><a aria-hidden="true" href="#cb30-31" tabindex="-1"></a><span class="fu">ggplot</span>(plot_data, <span class="fu">aes</span>(<span class="at">x =</span> p, <span class="at">y =</span> log_likelihood)) <span class="sc">+</span></span>
<span id="cb30-32"><a aria-hidden="true" href="#cb30-32" tabindex="-1"></a><span class="fu">geom_line</span>(<span class="at">color =</span> <span class="st">"blue"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb30-33"><a aria-hidden="true" href="#cb30-33" tabindex="-1"></a><span class="fu">geom_vline</span>(<span class="at">xintercept =</span> mle_p, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">linewidth =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb30-34"><a aria-hidden="true" href="#cb30-34" tabindex="-1"></a><span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Log-Likelihood Function for Binomial Distribution"</span>,</span>
<span id="cb30-35"><a aria-hidden="true" href="#cb30-35" tabindex="-1"></a><span class="at">x =</span> <span class="st">"p"</span>,</span>
<span id="cb30-36"><a aria-hidden="true" href="#cb30-36" tabindex="-1"></a><span class="at">y =</span> <span class="st">"Log-Likelihood"</span>) <span class="sc">+</span></span>
<span id="cb30-37"><a aria-hidden="true" href="#cb30-37" tabindex="-1"></a><span class="fu">theme_minimal</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap11_files/figure-html/unnamed-chunk-5-1.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a aria-hidden="true" href="#cb31-1" tabindex="-1"></a><span class="co"># Print the MLE</span></span>
<span id="cb31-2"><a aria-hidden="true" href="#cb31-2" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"MLE for p:"</span>, mle_p, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>MLE for p: 0.5 </code></pre>
</div>
</div>
<p><strong>Intuitive explanation:</strong></p>
<p>This script calculates and plots the <strong>log-likelihood function</strong> for a binomial distribution and finds the <strong>maximum likelihood estimate (MLE)</strong> for the probability of success <span class="math inline">\(p\)</span>.</p>
<ol type="1">
<li><strong>Load Libraries and Set Seed</strong>: We load <code>ggplot2</code> and <code>dplyr</code> and set a seed for reproducibility.</li>
<li><strong>Generate Sample Data</strong>: We generate a random sample of size <span class="math inline">\(n = 50\)</span> from a Bernoulli distribution (binomial with size = 1) with probability of success <span class="math inline">\(p = 0.6\)</span> using the <code>rbinom</code> function. The text discusses the Binomial distribution in Example 11.7.</li>
<li><strong>Define Log-Likelihood Function</strong>: We define the log-likelihood function for a binomial distribution as described in Example 11.7 in the text:</li>
</ol>
<p><span class="math display">\[
l(p|X^n) = \sum_{i=1}^n X_i \log p + (1-X_i) \log(1-p)
\]</span> 4. <strong>Create Sequence of p Values</strong>: We create a sequence of <span class="math inline">\(p\)</span> values from 0.01 to 0.99 with a step of 0.01 using the <code>seq</code> function. 5. <strong>Calculate Log-Likelihood</strong>: We use <code>sapply</code> to apply the <code>log_likelihood</code> function to each <span class="math inline">\(p\)</span> value in the sequence, calculating the log-likelihood for each value given the sample data. 6. <strong>Find MLE</strong>: We find the MLE for <span class="math inline">\(p\)</span> by identifying the <span class="math inline">\(p\)</span> value that corresponds to the maximum log-likelihood value using <code>which.max</code>. 7. <strong>Plot Log-Likelihood Function</strong>: We create a data frame with the <span class="math inline">\(p\)</span> values and their corresponding log-likelihood values. We use <code>ggplot2</code> to plot the log-likelihood function, with a vertical dashed line at the MLE using <code>geom_vline</code>. This visually represents how the MLE is found as the peak of the log-likelihood function. 8. <strong>Print MLE</strong>: We print the MLE for <span class="math inline">\(p\)</span>.</p>
</section>
</section>
<section class="level2" id="youtube-videos">
<h2 class="anchored" data-anchor-id="youtube-videos">YouTube Videos</h2>
<p>Here are some YouTube videos that explain concepts related to the attached text on Estimation Theory:</p>
<section class="level3" id="videos-on-method-of-moments">
<h3 class="anchored" data-anchor-id="videos-on-method-of-moments">Videos on Method of Moments</h3>
<ol type="1">
<li><strong>Title:</strong> Method of Moments</li>
</ol>
<p><strong>Channel:</strong> ritvikmath</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=7hmGVf%20মেশিনে-পাঠযোগ্য-না">https://www.youtube.com/watch?v=7hmGVf মেশিনে-পাঠযোগ্য-না</a></p>
<p><strong>Description:</strong> This video provides a general introduction to the <strong>method of moments</strong> estimation. It explains how to estimate parameters by equating sample moments to population moments.</p>
<p><strong>Relation to Text:</strong> This video relates to Section 11.1.1, where the method of moments is introduced. The video complements the text by providing a visual and intuitive explanation of the concept, including examples of how to derive method of moments estimators.</p>
<ol start="2" type="1">
<li><strong>Title:</strong> Method of Moments Estimation EXPLAINED</li>
</ol>
<p><strong>Channel:</strong> Ben Lambert</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=7oDtsIz-H9M">https://www.youtube.com/watch?v=7oDtsIz-H9M</a></p>
<p><strong>Description:</strong> This video explains the intuition behind the method of moments estimation, using clear examples and analogies. It covers both single and multiple parameter estimations.</p>
<p><strong>Relation to Text:</strong> This video strongly relates to Section 11.1.1, where the method of moments and the analogy principle are discussed. It reinforces the concepts in the text by illustrating how sample moments can be used to estimate population parameters, in line with the analogy principle of substituting population quantities with their sample counterparts.</p>
</section>
<section class="level3" id="videos-on-maximum-likelihood-estimation">
<h3 class="anchored" data-anchor-id="videos-on-maximum-likelihood-estimation">Videos on Maximum Likelihood Estimation</h3>
<ol type="1">
<li><strong>Title:</strong> (ML 18.1) Maximum likelihood estimation: intuition (MLE)</li>
</ol>
<p><strong>Channel:</strong> mathematicalmonk</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=XepXtl9YKwc">https://www.youtube.com/watch?v=XepXtl9YKwc</a></p>
<p><strong>Description:</strong> This video provides an intuitive explanation of <strong>maximum likelihood estimation (MLE)</strong>. It uses graphical examples to illustrate how MLE finds the parameter values that maximize the likelihood of observing the given data.</p>
<p><strong>Relation to Text:</strong> This video relates to Section 11.1.2, where maximum likelihood estimation is introduced. It provides a visual and conceptual understanding of MLE, complementing the theoretical description in the text.</p>
<ol start="2" type="1">
<li><strong>Title:</strong> Maximum Likelihood Estimation (MLE) : How to estimate parameters of a distribution</li>
</ol>
<p><strong>Channel:</strong> Author’s Name</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=BfKanl1aSG0">https://www.youtube.com/watch?v=BfKanl1aSG0</a></p>
<p><strong>Description:</strong> This video provides a step-by-step guide on how to derive maximum likelihood estimators. It covers various examples, including the exponential and normal distributions.</p>
<p><strong>Relation to Text:</strong> This video directly relates to the concepts and examples of MLE presented in Section 11.1.2 and Examples 11.7, 11.8, 11.9, and 11.10. The examples in the video, such as deriving the MLE for the exponential distribution, closely mirror the derivations shown in the text.</p>
<ol start="3" type="1">
<li><strong>Title:</strong> Maximum Likelihood, clearly explained!!!</li>
</ol>
<p><strong>Channel:</strong> StatQuest with Josh Starmer</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=XepXtl9YKwc">https://www.youtube.com/watch?v=XepXtl9YKwc</a></p>
<p><strong>Description:</strong> This video gives a clear and concise explanation of maximum likelihood estimation, using simple examples and analogies. It covers the basic idea, how to find the MLE, and how to interpret the results.</p>
<p><strong>Relation to Text:</strong> This video relates to Section 11.1.2, providing a strong conceptual foundation for understanding maximum likelihood estimation. It helps to solidify the concepts in the text by illustrating how MLE is used to find the parameter values that best explain the observed data.</p>
</section>
<section class="level3" id="videos-on-properties-of-estimators">
<h3 class="anchored" data-anchor-id="videos-on-properties-of-estimators">Videos on Properties of Estimators</h3>
<ol type="1">
<li><strong>Title:</strong> 3.2 Mean Squared Error</li>
</ol>
<p><strong>Channel:</strong> MarinStatsLectures-R Programming &amp; Statistics</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=k-m-K-T-HM4">https://www.youtube.com/watch?v=k-m-K-T-HM4</a></p>
<p><strong>Description:</strong> This video explains the concept of <strong>mean squared error (MSE)</strong> and its use in evaluating the performance of estimators.</p>
<p><strong>Relation to Text:</strong> This video relates to Section 11.2, where MSE is defined (Definition 11.2) and used as a criterion for comparing estimators. The video complements the text by providing a clear explanation of MSE and its components (variance and bias), as discussed in Theorem 11.1.</p>
<ol start="2" type="1">
<li><strong>Title:</strong> Unbiased Estimators</li>
</ol>
<p><strong>Channel:</strong> MIT OpenCourseWare</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=b-j76R9Lhws">https://www.youtube.com/watch?v=b-j76R9Lhws</a></p>
<p><strong>Description:</strong> This lecture excerpt from MIT’s Probabilistic Systems Analysis and Applied Probability course defines <strong>unbiased estimators</strong> and discusses their properties.</p>
<p><strong>Relation to Text:</strong> This video relates to the discussion of unbiased estimators in Section 11.2. It complements the text’s definition of bias <span class="math inline">\(b(\theta) = E(\hat{\theta}) - \theta\)</span> by providing a formal explanation and examples of unbiasedness, enriching the understanding of this important property of estimators.</p>
<ol start="3" type="1">
<li><strong>Title:</strong> L21.3 Asymptotic Properties of Estimators</li>
</ol>
<p><strong>Channel:</strong> MIT OpenCourseWare</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=dMe_sx-q4fc">https://www.youtube.com/watch?v=dMe_sx-q4fc</a></p>
<p><strong>Description:</strong> This lecture excerpt from MIT’s Probabilistic Systems Analysis and Applied Probability course discusses <strong>asymptotic properties of estimators</strong>, including consistency and asymptotic normality.</p>
<p><strong>Relation to Text:</strong> This video directly relates to Section 11.2.1, where consistency and asymptotic normality are defined (Definitions 11.3 and 11.4). It complements the text by providing a formal treatment of these concepts and illustrating them with examples, enhancing the understanding of the large-sample behavior of estimators.</p>
</section>
<section class="level3" id="videos-on-computational-methods">
<h3 class="anchored" data-anchor-id="videos-on-computational-methods">Videos on Computational Methods</h3>
<ol type="1">
<li><strong>Title:</strong> Newton-Raphson Method</li>
</ol>
<p><strong>Channel:</strong> MIT OpenCourseWare</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=7ENgffqBn_A">https://www.youtube.com/watch?v=7ENgffqBn_A</a></p>
<p><strong>Description:</strong> This lecture excerpt from MIT’s Introduction to Numerical Analysis course explains the <strong>Newton-Raphson method</strong> for finding roots of equations.</p>
<p><strong>Relation to Text:</strong> This video relates to Section 11.1.3, where the Newton-Raphson method is mentioned as a numerical method for computing maximum likelihood estimators. It provides a detailed explanation of the algorithm, which is used to iteratively solve the equation <span class="math inline">\(s(\hat{\theta}|X^n) = 0\)</span> in the context of MLE. The video complements the text by showing how the method works, including its iterative formula and convergence properties.</p>
</section>
<section class="level3" id="videos-on-robustness">
<h3 class="anchored" data-anchor-id="videos-on-robustness">Videos on Robustness</h3>
<ol type="1">
<li><strong>Title:</strong> Robust statistics: robust estimators (trimmed mean, median,…)</li>
</ol>
<p><strong>Channel:</strong> Martin Vejmelka</p>
<p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=zJ-EN-s-9Rg">https://www.youtube.com/watch?v=zJ-EN-s-9Rg</a></p>
<p><strong>Description:</strong> This video introduces the concept of <strong>robust statistics</strong> and discusses robust estimators like the trimmed mean and median.</p>
<p><strong>Relation to Text:</strong> This video relates to Section 11.3, where the robustness of estimators is discussed. It complements the text by providing specific examples of robust estimators and explaining why they are less sensitive to outliers and deviations from assumed distributions compared to non-robust estimators like the sample mean.</p>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch11mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch11mcsolution1">MC Solution 1</a></p>
<p>The <strong>method of moments</strong> estimator is obtained by:</p>
<ol type="a">
<li><p>Maximizing the likelihood function.</p></li>
<li><p>Equating sample moments to population moments.</p></li>
<li><p>Minimizing the mean squared error.</p></li>
<li><p>Finding the unbiased estimator with the smallest variance.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch11mcsolution2">MC Solution 2</a></p>
<p>The <strong>maximum likelihood estimator (MLE)</strong> is the value of the parameter that:</p>
<ol type="a">
<li><p>Minimizes the sum of squared errors.</p></li>
<li><p>Maximizes the probability of observing the given sample.</p></li>
<li><p>Equals the sample mean.</p></li>
<li><p>Is always unbiased.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch11mcsolution3">MC Solution 3</a></p>
<p>Which of the following is NOT a desirable property of an estimator?</p>
<ol type="a">
<li><p>Unbiasedness</p></li>
<li><p>Consistency</p></li>
<li><p>Minimum variance</p></li>
<li><p>Dependence on unknown parameters</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch11mcsolution4">MC Solution 4</a></p>
<p>The <strong>bias</strong> of an estimator <span class="math inline">\(\hat{\theta}\)</span> for a parameter <span class="math inline">\(\theta\)</span> is defined as:</p>
<ol type="a">
<li><p><span class="math inline">\(\text{Var}(\hat{\theta})\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{E}(\hat{\theta}) - \theta\)</span></p></li>
<li><p><span class="math inline">\(\theta - \mathbb{E}(\hat{\theta})\)</span></p></li>
<li><p><span class="math inline">\(\mathbb{E}[(\hat{\theta} - \theta)^2]\)</span></p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch11mcsolution5">MC Solution 5</a></p>
<p>The <strong>mean squared error (MSE)</strong> of an estimator <span class="math inline">\(\hat{\theta}\)</span> is:</p>
<ol type="a">
<li><p>The variance of the estimator.</p></li>
<li><p>The bias of the estimator squared.</p></li>
<li><p>The sum of the variance and the squared bias of the estimator.</p></li>
<li><p>Always zero for an unbiased estimator.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch11mcsolution6">MC Solution 6</a></p>
<p>An estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be <strong>consistent</strong> if:</p>
<ol type="a">
<li><p><span class="math inline">\(\mathbb{E}(\hat{\theta}) = \theta\)</span></p></li>
<li><p><span class="math inline">\(\text{Var}(\hat{\theta})\)</span> is minimized</p></li>
<li><p><span class="math inline">\(\hat{\theta}\)</span> converges in probability to <span class="math inline">\(\theta\)</span> as the sample size increases.</p></li>
<li><p><span class="math inline">\(\hat{\theta}\)</span> is normally distributed.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch11mcsolution7">MC Solution 7</a></p>
<p>The <strong>Cramér-Rao lower bound</strong> provides:</p>
<ol type="a">
<li><p>A lower bound on the variance of any unbiased estimator.</p></li>
<li><p>An upper bound on the variance of any estimator.</p></li>
<li><p>The exact variance of the maximum likelihood estimator.</p></li>
<li><p>A method for finding the unbiased estimator with the smallest variance.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch11mcsolution8">MC Solution 8</a></p>
<p>An estimator is <strong>asymptotically normal</strong> if:</p>
<ol type="a">
<li><p>Its distribution is normal for any sample size.</p></li>
<li><p>Its distribution approaches a normal distribution as the sample size increases.</p></li>
<li><p>It is unbiased and consistent.</p></li>
<li><p>Its mean squared error is zero.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch11mcsolution9">MC Solution 9</a></p>
<p>The <strong>score function</strong> is defined as the:</p>
<ol type="a">
<li><p>Second derivative of the log-likelihood function.</p></li>
<li><p>First derivative of the log-likelihood function.</p></li>
<li><p>Logarithm of the likelihood function.</p></li>
<li><p>Maximum value of the likelihood function.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch11mcsolution10">MC Solution 10</a></p>
<p>The <strong>Newton-Raphson method</strong> is used to:</p>
<ol type="a">
<li><p>Find the maximum likelihood estimator numerically.</p></li>
<li><p>Calculate the mean squared error of an estimator.</p></li>
<li><p>Derive the method of moments estimator.</p></li>
<li><p>Test hypotheses about the parameters.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch11mcsolution11">MC Solution 11</a></p>
<p>Which of the following statements is true about the <strong>maximum likelihood estimator (MLE)</strong>?</p>
<ol type="a">
<li><p>It is always unbiased.</p></li>
<li><p>It is always consistent.</p></li>
<li><p>It is always efficient.</p></li>
<li><p>It is always easy to compute.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch11mcsolution12">MC Solution 12</a></p>
<p>The <strong>empirical distribution function</strong> <span class="math inline">\(F_n(x)\)</span> is defined as:</p>
<ol type="a">
<li><p>The probability density function of the sample.</p></li>
<li><p>The cumulative distribution function of the population.</p></li>
<li><p>The proportion of observations in the sample that are less than or equal to <span class="math inline">\(x\)</span>.</p></li>
<li><p>The inverse of the population distribution function.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch11mcsolution13">MC Solution 13</a></p>
<p>The <strong>order statistics</strong> of a sample <span class="math inline">\(X_1, \dots, X_n\)</span> are:</p>
<ol type="a">
<li><p>The sample moments.</p></li>
<li><p>The sample observations arranged in increasing order.</p></li>
<li><p>The parameters of the distribution.</p></li>
<li><p>The values of the score function.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch11mcsolution14">MC Solution 14</a></p>
<p>The <strong>invariance property of MLE</strong> states that if <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then for a function <span class="math inline">\(\tau(\theta)\)</span>, the MLE of <span class="math inline">\(\tau(\theta)\)</span> is:</p>
<ol type="a">
<li><p><span class="math inline">\(\tau(\hat{\theta})\)</span></p></li>
<li><p><span class="math inline">\(\hat{\theta}\)</span></p></li>
<li><p><span class="math inline">\(\dfrac{1}{\tau(\hat{\theta})}\)</span></p></li>
<li><p>Not determinable from the given information.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch11mcsolution15">MC Solution 15</a></p>
<p>A <strong>robust estimator</strong> is:</p>
<ol type="a">
<li><p>Always unbiased.</p></li>
<li><p>Always efficient.</p></li>
<li><p>Insensitive to outliers or deviations from the assumed distribution.</p></li>
<li><p>Always easy to compute.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch11mcsolution16">MC Solution 16</a></p>
<p>The <strong>Hessian matrix</strong> in the context of maximum likelihood estimation is:</p>
<ol type="a">
<li><p>The matrix of first derivatives of the log-likelihood function.</p></li>
<li><p>The matrix of second derivatives of the log-likelihood function.</p></li>
<li><p>The inverse of the score function.</p></li>
<li><p>The matrix of sample means.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch11mcsolution17">MC Solution 17</a></p>
<p>The <strong>information matrix</strong> is:</p>
<ol type="a">
<li><p>The negative of the expected value of the Hessian matrix.</p></li>
<li><p>The inverse of the Hessian matrix.</p></li>
<li><p>The matrix of sample variances and covariances.</p></li>
<li><p>The matrix of order statistics.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch11mcsolution18">MC Solution 18</a></p>
<p>The <strong>analogy principle</strong> suggests that:</p>
<ol type="a">
<li><p>Population parameters should be estimated by maximizing the likelihood function.</p></li>
<li><p>Population parameters should be estimated by the corresponding sample quantities.</p></li>
<li><p>Estimators should be unbiased and efficient.</p></li>
<li><p>Estimators should be robust to outliers.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch11mcsolution19">MC Solution 19</a></p>
<p>A <strong>sufficient statistic</strong> for a parameter <span class="math inline">\(\theta\)</span> is a statistic that:</p>
<ol type="a">
<li><p>Is an unbiased estimator of <span class="math inline">\(\theta\)</span>.</p></li>
<li><p>Contains all the information about <span class="math inline">\(\theta\)</span> available in the sample.</p></li>
<li><p>Is normally distributed.</p></li>
<li><p>Minimizes the mean squared error.</p></li>
</ol>
</section>
<section class="level3" id="sec-ch11mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch11mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch11mcsolution20">MC Solution 20</a></p>
<p>The <strong>profile likelihood</strong> is used when:</p>
<ol type="a">
<li><p>The likelihood function is difficult to compute.</p></li>
<li><p>There are multiple parameters, and we want to estimate a subset of them.</p></li>
<li><p>The estimator is not consistent.</p></li>
<li><p>The sample size is small.</p></li>
</ol>
</section>
</section>
<section class="level2" id="multiple-choice-solutions">
<h2 class="anchored" data-anchor-id="multiple-choice-solutions">Multiple Choice Solutions</h2>
<section class="level3" id="sec-ch11mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch11mcexercise1">MC Exercise 1</a></p>
<p><strong>Correct Answer:</strong> b) Equating sample moments to population moments.</p>
<p><strong>Explanation:</strong> The method of moments estimator is obtained by equating sample moments (like the sample mean) to the corresponding population moments (like the population mean) and solving for the unknown parameters. This is stated in Section 11.1.1 of the text.</p>
</section>
<section class="level3" id="sec-ch11mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch11mcexercise2">MC Exercise 2</a></p>
<p><strong>Correct Answer:</strong> b) Maximizes the probability of observing the given sample.</p>
<p><strong>Explanation:</strong> The maximum likelihood estimator (MLE) is the value of the parameter that maximizes the likelihood function. The likelihood function represents the probability (or probability density) of observing the given sample as a function of the parameter. This is explained in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch11mcexercise3">MC Exercise 3</a></p>
<p><strong>Correct Answer:</strong> d) Dependence on unknown parameters</p>
<p><strong>Explanation:</strong> Desirable properties of an estimator include unbiasedness, consistency, and minimum variance (efficiency). An estimator should not depend on unknown parameters, as these are what we are trying to estimate. The properties of estimators are discussed in Section 11.2.</p>
</section>
<section class="level3" id="sec-ch11mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch11mcexercise4">MC Exercise 4</a></p>
<p><strong>Correct Answer:</strong> b) <span class="math inline">\(\mathbb{E}(\hat{\theta}) - \theta\)</span></p>
<p><strong>Explanation:</strong> The bias of an estimator <span class="math inline">\(\hat{\theta}\)</span> for a parameter <span class="math inline">\(\theta\)</span> is defined as the difference between the expected value of the estimator and the true value of the parameter, i.e., <span class="math inline">\(\text{bias}(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta\)</span>. This is consistent with the definition in Section 11.2.</p>
</section>
<section class="level3" id="sec-ch11mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch11mcexercise5">MC Exercise 5</a></p>
<p><strong>Correct Answer:</strong> c) The sum of the variance and the squared bias of the estimator.</p>
<p><strong>Explanation:</strong> The mean squared error (MSE) of an estimator is defined as <span class="math inline">\(\text{MSE}(\hat{\theta}, \theta) = \mathbb{E}[(\hat{\theta} - \theta)^2]\)</span>. According to Theorem 11.1, it can be decomposed into the sum of the variance and the squared bias: <span class="math inline">\(\text{MSE}(\hat{\theta}, \theta) = \text{var}(\hat{\theta}) + \text{bias}^2(\hat{\theta})\)</span>.</p>
</section>
<section class="level3" id="sec-ch11mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch11mcexercise6">MC Exercise 6</a></p>
<p><strong>Correct Answer:</strong> c) <span class="math inline">\(\hat{\theta}\)</span> converges in probability to <span class="math inline">\(\theta\)</span> as the sample size increases.</p>
<p><strong>Explanation:</strong> An estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be consistent if it converges in probability to the true parameter value <span class="math inline">\(\theta\)</span> as the sample size <span class="math inline">\(n\)</span> goes to infinity. This is stated in Definition 11.3.</p>
</section>
<section class="level3" id="sec-ch11mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch11mcexercise7">MC Exercise 7</a></p>
<p><strong>Correct Answer:</strong> a) A lower bound on the variance of any unbiased estimator.</p>
<p><strong>Explanation:</strong> The Cramér-Rao lower bound (CRLB) provides a lower bound on the variance of any unbiased estimator. Theorem 11.3 states that for any unbiased estimator <span class="math inline">\(\hat{\theta}\)</span>, <span class="math inline">\(\text{var}(\hat{\theta}) \geq \mathcal{I}^{-1}(\theta)\)</span>, where <span class="math inline">\(\mathcal{I}(\theta)\)</span> is the information matrix.</p>
</section>
<section class="level3" id="sec-ch11mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch11mcexercise8">MC Exercise 8</a></p>
<p><strong>Correct Answer:</strong> b) Its distribution approaches a normal distribution as the sample size increases.</p>
<p><strong>Explanation:</strong> An estimator is asymptotically normal if its distribution, when properly standardized, approaches a normal distribution as the sample size <span class="math inline">\(n\)</span> goes to infinity. This is stated in Definition 11.4.</p>
</section>
<section class="level3" id="sec-ch11mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch11mcexercise9">MC Exercise 9</a></p>
<p><strong>Correct Answer:</strong> b) First derivative of the log-likelihood function.</p>
<p><strong>Explanation:</strong> The score function is defined as the first derivative (or gradient) of the log-likelihood function with respect to the parameter(s). This is shown in equation (11.5).</p>
</section>
<section class="level3" id="sec-ch11mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch11mcexercise10">MC Exercise 10</a></p>
<p><strong>Correct Answer:</strong> a) Find the maximum likelihood estimator numerically.</p>
<p><strong>Explanation:</strong> The Newton-Raphson method is an iterative numerical method used to find successively better approximations to the roots (or zeros) of a real-valued function. In the context of maximum likelihood estimation, it is used to find the value of the parameter that maximizes the log-likelihood function (or equivalently, sets the score function to zero), as mentioned in Section 11.1.3.</p>
</section>
<section class="level3" id="sec-ch11mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch11mcexercise11">MC Exercise 11</a></p>
<p><strong>Correct Answer:</strong> b) It is always consistent.</p>
<p><strong>Explanation:</strong> While the MLE has many desirable properties, it is not always unbiased or efficient in finite samples. However, under certain regularity conditions, the MLE is consistent, meaning it converges in probability to the true parameter value as the sample size increases. The MLE can also be computationally intensive. The properties of the MLE are discussed in Sections 11.1.2, 11.2.1, and 11.3.</p>
</section>
<section class="level3" id="sec-ch11mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch11mcexercise12">MC Exercise 12</a></p>
<p><strong>Correct Answer:</strong> c) The proportion of observations in the sample that are less than or equal to <span class="math inline">\(x\)</span>.</p>
<p><strong>Explanation:</strong> The empirical distribution function <span class="math inline">\(F_n(x)\)</span> is a non-parametric estimate of the cumulative distribution function (CDF) based on the sample. It is defined as the proportion of observations in the sample that are less than or equal to <span class="math inline">\(x\)</span>. This is introduced in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch11mcexercise13">MC Exercise 13</a></p>
<p><strong>Correct Answer:</strong> b) The sample observations arranged in increasing order.</p>
<p><strong>Explanation:</strong> The order statistics of a sample <span class="math inline">\(X_1, \dots, X_n\)</span> are the sample values arranged in ascending order, denoted by <span class="math inline">\(X_{(1)} \leq X_{(2)} \leq \dots \leq X_{(n)}\)</span>. This is defined in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch11mcexercise14">MC Exercise 14</a></p>
<p><strong>Correct Answer:</strong> a) <span class="math inline">\(\tau(\hat{\theta})\)</span></p>
<p><strong>Explanation:</strong> The invariance property of MLE states that if <span class="math inline">\(\hat{\theta}\)</span> is the MLE of <span class="math inline">\(\theta\)</span>, then for a function <span class="math inline">\(\tau(\theta)\)</span>, the MLE of <span class="math inline">\(\tau(\theta)\)</span> is <span class="math inline">\(\tau(\hat{\theta})\)</span>. This is discussed in Section 11.1.2.</p>
</section>
<section class="level3" id="sec-ch11mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch11mcexercise15">MC Exercise 15</a></p>
<p><strong>Correct Answer:</strong> c) Insensitive to outliers or deviations from the assumed distribution.</p>
<p><strong>Explanation:</strong> A robust estimator is one that is not overly sensitive to outliers or small deviations from the assumed distribution. This means that the estimator’s performance remains relatively stable even if the data contains some unusual observations or if the true distribution differs slightly from the assumed one. This is discussed in Section 11.3.</p>
</section>
<section class="level3" id="sec-ch11mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch11mcexercise16">MC Exercise 16</a></p>
<p><strong>Correct Answer:</strong> b) The matrix of second derivatives of the log-likelihood function.</p>
<p><strong>Explanation:</strong> The Hessian matrix in the context of maximum likelihood estimation is the matrix of second partial derivatives of the log-likelihood function with respect to the parameters. It is denoted by <span class="math inline">\(h(\theta|X^n)\)</span> in equation (11.6).</p>
</section>
<section class="level3" id="sec-ch11mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch11mcexercise17">MC Exercise 17</a></p>
<p><strong>Correct Answer:</strong> a) The negative of the expected value of the Hessian matrix.</p>
<p><strong>Explanation:</strong> The information matrix, denoted by <span class="math inline">\(\mathcal{I}(\theta)\)</span>, is the negative of the expected value of the Hessian matrix: <span class="math inline">\(\mathcal{I}(\theta) = -\mathbb{E}[h(\theta|X^n)]\)</span>. This is stated in equation (11.17).</p>
</section>
<section class="level3" id="sec-ch11mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch11mcexercise18">MC Exercise 18</a></p>
<p><strong>Correct Answer:</strong> b) Population parameters should be estimated by the corresponding sample quantities.</p>
<p><strong>Explanation:</strong> The analogy principle, also known as the method of moments, suggests that population parameters should be estimated by the corresponding sample quantities (e.g., estimating the population mean by the sample mean). This is described in Section 11.1.1.</p>
</section>
<section class="level3" id="sec-ch11mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch11mcexercise19">MC Exercise 19</a></p>
<p><strong>Correct Answer:</strong> b) Contains all the information about <span class="math inline">\(\theta\)</span> available in the sample.</p>
<p><strong>Explanation:</strong> A sufficient statistic for a parameter <span class="math inline">\(\theta\)</span> is a statistic that contains all the information about <span class="math inline">\(\theta\)</span> that is available in the sample. This means that the conditional distribution of the sample given the sufficient statistic does not depend on <span class="math inline">\(\theta\)</span>. Although the concept of sufficiency is not explicitly defined in the text, it is implicitly related to the discussion of sufficient statistics.</p>
</section>
<section class="level3" id="sec-ch11mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch11mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch11mcexercise20">MC Exercise 20</a></p>
<p><strong>Correct Answer:</strong> b) There are multiple parameters, and we want to estimate a subset of them.</p>
<p><strong>Explanation:</strong> The profile likelihood is used when there are multiple parameters, and we want to estimate a subset of them while treating the other parameters as nuisance parameters. The profile likelihood for a parameter of interest is obtained by maximizing the likelihood function with respect to the nuisance parameters for each fixed value of the parameter of interest. This is discussed in Section 11.1.3.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>