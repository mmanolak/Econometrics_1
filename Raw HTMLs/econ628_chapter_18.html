<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 18: Linear Model – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap19.html" rel="next"/>
<link href="../chapters/chap17.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 18: Linear Model</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level2" id="introduction">
<h2 class="anchored" data-anchor-id="introduction">18.1 INTRODUCTION</h2>
<p>The <strong>linear model</strong> is fundamental to econometrics. These notes discuss the probabilistic assumptions underlying the linear model and how they allow us to understand the properties of the Ordinary Least Squares (OLS) estimator.</p>
</section>
<section class="level2" id="the-model">
<h2 class="anchored" data-anchor-id="the-model">18.2 THE MODEL</h2>
<p>We specify a model describing how the data</p>
<p><span class="math inline">\(\qquad y, X \in \mathbb{R}^{n \times (K+1)}\)</span></p>
<p>were generated. We assume there is an underlying random mechanism. The observed data represents one realization from an infinite set of potential outcomes. The model focuses on how <span class="math inline">\(y\)</span> responds to <span class="math inline">\(X\)</span>, with minimal assumptions about <span class="math inline">\(X\)</span> itself. We assume throughout that</p>
<p><span class="math inline">\(\qquad \text{rank}(X) = K &lt; n\)</span>.</p>
<p>This assumption is immediately verifiable from the data.</p>
<section class="level3" id="assumption-a">
<h3 class="anchored" data-anchor-id="assumption-a">Assumption A</h3>
<ol type="1">
<li><p><strong>Linearity in Parameters:</strong> There exists a vector <span class="math inline">\(\beta \in \mathbb{R}^{K}\)</span> such that with probability one,</p>
<p><span class="math inline">\(\qquad E(y|X) = X\beta\)</span>.</p>
<p><em>Intuition:</em> The expected value of the dependent variable <span class="math inline">\(y\)</span>, given the independent variables <span class="math inline">\(X\)</span>, is a linear function of <span class="math inline">\(X\)</span>. The coefficients of this linear function are represented by the vector <span class="math inline">\(\beta\)</span>. <em>Real world example:</em> Suppose <span class="math inline">\(y\)</span> is wage and <span class="math inline">\(X\)</span> includes education and experience. <span class="math inline">\(\beta\)</span> would contain coefficients for education and experience’s respective impacts on wages.</p></li>
<li><p><strong>Conditional Variance:</strong> There exists a positive definite finite <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(\Sigma(X)\)</span> such that with probability one,</p>
<p><span class="math inline">\(\qquad \text{var}(y|X) = \Sigma(X)\)</span>.</p>
<p><em>Intuition:</em> The variance of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span> is not necessarily constant. It is captured by the matrix <span class="math inline">\(\Sigma(X)\)</span>, which can depend on <span class="math inline">\(X\)</span>. <em>Real world example:</em> The variability of wages might be higher for individuals with higher education levels, meaning that the variance of wage is not constant and depends on X.</p></li>
<li><p><strong>Conditional Normality:</strong> Conditional on <span class="math inline">\(X\)</span>, we have with probability one,</p>
<p><span class="math inline">\(\qquad y \sim N(X\beta, \Sigma(X))\)</span>.</p>
<p><em>Intuition:</em> Given <span class="math inline">\(X\)</span>, the dependent variable <span class="math inline">\(y\)</span> follows a normal distribution with mean <span class="math inline">\(X\beta\)</span> and variance <span class="math inline">\(\Sigma(X)\)</span>. <em>Real world example:</em> If we look at individuals with the same levels of education and experience, their wages will be normally distributed around a mean wage determined by their specific education and experience.</p></li>
<li><p><strong>Conditional Normality with Homoskedasticity and no Autocorrelation:</strong> Conditional on X, we have with probability one</p>
<p><span class="math inline">\(\qquad y \sim N(X\beta, \sigma^2 I)\)</span></p>
<p><em>Intuition:</em> This is a simplification of assumption 3. Now the variance is a constant (<span class="math inline">\(\sigma^2\)</span>) multiplied by the identity matrix. This is a special case, important for OLS.</p></li>
</ol>
</section>
<section class="level3" id="regression-model-in-familiar-form">
<h3 class="anchored" data-anchor-id="regression-model-in-familiar-form">Regression Model in Familiar Form</h3>
<p>We can rewrite the regression model using an error term. Define</p>
<p><span class="math inline">\(\qquad \varepsilon = y - X\beta = \begin{pmatrix}
\varepsilon_1 \\
\vdots \\
\varepsilon_n
\end{pmatrix}\)</span>.</p>
<p>Then,</p>
<p><span class="math inline">\(\qquad y = X\beta + \varepsilon\)</span>.</p>
<p>Assumptions A1 and A2 are equivalent to</p>
<p><span class="math inline">\(\qquad E(\varepsilon|X) = 0\)</span>,</p>
<p><span class="math inline">\(\qquad E(\varepsilon\varepsilon^T|X) = \Sigma(X)\)</span>.</p>
<p>with probability one.</p>
<p>The model is often stated in terms of the unobservable error <span class="math inline">\(\varepsilon\)</span> rather than directly on the observable <span class="math inline">\(y\)</span>. The assumptions about <span class="math inline">\(\varepsilon\)</span> are weak (and incomplete, as we only specify some moments).</p>
</section>
<section class="level3" id="assumption-a1-explained">
<h3 class="anchored" data-anchor-id="assumption-a1-explained">Assumption A1 Explained:</h3>
<p><span class="math inline">\(E(\varepsilon|X) = 0\)</span> can be expressed as (local notation change)</p>
<p><span class="math inline">\(\qquad E \left( \varepsilon_i | X_1, \dots, X_n  \right) = 0\)</span>, where <span class="math inline">\(X_i = \begin{pmatrix}
x_{i1} \\
\vdots \\
x_{iK}
\end{pmatrix} \in \mathbb{R}^K\)</span>,</p>
<p>with probability one.</p>
<p><em>Intuition:</em> This condition means that the expected value of each error term <span class="math inline">\(\varepsilon_i\)</span> is zero, regardless of the values of the regressors <span class="math inline">\(X\)</span>. The error term is uncorrelated with both past, present, and future values of the regressors. This holds if we have fixed design (e.g. <span class="math inline">\(x_i = i\)</span>) or with i.i.d. data <span class="math inline">\((x_i, \varepsilon_i)\)</span>. In a time series context, this condition is called <strong>strong exogeneity</strong>, and implies, among other conditions, no lagged dependent variable included in X.</p>
</section>
<section class="level3" id="condition-18.3-explained">
<h3 class="anchored" data-anchor-id="condition-18.3-explained">Condition 18.3 Explained:</h3>
<p>The condition <span class="math inline">\(E(\varepsilon\varepsilon^{T}|X) = \Sigma(X)\)</span> implies that the conditional variance is finite with probability one. The <span class="math inline">\(n \times n\)</span> covariance matrix <span class="math inline">\(\Sigma(X)\)</span> can depend on the covariates <span class="math inline">\(X\)</span> and allows for correlation between error terms.</p>
</section>
<section class="level3" id="special-cases-for-sigmax">
<h3 class="anchored" data-anchor-id="special-cases-for-sigmax">Special Cases for <span class="math inline">\(\Sigma(X)\)</span>:</h3>
<ol type="1">
<li><p><strong>Homoskedastic and Uncorrelated Case:</strong></p>
<p><span class="math inline">\(\qquad \Sigma(X) = \sigma^2 I_n\)</span>,</p>
<p>where <span class="math inline">\(I_n\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix. This means the errors have constant variance (<span class="math inline">\(\sigma^2\)</span>) and are uncorrelated with each other.</p></li>
<li><p><strong>Heteroskedastic Case:</strong></p>
<p><span class="math inline">\(\qquad \Sigma(X) = \begin{pmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_n^2
\end{pmatrix}\)</span>,</p>
<p>where the <span class="math inline">\(\sigma_i^2\)</span> may all be distinct. This allows for different variances for each observation.</p></li>
<li><p><strong>Clustered or Block-Diagonal Case:</strong></p>
<p><span class="math inline">\(\qquad \Sigma(X) = \begin{pmatrix}
B_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; B_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; B_p
\end{pmatrix}\)</span>,</p>
<p>where <span class="math inline">\(B_j\)</span> are non-zero blocks with dimensions <span class="math inline">\(n_j \times n_j\)</span>, <span class="math inline">\(j=1, \dots, p\)</span>, such that <span class="math inline">\(\sum_{j=1}^{p}n_j = n\)</span>, and the off-diagonal elements are zero. This represents situations where groups of observations have correlated errors, but errors are uncorrelated across groups.</p></li>
<li><p><strong>Time Series Case (Autocorrelation):</strong> <span class="math inline">\(\Sigma(X)\)</span> may have all entries non-zero, but the entries get smaller as they move further away from the diagonal. This represents situations where closer observations in time are more correlated than those farther apart.</p></li>
</ol>
</section>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch18exercise1">
<h3 class="anchored" data-anchor-id="sec-ch18exercise1">Exercise 1</h3>
<p><a href="#sec-ch18solution1">Solution 1</a></p>
<p>Explain the meaning of the assumption <span class="math inline">\(\text{rank}(X) = K &lt; n\)</span> in the context of the linear model. What would be the implications if <span class="math inline">\(\text{rank}(X) &lt; K\)</span>?</p>
</section>
<section class="level3" id="sec-ch18exercise2">
<h3 class="anchored" data-anchor-id="sec-ch18exercise2">Exercise 2</h3>
<p><a href="#sec-ch18solution2">Solution 2</a></p>
<p>Describe the difference between assumptions A1 and A3 regarding the distribution of <span class="math inline">\(y\)</span> conditional on <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="sec-ch18exercise3">
<h3 class="anchored" data-anchor-id="sec-ch18exercise3">Exercise 3</h3>
<p><a href="#sec-ch18solution3">Solution 3</a></p>
<p>Given the model <span class="math inline">\(y = X\beta + \varepsilon\)</span>, derive the expression for <span class="math inline">\(E(\varepsilon \varepsilon^T | X)\)</span> under the assumption of homoskedasticity and no autocorrelation.</p>
</section>
<section class="level3" id="sec-ch18exercise4">
<h3 class="anchored" data-anchor-id="sec-ch18exercise4">Exercise 4</h3>
<p><a href="#sec-ch18solution4">Solution 4</a></p>
<p>Provide an example of a real-world scenario where the assumption <span class="math inline">\(E(\varepsilon|X) = 0\)</span> is likely to be violated. Explain why.</p>
</section>
<section class="level3" id="sec-ch18exercise5">
<h3 class="anchored" data-anchor-id="sec-ch18exercise5">Exercise 5</h3>
<p><a href="#sec-ch18solution5">Solution 5</a></p>
<p>Explain the concept of <strong>strong exogeneity</strong> in a time series context and how it relates to the assumption <span class="math inline">\(E(\varepsilon|X) = 0\)</span>.</p>
</section>
<section class="level3" id="sec-ch18exercise6">
<h3 class="anchored" data-anchor-id="sec-ch18exercise6">Exercise 6</h3>
<p><a href="#sec-ch18solution6">Solution 6</a></p>
<p>Consider a dataset of student test scores (<span class="math inline">\(y\)</span>) and hours studied (<span class="math inline">\(X\)</span>). Formulate a linear model, and discuss potential sources of heteroskedasticity in the error term.</p>
</section>
<section class="level3" id="sec-ch18exercise7">
<h3 class="anchored" data-anchor-id="sec-ch18exercise7">Exercise 7</h3>
<p><a href="#sec-ch18solution7">Solution 7</a></p>
<p>What does it mean for the matrix <span class="math inline">\(\Sigma(X)\)</span> to be positive definite in Assumption A2? What are the implications if <span class="math inline">\(\Sigma(X)\)</span> is not positive definite?</p>
</section>
<section class="level3" id="sec-ch18exercise8">
<h3 class="anchored" data-anchor-id="sec-ch18exercise8">Exercise 8</h3>
<p><a href="#sec-ch18solution8">Solution 8</a></p>
<p>Explain the difference between the clustered and heteroskedastic cases for the covariance matrix <span class="math inline">\(\Sigma(X)\)</span>.</p>
</section>
<section class="level3" id="sec-ch18exercise9">
<h3 class="anchored" data-anchor-id="sec-ch18exercise9">Exercise 9</h3>
<p><a href="#sec-ch18solution9">Solution 9</a></p>
<p>Suppose you are analyzing panel data on firm profits over several years. How might you expect the error terms to be correlated, and which form of <span class="math inline">\(\Sigma(X)\)</span> would be appropriate?</p>
</section>
<section class="level3" id="sec-ch18exercise10">
<h3 class="anchored" data-anchor-id="sec-ch18exercise10">Exercise 10</h3>
<p><a href="#sec-ch18solution10">Solution 10</a></p>
<p>Explain how Assumption A1, <span class="math inline">\(E(y|X) = X\beta\)</span>, implies that the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span> is linear in parameters. Provide an example of a model that is linear in parameters and one that is not.</p>
</section>
<section class="level3" id="sec-ch18exercise11">
<h3 class="anchored" data-anchor-id="sec-ch18exercise11">Exercise 11</h3>
<p><a href="#sec-ch18solution11">Solution 11</a></p>
<p>If <span class="math inline">\(y\)</span> represents household consumption and <span class="math inline">\(X\)</span> includes household income and household size, explain how you would interpret the elements of the <span class="math inline">\(\beta\)</span> vector.</p>
</section>
<section class="level3" id="sec-ch18exercise12">
<h3 class="anchored" data-anchor-id="sec-ch18exercise12">Exercise 12</h3>
<p><a href="#sec-ch18solution12">Solution 12</a></p>
<p>Explain why we model the unobservable <span class="math inline">\(\epsilon\)</span> instead of directly modeling the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="sec-ch18exercise13">
<h3 class="anchored" data-anchor-id="sec-ch18exercise13">Exercise 13</h3>
<p><a href="#sec-ch18solution13">Solution 13</a></p>
<p>Describe a situation where you would expect the errors in a linear regression model to be both heteroskedastic and autocorrelated.</p>
</section>
<section class="level3" id="sec-ch18exercise14">
<h3 class="anchored" data-anchor-id="sec-ch18exercise14">Exercise 14</h3>
<p><a href="#sec-ch18solution14">Solution 14</a></p>
<p>Why is Assumption A, specifically condition 3, considered a strong assumption? What are its advantages?</p>
</section>
<section class="level3" id="sec-ch18exercise15">
<h3 class="anchored" data-anchor-id="sec-ch18exercise15">Exercise 15</h3>
<p><a href="#sec-ch18solution15">Solution 15</a></p>
<p>Consider the model <span class="math inline">\(y_i = \beta_0 + \beta_1 x_i + \varepsilon_i\)</span>. If <span class="math inline">\(E(\varepsilon_i|x_i) = x_i\)</span>, does this model satisfy Assumption A1? Explain.</p>
</section>
<section class="level3" id="sec-ch18exercise16">
<h3 class="anchored" data-anchor-id="sec-ch18exercise16">Exercise 16</h3>
<p><a href="#sec-ch18solution16">Solution 16</a></p>
<p>Explain the implications of using Assumption A4 instead of Assumption A3 for the properties of the OLS estimator.</p>
</section>
<section class="level3" id="sec-ch18exercise17">
<h3 class="anchored" data-anchor-id="sec-ch18exercise17">Exercise 17</h3>
<p><a href="#sec-ch18solution17">Solution 17</a></p>
<p>Explain the intuitive meaning of <span class="math inline">\(E(\varepsilon|X)=0\)</span>.</p>
</section>
<section class="level3" id="sec-ch18exercise18">
<h3 class="anchored" data-anchor-id="sec-ch18exercise18">Exercise 18</h3>
<p><a href="#sec-ch18solution18">Solution 18</a></p>
<p>Suppose <span class="math inline">\(y_i\)</span> represents the sales of a store and <span class="math inline">\(X_i\)</span> is the advertising spending on day <span class="math inline">\(i\)</span>. Provide a plausible explanation for correlation between the errors of adjacent days.</p>
</section>
<section class="level3" id="sec-ch18exercise19">
<h3 class="anchored" data-anchor-id="sec-ch18exercise19">Exercise 19</h3>
<p><a href="#sec-ch18solution19">Solution 19</a></p>
<p>Give an example of a dataset where the clustered error structure would be a reasonable assumption.</p>
</section>
<section class="level3" id="sec-ch18exercise20">
<h3 class="anchored" data-anchor-id="sec-ch18exercise20">Exercise 20</h3>
<p><a href="#sec-ch18solution20">Solution 20</a></p>
<p>How does the assumption of the linear model relate to the existence of an infinity of potential outcomes?</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch18solution1">
<h3 class="anchored" data-anchor-id="sec-ch18solution1">Solution 1</h3>
<p><a href="#sec-ch18exercise1">Exercise 1</a></p>
<p>The assumption <span class="math inline">\(\text{rank}(X) = K &lt; n\)</span> means that the matrix <span class="math inline">\(X\)</span> has full column rank, and the number of columns (<span class="math inline">\(K\)</span>, representing the number of regressors plus the constant term) is less than the number of rows (<span class="math inline">\(n\)</span>, representing the number of observations).</p>
<ul>
<li><strong>Full column rank:</strong> This means that no column in <span class="math inline">\(X\)</span> can be written as a linear combination of the other columns. In other words, there is no perfect multicollinearity among the regressors. Each regressor provides unique information.</li>
<li><strong><span class="math inline">\(K &lt; n\)</span>:</strong> This condition ensures that we have more observations than parameters to estimate. This is necessary for degrees of freedom in the estimation.</li>
</ul>
<p>If <span class="math inline">\(\text{rank}(X) &lt; K\)</span>, it means that there is perfect multicollinearity. At least one regressor is a perfect linear combination of the others. This makes it impossible to uniquely identify the individual effects of the regressors (<span class="math inline">\(\beta\)</span>) because multiple combinations of coefficients would produce the same fitted values. The OLS estimator would not be unique.</p>
</section>
<section class="level3" id="sec-ch18solution2">
<h3 class="anchored" data-anchor-id="sec-ch18solution2">Solution 2</h3>
<p><a href="#sec-ch18exercise2">Exercise 2</a></p>
<ul>
<li><strong>Assumption A1:</strong> <span class="math inline">\(E(y|X) = X\beta\)</span>. This assumption only specifies the <em>conditional mean</em> of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span>. It states that the expected value of <span class="math inline">\(y\)</span> is a linear function of <span class="math inline">\(X\)</span>. It says nothing about the distribution of y other than its mean.</li>
<li><strong>Assumption A3:</strong> <span class="math inline">\(y \sim N(X\beta, \Sigma(X))\)</span>. This assumption specifies the <em>entire conditional distribution</em> of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span>. It states that <span class="math inline">\(y\)</span> is normally distributed with a mean of <span class="math inline">\(X\beta\)</span> and a variance-covariance matrix of <span class="math inline">\(\Sigma(X)\)</span>. A3 implies A1 because if <span class="math inline">\(y\)</span> is conditionally normal, its conditional expectation is the mean of that normal distribution, which is <span class="math inline">\(X\beta\)</span>.</li>
</ul>
<p>In short, A3 is a much stronger assumption than A1. A3 implies A1, but A1 does not imply A3.</p>
</section>
<section class="level3" id="sec-ch18solution3">
<h3 class="anchored" data-anchor-id="sec-ch18solution3">Solution 3</h3>
<p><a href="#sec-ch18exercise3">Exercise 3</a></p>
<p>Under homoskedasticity and no autocorrelation, the variance-covariance matrix of the error term is <span class="math inline">\(\Sigma(X) = \sigma^2 I_n\)</span>. Therefore:</p>
<p><span class="math inline">\(\qquad E(\varepsilon \varepsilon^T | X) = \Sigma(X) = \sigma^2 I_n = \begin{pmatrix}
\sigma^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma^2
\end{pmatrix}\)</span>.</p>
<p>This means:</p>
<ul>
<li><span class="math inline">\(E(\varepsilon_i^2 | X) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span> (homoskedasticity: constant variance).</li>
<li><span class="math inline">\(E(\varepsilon_i \varepsilon_j | X) = 0\)</span> for all <span class="math inline">\(i \neq j\)</span> (no autocorrelation: errors are uncorrelated).</li>
</ul>
</section>
<section class="level3" id="sec-ch18solution4">
<h3 class="anchored" data-anchor-id="sec-ch18solution4">Solution 4</h3>
<p><a href="#sec-ch18exercise4">Exercise 4</a></p>
<p>Consider a model where <span class="math inline">\(y\)</span> is income and <span class="math inline">\(X\)</span> includes years of education. It’s possible that omitted variables, such as innate ability, are correlated with both education and income. People with higher innate ability might choose to pursue more education <em>and</em> also earn higher incomes, even holding education constant.</p>
<p>If innate ability is omitted, it becomes part of the error term <span class="math inline">\(\varepsilon\)</span>. Since innate ability is positively correlated with education (<span class="math inline">\(X\)</span>), we have <span class="math inline">\(E(\varepsilon|X) \neq 0\)</span>. Specifically, we would expect <span class="math inline">\(E(\varepsilon|X)\)</span> to be an increasing function of <span class="math inline">\(X\)</span> (higher education is associated with a more positive error term due to higher omitted ability).</p>
</section>
<section class="level3" id="sec-ch18solution5">
<h3 class="anchored" data-anchor-id="sec-ch18solution5">Solution 5</h3>
<p><a href="#sec-ch18exercise5">Exercise 5</a></p>
<p>In a time series setting, <strong>strong exogeneity</strong> means that the error term at any time period <span class="math inline">\(t\)</span> is uncorrelated with the regressors in <em>all</em> time periods (past, present, and future). Formally:</p>
<p><span class="math inline">\(\qquad E(\varepsilon_t | X_1, \dots, X_n) = 0\)</span>,</p>
<p>where <span class="math inline">\(X_i\)</span> is the vector of regressors at time <span class="math inline">\(i\)</span>.</p>
<p>This is a stronger condition than simply requiring contemporaneous exogeneity, which would only require <span class="math inline">\(E(\varepsilon_t | X_t) = 0\)</span>. Strong exogeneity rules out feedback from <span class="math inline">\(y\)</span> to future values of <span class="math inline">\(X\)</span>. This relates to the text as mentioned that <span class="math inline">\(E(\epsilon|X) = 0\)</span> can be expressed as <span class="math inline">\(E \left( \varepsilon_i | X_1, \dots, X_n  \right) = 0\)</span>.</p>
</section>
<section class="level3" id="sec-ch18solution6">
<h3 class="anchored" data-anchor-id="sec-ch18solution6">Solution 6</h3>
<p><a href="#sec-ch18exercise6">Exercise 6</a></p>
<p>Linear model:</p>
<p><span class="math inline">\(\qquad \text{Test Score}_i = \beta_0 + \beta_1 \text{Hours Studied}_i + \varepsilon_i\)</span>.</p>
<p><strong>Heteroskedasticity:</strong> The variance of the error term might not be constant. For example:</p>
<ul>
<li>Students who study very little might have test scores clustered close to zero (low variance).</li>
<li>Students who study a moderate amount might have a wide range of scores (high variance) due to differences in learning ability, test-taking skills, etc.</li>
<li>Students who study a lot might have scores clustered near the top (low variance).</li>
</ul>
<p>Thus, <span class="math inline">\(\text{Var}(\varepsilon_i | \text{Hours Studied}_i)\)</span> would not be constant, violating the homoskedasticity assumption.</p>
</section>
<section class="level3" id="sec-ch18solution7">
<h3 class="anchored" data-anchor-id="sec-ch18solution7">Solution 7</h3>
<p><a href="#sec-ch18exercise7">Exercise 7</a></p>
<p>A matrix <span class="math inline">\(\Sigma(X)\)</span> is <strong>positive definite</strong> if for any non-zero vector <span class="math inline">\(z\)</span>, the quadratic form <span class="math inline">\(z^T \Sigma(X) z\)</span> is strictly positive:</p>
<p><span class="math inline">\(\qquad z^T \Sigma(X) z &gt; 0\)</span> for all <span class="math inline">\(z \neq 0\)</span>.</p>
<p>Implications:</p>
<ol type="1">
<li><strong>Variances are positive:</strong> All diagonal elements of <span class="math inline">\(\Sigma(X)\)</span> (which represent variances) must be positive.</li>
<li><strong>Invertibility:</strong> A positive definite matrix is always invertible (non-singular). This is important for many statistical procedures, including OLS estimation.</li>
</ol>
<p>If <span class="math inline">\(\Sigma(X)\)</span> is <em>not</em> positive definite, several problems arise:</p>
<ul>
<li><strong>Zero or negative variances:</strong> It could imply zero or negative variances for some linear combinations of the error terms, which is nonsensical.</li>
<li><strong>Singularity:</strong> <span class="math inline">\(\Sigma(X)\)</span> might be singular (non-invertible), leading to problems in estimation.</li>
</ul>
</section>
<section class="level3" id="sec-ch18solution8">
<h3 class="anchored" data-anchor-id="sec-ch18solution8">Solution 8</h3>
<p><a href="#sec-ch18exercise8">Exercise 8</a></p>
<ul>
<li><p><strong>Heteroskedastic Case:</strong> The variances of the error terms can be different for <em>each</em> observation. The covariance matrix is diagonal, meaning there is no correlation between errors of different observations.</p>
<p><span class="math inline">\(\qquad \Sigma(X) = \begin{pmatrix}
\sigma_1^2 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; \sigma_2^2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; \sigma_n^2
\end{pmatrix}\)</span></p></li>
<li><p><strong>Clustered Case:</strong> Observations are grouped into clusters. Within each cluster, the errors can be correlated, and the variances can differ across clusters. However, errors from different clusters are uncorrelated. The covariance matrix is block-diagonal.</p>
<p><span class="math inline">\(\qquad \Sigma(X) = \begin{pmatrix}
B_1 &amp; 0 &amp; \cdots &amp; 0 \\
0 &amp; B_2 &amp; \cdots &amp; 0 \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
0 &amp; 0 &amp; \cdots &amp; B_p
\end{pmatrix}\)</span></p>
<p>Each <span class="math inline">\(B_j\)</span> is a submatrix representing the covariance structure within cluster <span class="math inline">\(j\)</span>.</p></li>
</ul>
<p>The key difference is that heteroskedasticity allows for individual variation in variances, while the clustered case allows for group-wise correlation and variance differences.</p>
</section>
<section class="level3" id="sec-ch18solution9">
<h3 class="anchored" data-anchor-id="sec-ch18solution9">Solution 9</h3>
<p><a href="#sec-ch18exercise9">Exercise 9</a></p>
<p>In panel data on firm profits, we might expect errors to be correlated <em>within</em> each firm over time (autocorrelation). This is because unobserved factors affecting a firm’s profitability (e.g., management quality, firm-specific shocks) are likely to persist over time. However, we might assume that errors are uncorrelated <em>across</em> different firms.</p>
<p>The appropriate form of <span class="math inline">\(\Sigma(X)\)</span> would be the <strong>clustered</strong> case, where each firm represents a cluster. Each block <span class="math inline">\(B_j\)</span> on the diagonal of <span class="math inline">\(\Sigma(X)\)</span> would represent the covariance matrix of the error terms for firm <span class="math inline">\(j\)</span>. These blocks could potentially have an autocorrelated structure.</p>
</section>
<section class="level3" id="sec-ch18solution10">
<h3 class="anchored" data-anchor-id="sec-ch18solution10">Solution 10</h3>
<p><a href="#sec-ch18exercise10">Exercise 10</a></p>
<p><span class="math inline">\(E(y|X) = X\beta\)</span> implies linearity in parameters because the expected value of <span class="math inline">\(y\)</span> is a weighted sum of the elements of <span class="math inline">\(X\)</span>, where the weights are the <em>parameters</em> in <span class="math inline">\(\beta\)</span>. The parameters (<span class="math inline">\(\beta\)</span>) enter the equation linearly; they are not raised to any powers, multiplied together, or transformed in any non-linear way.</p>
<ul>
<li><strong>Linear in parameters:</strong> <span class="math inline">\(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2}^2 + \varepsilon_i\)</span>. Even though <span class="math inline">\(x_{i2}\)</span> is squared, the <em>parameter</em> <span class="math inline">\(\beta_2\)</span> is not.</li>
<li><strong>Not linear in parameters:</strong> <span class="math inline">\(y_i = \beta_0 + \beta_1 x_{i1} + \beta_2^{\beta_3}x_{i2} + \varepsilon_i\)</span>. Here the parameter <span class="math inline">\(\beta_2\)</span> is raised to the power of parameter <span class="math inline">\(\beta_3\)</span> which is a non-linear transformation of parameters.</li>
</ul>
</section>
<section class="level3" id="sec-ch18solution11">
<h3 class="anchored" data-anchor-id="sec-ch18solution11">Solution 11</h3>
<p><a href="#sec-ch18solution11">Exercise 11</a></p>
<p>The model would be:</p>
<p><span class="math inline">\(\qquad \text{Consumption}_i = \beta_0 + \beta_1 \text{Income}_i + \beta_2 \text{Household Size}_i + \varepsilon_i\)</span>.</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span>: The intercept, representing autonomous consumption (consumption when income and household size are zero).</li>
<li><span class="math inline">\(\beta_1\)</span>: The marginal propensity to consume out of income. It represents the expected increase in consumption for a one-unit increase in income, holding household size constant.</li>
<li><span class="math inline">\(\beta_2\)</span>: The expected change in consumption for a one-unit increase in household size, holding income constant.</li>
</ul>
</section>
<section class="level3" id="sec-ch18solution12">
<h3 class="anchored" data-anchor-id="sec-ch18solution12">Solution 12</h3>
<p><a href="#sec-ch18exercise12">Exercise 12</a></p>
<p>We model the unobservable <span class="math inline">\(\epsilon\)</span> because it represents the combined effects of all factors influencing <span class="math inline">\(y\)</span> that are <em>not</em> included in <span class="math inline">\(X\)</span>. It’s a convenient way to capture the inherent randomness and unexplained variation in <span class="math inline">\(y\)</span>. Directly modeling the <em>conditional</em> distribution <span class="math inline">\(P(y|X)\)</span> would be very complex and might require strong distributional assumptions that we don’t want to, or can’t, make. By modeling the error we can focus on <span class="math inline">\(E(y|X)\)</span> instead of the complete distribution, as a first step.</p>
</section>
<section class="level3" id="sec-ch18solution13">
<h3 class="anchored" data-anchor-id="sec-ch18solution13">Solution 13</h3>
<p><a href="#sec-ch18solution13">Exercise 13</a></p>
<p>Consider daily stock prices (<span class="math inline">\(y\)</span>) regressed on some economic indicators (<span class="math inline">\(X\)</span>).</p>
<ul>
<li><strong>Heteroskedasticity:</strong> Volatility in stock markets tends to cluster. Days with large price swings (either positive or negative) are often followed by other days with large swings. Thus, the variance of the error term might be higher on days following large market movements.</li>
<li><strong>Autocorrelation:</strong> Unobserved factors influencing stock prices (e.g., news, investor sentiment) can persist for several days. Therefore, the error term on one day is likely to be correlated with the error term on the following day.</li>
</ul>
</section>
<section class="level3" id="sec-ch18solution14">
<h3 class="anchored" data-anchor-id="sec-ch18solution14">Solution 14</h3>
<p><a href="#sec-ch18exercise14">Exercise 14</a></p>
<p>Assumption A, condition 3 (<span class="math inline">\(y \sim N(X\beta, \Sigma(X))\)</span>) is strong because it assumes that the conditional distribution of <span class="math inline">\(y\)</span> is <em>normal</em>. This is a specific distributional assumption that may not hold in all real-world situations. Many variables are not normally distributed.</p>
<p>Advantages:</p>
<ul>
<li><strong>Simplifies inference:</strong> Normality makes statistical inference (hypothesis testing, confidence intervals) much easier. Many statistical results are derived under the assumption of normality.</li>
<li><strong>Maximum Likelihood:</strong> With normality the OLS estimator coincides with the Maximum Likelihood Estimator (MLE).</li>
</ul>
</section>
<section class="level3" id="sec-ch18solution15">
<h3 class="anchored" data-anchor-id="sec-ch18solution15">Solution 15</h3>
<p><a href="#sec-ch18solution15">Exercise 15</a></p>
<p>No, this model does <em>not</em> satisfy Assumption A1. Assumption A1 requires <span class="math inline">\(E(\varepsilon|X) = 0\)</span>. In this case, <span class="math inline">\(E(\varepsilon_i|x_i) = x_i\)</span>, which is not zero unless <span class="math inline">\(x_i = 0\)</span>. The expected value of the error term depends on the value of the regressor, violating the assumption.</p>
</section>
<section class="level3" id="sec-ch18solution16">
<h3 class="anchored" data-anchor-id="sec-ch18solution16">Solution 16</h3>
<p><a href="#sec-ch18solution16">Exercise 16</a></p>
<p>Assumption A4 (<span class="math inline">\(y \sim N(X\beta, \sigma^2 I)\)</span>) assumes both normality and homoskedasticity (and no autocorrelation) of the errors. Assumption A3 (<span class="math inline">\(y \sim N(X\beta, \Sigma(X))\)</span>) only assumes normality, allowing for a more general covariance structure <span class="math inline">\(\Sigma(X)\)</span>.</p>
<p>Implications for OLS:</p>
<ul>
<li><strong>Under A4:</strong> The OLS estimator is the Best Linear Unbiased Estimator (BLUE) and also the Maximum Likelihood Estimator (MLE). Its standard errors are straightforward to calculate.</li>
<li><strong>Under A3:</strong> The OLS estimator is <em>still</em> unbiased and consistent, but it is no longer necessarily the most efficient (it’s not BLUE). We need to use different formulas for standard errors (e.g., robust standard errors) to account for heteroskedasticity or autocorrelation.</li>
</ul>
</section>
<section class="level3" id="sec-ch18solution17">
<h3 class="anchored" data-anchor-id="sec-ch18solution17">Solution 17</h3>
<p><a href="#sec-ch18solution17">Exercise 17</a></p>
<p><span class="math inline">\(E(\varepsilon|X) = 0\)</span> means that, on average, the errors are zero for <em>any</em> given value of the regressors <span class="math inline">\(X\)</span>. The unobserved factors captured by <span class="math inline">\(\varepsilon\)</span> are not systematically related to the regressors. There’s no systematic over- or under-prediction for any particular values of <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="sec-ch18solution18">
<h3 class="anchored" data-anchor-id="sec-ch18solution18">Solution 18</h3>
<p><a href="#sec-ch18solution18">Exercise 18</a></p>
<p>Suppose there’s a major, unexpected event (e.g., a product recall or a positive news story) that affects store sales on day <span class="math inline">\(i\)</span>. This event is not included in <span class="math inline">\(X_i\)</span> (advertising spending), so it will be part of the error term <span class="math inline">\(\varepsilon_i\)</span>. If the effects of this event linger for several days (e.g., customers continue to avoid the store after the recall or continue to be drawn in by the positive news), then <span class="math inline">\(\varepsilon_{i+1}\)</span> will also be affected. This creates a positive correlation between <span class="math inline">\(\varepsilon_i\)</span> and <span class="math inline">\(\varepsilon_{i+1}\)</span>.</p>
</section>
<section class="level3" id="sec-ch18solution19">
<h3 class="anchored" data-anchor-id="sec-ch18solution19">Solution 19</h3>
<p><a href="#sec-ch18solution19">Exercise 19</a></p>
<p>A dataset of student test scores within classrooms across multiple schools. Students within the same classroom (cluster) are likely to have correlated errors due to shared factors like teacher quality, classroom environment, and peer effects. However, students in different classrooms (different clusters) are less likely to have correlated errors.</p>
</section>
<section class="level3" id="sec-ch18solution20">
<h3 class="anchored" data-anchor-id="sec-ch18solution20">Solution 20</h3>
<p><a href="#sec-ch18solution20">Exercise 20</a></p>
<p>The linear model assumes that the observed data <span class="math inline">\((y, X)\)</span> is just <em>one</em> possible realization from a random process. There’s an underlying data-generating process, and we only see one draw from it. There’s an infinity of <em>potential</em> outcomes that we <em>could</em> have observed, but we only observe one. The error term <span class="math inline">\(\varepsilon\)</span> captures the difference between the observed outcome and the expected outcome given <span class="math inline">\(X\)</span>, representing the influence of unobserved factors and randomness in this data-generating process.</p>
</section>
</section>
<section class="level2" id="r-script-examples">
<h2 class="anchored" data-anchor-id="r-script-examples">R Script Examples</h2>
<section class="level3" id="r-script-1-simulating-a-linear-model-with-homoskedastic-errors">
<h3 class="anchored" data-anchor-id="r-script-1-simulating-a-linear-model-with-homoskedastic-errors">R Script 1: Simulating a Linear Model with Homoskedastic Errors</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="fu">library</span>(MASS) <span class="co"># For mvrnorm function</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'MASS'

The following object is masked from 'package:dplyr':

    select</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a aria-hidden="true" href="#cb5-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb5-2"><a aria-hidden="true" href="#cb5-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb5-3"><a aria-hidden="true" href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a aria-hidden="true" href="#cb5-4" tabindex="-1"></a><span class="co"># Number of observations</span></span>
<span id="cb5-5"><a aria-hidden="true" href="#cb5-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb5-6"><a aria-hidden="true" href="#cb5-6" tabindex="-1"></a></span>
<span id="cb5-7"><a aria-hidden="true" href="#cb5-7" tabindex="-1"></a><span class="co"># Number of regressors (excluding the intercept)</span></span>
<span id="cb5-8"><a aria-hidden="true" href="#cb5-8" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb5-9"><a aria-hidden="true" href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a aria-hidden="true" href="#cb5-10" tabindex="-1"></a><span class="co"># Generate regressors (X)</span></span>
<span id="cb5-11"><a aria-hidden="true" href="#cb5-11" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n <span class="sc">*</span> k), <span class="at">nrow =</span> n, <span class="at">ncol =</span> k)</span>
<span id="cb5-12"><a aria-hidden="true" href="#cb5-12" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X) <span class="co"># Add a column of 1s for the intercept</span></span>
<span id="cb5-13"><a aria-hidden="true" href="#cb5-13" tabindex="-1"></a></span>
<span id="cb5-14"><a aria-hidden="true" href="#cb5-14" tabindex="-1"></a><span class="co"># True coefficients (beta)</span></span>
<span id="cb5-15"><a aria-hidden="true" href="#cb5-15" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb5-16"><a aria-hidden="true" href="#cb5-16" tabindex="-1"></a></span>
<span id="cb5-17"><a aria-hidden="true" href="#cb5-17" tabindex="-1"></a><span class="co"># Error variance (sigma^2)</span></span>
<span id="cb5-18"><a aria-hidden="true" href="#cb5-18" tabindex="-1"></a>sigma2 <span class="ot">&lt;-</span> <span class="dv">4</span></span>
<span id="cb5-19"><a aria-hidden="true" href="#cb5-19" tabindex="-1"></a></span>
<span id="cb5-20"><a aria-hidden="true" href="#cb5-20" tabindex="-1"></a><span class="co"># Generate errors (epsilon) from a normal distribution with mean 0 and variance sigma2</span></span>
<span id="cb5-21"><a aria-hidden="true" href="#cb5-21" tabindex="-1"></a><span class="co">#   This satisfies the homoskedasticity assumption (Sigma(X) = sigma2 * I)</span></span>
<span id="cb5-22"><a aria-hidden="true" href="#cb5-22" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2))</span>
<span id="cb5-23"><a aria-hidden="true" href="#cb5-23" tabindex="-1"></a></span>
<span id="cb5-24"><a aria-hidden="true" href="#cb5-24" tabindex="-1"></a><span class="co"># Generate dependent variable (y)</span></span>
<span id="cb5-25"><a aria-hidden="true" href="#cb5-25" tabindex="-1"></a><span class="co"># y = X %*% beta + epsilon corresponds to the linear model y = Xβ + ε</span></span>
<span id="cb5-26"><a aria-hidden="true" href="#cb5-26" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> epsilon</span>
<span id="cb5-27"><a aria-hidden="true" href="#cb5-27" tabindex="-1"></a></span>
<span id="cb5-28"><a aria-hidden="true" href="#cb5-28" tabindex="-1"></a><span class="co"># Combine into a data frame</span></span>
<span id="cb5-29"><a aria-hidden="true" href="#cb5-29" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">X =</span> X[, <span class="sc">-</span><span class="dv">1</span>]) <span class="co"># Exclude the intercept column</span></span>
<span id="cb5-30"><a aria-hidden="true" href="#cb5-30" tabindex="-1"></a></span>
<span id="cb5-31"><a aria-hidden="true" href="#cb5-31" tabindex="-1"></a><span class="co"># Visualize the relationship between y and one of the regressors (X.1)</span></span>
<span id="cb5-32"><a aria-hidden="true" href="#cb5-32" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> X<span class="fl">.1</span>, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb5-33"><a aria-hidden="true" href="#cb5-33" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb5-34"><a aria-hidden="true" href="#cb5-34" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span> <span class="co"># Add the true regression line</span></span>
<span id="cb5-35"><a aria-hidden="true" href="#cb5-35" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Simulated Linear Model with Homoskedastic Errors"</span>,</span>
<span id="cb5-36"><a aria-hidden="true" href="#cb5-36" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"X1"</span>,</span>
<span id="cb5-37"><a aria-hidden="true" href="#cb5-37" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"y"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap18_files/figure-html/unnamed-chunk-1-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong> We load the necessary libraries (<code>tidyverse</code> for data manipulation and visualization, <code>MASS</code> for multivariate normal simulation). We set a seed for reproducibility.</li>
<li><strong>Define Parameters:</strong> We define the number of observations (<code>n</code>), the number of regressors (<code>k</code>), the true coefficient vector (<code>beta</code>), and the error variance (<code>sigma2</code>).</li>
<li><strong>Generate Regressors (X):</strong> We generate the regressor matrix <code>X</code>. <code>runif(n * k)</code> creates a vector of <code>n*k</code> uniformly distributed random numbers, which are then arranged into an <code>n x k</code> matrix. We then <code>cbind</code> a column of 1s to <code>X</code> to represent the intercept term. This step relates to the text where it defines <span class="math inline">\(X \in \mathbb{R}^{n \times (K+1)}\)</span>.</li>
<li><strong>Generate Errors (ε):</strong> We generate the error terms, <code>epsilon</code>, from a normal distribution with mean 0 and standard deviation <code>sqrt(sigma2)</code>. This ensures <span class="math inline">\(E(\varepsilon|X) = 0\)</span> (Assumption A1) and <span class="math inline">\(\text{Var}(\varepsilon|X) = \sigma^2 I_n\)</span> (homoskedasticity, Assumption A2 specialized to equation 18.4).</li>
<li><strong>Generate Dependent Variable (y):</strong> We generate the dependent variable <code>y</code> using the linear model equation: <span class="math inline">\(y = X\beta + \varepsilon\)</span>. This corresponds directly to the core equation of the linear model in the text.</li>
<li><strong>Combine into Data Frame:</strong> We combine <code>y</code> and <code>X</code> into a data frame for easier manipulation.</li>
<li><strong>Visualization:</strong> We create a scatter plot of <code>y</code> against one of the regressors (<code>X.1</code>) and add a smoothed line using <code>geom_smooth(method = "lm")</code>, which fits a linear model. This visualizes the linear relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span>, reflecting <span class="math inline">\(E(y|X) = X\beta\)</span> (Assumption A1).</li>
</ol>
</section>
<section class="level3" id="r-script-2-simulating-a-linear-model-with-heteroskedastic-errors">
<h3 class="anchored" data-anchor-id="r-script-2-simulating-a-linear-model-with-heteroskedastic-errors">R Script 2: Simulating a Linear Model with Heteroskedastic Errors</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a aria-hidden="true" href="#cb7-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb7-2"><a aria-hidden="true" href="#cb7-2" tabindex="-1"></a></span>
<span id="cb7-3"><a aria-hidden="true" href="#cb7-3" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb7-4"><a aria-hidden="true" href="#cb7-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb7-5"><a aria-hidden="true" href="#cb7-5" tabindex="-1"></a></span>
<span id="cb7-6"><a aria-hidden="true" href="#cb7-6" tabindex="-1"></a><span class="co"># Number of observations</span></span>
<span id="cb7-7"><a aria-hidden="true" href="#cb7-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb7-8"><a aria-hidden="true" href="#cb7-8" tabindex="-1"></a></span>
<span id="cb7-9"><a aria-hidden="true" href="#cb7-9" tabindex="-1"></a><span class="co"># Number of regressors (excluding the intercept)</span></span>
<span id="cb7-10"><a aria-hidden="true" href="#cb7-10" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb7-11"><a aria-hidden="true" href="#cb7-11" tabindex="-1"></a></span>
<span id="cb7-12"><a aria-hidden="true" href="#cb7-12" tabindex="-1"></a><span class="co"># Generate regressors (X)</span></span>
<span id="cb7-13"><a aria-hidden="true" href="#cb7-13" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n <span class="sc">*</span> k), <span class="at">nrow =</span> n, <span class="at">ncol =</span> k)</span>
<span id="cb7-14"><a aria-hidden="true" href="#cb7-14" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X) <span class="co"># Add a column of 1s for the intercept</span></span>
<span id="cb7-15"><a aria-hidden="true" href="#cb7-15" tabindex="-1"></a></span>
<span id="cb7-16"><a aria-hidden="true" href="#cb7-16" tabindex="-1"></a><span class="co"># True coefficients (beta)</span></span>
<span id="cb7-17"><a aria-hidden="true" href="#cb7-17" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb7-18"><a aria-hidden="true" href="#cb7-18" tabindex="-1"></a></span>
<span id="cb7-19"><a aria-hidden="true" href="#cb7-19" tabindex="-1"></a><span class="co"># Generate heteroskedastic errors</span></span>
<span id="cb7-20"><a aria-hidden="true" href="#cb7-20" tabindex="-1"></a><span class="co">#   Here, the variance depends on the first regressor (X[, 2])</span></span>
<span id="cb7-21"><a aria-hidden="true" href="#cb7-21" tabindex="-1"></a>sigma2_i <span class="ot">&lt;-</span> <span class="fu">exp</span>(X[, <span class="dv">2</span>])  <span class="co"># Variance increases exponentially with X[, 2]</span></span>
<span id="cb7-22"><a aria-hidden="true" href="#cb7-22" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="fu">sqrt</span>(sigma2_i))</span>
<span id="cb7-23"><a aria-hidden="true" href="#cb7-23" tabindex="-1"></a></span>
<span id="cb7-24"><a aria-hidden="true" href="#cb7-24" tabindex="-1"></a><span class="co"># Generate dependent variable (y)</span></span>
<span id="cb7-25"><a aria-hidden="true" href="#cb7-25" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> epsilon</span>
<span id="cb7-26"><a aria-hidden="true" href="#cb7-26" tabindex="-1"></a></span>
<span id="cb7-27"><a aria-hidden="true" href="#cb7-27" tabindex="-1"></a><span class="co"># Combine into a data frame</span></span>
<span id="cb7-28"><a aria-hidden="true" href="#cb7-28" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">X =</span> X[, <span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb7-29"><a aria-hidden="true" href="#cb7-29" tabindex="-1"></a></span>
<span id="cb7-30"><a aria-hidden="true" href="#cb7-30" tabindex="-1"></a><span class="co"># Visualize the heteroskedasticity</span></span>
<span id="cb7-31"><a aria-hidden="true" href="#cb7-31" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> X<span class="fl">.1</span>, <span class="at">y =</span> y)) <span class="sc">+</span></span>
<span id="cb7-32"><a aria-hidden="true" href="#cb7-32" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb7-33"><a aria-hidden="true" href="#cb7-33" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Simulated Linear Model with Heteroskedastic Errors"</span>,</span>
<span id="cb7-34"><a aria-hidden="true" href="#cb7-34" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"X1"</span>,</span>
<span id="cb7-35"><a aria-hidden="true" href="#cb7-35" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"y"</span>) <span class="sc">+</span></span>
<span id="cb7-36"><a aria-hidden="true" href="#cb7-36" tabindex="-1"></a>    <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se=</span><span class="cn">FALSE</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap18_files/figure-html/unnamed-chunk-2-1.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a aria-hidden="true" href="#cb9-1" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> X<span class="fl">.1</span>, <span class="at">y =</span> epsilon<span class="sc">^</span><span class="dv">2</span>)) <span class="sc">+</span></span>
<span id="cb9-2"><a aria-hidden="true" href="#cb9-2" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb9-3"><a aria-hidden="true" href="#cb9-3" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Squared Residuals vs. X1 (Illustrating Heteroskedasticity)"</span>,</span>
<span id="cb9-4"><a aria-hidden="true" href="#cb9-4" tabindex="-1"></a>         <span class="at">x =</span> <span class="st">"X1"</span>,</span>
<span id="cb9-5"><a aria-hidden="true" href="#cb9-5" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Squared Residuals"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap18_files/figure-html/unnamed-chunk-2-2.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong> Similar to the previous example, we load <code>tidyverse</code>, set a seed, and define <code>n</code>, <code>k</code>, <code>X</code>, and <code>beta</code>.</li>
<li><strong>Generate Heteroskedastic Errors:</strong> This is the key difference. Instead of a constant <code>sigma2</code>, we create <code>sigma2_i</code>, which varies for each observation. Here, we make the variance a function of the first regressor (<code>X[, 2]</code>): <code>sigma2_i &lt;- exp(X[, 2])</code>. This means the variance of the error term increases exponentially with the value of <code>X[, 2]</code>. We then generate <code>epsilon</code> from a normal distribution with mean 0 and standard deviation <code>sqrt(sigma2_i)</code>. This satisfies Assumption A2, where <span class="math inline">\(\text{Var}(y|X) = \Sigma(X)\)</span>, but <em>not</em> the homoskedastic special case. <span class="math inline">\(\Sigma(X)\)</span> is now a diagonal matrix with the <code>sigma2_i</code> values on the diagonal.</li>
<li><strong>Generate Dependent Variable (y):</strong> Same as before: <span class="math inline">\(y = X\beta + \varepsilon\)</span>.</li>
<li><strong>Combine into Data Frame:</strong> We combine y and X into a data frame.</li>
<li><strong>Visualization:</strong>
<ul>
<li>The first plot shows the relationship between <span class="math inline">\(y\)</span> and <code>X.1</code>. The heteroskedasticity might not be immediately obvious from this plot alone.</li>
<li>The second plot is more revealing. We plot the <em>squared residuals</em> (<code>epsilon^2</code>) against <code>X.1</code>. Since the variance of <span class="math inline">\(\varepsilon\)</span> depends on <code>X.1</code>, we expect to see a pattern in this plot (in this case, an increasing spread), indicating heteroskedasticity.</li>
</ul></li>
</ol>
</section>
<section class="level3" id="r-script-3-simulating-a-linear-model-with-autocorrelated-errors-ar1">
<h3 class="anchored" data-anchor-id="r-script-3-simulating-a-linear-model-with-autocorrelated-errors-ar1">R Script 3: Simulating a Linear Model with Autocorrelated Errors (AR(1))</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a aria-hidden="true" href="#cb10-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb10-2"><a aria-hidden="true" href="#cb10-2" tabindex="-1"></a></span>
<span id="cb10-3"><a aria-hidden="true" href="#cb10-3" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb10-4"><a aria-hidden="true" href="#cb10-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb10-5"><a aria-hidden="true" href="#cb10-5" tabindex="-1"></a></span>
<span id="cb10-6"><a aria-hidden="true" href="#cb10-6" tabindex="-1"></a><span class="co"># Number of observations</span></span>
<span id="cb10-7"><a aria-hidden="true" href="#cb10-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb10-8"><a aria-hidden="true" href="#cb10-8" tabindex="-1"></a></span>
<span id="cb10-9"><a aria-hidden="true" href="#cb10-9" tabindex="-1"></a><span class="co"># Number of regressors (excluding the intercept)</span></span>
<span id="cb10-10"><a aria-hidden="true" href="#cb10-10" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb10-11"><a aria-hidden="true" href="#cb10-11" tabindex="-1"></a></span>
<span id="cb10-12"><a aria-hidden="true" href="#cb10-12" tabindex="-1"></a><span class="co"># Generate regressors (X)</span></span>
<span id="cb10-13"><a aria-hidden="true" href="#cb10-13" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n <span class="sc">*</span> k), <span class="at">nrow =</span> n, <span class="at">ncol =</span> k)</span>
<span id="cb10-14"><a aria-hidden="true" href="#cb10-14" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X) <span class="co"># Add a column of 1s for the intercept</span></span>
<span id="cb10-15"><a aria-hidden="true" href="#cb10-15" tabindex="-1"></a></span>
<span id="cb10-16"><a aria-hidden="true" href="#cb10-16" tabindex="-1"></a><span class="co"># True coefficients (beta)</span></span>
<span id="cb10-17"><a aria-hidden="true" href="#cb10-17" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="fl">0.5</span>)</span>
<span id="cb10-18"><a aria-hidden="true" href="#cb10-18" tabindex="-1"></a></span>
<span id="cb10-19"><a aria-hidden="true" href="#cb10-19" tabindex="-1"></a><span class="co"># Autocorrelation parameter (rho)</span></span>
<span id="cb10-20"><a aria-hidden="true" href="#cb10-20" tabindex="-1"></a>rho <span class="ot">&lt;-</span> <span class="fl">0.7</span></span>
<span id="cb10-21"><a aria-hidden="true" href="#cb10-21" tabindex="-1"></a></span>
<span id="cb10-22"><a aria-hidden="true" href="#cb10-22" tabindex="-1"></a><span class="co"># Generate autocorrelated errors</span></span>
<span id="cb10-23"><a aria-hidden="true" href="#cb10-23" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb10-24"><a aria-hidden="true" href="#cb10-24" tabindex="-1"></a>epsilon[<span class="dv">1</span>] <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(<span class="dv">1</span>)  <span class="co"># Initialize the first error term</span></span>
<span id="cb10-25"><a aria-hidden="true" href="#cb10-25" tabindex="-1"></a><span class="cf">for</span> (t <span class="cf">in</span> <span class="dv">2</span><span class="sc">:</span>n) {</span>
<span id="cb10-26"><a aria-hidden="true" href="#cb10-26" tabindex="-1"></a>  epsilon[t] <span class="ot">&lt;-</span> rho <span class="sc">*</span> epsilon[t<span class="dv">-1</span>] <span class="sc">+</span> <span class="fu">rnorm</span>(<span class="dv">1</span>) <span class="co"># AR(1) process</span></span>
<span id="cb10-27"><a aria-hidden="true" href="#cb10-27" tabindex="-1"></a>}</span>
<span id="cb10-28"><a aria-hidden="true" href="#cb10-28" tabindex="-1"></a></span>
<span id="cb10-29"><a aria-hidden="true" href="#cb10-29" tabindex="-1"></a><span class="co"># Generate dependent variable (y)</span></span>
<span id="cb10-30"><a aria-hidden="true" href="#cb10-30" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> epsilon</span>
<span id="cb10-31"><a aria-hidden="true" href="#cb10-31" tabindex="-1"></a></span>
<span id="cb10-32"><a aria-hidden="true" href="#cb10-32" tabindex="-1"></a><span class="co"># Combine into a data frame</span></span>
<span id="cb10-33"><a aria-hidden="true" href="#cb10-33" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">X =</span> X[, <span class="sc">-</span><span class="dv">1</span>], <span class="at">Time =</span> <span class="dv">1</span><span class="sc">:</span>n)</span>
<span id="cb10-34"><a aria-hidden="true" href="#cb10-34" tabindex="-1"></a></span>
<span id="cb10-35"><a aria-hidden="true" href="#cb10-35" tabindex="-1"></a><span class="co"># Visualize the autocorrelation</span></span>
<span id="cb10-36"><a aria-hidden="true" href="#cb10-36" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> Time, <span class="at">y =</span> epsilon)) <span class="sc">+</span></span>
<span id="cb10-37"><a aria-hidden="true" href="#cb10-37" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb10-38"><a aria-hidden="true" href="#cb10-38" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Autocorrelated Errors over Time"</span>,</span>
<span id="cb10-39"><a aria-hidden="true" href="#cb10-39" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Time"</span>,</span>
<span id="cb10-40"><a aria-hidden="true" href="#cb10-40" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Error Term"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap18_files/figure-html/unnamed-chunk-3-1.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a aria-hidden="true" href="#cb11-1" tabindex="-1"></a><span class="fu">acf</span>(epsilon, <span class="at">main =</span> <span class="st">"ACF of Error Terms"</span>) <span class="co"># Autocorrelation function</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap18_files/figure-html/unnamed-chunk-3-2.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong> We load <code>tidyverse</code>, set the seed, define <code>n</code>, <code>k</code>, <code>X</code>, and <code>beta</code> as before.</li>
<li><strong>Autocorrelation Parameter (ρ):</strong> We define <code>rho</code>, the autocorrelation coefficient, which determines the strength of the correlation between consecutive error terms.</li>
<li><strong>Generate Autocorrelated Errors:</strong> This is the core part. We simulate an AR(1) process:
<ul>
<li>We initialize the first error term, <code>epsilon[1]</code>, with a standard normal random variable.</li>
<li>For subsequent time periods (<code>t in 2:n</code>), we generate <code>epsilon[t]</code> using the AR(1) equation: <span class="math inline">\(\varepsilon_t = \rho \varepsilon_{t-1} + u_t\)</span>, where <span class="math inline">\(u_t\)</span> is a standard normal random variable. This means the current error term is equal to <code>rho</code> times the previous error term plus a new random shock. This corresponds to a special case of Assumption A2 where the off-diagonal entries of <span class="math inline">\(\Sigma(X)\)</span> get smaller the further apart they are.</li>
</ul></li>
<li><strong>Generate Dependent Variable (y):</strong> We generate the dependent variable: <span class="math inline">\(y = X\beta + \varepsilon\)</span>.</li>
<li><strong>Combine into a Data Frame</strong>: We create a data frame including the y and X variables.</li>
<li><strong>Visualization:</strong>
<ul>
<li>We plot the error term (<code>epsilon</code>) over time. The AR(1) process will typically exhibit some “stickiness” or persistence, where positive errors tend to be followed by positive errors, and negative errors by negative errors.</li>
<li>We use the <code>acf()</code> function (Autocorrelation Function) to display the autocorrelation of the error terms. This will show significant autocorrelation at lag 1 (and possibly other lags, depending on the value of <code>rho</code>).</li>
</ul></li>
</ol>
</section>
<section class="level3" id="r-script-4-illustrating-the-rank-condition">
<h3 class="anchored" data-anchor-id="r-script-4-illustrating-the-rank-condition">R Script 4: Illustrating the Rank Condition</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a aria-hidden="true" href="#cb12-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb12-2"><a aria-hidden="true" href="#cb12-2" tabindex="-1"></a></span>
<span id="cb12-3"><a aria-hidden="true" href="#cb12-3" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb12-4"><a aria-hidden="true" href="#cb12-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101</span>)</span>
<span id="cb12-5"><a aria-hidden="true" href="#cb12-5" tabindex="-1"></a></span>
<span id="cb12-6"><a aria-hidden="true" href="#cb12-6" tabindex="-1"></a><span class="co"># Number of observations</span></span>
<span id="cb12-7"><a aria-hidden="true" href="#cb12-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb12-8"><a aria-hidden="true" href="#cb12-8" tabindex="-1"></a></span>
<span id="cb12-9"><a aria-hidden="true" href="#cb12-9" tabindex="-1"></a><span class="co"># Create a matrix X with rank deficiency</span></span>
<span id="cb12-10"><a aria-hidden="true" href="#cb12-10" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n <span class="sc">*</span> <span class="dv">3</span>), <span class="at">nrow =</span> n, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb12-11"><a aria-hidden="true" href="#cb12-11" tabindex="-1"></a>X[, <span class="dv">3</span>] <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="sc">*</span> X[, <span class="dv">1</span>] <span class="sc">+</span> <span class="dv">3</span> <span class="sc">*</span> X[, <span class="dv">2</span>]  <span class="co"># Make the third column a linear combination of the first two</span></span>
<span id="cb12-12"><a aria-hidden="true" href="#cb12-12" tabindex="-1"></a></span>
<span id="cb12-13"><a aria-hidden="true" href="#cb12-13" tabindex="-1"></a><span class="co"># Calculate the rank of X</span></span>
<span id="cb12-14"><a aria-hidden="true" href="#cb12-14" tabindex="-1"></a>rank_X <span class="ot">&lt;-</span> <span class="fu">qr</span>(X)<span class="sc">$</span>rank</span>
<span id="cb12-15"><a aria-hidden="true" href="#cb12-15" tabindex="-1"></a></span>
<span id="cb12-16"><a aria-hidden="true" href="#cb12-16" tabindex="-1"></a><span class="co"># Print the rank</span></span>
<span id="cb12-17"><a aria-hidden="true" href="#cb12-17" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Rank of X:"</span>, rank_X))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Rank of X: 2"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a aria-hidden="true" href="#cb14-1" tabindex="-1"></a><span class="co"># Attempt to calculate (X'X)^-1 (will fail)</span></span>
<span id="cb14-2"><a aria-hidden="true" href="#cb14-2" tabindex="-1"></a><span class="co"># solve(t(X) %*% X) # This line would produce an error</span></span>
<span id="cb14-3"><a aria-hidden="true" href="#cb14-3" tabindex="-1"></a></span>
<span id="cb14-4"><a aria-hidden="true" href="#cb14-4" tabindex="-1"></a><span class="co"># Create a matrix X with full column rank</span></span>
<span id="cb14-5"><a aria-hidden="true" href="#cb14-5" tabindex="-1"></a>X_full <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n <span class="sc">*</span> <span class="dv">3</span>), <span class="at">nrow =</span> n, <span class="at">ncol =</span> <span class="dv">3</span>)</span>
<span id="cb14-6"><a aria-hidden="true" href="#cb14-6" tabindex="-1"></a></span>
<span id="cb14-7"><a aria-hidden="true" href="#cb14-7" tabindex="-1"></a><span class="co"># Calculate the rank of X_full</span></span>
<span id="cb14-8"><a aria-hidden="true" href="#cb14-8" tabindex="-1"></a>rank_X_full <span class="ot">&lt;-</span> <span class="fu">qr</span>(X_full)<span class="sc">$</span>rank</span>
<span id="cb14-9"><a aria-hidden="true" href="#cb14-9" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Rank of X_full:"</span>, rank_X_full))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Rank of X_full: 3"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a aria-hidden="true" href="#cb16-1" tabindex="-1"></a><span class="co"># Calculate (X_full'X_full)^-1 (will succeed)</span></span>
<span id="cb16-2"><a aria-hidden="true" href="#cb16-2" tabindex="-1"></a><span class="fu">solve</span>(<span class="fu">t</span>(X_full) <span class="sc">%*%</span> X_full)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]        [,2]        [,3]
[1,]  0.13624535 -0.04690227 -0.08378334
[2,] -0.04690227  0.20430759 -0.10415225
[3,] -0.08378334 -0.10415225  0.20643596</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong> Load libraries, and set seed.</li>
<li><strong>Create Rank-Deficient Matrix:</strong> We create a matrix <code>X</code> where the third column is a linear combination of the first two columns. This ensures that <code>X</code> does <em>not</em> have full column rank. This directly demonstrates a violation of the condition <span class="math inline">\(\text{rank}(X) = K &lt; n\)</span> stated in the text.</li>
<li><strong>Calculate Rank:</strong> We use <code>qr(X)$rank</code> to calculate the rank of <code>X</code>. The <code>qr()</code> function performs QR decomposition, and its <code>$rank</code> component gives the rank of the matrix.</li>
<li><strong>Illustrate Problem:</strong> We (commented out) attempt to calculate <span class="math inline">\((X'X)^{-1}\)</span>. This will result in an error because <span class="math inline">\((X'X)\)</span> is singular when <code>X</code> does not have full column rank. This is a critical issue in OLS estimation, as <span class="math inline">\((X'X)^{-1}\)</span> is required to calculate the OLS estimator.</li>
<li><strong>Create Full-Rank Matrix:</strong> We create another matrix, <code>X_full</code>, where the columns are linearly independent (generated from independent random uniform variables).</li>
<li><strong>Calculate Rank (Full Rank):</strong> We calculate the rank of <code>X_full</code>, which will be equal to the number of columns (3 in this case).</li>
<li><strong>Calculate Inverse:</strong> We successfully calculate <span class="math inline">\((X_{full}'X_{full})^{-1}\)</span>, demonstrating that the inverse exists when the matrix has full column rank.</li>
</ol>
</section>
<section class="level3" id="r-script-5-illustrating-clustered-errors">
<h3 class="anchored" data-anchor-id="r-script-5-illustrating-clustered-errors">R Script 5: Illustrating Clustered Errors</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a aria-hidden="true" href="#cb18-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb18-2"><a aria-hidden="true" href="#cb18-2" tabindex="-1"></a><span class="fu">library</span>(mvtnorm) <span class="co"># For multivariate normal simulation</span></span>
<span id="cb18-3"><a aria-hidden="true" href="#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a aria-hidden="true" href="#cb18-4" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb18-5"><a aria-hidden="true" href="#cb18-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">202</span>)</span>
<span id="cb18-6"><a aria-hidden="true" href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a aria-hidden="true" href="#cb18-7" tabindex="-1"></a><span class="co"># Number of clusters</span></span>
<span id="cb18-8"><a aria-hidden="true" href="#cb18-8" tabindex="-1"></a>n_clusters <span class="ot">&lt;-</span> <span class="dv">10</span></span>
<span id="cb18-9"><a aria-hidden="true" href="#cb18-9" tabindex="-1"></a></span>
<span id="cb18-10"><a aria-hidden="true" href="#cb18-10" tabindex="-1"></a><span class="co"># Observations per cluster</span></span>
<span id="cb18-11"><a aria-hidden="true" href="#cb18-11" tabindex="-1"></a>n_per_cluster <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb18-12"><a aria-hidden="true" href="#cb18-12" tabindex="-1"></a></span>
<span id="cb18-13"><a aria-hidden="true" href="#cb18-13" tabindex="-1"></a><span class="co"># Total observations</span></span>
<span id="cb18-14"><a aria-hidden="true" href="#cb18-14" tabindex="-1"></a>n <span class="ot">&lt;-</span> n_clusters <span class="sc">*</span> n_per_cluster</span>
<span id="cb18-15"><a aria-hidden="true" href="#cb18-15" tabindex="-1"></a></span>
<span id="cb18-16"><a aria-hidden="true" href="#cb18-16" tabindex="-1"></a><span class="co"># Generate regressors (X) - for simplicity, just one regressor</span></span>
<span id="cb18-17"><a aria-hidden="true" href="#cb18-17" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">runif</span>(n), <span class="at">nrow =</span> n, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb18-18"><a aria-hidden="true" href="#cb18-18" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, X) <span class="co"># Add intercept</span></span>
<span id="cb18-19"><a aria-hidden="true" href="#cb18-19" tabindex="-1"></a></span>
<span id="cb18-20"><a aria-hidden="true" href="#cb18-20" tabindex="-1"></a><span class="co"># True coefficients (beta)</span></span>
<span id="cb18-21"><a aria-hidden="true" href="#cb18-21" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb18-22"><a aria-hidden="true" href="#cb18-22" tabindex="-1"></a></span>
<span id="cb18-23"><a aria-hidden="true" href="#cb18-23" tabindex="-1"></a><span class="co"># Generate clustered errors</span></span>
<span id="cb18-24"><a aria-hidden="true" href="#cb18-24" tabindex="-1"></a><span class="co">#   Within each cluster, errors are correlated; between clusters, they are uncorrelated.</span></span>
<span id="cb18-25"><a aria-hidden="true" href="#cb18-25" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n)</span>
<span id="cb18-26"><a aria-hidden="true" href="#cb18-26" tabindex="-1"></a>rho <span class="ot">&lt;-</span> <span class="fl">0.8</span> <span class="co"># Within-cluster correlation</span></span>
<span id="cb18-27"><a aria-hidden="true" href="#cb18-27" tabindex="-1"></a></span>
<span id="cb18-28"><a aria-hidden="true" href="#cb18-28" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_clusters) {</span>
<span id="cb18-29"><a aria-hidden="true" href="#cb18-29" tabindex="-1"></a>  start_index <span class="ot">&lt;-</span> (i <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> n_per_cluster <span class="sc">+</span> <span class="dv">1</span></span>
<span id="cb18-30"><a aria-hidden="true" href="#cb18-30" tabindex="-1"></a>  end_index <span class="ot">&lt;-</span> i <span class="sc">*</span> n_per_cluster</span>
<span id="cb18-31"><a aria-hidden="true" href="#cb18-31" tabindex="-1"></a>  </span>
<span id="cb18-32"><a aria-hidden="true" href="#cb18-32" tabindex="-1"></a>  <span class="co"># Covariance matrix for the cluster</span></span>
<span id="cb18-33"><a aria-hidden="true" href="#cb18-33" tabindex="-1"></a>  cluster_cov <span class="ot">&lt;-</span> <span class="fu">matrix</span>(rho, <span class="at">nrow =</span> n_per_cluster, <span class="at">ncol =</span> n_per_cluster)</span>
<span id="cb18-34"><a aria-hidden="true" href="#cb18-34" tabindex="-1"></a>  <span class="fu">diag</span>(cluster_cov) <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># Variance of 1 for each error</span></span>
<span id="cb18-35"><a aria-hidden="true" href="#cb18-35" tabindex="-1"></a>  </span>
<span id="cb18-36"><a aria-hidden="true" href="#cb18-36" tabindex="-1"></a>  <span class="co"># Generate errors for the cluster using rmvnorm</span></span>
<span id="cb18-37"><a aria-hidden="true" href="#cb18-37" tabindex="-1"></a>  epsilon[start_index<span class="sc">:</span>end_index] <span class="ot">&lt;-</span> <span class="fu">rmvnorm</span>(<span class="dv">1</span>, <span class="at">mean =</span> <span class="fu">rep</span>(<span class="dv">0</span>, n_per_cluster), <span class="at">sigma =</span> cluster_cov)</span>
<span id="cb18-38"><a aria-hidden="true" href="#cb18-38" tabindex="-1"></a>}</span>
<span id="cb18-39"><a aria-hidden="true" href="#cb18-39" tabindex="-1"></a></span>
<span id="cb18-40"><a aria-hidden="true" href="#cb18-40" tabindex="-1"></a><span class="co"># Generate dependent variable (y)</span></span>
<span id="cb18-41"><a aria-hidden="true" href="#cb18-41" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> epsilon</span>
<span id="cb18-42"><a aria-hidden="true" href="#cb18-42" tabindex="-1"></a></span>
<span id="cb18-43"><a aria-hidden="true" href="#cb18-43" tabindex="-1"></a><span class="co"># Create data frame, including cluster ID</span></span>
<span id="cb18-44"><a aria-hidden="true" href="#cb18-44" tabindex="-1"></a>data <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">y =</span> y, <span class="at">X =</span> X[,<span class="sc">-</span><span class="dv">1</span>], <span class="at">Cluster =</span> <span class="fu">rep</span>(<span class="dv">1</span><span class="sc">:</span>n_clusters, <span class="at">each =</span> n_per_cluster))</span>
<span id="cb18-45"><a aria-hidden="true" href="#cb18-45" tabindex="-1"></a></span>
<span id="cb18-46"><a aria-hidden="true" href="#cb18-46" tabindex="-1"></a><span class="co"># Visualize errors within a few clusters</span></span>
<span id="cb18-47"><a aria-hidden="true" href="#cb18-47" tabindex="-1"></a><span class="fu">ggplot</span>(data, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">seq_along</span>(y), <span class="at">y =</span> epsilon, <span class="at">color =</span> <span class="fu">factor</span>(Cluster))) <span class="sc">+</span></span>
<span id="cb18-48"><a aria-hidden="true" href="#cb18-48" tabindex="-1"></a>  <span class="fu">geom_line</span>() <span class="sc">+</span></span>
<span id="cb18-49"><a aria-hidden="true" href="#cb18-49" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb18-50"><a aria-hidden="true" href="#cb18-50" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Clustered Errors"</span>,</span>
<span id="cb18-51"><a aria-hidden="true" href="#cb18-51" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Observation within Cluster"</span>,</span>
<span id="cb18-52"><a aria-hidden="true" href="#cb18-52" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Error Term"</span>,</span>
<span id="cb18-53"><a aria-hidden="true" href="#cb18-53" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"Cluster"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap18_files/figure-html/unnamed-chunk-5-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong> Load <code>tidyverse</code> and <code>mvtnorm</code> (for simulating from a multivariate normal distribution). Set seed.</li>
<li><strong>Define Parameters:</strong> Define the number of clusters (<code>n_clusters</code>), observations per cluster (<code>n_per_cluster</code>), total observations (<code>n</code>), regressors (<code>X</code>), coefficients (<code>beta</code>), and the within-cluster correlation (<code>rho</code>).</li>
<li><strong>Generate Clustered Errors:</strong> This is the key part.
<ul>
<li>We loop through each cluster.</li>
<li>Inside the loop, we define the start and end indices for the current cluster within the overall <code>epsilon</code> vector.</li>
<li>We create a covariance matrix <code>cluster_cov</code> for the current cluster. The diagonal elements are 1 (variance of each error term), and the off-diagonal elements are <code>rho</code> (the correlation between errors within the cluster).</li>
<li>We use <code>rmvnorm()</code> to generate <code>n_per_cluster</code> error terms from a multivariate normal distribution with mean 0 and the specified <code>cluster_cov</code>. This ensures that the errors within the cluster are correlated as desired. This corresponds to a special case of Assumption A2 with a block-diagonal matrix, as shown in the text.</li>
</ul></li>
<li><strong>Generate Dependent Variable (y):</strong> We create <code>y</code> using the usual linear model equation.</li>
<li><strong>Create Data Frame:</strong> Create a dataframe including cluster ids.</li>
<li><strong>Visualization:</strong> We plot the error terms for the first three clusters. Observations within each cluster are connected by lines, and different clusters have different colors. This visually demonstrates the within-cluster correlation.</li>
<li><strong>Sample Correlation:</strong> We calculate the sample correlation matrix of the errors within the first cluster to show that the generated errors indeed have a correlation close to the specified <code>rho</code>.</li>
</ol>
</section>
</section>
<section class="level2" id="youtube-video-suggestions-for-linear-model-concepts">
<h2 class="anchored" data-anchor-id="youtube-video-suggestions-for-linear-model-concepts">YouTube Video Suggestions for Linear Model Concepts</h2>
<p>Here are some YouTube video suggestions related to the concepts in the provided text, along with explanations of their relevance and verification of their availability (as of October 26, 2023):</p>
<section class="level3" id="video-1-the-linear-regression-model">
<h3 class="anchored" data-anchor-id="video-1-the-linear-regression-model">Video 1: The Linear Regression Model</h3>
<ul>
<li><strong>Title:</strong> “1.1. The linear regression model”</li>
<li><strong>Channel:</strong> Ben Lambert</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=zITIFTsivN8">https://www.youtube.com/watch?v=zITIFTsivN8</a></li>
<li><strong>Availability:</strong> Verified - video is currently available.</li>
<li><strong>Relevance:</strong> This video provides a good introduction to the <strong>linear regression model</strong>, covering the basic equation <span class="math inline">\(y = X\beta + \epsilon\)</span>. It discusses the components of the model, including the dependent variable (<span class="math inline">\(y\)</span>), independent variables (<span class="math inline">\(X\)</span>), coefficients (<span class="math inline">\(\beta\)</span>), and the error term (<span class="math inline">\(\epsilon\)</span>). It lays the groundwork for understanding the model presented in section 18.2 of the text. It visually relates data with the linear model and the error term.</li>
</ul>
</section>
<section class="level3" id="video-2-assumptions-of-linear-regression">
<h3 class="anchored" data-anchor-id="video-2-assumptions-of-linear-regression">Video 2: Assumptions of Linear Regression</h3>
<ul>
<li><strong>Title:</strong> “6. Regression Assumptions”</li>
<li><strong>Channel:</strong> zedstatistics</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=0MFpOQRY-t4">https://www.youtube.com/watch?v=0MFpOQRY-t4</a></li>
<li><strong>Availability:</strong> Verified - video is currently available.</li>
<li><strong>Relevance:</strong> This video discusses the key <strong>assumptions of linear regression</strong>, many of which align with Assumptions A1-A4 in the text. It covers topics like linearity (<span class="math inline">\(E(y|X) = X\beta\)</span>), homoskedasticity (<span class="math inline">\(\text{Var}(y|X) = \sigma^2 I\)</span>), and the lack of autocorrelation. Although it doesn’t delve into the matrix notation of <span class="math inline">\(\Sigma(X)\)</span> as deeply as the text, it provides a solid intuitive understanding of the assumptions.</li>
</ul>
</section>
<section class="level3" id="video-3-multicollinearity">
<h3 class="anchored" data-anchor-id="video-3-multicollinearity">Video 3: Multicollinearity</h3>
<ul>
<li><strong>Title:</strong> “Econometrics // Lecture 6: Multicollinearity”</li>
<li><strong>Channel:</strong> Wooldridge Lectures</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=4T1w9NEK54c">https://www.youtube.com/watch?v=4T1w9NEK54c</a></li>
<li><strong>Availability:</strong> Verified - video is currently available.</li>
<li><strong>Relevance:</strong> This video discusses <strong>multicollinearity</strong>, which is directly related to the assumption <span class="math inline">\(\text{rank}(X) = K &lt; n\)</span> (Equation 18.1 in the text). It explains what multicollinearity is (linear dependence among regressors), its consequences (inflated standard errors, unstable coefficients), and how to detect and address it. The video makes clear why full column rank is crucial for OLS estimation.</li>
</ul>
</section>
<section class="level3" id="video-4-heteroskedasticity">
<h3 class="anchored" data-anchor-id="video-4-heteroskedasticity">Video 4: Heteroskedasticity</h3>
<ul>
<li><strong>Title:</strong> “Heteroskedasticity”</li>
<li><strong>Channel:</strong> Steve Grams</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=cREHqG274Ag">https://www.youtube.com/watch?v=cREHqG274Ag</a></li>
<li><strong>Availability:</strong> Verified - video is currently available.</li>
<li><strong>Relevance:</strong> This video explains <strong>heteroskedasticity</strong>, which is a violation of the assumption of constant error variance. The text discusses this in the context of <span class="math inline">\(\Sigma(X)\)</span> not being equal to <span class="math inline">\(\sigma^2 I_n\)</span>. The video provides a clear visual explanation of heteroskedasticity (non-constant spread of residuals) and discusses its consequences and potential remedies.</li>
</ul>
</section>
<section class="level3" id="video-5-autocorrelation">
<h3 class="anchored" data-anchor-id="video-5-autocorrelation">Video 5: Autocorrelation</h3>
<ul>
<li><strong>Title:</strong> “Autocorrelation”</li>
<li><strong>Channel:</strong> DATAtab</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=w_w6ZkEeUo8">https://www.youtube.com/watch?v=w_w6ZkEeUo8</a></li>
<li><strong>Availability:</strong> Verified - video is currently available.</li>
<li><strong>Relevance</strong>: This video provides a detailed explanation of <strong>autocorrelation</strong>. This is the condition where errors from different time periods are correlated with each other. The video gives an intuitive explanation, useful examples, and visual aids of how to detect autocorrelation. The text describes autocorrelation as a case where <span class="math inline">\(\Sigma(X)\)</span> may have all entries non-zero but have the property that entries get smaller and smaller the further they are away from the diagonal.</li>
</ul>
</section>
<section class="level3" id="video-6-strong-exogeneity">
<h3 class="anchored" data-anchor-id="video-6-strong-exogeneity">Video 6: Strong Exogeneity</h3>
<ul>
<li><strong>Title</strong>: “What is Exogeneity? - Causal Research”</li>
<li><strong>Channel</strong>: John Antonakis</li>
<li><strong>Link</strong>: <a href="https://www.youtube.com/watch?v=efeDP-FagjA">https://www.youtube.com/watch?v=efeDP-FagjA</a></li>
<li><strong>Availability</strong>: Verified - video is currently available</li>
<li><strong>Relevance</strong>: This video introduces and provides intuitive examples of <strong>strong exogeneity</strong>. The video explains what it means for an independent variable to be exogenous, as the condition that the independent variable cannot be correlated with the error term. The text describes strong exogeneity as a condition where the error term is uncorrelated with past, present and future values of the independent variable.</li>
</ul>
<p>These videos provide a good visual and intuitive complement to the more formal mathematical treatment in the text. They cover the core assumptions and potential issues in the linear model, aligning well with the concepts discussed in Chapter 18.</p>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch18mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch18mcsolution1">MC Solution 1</a></p>
<p>The assumption <span class="math inline">\(\text{rank}(X) = K &lt; n\)</span> in the linear model implies:</p>
<ol type="a">
<li>The number of regressors is greater than the number of observations.</li>
<li>There is perfect multicollinearity among the regressors.</li>
<li>The matrix <span class="math inline">\(X\)</span> has full column rank, and the number of observations is greater than the number of regressors (including the intercept).</li>
<li>The error terms are normally distributed.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch18mcsolution2">MC Solution 2</a></p>
<p>Assumption A1, <span class="math inline">\(E(y|X) = X\beta\)</span>, states that:</p>
<ol type="a">
<li>The variance of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span> is constant.</li>
<li>The expected value of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span> is a linear function of <span class="math inline">\(X\)</span>.</li>
<li><span class="math inline">\(y\)</span> is normally distributed.</li>
<li>The error terms are uncorrelated.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch18mcsolution3">MC Solution 3</a></p>
<p>Assumption A2, <span class="math inline">\(\text{var}(y|X) = \Sigma(X)\)</span>, implies that:</p>
<ol type="a">
<li>The variance of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span> is constant.</li>
<li>The variance of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span> can depend on <span class="math inline">\(X\)</span>.</li>
<li>The errors are homoskedastic.</li>
<li><span class="math inline">\(y\)</span> is normally distributed.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch18mcsolution4">MC Solution 4</a></p>
<p>Which assumption implies that <span class="math inline">\(y\)</span> is normally distributed conditional on <span class="math inline">\(X\)</span>?</p>
<ol type="a">
<li>A1</li>
<li>A2</li>
<li>A3</li>
<li>A4 and A1</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch18mcsolution5">MC Solution 5</a></p>
<p>The equation <span class="math inline">\(y = X\beta + \varepsilon\)</span> represents:</p>
<ol type="a">
<li>The conditional expectation of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span>.</li>
<li>The variance of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span>.</li>
<li>The linear regression model with an error term.</li>
<li>The normal distribution of <span class="math inline">\(y\)</span>.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch18mcsolution6">MC Solution 6</a></p>
<p><span class="math inline">\(E(\varepsilon|X) = 0\)</span> implies:</p>
<ol type="a">
<li>The errors are homoskedastic.</li>
<li>The errors are normally distributed.</li>
<li>The expected value of the error term, conditional on <span class="math inline">\(X\)</span>, is zero.</li>
<li>The variance of the error term is constant.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch18mcsolution7">MC Solution 7</a></p>
<p><strong>Strong exogeneity</strong> in a time series setting means:</p>
<ol type="a">
<li>The error term is only correlated with the current value of the regressors.</li>
<li>The error term is uncorrelated with past, present, and future values of the regressors.</li>
<li>The regressors are normally distributed.</li>
<li>The error term is homoskedastic.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch18mcsolution8">MC Solution 8</a></p>
<p>The <strong>homoskedastic</strong> case is characterized by:</p>
<ol type="a">
<li><span class="math inline">\(\Sigma(X) = \sigma^2 I_n\)</span></li>
<li><span class="math inline">\(\Sigma(X)\)</span> being a diagonal matrix with different values on the diagonal.</li>
<li><span class="math inline">\(\Sigma(X)\)</span> being a block-diagonal matrix.</li>
<li><span class="math inline">\(\Sigma(X)\)</span> having all entries non-zero.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch18mcsolution9">MC Solution 9</a></p>
<p>The <strong>heteroskedastic</strong> case is characterized by:</p>
<ol type="a">
<li><span class="math inline">\(\Sigma(X) = \sigma^2 I_n\)</span></li>
<li><span class="math inline">\(\Sigma(X)\)</span> being a diagonal matrix with potentially different values on the diagonal.</li>
<li><span class="math inline">\(\Sigma(X)\)</span> being a block-diagonal matrix.</li>
<li><span class="math inline">\(\Sigma(X)\)</span> having all entries non-zero.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch18mcsolution10">MC Solution 10</a></p>
<p>The <strong>clustered</strong> error structure is characterized by:</p>
<ol type="a">
<li><span class="math inline">\(\Sigma(X) = \sigma^2 I_n\)</span></li>
<li><span class="math inline">\(\Sigma(X)\)</span> being a diagonal matrix with different values on the diagonal.</li>
<li><span class="math inline">\(\Sigma(X)\)</span> being a block-diagonal matrix.</li>
<li><span class="math inline">\(\Sigma(X)\)</span> having all entries non-zero and constant.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch18mcsolution11">MC Solution 11</a></p>
<p>In the time series case, the covariance matrix <span class="math inline">\(\Sigma(X)\)</span> typically has:</p>
<ol type="a">
<li>All entries equal to zero.</li>
<li>All entries equal to a constant <span class="math inline">\(\sigma^2\)</span>.</li>
<li>Non-zero entries that decrease in magnitude as they move away from the diagonal.</li>
<li>A block-diagonal structure.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch18mcsolution12">MC Solution 12</a></p>
<p>A positive definite matrix <span class="math inline">\(\Sigma(X)\)</span> implies:</p>
<ol type="a">
<li>All its diagonal elements are negative.</li>
<li>It is always singular (non-invertible).</li>
<li>For any non-zero vector <span class="math inline">\(z\)</span>, <span class="math inline">\(z^T \Sigma(X) z &gt; 0\)</span>.</li>
<li>It represents a uniform distribution.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch18mcsolution13">MC Solution 13</a></p>
<p>If <span class="math inline">\(E(\epsilon_i|X_1,...,X_n) = 0\)</span>, this implies:</p>
<ol type="a">
<li>The error term <span class="math inline">\(\epsilon_i\)</span> is heteroskedastic.</li>
<li>Strong exogeneity.</li>
<li><span class="math inline">\(\epsilon_i\)</span> follows a normal distribution.</li>
<li>Weak exogeneity.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch18mcsolution14">MC Solution 14</a></p>
<p>Assumption A4, <span class="math inline">\(y \sim N(X\beta, \sigma^2 I)\)</span>, implies:</p>
<ol type="a">
<li>Heteroskedasticity.</li>
<li>Autocorrelation.</li>
<li>Homoskedasticity and no autocorrelation.</li>
<li>Clustered errors.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch18mcsolution15">MC Solution 15</a></p>
<p>Which of the following statements is true about the linear model?</p>
<ol type="a">
<li>It always assumes that the dependent variable is normally distributed.</li>
<li>It always assumes homoskedasticity.</li>
<li>It models the conditional expectation of the dependent variable as a linear function of the regressors.</li>
<li>It cannot handle time series data.</li>
</ol>
</section>
<section class="level3" id="sec-ch18mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch18mcsolution16">MC Solution 16</a> Consider a model explaining house prices as a function of square footage and the number of bedrooms. Which assumption is most likely to be violated? a) <span class="math inline">\(E(y|X)= X\beta\)</span>. b) <span class="math inline">\(\text{rank}(X) = K &lt;n\)</span>. c) Homoskedasticity. d) <span class="math inline">\(y\)</span> is normally distributed.</p>
</section>
<section class="level3" id="sec-ch18mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch18mcsolution17">MC Solution 17</a></p>
<p>The notation <span class="math inline">\(\varepsilon = y - X\beta\)</span> defines: a) The predicted value of <span class="math inline">\(y\)</span>. b) The error term of the linear model. c) The variance of <span class="math inline">\(y\)</span>. d) The coefficient vector.</p>
</section>
<section class="level3" id="sec-ch18mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch18mcsolution18">MC Solution 18</a></p>
<p>Which of the following cases of <span class="math inline">\(\Sigma(X)\)</span> would be most applicable for panel data where observations are grouped by individuals over time? a) <span class="math inline">\(\Sigma(X) = \sigma^2 I_n\)</span> b) A diagonal matrix with potentially different values on the diagonal. c) A block-diagonal matrix. d) A matrix with all entries identical and non-zero.</p>
</section>
<section class="level3" id="sec-ch18mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch18mcsolution19">MC Solution 19</a></p>
<p>The linear model assumes there is a random mechanism behind the data generation. This implies: a) The observed data is the only possible outcome. b) The observed data represents one realization of many potential outcomes. c) The data is deterministic. d) The data is normally distributed.</p>
</section>
<section class="level3" id="sec-ch18mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch18mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch18mcsolution20">MC Solution 20</a></p>
<p>Assumption A3 is a _____ assumption than Assumption A1: a) Weaker b) Stronger c) Less restrictive d) Simpler</p>
</section>
</section>
<section class="level2" id="multiple-choice-solutions">
<h2 class="anchored" data-anchor-id="multiple-choice-solutions">Multiple Choice Solutions</h2>
<section class="level3" id="sec-ch18mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch18mcexercise1">MC Exercise 1</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> The assumption <span class="math inline">\(\text{rank}(X) = K &lt; n\)</span> has two parts:
<ul>
<li><span class="math inline">\(\text{rank}(X) = K\)</span>: This means the matrix <span class="math inline">\(X\)</span> has full column rank. No regressor is a perfect linear combination of other regressors.</li>
<li><span class="math inline">\(K &lt; n\)</span>: This means the number of regressors (including the intercept, hence <span class="math inline">\(K\)</span>, not <span class="math inline">\(K+1\)</span> as in the text), is less than the number of observations. This is necessary to have degrees of freedom for estimation.</li>
</ul></li>
<li>Options a) and b) are incorrect, and d) is not related to the rank of the matrix.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch18mcexercise2">MC Exercise 2</a></p>
<p><strong>Correct Answer: b)</strong></p>
<ul>
<li><strong>Explanation:</strong> <span class="math inline">\(E(y|X) = X\beta\)</span> states that the <em>expected value</em> (or mean) of the dependent variable <span class="math inline">\(y\)</span>, given the values of the independent variables <span class="math inline">\(X\)</span>, is a <em>linear function</em> of <span class="math inline">\(X\)</span>. The coefficients of this linear function are the elements of the vector <span class="math inline">\(\beta\)</span>.</li>
<li>Options a), c), and d) refer to other assumptions or properties, not the definition of the conditional expectation.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch18mcexercise3">MC Exercise 3</a></p>
<p><strong>Correct Answer: b)</strong></p>
<ul>
<li><strong>Explanation:</strong> <span class="math inline">\(\text{var}(y|X) = \Sigma(X)\)</span> states that the <em>variance</em> of <span class="math inline">\(y\)</span> conditional on <span class="math inline">\(X\)</span> is given by the matrix <span class="math inline">\(\Sigma(X)\)</span>. The key here is that <span class="math inline">\(\Sigma(X)\)</span> can <em>depend on</em> <span class="math inline">\(X\)</span>. This allows for the possibility of heteroskedasticity (non-constant variance).</li>
<li>Option a) is incorrect because it would imply homoskedasticity. Options c) and d) are not directly implied by Assumption A2.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch18mcexercise4">MC Exercise 4</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> Assumption A3, <span class="math inline">\(y \sim N(X\beta, \Sigma(X))\)</span>, explicitly states that <span class="math inline">\(y\)</span> follows a <em>normal distribution</em> with mean <span class="math inline">\(X\beta\)</span> and variance-covariance matrix <span class="math inline">\(\Sigma(X)\)</span>.</li>
<li>Assumptions A1 and A2 only specify the first two moments (mean and variance) and do not assume normality. A4 <em>also</em> assumes normality, but with a specific, restrictive form of <span class="math inline">\(\Sigma(X)\)</span>, so c) is a more general answer.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch18mcexercise5">MC Exercise 5</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> This is the standard representation of the <strong>linear regression model</strong>, where <span class="math inline">\(y\)</span> is the dependent variable, <span class="math inline">\(X\)</span> is the matrix of regressors, <span class="math inline">\(\beta\)</span> is the vector of coefficients, and <span class="math inline">\(\varepsilon\)</span> is the error term.</li>
<li>Options a) and b) refer to specific parts or characteristics of the model, but not the complete equation. Option d) is an assumption, not the definition.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch18mcexercise6">MC Exercise 6</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> <span class="math inline">\(E(\varepsilon|X) = 0\)</span> states that the <em>expected value</em> (or mean) of the error term <span class="math inline">\(\varepsilon\)</span>, conditional on the values of the regressors <span class="math inline">\(X\)</span>, is <em>zero</em>. This means that, on average, the errors are zero for any given values of the regressors.</li>
<li>Options a), b), and d) refer to other properties of the error term (homoskedasticity, normality, constant variance), which are separate assumptions.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch18mcexercise7">MC Exercise 7</a></p>
<p><strong>Correct Answer: b)</strong></p>
<ul>
<li><strong>Explanation:</strong> <strong>Strong exogeneity</strong> means that the error term at any time period is uncorrelated with the regressors in <em>all</em> time periods (past, present, and future). This is a crucial assumption for causal inference in time series models.</li>
<li>Option a) describes a weaker form of exogeneity (contemporaneous exogeneity). Options c) and d) are unrelated to exogeneity.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch18mcexercise8">MC Exercise 8</a></p>
<p><strong>Correct Answer: a)</strong></p>
<ul>
<li><strong>Explanation:</strong> The <strong>homoskedastic</strong> case means the variance of the error term is <em>constant</em> and the errors are <em>uncorrelated</em>. This is represented by <span class="math inline">\(\Sigma(X) = \sigma^2 I_n\)</span>, where <span class="math inline">\(\sigma^2\)</span> is the constant variance and <span class="math inline">\(I_n\)</span> is the identity matrix, indicating no correlation between different error terms.</li>
<li>The other options describe heteroskedasticity (b), clustered errors (c), or a general covariance structure (d).</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch18mcexercise9">MC Exercise 9</a></p>
<p><strong>Correct Answer: b)</strong></p>
<ul>
<li><strong>Explanation:</strong> The <strong>heteroskedastic</strong> case means the variance of the error term is <em>not constant</em>. It can vary across observations. This is represented by <span class="math inline">\(\Sigma(X)\)</span> being a diagonal matrix with potentially <em>different</em> values on the diagonal (representing different variances for each observation).</li>
<li>Option a) represents homoskedasticity. Options c) and d) describe other types of covariance structures.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch18mcexercise10">MC Exercise 10</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> <strong>Clustered errors</strong> occur when observations are grouped into clusters, and errors within the same cluster are correlated, but errors in different clusters are uncorrelated. This is represented by a block-diagonal matrix <span class="math inline">\(\Sigma(X)\)</span>, where each block corresponds to a cluster and captures the within-cluster correlation.</li>
<li>Option a) represents homoskedasticity. Option b) represents heteroskedasticity. Option d) is too general.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch18mcexercise11">MC Exercise 11</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> In time series data, it’s common for observations closer in time to be more correlated than observations farther apart. This means the covariance matrix <span class="math inline">\(\Sigma(X)\)</span> will have non-zero entries, but these entries will <em>decrease in magnitude</em> as you move away from the diagonal (representing the correlation between observations further apart in time).</li>
<li>Options a) and b) imply no autocorrelation. Option d) is more typical of panel data.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch18mcexercise12">MC Exercise 12</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> This is the definition of a <strong>positive definite</strong> matrix. It means that the quadratic form <span class="math inline">\(z^T \Sigma(X) z\)</span> is always positive for any non-zero vector <span class="math inline">\(z\)</span>. This property is important for ensuring that variances are positive and that the matrix is invertible.</li>
<li>Option a) is incorrect. Positive definite matrices have positive diagonal entries. Option b) is incorrect, positive definite matrices are always invertible. Option d) is unrelated.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch18mcexercise13">MC Exercise 13</a></p>
<p><strong>Correct Answer: b)</strong></p>
<ul>
<li><strong>Explanation:</strong> The condition <span class="math inline">\(E(\epsilon_i|X_1,...,X_n) = 0\)</span> means that the expected value of the error term in observation <em>i</em> is zero conditional on <em>all</em> values of the regressors in the data set. This encompasses past, present, and future values, indicating strong exogeneity.</li>
<li>Option a) is unrelated to the conditional expectation. Option c) is not implied by this condition. Option d) is a concept related to, but not the same as <span class="math inline">\(E(\epsilon_i|X_i) = 0\)</span>.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch18mcexercise14">MC Exercise 14</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> Assumption A4 combines normality with <span class="math inline">\(\Sigma(X) = \sigma^2 I_n\)</span>. The <span class="math inline">\(\sigma^2 I_n\)</span> part implies <strong>homoskedasticity</strong> (constant variance) and <strong>no autocorrelation</strong> (errors are uncorrelated).</li>
<li>The other options represent violations of these assumptions.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch18mcexercise15">MC Exercise 15</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> The core of the linear model is that it models the <em>conditional expectation</em> of the dependent variable (<span class="math inline">\(y\)</span>) as a <em>linear function</em> of the regressors (<span class="math inline">\(X\)</span>): <span class="math inline">\(E(y|X) = X\beta\)</span>.</li>
<li>Options a) and b) are common assumptions but not always required. Option d) is incorrect; the linear model can be adapted for time series data.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch18mcexercise16">MC Exercise 16</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> It is common for house prices to be <strong>heteroskedastic</strong>. The variance of house prices is very often an increasing function of the size of the house.</li>
<li>Option a) is likely to be satisfied with the proposed model. Options b) would not likely be violated if we have a sufficiently large number of observations. Finally, while it is not impossible for house prices to be normally distributed, this is not a strict requirement of the linear model (option d).</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch18mcexercise17">MC Exercise 17</a></p>
<p><strong>Correct Answer: b)</strong></p>
<ul>
<li><strong>Explanation:</strong> This equation <em>defines</em> the <strong>error term</strong> (<span class="math inline">\(\varepsilon\)</span>) as the difference between the actual value of the dependent variable (<span class="math inline">\(y\)</span>) and the predicted value based on the linear model (<span class="math inline">\(X\beta\)</span>).</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch18mcexercise18">MC Exercise 18</a></p>
<p><strong>Correct Answer: c)</strong></p>
<ul>
<li><strong>Explanation:</strong> Panel data (individuals observed over time) often exhibits a <strong>clustered error structure</strong>. Errors are likely to be correlated <em>within</em> each individual over time (due to unobserved individual-specific factors), but uncorrelated <em>across</em> individuals. This is best represented by a <strong>block-diagonal</strong> matrix, where each block corresponds to an individual and captures the within-individual correlation.</li>
<li>Option a) represents homoskedasticity and no autocorrelation. Option b) represents heteroskedasticity. Option d) is not a realistic covariance structure.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch18mcexercise19">MC Exercise 19</a></p>
<p><strong>Correct Answer: b)</strong></p>
<ul>
<li><strong>Explanation:</strong> The linear model assumes there’s an underlying <strong>random</strong> data-generating process. We only observe <em>one</em> possible realization of this process. There’s an infinite number of potential outcomes we <em>could</em> have observed.</li>
<li>Option a) is the opposite of what the linear model assumes. Options c) and d) are not inherent to the idea of a random mechanism.</li>
</ul>
</section>
<section class="level3" id="sec-ch18mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch18mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch18mcexercise20">MC Exercise 20</a></p>
<p><strong>Correct Answer: b)</strong> * <strong>Explanation:</strong> Assumption A3 (<span class="math inline">\(y \sim N(X\beta, \Sigma(X))\)</span>) makes a specific distributional assumption (normality) in addition to specifying the mean and variance. Assumption A1 (<span class="math inline">\(E(y|X) = X\beta\)</span>) <em>only</em> specifies the conditional mean. A3 therefore implies A1, but not vice versa. Therefore, A3 is the <strong>stronger</strong> assumption.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>