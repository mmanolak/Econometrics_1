<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 16: Linear Algebra – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap17.html" rel="next"/>
<link href="../chapters/chap15.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 16: Linear Algebra</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level2" id="matrices">
<h2 class="anchored" data-anchor-id="matrices">16.1 Matrices</h2>
<section class="level3" id="define-the-n-times-1-column-vector">
<h3 class="anchored" data-anchor-id="define-the-n-times-1-column-vector">Define the <span class="math inline">\(n \times 1\)</span> column <strong>vector</strong></h3>
<p><span class="math display">\[
x = \begin{pmatrix}
x_1 \\
\vdots \\
x_n
\end{pmatrix}
\]</span></p>
<p>and the <span class="math inline">\(n \times K\)</span> <strong>matrix</strong></p>
<p><span class="math display">\[
A = \begin{pmatrix}
a_{11} &amp; \cdots &amp; a_{1K} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; \cdots &amp; a_{nK}
\end{pmatrix}
= (a_{ij})_{i,j}.
\]</span></p>
<p>The <strong>transpose</strong> of a matrix <span class="math inline">\(A = (a_{ij})\)</span> is the matrix <span class="math inline">\(A^T = (a_{ji})\)</span>. For example</p>
<p><span class="math display">\[
\begin{pmatrix}
1 &amp; 2 &amp; 3 \\
4 &amp; 5 &amp; 6
\end{pmatrix}^T
=
\begin{pmatrix}
1 &amp; 4 \\
2 &amp; 5 \\
3 &amp; 6
\end{pmatrix}.
\]</span></p>
<p>Matrices can be added in an obvious way, provided they are <strong>conformable</strong> meaning they have the same (row and column) dimensions</p>
<p><span class="math display">\[
A + B = \begin{pmatrix}
a_{11} + b_{11} &amp; \cdots &amp; a_{1K} + b_{1K} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{n1} + b_{n1} &amp; \cdots &amp; a_{nK} + b_{nK}
\end{pmatrix}.
\]</span></p>
<p><strong>Multiplication</strong> is a little more complicated and requires that the number of columns of <span class="math inline">\(A\)</span> be equal to the number of rows of <span class="math inline">\(B\)</span>. Suppose that <span class="math inline">\(A\)</span> is <span class="math inline">\(n \times K\)</span> and <span class="math inline">\(B\)</span> is <span class="math inline">\(K \times m\)</span>, then</p>
<p><span class="math display">\[
AB = \begin{pmatrix}
\sum_{j=1}^K a_{1j}b_{j1} &amp; \cdots &amp; \sum_{j=1}^K a_{1j}b_{jm} \\
\vdots &amp; \ddots &amp; \vdots \\
\sum_{j=1}^K a_{nj}b_{j1} &amp; \cdots &amp; \sum_{j=1}^K a_{nj}b_{jm}
\end{pmatrix}.
\]</span> The resulting matrix will have dimensions equal to the number of rows of the first matrix, and the number of columns of the second matrix. Thus, the product of an <span class="math inline">\(n \times K\)</span> matrix and a <span class="math inline">\(K \times m\)</span> matrix is an <span class="math inline">\(n \times m\)</span> matrix.</p>
<p><span class="math inline">\(AB\)</span> is an <span class="math inline">\(n \times m\)</span> matrix. If <span class="math inline">\(n = K\)</span>, the matrix <span class="math inline">\(A\)</span> is <strong>square</strong>. If <span class="math inline">\(m = n\)</span>, then we can define both <span class="math inline">\(AB\)</span> and <span class="math inline">\(BA\)</span>, which are both <span class="math inline">\(n \times n\)</span> matrices. Matrix multiplication is <strong>distributive</strong>, meaning that <span class="math inline">\((AB)C = A(BC)\)</span>. Note however that in general matrices do not <strong>commute</strong> so that <span class="math inline">\(AB \neq BA\)</span> (in general, when <span class="math inline">\(m \neq n\)</span>, <span class="math inline">\(AB\)</span> and <span class="math inline">\(BA\)</span> may not even have the same dimensions).</p>
<p>For vectors <span class="math inline">\(x, y \in \mathbb{R}^n\)</span>, we define the <strong>inner product</strong></p>
<p><span class="math display">\[
x^Ty = \sum_{i=1}^n x_iy_i \in \mathbb{R}.
\]</span></p>
<p>Two vectors are <strong>orthogonal</strong> if <span class="math inline">\(x^Ty = 0\)</span>. The Euclidean <strong>norm</strong> of a vector is denoted</p>
<p><span class="math display">\[
||x|| = (x^Tx)^{1/2} = \left( \sum_{i=1}^n x_i^2 \right)^{1/2} \in \mathbb{R}^+.
\]</span></p>
<p>This measures the length of the vector. We have the triangle inequality for two vectors <span class="math inline">\(x, y\)</span></p>
<p><span class="math display">\[
||x + y|| \leq ||x|| + ||y||.
\]</span></p>
<p>This says that the length of the sum is less than or equal to the sum of the lengths.</p>
<p>The <strong>identity matrix</strong> is a special kind of square matrix</p>
<p><span class="math display">\[
I_n = \begin{pmatrix}
1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; 1
\end{pmatrix}
\]</span></p>
<p>and satisfies <span class="math inline">\(AI = IA = A\)</span> for all square conformable matrices. The <strong>zero matrix</strong> <span class="math inline">\(0\)</span> consists of zeros, and satisfies <span class="math inline">\(A + 0 = 0 + A = 0\)</span> for any conformable <span class="math inline">\(A\)</span>. A square matrix <span class="math inline">\(B\)</span> such that <span class="math inline">\(AB = BA = I\)</span> is called the <strong>inverse</strong> of <span class="math inline">\(A\)</span>; not all square matrices have an inverse, but if they do then it is unique, i.e., if <span class="math inline">\(B^*\)</span> also satisfies <span class="math inline">\(AB^* = B^*A = I\)</span>, then <span class="math inline">\(B = B^*\)</span>. This is left as an exercise. If the matrix <span class="math inline">\(A\)</span> has an inverse it is called <strong>nonsingular</strong> otherwise it is called <strong>singular</strong>.</p>
<p>For example, a diagonal matrix</p>
<p><span class="math display">\[
D = \begin{pmatrix}
d_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; d_n
\end{pmatrix}
\]</span></p>
<p>has inverse</p>
<p><span class="math display">\[
D^{-1} = \begin{pmatrix}
d_1^{-1} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; d_n^{-1}
\end{pmatrix}.
\]</span></p>
<p>Diagonal matrices are quite important. They are automatically symmetric and they have the special feature that when pre and postmultiplying an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> by a diagonal <span class="math inline">\(D\)</span> one gets</p>
<p><span class="math display">\[
\begin{aligned}
D^{-1}AD^{-1} &amp;= \begin{pmatrix}
d_1^{-1} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; d_n^{-1}
\end{pmatrix}
\begin{pmatrix}
a_{11} &amp; \cdots &amp; a_{1n} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{n1} &amp; \cdots &amp; a_{nn}
\end{pmatrix}
\begin{pmatrix}
d_1^{-1} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; d_n^{-1}
\end{pmatrix}
\\
&amp;=
\begin{pmatrix}
\frac{a_{11}}{d_1^2} &amp; \cdots &amp; \frac{a_{1n}}{d_1 d_n} \\
\vdots &amp; \ddots &amp; \vdots \\
\frac{a_{n1}}{d_1 d_n} &amp; \cdots &amp; \frac{a_{nn}}{d_n^2}
\end{pmatrix} \\
&amp;= (a_{ij}/d_id_j)_{i,j}.
\end{aligned}
\]</span></p>
</section>
<section class="level3" id="example-16.1.">
<h3 class="anchored" data-anchor-id="example-16.1.">Example 16.1.</h3>
<p>Derive the inverse of the reverse diagonal matrix</p>
<p><span class="math display">\[
R = \begin{pmatrix}
0 &amp; \cdots &amp; d_1 \\
\vdots &amp; \ddots &amp; \vdots \\
d_n &amp; \cdots &amp; 0
\end{pmatrix}.
\]</span> <strong>Solution</strong>: The inverse matrix <span class="math inline">\(R^{-1}\)</span> is given by <span class="math display">\[
R^{-1} = \begin{pmatrix}
0 &amp; \cdots &amp; d_n^{-1} \\
\vdots &amp; \ddots &amp; \vdots \\
d_1^{-1} &amp; \cdots &amp; 0
\end{pmatrix}.
\]</span></p>
<p><strong>Diagonal matrices</strong> are examples of <strong>sparse matrices</strong>, that is, they have many zeros relative to the non-zero elements.</p>
<p>We next consider the relationship between different vectors.</p>
</section>
<section class="level3" id="definition-16.1.">
<h3 class="anchored" data-anchor-id="definition-16.1.">Definition 16.1.</h3>
<p>A set of vectors <span class="math inline">\(\{x_1, \dots, x_K\}\)</span> with <span class="math inline">\(x_i \in \mathbb{R}^n\)</span> is called <strong>linearly dependent</strong> if there exist scalars <span class="math inline">\(\alpha_1, \dots, \alpha_K\)</span> not all zero such that</p>
<p><span class="math display">\[
\alpha_1 x_1 + \dots + \alpha_K x_K = 0.
\]</span></p>
<p>Suppose that <span class="math inline">\(\alpha_i \neq 0\)</span>, then we can write</p>
<p><span class="math display">\[
x_i = \frac{-1}{\alpha_i} \sum_{j \neq i} \alpha_j x_j
\]</span></p>
<p>so that <span class="math inline">\(x_i\)</span> is a linear combination of the other vectors.</p>
</section>
<section class="level3" id="example-16.2.">
<h3 class="anchored" data-anchor-id="example-16.2.">Example 16.2.</h3>
<p>The two vectors</p>
<p><span class="math display">\[
x_1 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}; \quad
x_2 = \begin{pmatrix} -3 \\ -3 \end{pmatrix}
\]</span></p>
<p>are linearly dependent because</p>
<p><span class="math display">\[
3x_1 + x_2 = 0.
\]</span></p>
</section>
<section class="level3" id="definition-16.2.">
<h3 class="anchored" data-anchor-id="definition-16.2.">Definition 16.2.</h3>
<p>A set of vectors <span class="math inline">\(\{x_1, \dots, x_K\}\)</span> with <span class="math inline">\(x_i \in \mathbb{R}^n\)</span> is <strong>linearly independent</strong> if whenever for some scalars <span class="math inline">\(\alpha_1, \dots, \alpha_K\)</span></p>
<p><span class="math display">\[
\alpha_1 x_1 + \dots + \alpha_K x_K = 0,
\]</span></p>
<p>then necessarily <span class="math inline">\(\alpha_1 = 0, \dots, \alpha_K = 0\)</span>. If <span class="math inline">\(X\alpha = 0\)</span>, where <span class="math inline">\(X\)</span> is the matrix <span class="math inline">\(X = (x_1, \dots, x_K)\)</span>, then <span class="math inline">\(\alpha = 0\)</span> where <span class="math inline">\(\alpha = (\alpha_1, \dots, \alpha_K)^T\)</span>.</p>
</section>
<section class="level3" id="definition-16.3.">
<h3 class="anchored" data-anchor-id="definition-16.3.">Definition 16.3.</h3>
<p>The <strong>column rank</strong> of an <span class="math inline">\(n \times K\)</span> matrix <span class="math inline">\(A\)</span> is the dimension of the column space of <span class="math inline">\(A\)</span>, which is equal to the number of linearly independent columns, while the <strong>row rank</strong> of <span class="math inline">\(A\)</span> is the dimension of the row space of <span class="math inline">\(A\)</span>. The column rank and the row rank are always equal. This number is simply called the <strong>rank</strong> of <span class="math inline">\(A\)</span>. A square matrix of full rank is invertible.</p>
<p>The rank of an <span class="math inline">\(n \times K\)</span> matrix can be: <span class="math inline">\(0, 1, \dots,\)</span> or <span class="math inline">\(\text{min}\{n, K\}\)</span>. A full rank square matrix has rank <span class="math inline">\(n = K\)</span>, and is nonsingular. The zero matrix is of rank zero by convention, otherwise the rank has to be greater than or equal to one.</p>
<p>The matrix</p>
<p><span class="math display">\[
A = \begin{pmatrix}
1 &amp; \cdots &amp; 1 \\
\vdots &amp; \ddots &amp; \vdots \\
1 &amp; \cdots &amp; 1
\end{pmatrix}
= ii^T,
\]</span></p>
<p>where <span class="math inline">\(i = (1, \dots, 1)^T\)</span>, is of rank one because the column vector <span class="math inline">\(i\)</span> is technically linearly independent in the sense that <span class="math inline">\(\alpha i = 0\)</span> if and only if <span class="math inline">\(\alpha = 0\)</span>. In fact, suppose that <span class="math inline">\(v\)</span> is any <span class="math inline">\(n \times 1\)</span> column vector. Then</p>
<p><span class="math display">\[
A = vv^T = \begin{pmatrix}
v_1^2 &amp; v_1 v_2 &amp; \cdots &amp; v_1 v_n \\
v_2 v_1 &amp; v_2^2 &amp; \cdots &amp; v_2 v_n \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
v_n v_1 &amp; v_n v_2 &amp; \cdots &amp; v_n^2
\end{pmatrix}
= (a_1, \dots, a_n)
\]</span></p>
<p>is a rank one matrix. This can be seen by the following argument. Suppose that for some <span class="math inline">\(\alpha_j\)</span> we have</p>
<p><span class="math display">\[
\sum_{j=1}^n \alpha_j a_j = 0.
\]</span></p>
<p>Then</p>
<p><span class="math display">\[
\sum_{j=1}^n \alpha_j v_j \times v = 0,
\]</span></p>
<p>which is possible whenever <span class="math inline">\(\sum_{j=1}^n \alpha_j v_j = 0\)</span>. But there are many candidates for this.</p>
<p>Let <span class="math inline">\(V\)</span> be an <span class="math inline">\(n \times K\)</span> full rank matrix with <span class="math inline">\(K \leq n\)</span>. Then the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(VV^T\)</span> is of rank <span class="math inline">\(K\)</span>, while the <span class="math inline">\(K \times K\)</span> matrix <span class="math inline">\(V^TV\)</span> is of rank <span class="math inline">\(K\)</span> and hence invertible. The <strong>Trace</strong> of a square matrix <span class="math inline">\(A\)</span>, denoted <span class="math inline">\(\text{tr}(A)\)</span>, is defined as the sum of the diagonal elements. For conformable matrices</p>
<p><span class="math display">\[
\begin{aligned}
\text{tr}(A + B) &amp;= \text{tr}(A) + \text{tr}(B) \\
\sum_{i=1}^n \sum_{j=1}^n a_{ij} b_{ji} &amp;= \text{tr}(AB) = \text{tr}(BA) = \sum_{i=1}^n \sum_{j=1}^n b_{ij} a_{ji}
\end{aligned}
\]</span></p>
<p>This is true even if <span class="math inline">\(AB \neq BA\)</span>. For example, when <span class="math inline">\(V\)</span> is <span class="math inline">\(n \times K\)</span> matrix <span class="math inline">\(\text{tr}(VV^T) = \text{tr}(V^TV)\)</span>. For example for column vector <span class="math inline">\(x\)</span> and conformable square matrix <span class="math inline">\(A\)</span>, we have</p>
<p><span class="math display">\[
x^T Ax = \text{tr}(x^T Ax) = \text{tr}(xx^T A)
\]</span></p>
<p>The <strong>determinant</strong> of square matrix is defined recursively. For <span class="math inline">\(2 \times 2\)</span> matrices</p>
<p><span class="math display">\[
\text{det}(A) = \text{det} \begin{pmatrix}
a_{11} &amp; a_{12} \\
a_{21} &amp; a_{22}
\end{pmatrix}
= a_{11}a_{22} - a_{12}a_{21}.
\]</span></p>
<p>For a <span class="math inline">\(3 \times 3\)</span> matrix</p>
<p><span class="math display">\[
A = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{pmatrix}
\]</span></p>
<p>we have</p>
<p><span class="math display">\[
\text{det}(A) = a_{11} \text{det}(A_{11}) - a_{12} \text{det}(A_{12}) + a_{13} \text{det}(A_{13}),
\]</span></p>
<p>where the matrices <span class="math inline">\(A_{ij}\)</span> are defined as the original matrix with the <span class="math inline">\(i\)</span>th row and <span class="math inline">\(j\)</span>th column deleted</p>
<p><span class="math display">\[
A_{11} = \begin{pmatrix} a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33} \end{pmatrix}; \quad
A_{12} = \begin{pmatrix} a_{21} &amp; a_{23} \\ a_{31} &amp; a_{33} \end{pmatrix}; \quad
A_{13} = \begin{pmatrix} a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32} \end{pmatrix}.
\]</span></p>
<p>In general we have</p>
<p><span class="math display">\[
\text{det}(A) = \sum_{j=1}^n (-1)^{j-1} \text{det}(A_{1j}),
\]</span></p>
<p>where <span class="math inline">\(A_{ij}\)</span> is the <span class="math inline">\(n-1 \times n-1\)</span> matrix with the <span class="math inline">\(i\)</span>th row and the <span class="math inline">\(j\)</span>th column removed so that for example</p>
<p><span class="math display">\[
A_{11} = \begin{pmatrix}
a_{22} &amp; \cdots &amp; a_{2n} \\
\vdots &amp; \ddots &amp; \vdots \\
a_{n2} &amp; \cdots &amp; a_{nn}
\end{pmatrix}.
\]</span></p>
<p>For conformable matrices</p>
<p><span class="math display">\[
\begin{aligned}
\text{tr}(AB) &amp;= \text{tr}(BA) \\
\text{det}(AB) &amp;= \text{det}(BA).
\end{aligned}
\]</span></p>
<p>This is true even if <span class="math inline">\(AB \neq BA\)</span>.</p>
<p>A <strong>symmetric matrix</strong>, necessarily square, is one in which <span class="math inline">\(a_{ij} = a_{ji}\)</span> for all <span class="math inline">\(i, j = 1, \dots, n\)</span>. For example,</p>
<p><span class="math display">\[
A = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{12} &amp; a_{22} &amp; a_{23} \\
a_{13} &amp; a_{23} &amp; a_{33}
\end{pmatrix} = A^T.
\]</span></p>
<p>In general, an <span class="math inline">\(n \times n\)</span> symmetric matrix contains <span class="math inline">\(n(n + 1)/2\)</span> unique elements. Matrices can be viewed as linear transformations. Suppose that <span class="math inline">\(A\)</span> is <span class="math inline">\(n \times K\)</span> and of full rank, and <span class="math inline">\(u, v\)</span> are <span class="math inline">\(K \times 1\)</span> vectors. Then we have <span class="math inline">\(Au \in \mathbb{R}^n\)</span>, so that <span class="math inline">\(A: \mathbb{R}^K \to \mathbb{R}^n\)</span>. They are linear because for <span class="math inline">\(c \in \mathbb{R}\)</span></p>
<p><span class="math display">\[
\begin{aligned}
A(cu) &amp;= cAu \\
A(u + v) &amp;= Au + Av.
\end{aligned}
\]</span></p>
</section>
<section class="level3" id="linear-spaces">
<h3 class="anchored" data-anchor-id="linear-spaces">16.1.1 Linear Spaces</h3>
</section>
<section class="level3" id="definition-16.4.">
<h3 class="anchored" data-anchor-id="definition-16.4.">Definition 16.4.</h3>
<p>The Euclidean space <span class="math inline">\(\mathbb{R}^n\)</span> is a <strong>vector space</strong> (linear space) since it has addition and scalar multiplication defined. A subset <span class="math inline">\(L\)</span> is a subspace if, for any <span class="math inline">\(x, z \in L \subset \mathbb{R}^n\)</span> and any <span class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span> we have</p>
<p><span class="math display">\[
\alpha x + \beta z \in L
\]</span></p>
<p>For example,</p>
<p><span class="math display">\[
L = \left\{ x: \sum_{j=1}^n x_j = 0 \right\}
\]</span></p>
<p>is a proper subspace of <span class="math inline">\(\mathbb{R}^n\)</span> of dimension <span class="math inline">\(n - 1\)</span>. Show this. <strong>Solution</strong>: Let <span class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span> and let <span class="math inline">\(x, z \in L\)</span>. Then, because <span class="math inline">\(x, z \in L\)</span>,</p>
<p><span class="math display">\[
\sum_{j=1}^n x_j = 0 \quad \text{and} \quad \sum_{j=1}^n z_j = 0
\]</span></p>
<p>Let <span class="math inline">\(y = \alpha x + \beta z\)</span>. Then,</p>
<p><span class="math display">\[
\sum_{j=1}^n y_j = \sum_{j=1}^n (\alpha x_j + \beta z_j) = \alpha \sum_{j=1}^n x_j + \beta \sum_{j=1}^n z_j = \alpha \cdot 0 + \beta \cdot 0 = 0
\]</span></p>
<p>Thus, <span class="math inline">\(y \in L\)</span>. This proves that <span class="math inline">\(L\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. We will show that the dimension of <span class="math inline">\(L\)</span> is <span class="math inline">\(n - 1\)</span> by finding a set of <span class="math inline">\(n - 1\)</span> linearly independent vectors in <span class="math inline">\(L\)</span>. For <span class="math inline">\(i = 1, 2, \dots, n-1\)</span>, let</p>
<p><span class="math display">\[
v_i = \begin{pmatrix} 0 \\ \vdots \\ 0 \\ 1 \\ 0 \\ \vdots \\ 0 \\ -1 \end{pmatrix},
\]</span> where <span class="math inline">\(v_i\)</span> is an <span class="math inline">\(n \times 1\)</span> column vector with zeros everywhere, except the <span class="math inline">\(i\)</span>-th element is <span class="math inline">\(1\)</span> and the <span class="math inline">\(n\)</span>-th element is <span class="math inline">\(-1\)</span>. Clearly, <span class="math inline">\(\{v_i\}_{i=1}^{n-1} \in L\)</span>. We will now show that these vectors are linearly independent. Suppose that</p>
<p><span class="math display">\[
\sum_{i=1}^{n-1} \alpha_i v_i = \mathbf{0}.
\]</span></p>
<p>Then <span class="math inline">\(\alpha_i = 0\)</span> for <span class="math inline">\(i = 1, 2, \dots, n-1\)</span>. If we add an additional linearly independent vector to the set, for example the vector</p>
<p><span class="math display">\[v_n = \begin{pmatrix} 1 \\ 0 \\ \vdots \\ 0 \\ 0 \end{pmatrix},\]</span> we will have a set of <span class="math inline">\(n\)</span> linearly independent vectors in <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</section>
<section class="level3" id="definition-16.5.">
<h3 class="anchored" data-anchor-id="definition-16.5.">Definition 16.5.</h3>
<p>A set of vectors <span class="math inline">\(x_1, \dots, x_K \in \mathbb{R}^n\)</span> generate a subspace of <span class="math inline">\(\mathbb{R}^n\)</span> called the <strong>span</strong> of <span class="math inline">\(x_1, \dots, x_K\)</span></p>
<p><span class="math display">\[
C(x_1, \dots, x_K) = \{\alpha_1 x_1 + \dots + \alpha_K x_K : \alpha_1, \dots, \alpha_K \in \mathbb{R} \} = \{X\alpha, \quad \alpha \in \mathbb{R}^K \}.
\]</span></p>
</section>
<section class="level3" id="definition-16.6.">
<h3 class="anchored" data-anchor-id="definition-16.6.">Definition 16.6.</h3>
<p>The <strong>null space</strong> of <span class="math inline">\(x_1, \dots, x_K\)</span></p>
<p><span class="math display">\[
N(x_1, \dots, x_K) = \{\alpha_1 x_1 + \dots + \alpha_K x_K = 0 : \alpha_1, \dots, \alpha_K \in \mathbb{R} \}
\]</span></p>
<p>is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. This is also denoted by <span class="math inline">\(C^{\perp}(X)\)</span> and called the <strong>orthocomplement</strong> of <span class="math inline">\(C(X)\)</span>.</p>
</section>
<section class="level3" id="definition-16.7.">
<h3 class="anchored" data-anchor-id="definition-16.7.">Definition 16.7.</h3>
<p>A <strong>basis</strong> for a space <span class="math inline">\(L\)</span> is a set of linearly independent vectors <span class="math inline">\(x_1, \dots, x_K \in \mathbb{R}^n\)</span> such that for any <span class="math inline">\(x \in L\)</span>, there exist scalars <span class="math inline">\(\alpha_1, \dots, \alpha_K\)</span> such that</p>
<p><span class="math display">\[
x = \alpha_1 x_1 + \dots + \alpha_K x_K
\]</span></p>
<p>The dimension of the space <span class="math inline">\(L\)</span> is <span class="math inline">\(K\)</span>.</p>
<p>If the vectors <span class="math inline">\(x_1, \dots, x_K\)</span> are linearly independent then the dimension of <span class="math inline">\(C(x_1, \dots, x_K)\)</span> is <span class="math inline">\(K\)</span> and the dimension of <span class="math inline">\(N(x_1, \dots, x_K)\)</span> is <span class="math inline">\(n - K\)</span>. A basis is not unique.</p>
</section>
<section class="level3" id="definition-16.8.">
<h3 class="anchored" data-anchor-id="definition-16.8.">Definition 16.8.</h3>
<p>Let <span class="math inline">\(x_1, \dots, x_K \in \mathbb{R}^n\)</span> be a basis for the space <span class="math inline">\(L\)</span>. Suppose that <span class="math inline">\(x_i^T x_j = 0\)</span> for <span class="math inline">\(i \neq j\)</span> and <span class="math inline">\(x_i^T x_i = 1\)</span>. Then the basis is <strong>orthonormal</strong> and there is only one such orthonormal basis.</p>
</section>
<section class="level3" id="example-16.3.">
<h3 class="anchored" data-anchor-id="example-16.3.">Example 16.3.</h3>
<p>For example <span class="math inline">\(\mathbb{R}^n\)</span> has orthonormal basis <span class="math inline">\(e_1 = (1, 0, \dots, 0)^T, \dots, e_n = (0, 0, \dots, 0, 1)^T\)</span>.</p>
</section>
<section class="level3" id="eigenvectors-and-eigenvalues">
<h3 class="anchored" data-anchor-id="eigenvectors-and-eigenvalues">16.1.2 Eigenvectors and Eigenvalues</h3>
<p>We next define the concept of <strong>Eigenvectors</strong> and <strong>Eigenvalues</strong>.</p>
</section>
<section class="level3" id="definition-16.9.">
<h3 class="anchored" data-anchor-id="definition-16.9.">Definition 16.9.</h3>
<p>For a real matrix <span class="math inline">\(A\)</span> with dimensions <span class="math inline">\(n \times n\)</span>, a vector <span class="math inline">\(u \in \mathbb{R}^n\)</span> with <span class="math inline">\(u \neq 0\)</span> and scalar <span class="math inline">\(\lambda \in \mathbb{R}\)</span> are called an <strong>eigenvector</strong> and <strong>eigenvalue</strong> respectively of the matrix <span class="math inline">\(A\)</span> if</p>
<p><span class="math display">\[
Au = \lambda u.
\]</span></p>
<p>Clearly, <span class="math inline">\(u = 0\)</span> trivially satisfies this equation for any <span class="math inline">\(\lambda\)</span>, which is why we don’t consider it. The interpretation of eigenvector is a direction that is invariant under the transformation <span class="math inline">\(A\)</span>. Most vectors <span class="math inline">\(u\)</span> will have their “direction” changed by a given transformation <span class="math inline">\(A\)</span>, the eigenvectors are the special ones that are unchanged in direction and just scaled according to <span class="math inline">\(\lambda\)</span> by the transformation. For an identity matrix <span class="math inline">\(A = I_n\)</span>, the eigenvectors are any vectors <span class="math inline">\(u \in \mathbb{R}^n\)</span> and the eigenvalues are all <span class="math inline">\(\lambda = 1\)</span> because <span class="math inline">\(Iu = u\)</span> for all <span class="math inline">\(u \in \mathbb{R}^n\)</span>, the identity transformation does not change anything. Do eigenvalues/eigenvectors always exist, and how many of them are there?</p>
<p>We can write the equation <span class="math inline">\(Au = \lambda u\)</span> as</p>
<p><span class="math display">\[
(A - \lambda I)u = 0,
\]</span></p>
<p>where <span class="math inline">\(0\)</span> is a vector of zeros. If <span class="math inline">\(u \neq 0\)</span>, then <span class="math inline">\(A - \lambda I\)</span> must be singular otherwise we would have a contradiction. Therefore,</p>
<p><span class="math display">\[
\text{det} (A - \lambda I) = 0,
\]</span></p>
<p>which gives one way of finding the eigenvalues. The left hand side of the equation is an <span class="math inline">\(n\)</span>th order polynomial in <span class="math inline">\(\lambda\)</span>, denoted <span class="math inline">\(p_n(\lambda)\)</span>, called the <strong>characteristic polynomial</strong>. We know from the mathematical analysis of polynomial equations that <span class="math inline">\(p_n(\lambda) = 0\)</span> will always have at least one solution, although some of them may be complex valued. In general, there may be one solution, two solutions, or even as many as <span class="math inline">\(n\)</span> distinct solutions, but no more than that, that is, we can write</p>
<p><span class="math display">\[
p_n(\lambda) = (\lambda_1 - \lambda)^{k_1} \times \dots \times (\lambda_p - \lambda)^{k_p},
\]</span></p>
<p>where <span class="math inline">\(\lambda_j, j = 1, \dots, p\)</span> are distinct solutions and <span class="math inline">\(k_j\)</span> are called the multiplicity of the solution, where <span class="math inline">\(p \leq n\)</span> and <span class="math inline">\(\sum_{j=1}^p k_j = n\)</span>. For example, <span class="math inline">\(k_j = 1\)</span> for all <span class="math inline">\(j\)</span> is the case where there are <span class="math inline">\(n\)</span> distinct solutions <span class="math inline">\(\lambda_1, \dots, \lambda_n\)</span>. The <strong>Cayley Hamilton Theorem</strong> says that the matrix <span class="math inline">\(A\)</span> satisfies its own characteristic polynomial, i.e. <span class="math inline">\(p_n(A) = 0\)</span>. The eigenvectors are then found by finding the vectors <span class="math inline">\(u \in \mathbb{R}^n\)</span> that solve <span class="math inline">\((A - \lambda I)u = 0\)</span>.</p>
<p>For real symmetric matrices <span class="math inline">\(A\)</span> the eigenvalues are all real valued because in that case the polynomial coefficients satisfy the conditions to guarantee real solutions. A proof of this result is simple but requires complex numbers.</p>
</section>
<section class="level3" id="example-16.4.">
<h3 class="anchored" data-anchor-id="example-16.4.">Example 16.4.</h3>
<p>In the <span class="math inline">\(2 \times 2\)</span> case we may use this to find explicitly the eigenvalues and hence the eigenvectors. Suppose that</p>
<p><span class="math display">\[
A = \begin{pmatrix} a &amp; b \\ c &amp; d \end{pmatrix}
\]</span></p>
<p>Then we must solve the quadratic equation</p>
<p><span class="math display">\[
\text{det} (A - \lambda I) = (a - \lambda)(d - \lambda) - bc = \lambda^2 - (a + d)\lambda + ad - bc = 0
\]</span></p>
<p>for <span class="math inline">\(\lambda\)</span>. There will always be a solution in the complex plane (fundamental theorem of algebra); the solution will be real provided</p>
<p><span class="math display">\[
\Delta = (a + d)^2 - 4(ad - bc) \geq 0.
\]</span></p>
<p>The general solutions are <span class="math inline">\(\frac{1}{2}(a + d + \sqrt{\Delta})\)</span> and <span class="math inline">\(\frac{1}{2}(a + d - \sqrt{\Delta})\)</span>. In the special case that <span class="math inline">\(c = b = 0\)</span> it is clear that <span class="math inline">\(\lambda = a\)</span> and <span class="math inline">\(\lambda = d\)</span> are the solutions. In the special case that <span class="math inline">\(a = d = 1\)</span> and <span class="math inline">\(b = c = \rho\)</span>, the eigenvalues are <span class="math inline">\(1 + \rho\)</span> and <span class="math inline">\(1 - \rho\)</span>. How to find the eigenvectors? We look for solutions to</p>
<p><span class="math display">\[
\begin{pmatrix}
-\rho &amp; \rho \\
\rho &amp; -\rho
\end{pmatrix}
\begin{pmatrix} u \\ v \end{pmatrix}
=
\begin{pmatrix} 0 \\ 0 \end{pmatrix}
\]</span></p>
<p>That is <span class="math inline">\(v = u\)</span>, which implies that any multiple of <span class="math inline">\((1, 1)\)</span> is a candidate eigenvector. For the other eigenvector we have to solve</p>
<p><span class="math display">\[
\begin{pmatrix} \rho &amp; \rho \\ \rho &amp; \rho \end{pmatrix}
\begin{pmatrix} u \\ v \end{pmatrix}
=
\begin{pmatrix} 0 \\ 0 \end{pmatrix}
\]</span></p>
<p>so that <span class="math inline">\(u = -v\)</span>, and any multiple of <span class="math inline">\((1, -1)\)</span> is a candidate.</p>
<p>If there are <span class="math inline">\(n\)</span> distinct eigenvalues, then eigenvectors are unique up to a scaling factor. Note that if <span class="math inline">\(Au = \lambda u\)</span>, then <span class="math inline">\(Au' = \lambda u'\)</span>, where <span class="math inline">\(u' = ku\)</span> for any scalar <span class="math inline">\(k\)</span>.</p>
</section>
<section class="level3" id="theorem-16.1.">
<h3 class="anchored" data-anchor-id="theorem-16.1.">Theorem 16.1.</h3>
<p>Eigenvectors corresponding to distinct eigenvalues are orthogonal.</p>
<p><strong><em>Proof:</em></strong> Suppose that <span class="math inline">\(Au = \lambda u\)</span> and <span class="math inline">\(Av = \mu v\)</span>. Then <span class="math inline">\(v^T Au = \lambda v^T u\)</span> and <span class="math inline">\(u^T Av = \mu u^T v\)</span>. Therefore,</p>
<p><span class="math display">\[
0 = v^T Au - u^T Av = \lambda v^T u - \mu u^T v = (\lambda - \mu)v^T u,
\]</span></p>
<p>which means that <span class="math inline">\(v^T u = 0\)</span>. <span class="math inline">\(\square\)</span></p>
<p>Let <span class="math inline">\(\lambda^*\)</span> be such that <span class="math inline">\(p_n(\lambda^*) = 0\)</span> and suppose that <span class="math inline">\(u_1, \dots, u_m\)</span> are vectors such that <span class="math inline">\((A - \lambda^* I)u_i = 0\)</span>, then for any <span class="math inline">\(v = \sum_{i=1}^m \alpha_i u_i\)</span> with <span class="math inline">\(\alpha_i \in \mathbb{R}\)</span>, we have</p>
<p><span class="math display">\[
(A - \lambda^* I)v = 0.
\]</span></p>
<p>Some matrices have repeated eigenvalues, in which case the corresponding eigenvectors form a vector space of the same dimensions as the cardinality of multiplicity.</p>
</section>
<section class="level3" id="example-16.5.">
<h3 class="anchored" data-anchor-id="example-16.5.">Example 16.5.</h3>
<p>Consider the real symmetric matrix</p>
<p><span class="math display">\[
A = \begin{pmatrix}
1 &amp; \rho &amp; \rho \\
\rho &amp; 1 &amp; \rho \\
\rho &amp; \rho &amp; 1
\end{pmatrix}.
\]</span></p>
<p>We have two distinct eigenvalues <span class="math inline">\(\lambda = 1 - \rho\)</span> and <span class="math inline">\(\mu = 1 + 2\rho\)</span>. Define the vectors:</p>
<p><span class="math display">\[
u_1 = \begin{pmatrix} -1 \\ 1 \\ 0 \end{pmatrix}; \quad
u_2 = \begin{pmatrix} -1 \\ 0 \\ 1 \end{pmatrix}; \quad
u_3 = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}.
\]</span></p>
<p>Then check that <span class="math inline">\(u_1\)</span> and <span class="math inline">\(u_2\)</span> are distinct eigenvectors associated with the eigenvalue <span class="math inline">\(\lambda\)</span>, while <span class="math inline">\(u_3\)</span> is the eigenvector associated with <span class="math inline">\(\mu\)</span>. We have <span class="math inline">\(u_1^T u_3 = u_2^T u_3 = 0\)</span> but <span class="math inline">\(u_1^T u_2 = 1 \neq 0\)</span>. Define the space</p>
<p><span class="math display">\[
L = \{\alpha u_1 + \beta u_2 : \alpha, \beta \in \mathbb{R} \}.
\]</span></p>
<p>This space has dimensions two since <span class="math inline">\(u_1, u_2\)</span> are linearly independent. Define <span class="math inline">\(e_1 = u_1, e_2 = u_1 - 2u_2\)</span>, and <span class="math inline">\(e_3 = u_3\)</span>. Then <span class="math inline">\(\{e_1, e_2 \}\)</span> is an orthogonal basis for <span class="math inline">\(L\)</span>. Note also that <span class="math inline">\(e_1, e_2\)</span> are eigenvectors associated with <span class="math inline">\(\lambda\)</span>. Finally, we may scale the vectors <span class="math inline">\(e_j\)</span> to have unit length; for example let <span class="math inline">\(e_3^* = e_3 / \sqrt{3}\)</span>.</p>
</section>
<section class="level3" id="theorem-16.2.">
<h3 class="anchored" data-anchor-id="theorem-16.2.">Theorem 16.2.</h3>
<p>Suppose that <span class="math inline">\(A\)</span> is a real symmetric matrix, then it possesses an <strong>Eigendecomposition</strong>, whereby it can be written as</p>
<p><span class="math display">\[
A = Q \Lambda Q^{-1}
\]</span></p>
<p><span class="math display">\[
\Lambda = \begin{pmatrix}
\lambda_1 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n
\end{pmatrix}
\]</span></p>
<p>where <span class="math inline">\(Q = (q_1, \dots, q_n)\)</span> are linearly independent eigenvectors of <span class="math inline">\(A\)</span> associated with the eigenvalues in <span class="math inline">\(\Lambda\)</span>. By convention we organize the eigenvalues in decreasing order so that <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \dots \geq \lambda_n\)</span>. Moreover, there exists a unique orthonormal matrix <span class="math inline">\(U\)</span></p>
<p><span class="math display">\[
\begin{aligned}
A = U \Lambda U^T &amp;= \sum_{i=1}^n \lambda_i u_i u_i^T \\
UU^T = (u_i^T u_j)_{i,j} &amp;= I_n = U^TU
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(u_i, \lambda_i, i = 1, \dots, n\)</span> are the real eigenvectors and real valued eigenvalues (not necessarily distinct) of the matrix <span class="math inline">\(A\)</span>. The matrix <span class="math inline">\(U\)</span> is orthonormal and satisfies <span class="math inline">\(AU = \Lambda U\)</span>. We may equivalently write</p>
<p><span class="math display">\[
U^T A U = \Lambda
\]</span></p>
<p>In general finding eigenvectors requires we solve the equation <span class="math inline">\(Bu = 0\)</span> for a singular matrix <span class="math inline">\(B = A - \lambda I\)</span>. There are many methods for doing this but they are quite involved to describe, so we refer the reader to elsewhere. In low dimensional cases as we have seen it is easy to see how to find the solutions. It follows from this theorem that</p>
<p><span class="math display">\[
\begin{aligned}
\text{tr}(A) = \text{tr}(U \Lambda U^T) &amp;= \text{tr}(U^T U \Lambda) = \sum_{i=1}^n \lambda_i \\
\text{det}(A) = \text{det}(U \Lambda U^T) &amp;= \text{det}(U^T U \Lambda) = \prod_{i=1}^n \lambda_i.
\end{aligned}
\]</span></p>
<p>It also follows that if <span class="math inline">\(c \in \mathbb{R}\)</span>, the eigenvalues of <span class="math inline">\(A + cI_n\)</span> are <span class="math inline">\(\lambda_1 + c, \dots, \lambda_n + c\)</span> and the eigenvectors of <span class="math inline">\(A + cI_n\)</span> are the same as the eigenvectors of <span class="math inline">\(A\)</span> since for eigenvalue, eigenvector pair <span class="math inline">\(\lambda, u\)</span> we have</p>
<p><span class="math display">\[
(A + cI_n)u = Au + cu = (\lambda + c)u.
\]</span></p>
</section>
<section class="level3" id="corollary-16.1.">
<h3 class="anchored" data-anchor-id="corollary-16.1.">Corollary 16.1.</h3>
<p>(Singular Value Decomposition) Suppose that <span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times K\)</span> real matrix. Then there exists a factorization, called a <strong>singular value decomposition</strong> of <span class="math inline">\(A\)</span> of the form</p>
<p><span class="math display">\[
A = USV^T,
\]</span></p>
<p>where <span class="math inline">\(U\)</span> is an <span class="math inline">\(n \times n\)</span> orthonormal matrix, <span class="math inline">\(V\)</span> is a <span class="math inline">\(K \times K\)</span> orthonormal matrix, and <span class="math inline">\(S\)</span> is an <span class="math inline">\(n \times K\)</span> matrix with non-negative real numbers on the diagonal, of the form</p>
<p><span class="math display">\[
S = \begin{pmatrix}
s_1 &amp; \cdots &amp; 0 &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots &amp; \cdots &amp; \vdots \\
0 &amp; \cdots &amp; s_K &amp; \cdots &amp; 0
\end{pmatrix}
= \text{diag}\{s_1, \dots, s_K \} | 0_{n-K \times K}.
\]</span></p>
<p>It follows that for any matrix <span class="math inline">\(A\)</span> we obtain <span class="math inline">\(A^T A = VS^2 V^T = V \Lambda V^T\)</span>, where <span class="math inline">\(\Lambda\)</span> is the matrix of eigenvalues.</p>
</section>
<section class="level3" id="definition-16.10.">
<h3 class="anchored" data-anchor-id="definition-16.10.">Definition 16.10.</h3>
<p>A positive (semi)definite matrix (psd) <span class="math inline">\(A\)</span> satisfies</p>
<p><span class="math display">\[
x^T Ax = \sum_{i=1}^n \sum_{j=1}^n a_{ij}x_i x_j \geq 0
\]</span></p>
<p>for all vectors <span class="math inline">\(x\)</span>. A strictly positive definite matrix (pd) is one for which <span class="math inline">\(x^T Ax &gt; 0\)</span> for all <span class="math inline">\(x\)</span>.</p>
<p>A negative semi-definite matrix satisfies <span class="math inline">\(x^T Ax \leq 0\)</span> for all vectors <span class="math inline">\(x\)</span>. A matrix may be indefinite, i.e., <span class="math inline">\(x^T Ax &gt; 0\)</span> for some <span class="math inline">\(x\)</span> and <span class="math inline">\(x^T Ax &lt; 0\)</span> for other <span class="math inline">\(x\)</span>. The definiteness question can be answered for a real symmetric matrix from its eigenvalues. From the eigendecomposition we see that for any vector <span class="math inline">\(x\)</span></p>
<p><span class="math display">\[
x^T Ax = x^T U \Lambda U^T x = y^T \Lambda y = \sum_{j=1}^n \lambda_j y_j^2,
\]</span></p>
<p>where <span class="math inline">\(y = U^T x\)</span> and <span class="math inline">\(x = Uy\)</span> are in one to one correspondence. A real symmetric matrix is psd if all its eigenvalues are nonnegative.</p>
</section>
<section class="level3" id="example-16.6.">
<h3 class="anchored" data-anchor-id="example-16.6.">Example 16.6.</h3>
<p>Consider the matrix</p>
<p><span class="math display">\[
A = \begin{pmatrix} 1 &amp; \rho \\ \rho &amp; 1 \end{pmatrix}.
\]</span></p>
<p>Then, for any <span class="math inline">\(x = (x_1, x_2)^T\)</span> we have</p>
<p><span class="math display">\[
x^T A x = x_1^2 + x_2^2 + 2x_1 x_2 \rho.
\]</span></p>
<p>The eigenvalues of <span class="math inline">\(A\)</span> are <span class="math inline">\(1 - \rho, \rho + 1\)</span>. Therefore, it is psd if and only if <span class="math inline">\(\rho \in [-1, 1]\)</span>.</p>
</section>
<section class="level3" id="definition-16.11.">
<h3 class="anchored" data-anchor-id="definition-16.11.">Definition 16.11.</h3>
<p>A matrix <span class="math inline">\(A \geq B\)</span> if and only if <span class="math inline">\(A - B\)</span> is psd.</p>
<p>The matrix order is only a partial order, meaning not all matrices can be compared, as the following example illustrates.</p>
</section>
<section class="level3" id="example-16.7.">
<h3 class="anchored" data-anchor-id="example-16.7.">Example 16.7.</h3>
<p><span class="math display">\[
A = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}, \quad
B = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 1/4 \end{pmatrix}
\]</span></p>
<p><span class="math display">\[
A - B = \begin{pmatrix} -1 &amp; 0 \\ 0 &amp; 3/4 \end{pmatrix}
\]</span></p>
<p>Scalar functions of a matrix give a total order, but different scalar functions yield different rankings</p>
<p><span class="math display">\[
\begin{aligned}
2 = \text{tr}(A) &amp;&lt; \text{tr}(B) = 9/4 \\
1 = \text{det}(A) &amp;&gt; \text{det}(B) = 1/2.
\end{aligned}
\]</span></p>
</section>
<section class="level3" id="theorem-16.3.">
<h3 class="anchored" data-anchor-id="theorem-16.3.">Theorem 16.3.</h3>
<p>Let <span class="math inline">\(A\)</span> be a real symmetric matrix. Then, the largest eigenvalue of <span class="math inline">\(A\)</span> denoted <span class="math inline">\(\lambda_1\)</span> or <span class="math inline">\(\lambda_{\text{max}}(A)\)</span>, is the value of the optimized criterion in the constrained optimization problem</p>
<p><span class="math display">\[
\max_{x: x^T x = 1} x^T A x
\]</span></p>
<p>and the optimizing choice of <span class="math inline">\(x\)</span> is the corresponding eigenvector.</p>
<p><strong><em>Proof:</em></strong> For every eigenvalue <span class="math inline">\(\lambda\)</span> there is <span class="math inline">\(x\)</span> such that <span class="math inline">\(Ax = \lambda x\)</span>, which implies that</p>
<p><span class="math display">\[
x^T A x = x^T \lambda x = \lambda
\]</span></p>
<p>so just take the largest such. <span class="math inline">\(\square\)</span></p>
<p>Likewise the smallest eigenvalue is defined through a minimization problem. That is,</p>
<p><span class="math display">\[
\lambda_{\text{min}}(A) = \min_{x: x^T x = 1} x^T A x.
\]</span></p>
<p>The eigendecomposition can be used to define functions of matrices. For example, the square root of a symmetric positive definite matrix is</p>
<p><span class="math display">\[
X = A^{1/2} = U \Lambda^{1/2} U^T = U \begin{pmatrix}
\lambda_1^{1/2} &amp; \cdots &amp; 0 \\
\vdots &amp; \ddots &amp; \vdots \\
0 &amp; \cdots &amp; \lambda_n^{1/2}
\end{pmatrix} U^T.
\]</span></p>
<p>We have <span class="math inline">\(XX = U \Lambda^{1/2} U^T U \Lambda^{1/2} U^T = U \Lambda^{1/2} \Lambda^{1/2} U^T = U \Lambda U^T\)</span>. One may define exponentials and logarithms of matrices in this way.</p>
<p>The eigendecomposition is also useful in establish bounds on quadratic forms.</p>
</section>
<section class="level3" id="theorem-16.4.">
<h3 class="anchored" data-anchor-id="theorem-16.4.">Theorem 16.4.</h3>
<p>Suppose that the <span class="math inline">\(n \times K\)</span> matrix <span class="math inline">\(A\)</span> has full rank with <span class="math inline">\(K \leq n\)</span>. Then for some <span class="math inline">\(C &gt; 0\)</span> we have for all <span class="math inline">\(x \in \mathbb{R}^K\)</span></p>
<p><span class="math display">\[
C||x|| \leq ||Ax|| \leq \frac{1}{C} ||x||.
\]</span> <strong><em>Proof:</em></strong> The matrix A is not necessarily symmetric or even square, but <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> are symmetric. Because A is of full rank, so is <span class="math inline">\(A^TA\)</span>. Therefore, we have. <span class="math display">\[
\lambda_{min}(A^TA) = inf(x/||x||)^TA^TA(x/||x||) \ge C &gt; 0
\]</span> which implies that for any x, <span class="math display">\[
x^TA^TAx \ge C||x||^2
\]</span> as required. Similarly for the upper bound.</p>
</section>
<section class="level3" id="applications">
<h3 class="anchored" data-anchor-id="applications">16.1.3 Applications</h3>
</section>
<section class="level3" id="definition-16.12.">
<h3 class="anchored" data-anchor-id="definition-16.12.">Definition 16.12.</h3>
<p>Suppose that <span class="math inline">\(X \in \mathbb{R}^d\)</span> is a random variable. The covariance matrix of the random vector <span class="math inline">\(X\)</span> is defined as</p>
<p><span class="math display">\[
\begin{aligned}
\text{cov}(X) = E[(X - \mu)(X - \mu)^T] &amp;= E[XX^T] - \mu \mu^T = \Sigma \\
&amp;= \begin{pmatrix}
\sigma_{11} &amp; \cdots &amp; \sigma_{1d} \\
\vdots &amp; \ddots &amp; \vdots \\
\sigma_{1d} &amp; \cdots &amp; \sigma_{dd}
\end{pmatrix},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mu = E(X)\)</span> and <span class="math inline">\(\sigma_{ij} = \text{cov}(X_i, X_j)\)</span>.</p>
<p>Because <span class="math inline">\(\Sigma\)</span> is a covariance matrix this means that it is symmetric (check this) and positive semi-definite, because for any vector <span class="math inline">\(w \in \mathbb{R}^d\)</span></p>
<p><span class="math display">\[
0 \leq \text{var}(w^T X) = E[w^T (X - \mu)(X - \mu)^T w] = w^T \Sigma w.
\]</span></p>
<p>Because it is real and symmetric we may define the inverse and the square root of <span class="math inline">\(\Sigma\)</span>, provided it is strictly positive definite. In fact, the matrix <span class="math inline">\(\Sigma^{-1/2}\)</span> can be defined from the eigendecomposition, as explained above. This is the matrix equivalent of “one over the standard deviation” and it allows us to “standardize” vector random variables.</p>
</section>
<section class="level3" id="theorem-16.5.">
<h3 class="anchored" data-anchor-id="theorem-16.5.">Theorem 16.5.</h3>
<p>Let <span class="math inline">\(Z = \Sigma^{-1/2} (X - \mu)\)</span>. Then <span class="math inline">\(EZ = 0\)</span> and <span class="math inline">\(\text{cov}(Z) = I_d\)</span>.</p>
<p><strong><em>Proof:</em></strong> We have</p>
<p><span class="math display">\[
\begin{aligned}
E(ZZ^T) &amp;= E[\Sigma^{-1/2} (X - \mu)(X - \mu)^T \Sigma^{-1/2}] \\
&amp;= \Sigma^{-1/2} E[(X - \mu)(X - \mu)^T] \Sigma^{-1/2} \\
&amp;= \Sigma^{-1/2} \Sigma \Sigma^{-1/2} \\
&amp;= I_d. \quad \square
\end{aligned}
\]</span></p>
</section>
<section class="level3" id="example-16.8.">
<h3 class="anchored" data-anchor-id="example-16.8.">Example 16.8.</h3>
<p>MULTIVARIATE NORMAL. We say that <span class="math inline">\(X = (X_1, \dots, X_k) \sim MVN_k(\mu, \Sigma)\)</span>, when</p>
<p><span class="math display">\[
f_X(x | \mu, \Sigma) = \frac{1}{(2\pi)^{k/2} \text{det}(\Sigma)^{1/2}} \text{exp} \left( -\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) \right),
\]</span></p>
<p>where <span class="math inline">\(\Sigma\)</span> is a <span class="math inline">\(k \times k\)</span> covariance matrix</p>
<p><span class="math display">\[
\Sigma = \begin{pmatrix}
\sigma_{11} &amp; \sigma_{12} &amp; \cdots &amp; \sigma_{1k} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\sigma_{k1} &amp; \sigma_{k2} &amp; \cdots &amp; \sigma_{kk}
\end{pmatrix}
\]</span></p>
<p>and <span class="math inline">\(\text{det}(\Sigma)\)</span> is the determinant of <span class="math inline">\(\Sigma\)</span>. The characteristic function of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[
E[\text{exp}(it^T X)] = \varphi_X(t | \mu, \Sigma) = \text{exp} \left( -t^T \mu + \frac{1}{2} t^T \Sigma t \right)
\]</span></p>
<p>for any <span class="math inline">\(t \in \mathbb{R}^k\)</span>.</p>
</section>
<section class="level3" id="theorem-16.6.">
<h3 class="anchored" data-anchor-id="theorem-16.6.">Theorem 16.6.</h3>
<p>Suppose that we partition <span class="math inline">\(X = (X_a^T, X_b^T)^T\)</span> and</p>
<p><span class="math display">\[
\mu = \begin{pmatrix} \mu_a \\ \mu_b \end{pmatrix}; \quad
\Sigma = \begin{pmatrix}
\Sigma_{aa} &amp; \Sigma_{ab} \\
\Sigma_{ba} &amp; \Sigma_{bb}
\end{pmatrix}
\]</span></p>
<ol type="a">
<li>If <span class="math inline">\(X \sim MVN_k(\mu, \Sigma)\)</span> then <span class="math inline">\(X_a \sim N(\mu_a, \Sigma_{aa})\)</span>. This is shown by integration of the joint density with respect to the other variables.</li>
<li>The conditional distributions of <span class="math inline">\(X\)</span> are Gaussian too, i.e.,</li>
</ol>
<p><span class="math display">\[
f_{X_a | X_b}(X_a) \sim N(\mu_{X_a | X_b}, \Sigma_{X_a | X_b}),
\]</span></p>
<p>where the conditional mean vector and conditional covariance matrix are given by</p>
<p><span class="math display">\[
\begin{aligned}
\mu_{X_a | X_b} = E(X_a | X_b) &amp;= \mu_a + \Sigma_{ab} \Sigma_{bb}^{-1} (X_b - \mu_b) \\
\Sigma_{X_a | X_b} &amp;= \Sigma_{aa} - \Sigma_{ab} \Sigma_{bb}^{-1} \Sigma_{ba}.
\end{aligned}
\]</span></p>
<ol start="3" type="a">
<li>If <span class="math inline">\(\Sigma\)</span> is diagonal, then <span class="math inline">\(X_1, \dots, X_k\)</span> are mutually independent. In this case</li>
</ol>
<p><span class="math display">\[
\begin{aligned}
\text{det}(\Sigma) &amp;= \sigma_{11} \times \dots \times \sigma_{kk} \\
-\frac{1}{2} (x - \mu)^T \Sigma^{-1} (x - \mu) &amp;= -\frac{1}{2} \sum_{l=1}^k \frac{(x_l - \mu_l)^2}{\sigma_{ll}}
\end{aligned}
\]</span> so that <span class="math display">\[
f_X(x | \mu, \sigma) = \frac{1}{\sigma_{11}^{1/2}\sqrt{2\pi}}exp(-\frac{1}{2}(\frac{x_1-\mu_1}{\sigma_{11}})^2)...
\frac{1}{\sigma_{kk}^{1/2}\sqrt{2\pi}}exp(-\frac{1}{2}(\frac{x_k-\mu_k}{\sigma_{kk}})^2)
\]</span></p>
</section>
<section class="level3" id="example-16.9.">
<h3 class="anchored" data-anchor-id="example-16.9.">Example 16.9.</h3>
<p>Suppose that <span class="math inline">\(X \in \mathbb{R}^d\)</span> and <span class="math inline">\(X \sim N(\mu, \Sigma)\)</span>. We have the eigendecomposition where <span class="math inline">\(\lambda_1 \geq \lambda_2 \geq \dots \geq 0\)</span></p>
<p><span class="math display">\[
\Sigma = \sum_{i=1}^d \lambda_i u_i u_i^T.
\]</span></p>
<p>The first <strong>Principal Component</strong> of the random vector <span class="math inline">\(X\)</span> is the scalar combination <span class="math inline">\(u^T X\)</span> such that</p>
<p><span class="math display">\[
\text{var}(u^T X) = E[u^T (X - \mu) (X - \mu)^T u] = u^T \Sigma u
\]</span></p>
<p>is maximized subject to <span class="math inline">\(u^T u = 1\)</span>, i.e., <span class="math inline">\(u\)</span> is the eigenvector of <span class="math inline">\(\Sigma\)</span> corresponding to the largest eigenvalue of <span class="math inline">\(\Sigma\)</span>: <span class="math inline">\(u_1\)</span>. The largest eigenvalue <span class="math inline">\(\lambda_1 = \text{var}(u_1^T X)\)</span>. Can define</p>
<p><span class="math display">\[
u_1^T X, \dots, u_n^T X
\]</span></p>
<p>as the Principal components of <span class="math inline">\(X\)</span> in decreasing order of importance. The field of Principal Components Analysis originates with Pearson (1901) and is now very widely applied to understand multivariate statistics. See Fig. 16.1.</p>
</section>
<section class="level3" id="example-16.10.">
<h3 class="anchored" data-anchor-id="example-16.10.">Example 16.10.</h3>
<p>Suppose that <span class="math inline">\(X \in \mathbb{R}^d\)</span> and</p>
<p><span class="math display">\[
X \sim N(\mu, \Sigma).
\]</span></p>
<p>The parameters <span class="math inline">\(\theta\)</span> contain all the elements of <span class="math inline">\(\mu = (\mu_1, \dots, \mu_d)\)</span> and the unique elements of the covariance matrix</p>
<p><span class="math display">\[
\Sigma = \begin{pmatrix}
\sigma_{11} &amp; \cdots &amp; \sigma_{1d} \\
\vdots &amp; \ddots &amp; \vdots \\
\sigma_{1d} &amp; \cdots &amp; \sigma_{dd}
\end{pmatrix}
\]</span></p>
<p>Suppose we have a sample <span class="math inline">\(X_1, \dots, X_n\)</span>. The log likelihood is</p>
<p><span class="math display">\[
l(\theta | X^n) = -\frac{nd}{2} \text{log} 2 \pi - \frac{n}{2} \text{log det}(\Sigma) - \frac{1}{2} \sum_{i=1}^n (X_i - \mu)^T \Sigma^{-1} (X_i - \mu)
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
l(\theta | X^n) &amp;= -\frac{nd}{2} \text{log} 2\pi - \frac{n}{2} \text{log det}(\Sigma) - \frac{1}{2} \sum_{i=1}^n \text{tr}((X_i - \mu)(X_i - \mu)^T \Sigma^{-1})\\
&amp;= -\frac{nd}{2} \text{log} 2\pi - \frac{n}{2} \text{log det}(\Sigma) - \frac{n}{2} \text{tr}(S\Sigma^{-1}) \\
&amp; -\frac{n}{2} (\bar{X} - \mu)^T \Sigma^{-1} (\bar{X} - \mu),
\end{aligned}
\]</span></p>
<p>where the sample mean and sample covariance matrix respectively are:</p>
<p><span class="math display">\[
\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i; \quad S = \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})(X_i - \bar{X})^T.
\]</span></p>
<p>This follows because</p>
<p><span class="math display">\[
\sum_{i=1}^n (X_i - \mu)(X_i - \mu)^T = \sum_{i=1}^n (X_i - \bar{X})(X_i - \bar{X})^T + n(\bar{X} - \mu)(\bar{X} - \mu)^T
\]</span></p>
<p>It can be shown that <span class="math inline">\(\bar{X}\)</span> is the MLE of <span class="math inline">\(\mu\)</span> (this is easy) and <span class="math inline">\(S\)</span> is the MLE of <span class="math inline">\(\Sigma\)</span> (this is hard).</p>
</section>
<section class="level3" id="example-16.11.">
<h3 class="anchored" data-anchor-id="example-16.11.">Example 16.11.</h3>
<p>The motivating idea behind the search engine Google is that you want the first items returned by a search to be the most important items. One way is to count the number of sites that contain a link to a given site, and the site that is linked to the most is then the most important site. This has the drawback that all links are treated as equal. If your site is referenced from the home page of Justin Bieber, it counts no more than if it’s referenced by an unknown person with no fan base. The Page rank method takes account of the importance of each website in terms of its links. Suppose there are <span class="math inline">\(N\)</span> web sites, the page rank vector <span class="math inline">\(r\)</span> satisfies</p>
<p><span class="math display">\[
r = \frac{1-d}{N} i + dAr,
\]</span></p>
<p>where <span class="math inline">\(i\)</span> is a vector of ones, while <span class="math inline">\(A = (A_{ij})\)</span> is the <strong>Adjacency matrix</strong> with <span class="math inline">\(A_{ij} = 0\)</span> if page <span class="math inline">\(j\)</span> does not link to page <span class="math inline">\(i\)</span>, and normalized such that, for each <span class="math inline">\(j\)</span>, <span class="math inline">\(\sum_{i=1}^n A_{ij} = 1\)</span>. Here, <span class="math inline">\(d \leq 1\)</span> is a dampening factor. When <span class="math inline">\(d = 1\)</span> we have</p>
<p><span class="math display">\[
Ar = r
\]</span></p>
<p>so that <span class="math inline">\(r\)</span> is the eigenvector of <span class="math inline">\(A\)</span> corresponding to unit eigenvalue.</p>
</section>
<section class="level3" id="example-16.12.">
<h3 class="anchored" data-anchor-id="example-16.12.">Example 16.12.</h3>
<p>Input output analysis. Suppose that the economy has <span class="math inline">\(n\)</span> sectors. Each sector produces <span class="math inline">\(x_i\)</span> units of a single homogeneous good. Assume that the <span class="math inline">\(j\)</span>th sector, in order to produce <span class="math inline">\(1\)</span> unit, must use <span class="math inline">\(a_{ij}\)</span> units from sector <span class="math inline">\(i\)</span>. Furthermore, assume that each sector sells some of its output to other sectors (intermediate output) and some of its output to consumers (final output, or final demand). Call final demand in the <span class="math inline">\(i\)</span>th sector <span class="math inline">\(d_i\)</span>. Then we might write</p>
<p><span class="math display">\[
x - Ax = d
\]</span></p>
<p>where <span class="math inline">\(A = (a_{ij})\)</span>. We can solve this equation by</p>
<p><span class="math display">\[
x = (I - A)^{-1} d
\]</span></p>
<p>provided the inverse exists.</p>
</section>
<section class="level3" id="theorem-16.7.">
<h3 class="anchored" data-anchor-id="theorem-16.7.">Theorem 16.7.</h3>
<p>Let <span class="math inline">\(A\)</span> be an <span class="math inline">\(n \times m\)</span> matrix of real numbers. Either there exists <span class="math inline">\(\pi \in \mathbb{R}^m\)</span> such that <span class="math inline">\(A\pi \geq 0\)</span> for all elements, or there exists <span class="math inline">\(\alpha \in \mathbb{R}^n\)</span> such that <span class="math inline">\(A^T \alpha &lt; 0\)</span> for all elements.</p>
</section>
<section class="level3" id="example-16.13.">
<h3 class="anchored" data-anchor-id="example-16.13.">Example 16.13.</h3>
<p>Dirty Harry offers the following odds on the Premiership title: Arsenal 2 to 1, Manchester City 3 to 1, Chelsea 4 to 1 and Manchester United 5 to 1. You may assume that there are only 4 teams with positive probability of winning. Is it possible to find a combination of bets at these odds that you will surely win no matter who wins the Premiership? We can represent the payoff matrix for the bettor as</p>
<p><span class="math display">\[
A = \begin{pmatrix}
3 &amp; -1 &amp; -1 &amp; -1 \\
-1 &amp; 4 &amp; -1 &amp; -1 \\
-1 &amp; -1 &amp; 5 &amp; -1 \\
-1 &amp; -1 &amp; -1 &amp; 6
\end{pmatrix}.
\]</span></p>
<p>This matrix is positive definite and has eigenvalues <span class="math inline">\(1.1961136, 4.4922513, 5.6077247, 6.7039104\)</span> with first eigenvector</p>
<p><span class="math display">\[
u = \begin{pmatrix} 0.687 \\ 0.507 \\ 0.401 \\ 0.332 \end{pmatrix}
\]</span></p>
<p>that is <span class="math inline">\(Au = \lambda u &gt; 0\)</span>. This says that you should bet in proportion to <span class="math inline">\(u_i / \sum_{i=1}^4 u_i\)</span>, i.e., <span class="math inline">\(0.357\)</span> on Arsenal, <span class="math inline">\(0.263\)</span> on Manchester City, <span class="math inline">\(0.208\)</span> on Chelsea, and <span class="math inline">\(0.172\)</span> on Manchester United. No matter what you will make <span class="math inline">\(1.196\)</span>.</p>
</section>
</section>
<section class="level2" id="systems-of-linear-equations-and-projection">
<h2 class="anchored" data-anchor-id="systems-of-linear-equations-and-projection">16.2 Systems of Linear Equations and Projection</h2>
<p>We next consider linear equations and their solution. Consider the system of equations</p>
<p><span class="math display">\[
Ax = y,
\]</span></p>
<p>where <span class="math inline">\(A\)</span> is <span class="math inline">\(n \times K\)</span> and <span class="math inline">\(y\)</span> is <span class="math inline">\(n \times 1\)</span>, and both are given. We seek the <span class="math inline">\(K \times 1\)</span> solution vector <span class="math inline">\(x\)</span>. In general, there may be no solution to these equations, many solutions, or a unique solution. We suppose that <span class="math inline">\(A\)</span> is of full rank. There are several cases.</p>
<ol type="1">
<li><p>Suppose that <span class="math inline">\(n = K\)</span>. Then, since <span class="math inline">\(A\)</span> is nonsingular we may write the unique solution</p>
<p><span class="math display">\[
x = A^{-1} y
\]</span></p></li>
<li><p>Second, suppose that <span class="math inline">\(n &lt; K\)</span>. In this case, there are multiple solutions. Suppose that <span class="math inline">\(y = 0\)</span>. Then for any <span class="math inline">\(K \times 1\)</span> vector <span class="math inline">\(w\)</span></p>
<p><span class="math display">\[
x = (I_K - A^T(AA^T)^{-1} A)w
\]</span></p>
<p>is a solution to <span class="math inline">\(Ax=y\)</span>. The set of solutions <span class="math inline">\(N = \{x \in \mathbb{R}^K : Ax = 0 \}\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^K\)</span> because <span class="math inline">\(0 \in N\)</span> and if <span class="math inline">\(x_1, x_2 \in N\)</span> then <span class="math inline">\(\alpha_1 x_1 + \alpha_2 x_2 \in N\)</span>. In fact, <span class="math inline">\(N\)</span> is of dimension <span class="math inline">\(K - n\)</span>. Now consider the general case with <span class="math inline">\(y \neq 0\)</span>.</p>
<p>Then for any <span class="math inline">\(K \times 1\)</span> vector <span class="math inline">\(w\)</span> the vector</p>
<p><span class="math display">\[
x = (I_K - A^T(AA^T)^{-1}A)w + A^T(AA^T)^{-1}y
\]</span></p>
<p>is a solution to <span class="math inline">\(Ax = y\)</span></p></li>
<li><p>Suppose that <span class="math inline">\(n &gt; K\)</span>. In this case, the are no solutions to <span class="math inline">\(Ax=y\)</span> except in trivial cases. In that case, the best we can hope for is to find a vector <span class="math inline">\(x\)</span> that minimizes the error, i.e.,</p>
<p><span class="math display">\[
x = \text{arg min}_{x \in \mathbb{R}^K} ||Ax - y|| = \text{arg min}_{x \in \mathbb{R}^K} (Ax - y)^T (Ax - y).
\]</span></p></li>
</ol>
<p>The <strong>Projection theorem</strong> is a famous result of convex analysis that gives the conditions under which there is a unique solution to the minimization problem and characterizes the solution.</p>
<section class="level3" id="theorem-16.8.">
<h3 class="anchored" data-anchor-id="theorem-16.8.">Theorem 16.8.</h3>
<p>Let <span class="math inline">\(x \in \mathbb{R}^n\)</span> and let <span class="math inline">\(L\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. Then there exists a unique point <span class="math inline">\(\hat{y} \in L\)</span> for which Euclidean distance</p>
<p><span class="math display">\[
||x - y||^2 = \sum_{i=1}^n (x_i - y_i)^2
\]</span></p>
<p>is minimized over <span class="math inline">\(L\)</span>. In that case, a necessary and sufficient condition for <span class="math inline">\(\hat{y}\)</span> is that the vector <span class="math inline">\(x - \hat{y}\)</span> be orthogonal to <span class="math inline">\(L\)</span>, meaning for any <span class="math inline">\(y \in L\)</span></p>
<p><span class="math display">\[
\langle y, x - \hat{y} \rangle = y^T(x - \hat{y}) = \sum_{i=1}^n y_i(x_i - \hat{y}_i) = 0
\]</span></p>
</section>
<section class="level3" id="example-16.14.">
<h3 class="anchored" data-anchor-id="example-16.14.">Example 16.14.</h3>
<p>Let <span class="math inline">\(n = 3\)</span> and</p>
<p><span class="math display">\[
X = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \\ 0 &amp; 0 \end{pmatrix}; \quad y = \begin{pmatrix} 1 \\ 1 \\ 1 \end{pmatrix}.
\]</span></p>
<p>Then <span class="math inline">\(C(X)\)</span> is the set of all vectors in <span class="math inline">\(\mathbb{R}^3\)</span> with third component zero. What is the closest point in <span class="math inline">\(C(X)\)</span> to <span class="math inline">\(y\)</span>? This is</p>
<p><span class="math display">\[
(1, 1, 0)^T = \hat{y}, \quad y - \hat{y} = (0, 0, 1)^T
\]</span></p>
<p>In fact <span class="math inline">\(y - \hat{y}\)</span> is orthogonal to <span class="math inline">\(C(X)\)</span> (i.e., to <span class="math inline">\(X\)</span> and any linear combination thereof), i.e., <span class="math inline">\(y - \hat{y} \in C^{\perp}(X) = \{(0, 0, \alpha)^T, \alpha \in \mathbb{R} \}\)</span>.</p>
</section>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch16exercise1">
<h3 class="anchored" data-anchor-id="sec-ch16exercise1">Exercise 1</h3>
<p><a href="#sec-ch16solution1">Solution 1</a></p>
<p>Given matrices <span class="math inline">\(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span> and <span class="math inline">\(B = \begin{pmatrix} 0 &amp; -1 \\ 2 &amp; 3 \end{pmatrix}\)</span>, compute <span class="math inline">\(A + B\)</span>, <span class="math inline">\(AB\)</span>, <span class="math inline">\(BA\)</span>, <span class="math inline">\(A^T\)</span>, and <span class="math inline">\(B^T\)</span>. Verify that <span class="math inline">\((AB)^T = B^T A^T\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise2">
<h3 class="anchored" data-anchor-id="sec-ch16exercise2">Exercise 2</h3>
<p><a href="#sec-ch16solution2">Solution 2</a></p>
<p>For vectors <span class="math inline">\(x = \begin{pmatrix} 1 \\ -2 \\ 3 \end{pmatrix}\)</span> and <span class="math inline">\(y = \begin{pmatrix} 0 \\ 4 \\ -1 \end{pmatrix}\)</span>, calculate the inner product <span class="math inline">\(x^Ty\)</span> and the Euclidean norms <span class="math inline">\(||x||\)</span> and <span class="math inline">\(||y||\)</span>. Are <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> orthogonal?</p>
</section>
<section class="level3" id="sec-ch16exercise3">
<h3 class="anchored" data-anchor-id="sec-ch16exercise3">Exercise 3</h3>
<p><a href="#sec-ch16solution3">Solution 3</a></p>
<p>Find the inverse of the matrix <span class="math inline">\(D = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 5 \end{pmatrix}\)</span>. Verify that <span class="math inline">\(DD^{-1} = I\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise4">
<h3 class="anchored" data-anchor-id="sec-ch16exercise4">Exercise 4</h3>
<p><a href="#sec-ch16solution4">Solution 4</a></p>
<p>Determine if the following set of vectors is linearly dependent or independent: <span class="math inline">\(x_1 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}\)</span>, <span class="math inline">\(x_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix}\)</span>, <span class="math inline">\(x_3 = \begin{pmatrix} 0 \\ 3 \end{pmatrix}\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise5">
<h3 class="anchored" data-anchor-id="sec-ch16exercise5">Exercise 5</h3>
<p><a href="#sec-ch16solution5">Solution 5</a></p>
<p>Calculate the rank of the matrix <span class="math inline">\(A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 1 &amp; 4 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise6">
<h3 class="anchored" data-anchor-id="sec-ch16exercise6">Exercise 6</h3>
<p><a href="#sec-ch16solution6">Solution 6</a></p>
<p>Compute the trace and determinant of the matrix <span class="math inline">\(A = \begin{pmatrix} 1 &amp; -1 \\ 2 &amp; 3 \end{pmatrix}\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise7">
<h3 class="anchored" data-anchor-id="sec-ch16exercise7">Exercise 7</h3>
<p><a href="#sec-ch16solution7">Solution 7</a></p>
<p>Show that the set <span class="math inline">\(L = \{x \in \mathbb{R}^2 : x_1 + 2x_2 = 0 \}\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^2\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise8">
<h3 class="anchored" data-anchor-id="sec-ch16exercise8">Exercise 8</h3>
<p><a href="#sec-ch16solution8">Solution 8</a></p>
<p>Find a basis for the subspace <span class="math inline">\(L\)</span> in Exercise 7. What is the dimension of <span class="math inline">\(L\)</span>?</p>
</section>
<section class="level3" id="sec-ch16exercise9">
<h3 class="anchored" data-anchor-id="sec-ch16exercise9">Exercise 9</h3>
<p><a href="#sec-ch16solution9">Solution 9</a></p>
<p>Given the matrix <span class="math inline">\(A = \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span>, find its eigenvalues and eigenvectors.</p>
</section>
<section class="level3" id="sec-ch16exercise10">
<h3 class="anchored" data-anchor-id="sec-ch16exercise10">Exercise 10</h3>
<p><a href="#sec-ch16solution10">Solution 10</a></p>
<p>Verify the Cayley-Hamilton theorem for the matrix <span class="math inline">\(A\)</span> in Exercise 9.</p>
</section>
<section class="level3" id="sec-ch16exercise11">
<h3 class="anchored" data-anchor-id="sec-ch16exercise11">Exercise 11</h3>
<p><a href="#sec-ch16solution11">Solution 11</a></p>
<p>Show that the eigenvectors corresponding to distinct eigenvalues of the matrix <span class="math inline">\(A\)</span> in Exercise 9 are orthogonal.</p>
</section>
<section class="level3" id="sec-ch16exercise12">
<h3 class="anchored" data-anchor-id="sec-ch16exercise12">Exercise 12</h3>
<p><a href="#sec-ch16solution12">Solution 12</a></p>
<p>Find the eigendecomposition of the matrix <span class="math inline">\(A\)</span> in Exercise 9. Express <span class="math inline">\(A\)</span> in the form <span class="math inline">\(U \Lambda U^T\)</span>, where <span class="math inline">\(U\)</span> is an orthonormal matrix and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix.</p>
</section>
<section class="level3" id="sec-ch16exercise13">
<h3 class="anchored" data-anchor-id="sec-ch16exercise13">Exercise 13</h3>
<p><a href="#sec-ch16solution13">Solution 13</a></p>
<p>Determine if the matrix <span class="math inline">\(A = \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span> is positive definite, negative definite, or indefinite.</p>
</section>
<section class="level3" id="sec-ch16exercise14">
<h3 class="anchored" data-anchor-id="sec-ch16exercise14">Exercise 14</h3>
<p><a href="#sec-ch16solution14">Solution 14</a></p>
<p>Given matrices <span class="math inline">\(A = \begin{pmatrix} 4 &amp; 1 \\ 1 &amp; 4 \end{pmatrix}\)</span> and <span class="math inline">\(B= \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span> , determine if <span class="math inline">\(A \geq B\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise15">
<h3 class="anchored" data-anchor-id="sec-ch16exercise15">Exercise 15</h3>
<p><a href="#sec-ch16solution15">Solution 15</a></p>
<p>Let <span class="math inline">\(X\)</span> be a random vector with covariance matrix <span class="math inline">\(\Sigma = \begin{pmatrix} 4 &amp; 1 \\ 1 &amp; 9 \end{pmatrix}\)</span>. Find a matrix <span class="math inline">\(A\)</span> such that <span class="math inline">\(A \Sigma A^T=I\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise16">
<h3 class="anchored" data-anchor-id="sec-ch16exercise16">Exercise 16</h3>
<p><a href="#sec-ch16solution16">Solution 16</a> Consider the matrix <span class="math inline">\(A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}\)</span>. Find the maximum value of <span class="math inline">\(x^TAx\)</span> subject to the constraint <span class="math inline">\(x^Tx = 1\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise17">
<h3 class="anchored" data-anchor-id="sec-ch16exercise17">Exercise 17</h3>
<p><a href="#sec-ch16solution17">Solution 17</a></p>
<p>Find <span class="math inline">\(A^{1/2}\)</span> for the matrix <span class="math inline">\(A\)</span> in Exercise 9.</p>
</section>
<section class="level3" id="sec-ch16exercise18">
<h3 class="anchored" data-anchor-id="sec-ch16exercise18">Exercise 18</h3>
<p><a href="#sec-ch16solution18">Solution 18</a></p>
<p>Consider the system of equations <span class="math inline">\(Ax = y\)</span> where <span class="math inline">\(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span>, and <span class="math inline">\(y = \begin{pmatrix} 5 \\ 11 \end{pmatrix}\)</span>. Find the solution vector <span class="math inline">\(x\)</span>.</p>
</section>
<section class="level3" id="sec-ch16exercise19">
<h3 class="anchored" data-anchor-id="sec-ch16exercise19">Exercise 19</h3>
<p><a href="#sec-ch16solution19">Solution 19</a> Show that for any matrix <span class="math inline">\(A\)</span>, both <span class="math inline">\(A^TA\)</span> and <span class="math inline">\(AA^T\)</span> are symmetric.</p>
</section>
<section class="level3" id="sec-ch16exercise20">
<h3 class="anchored" data-anchor-id="sec-ch16exercise20">Exercise 20</h3>
<p><a href="#sec-ch16solution20">Solution 20</a> Find a basis for the null space of matrix <span class="math inline">\(A= \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \end{bmatrix}\)</span>. What is the dimension of the null space?</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch16solution1">
<h3 class="anchored" data-anchor-id="sec-ch16solution1">Solution 1</h3>
<p><a href="#sec-ch16exercise1">Exercise 1</a></p>
<p>Given <span class="math inline">\(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span> and <span class="math inline">\(B = \begin{pmatrix} 0 &amp; -1 \\ 2 &amp; 3 \end{pmatrix}\)</span>:</p>
<ul>
<li><p><span class="math inline">\(A + B = \begin{pmatrix} 1+0 &amp; 2+(-1) \\ 3+2 &amp; 4+3 \end{pmatrix} = \begin{pmatrix} 1 &amp; 1 \\ 5 &amp; 7 \end{pmatrix}\)</span>. This follows from the definition of matrix <strong>addition</strong>, described in <strong>Section 16.1</strong>.</p></li>
<li><p><span class="math inline">\(AB = \begin{pmatrix} 1(0) + 2(2) &amp; 1(-1) + 2(3) \\ 3(0) + 4(2) &amp; 3(-1) + 4(3) \end{pmatrix} = \begin{pmatrix} 4 &amp; 5 \\ 8 &amp; 9 \end{pmatrix}\)</span>. This follows from the definition of matrix <strong>multiplication</strong>, described in <strong>Section 16.1</strong>.</p></li>
<li><p><span class="math inline">\(BA = \begin{pmatrix} 0(1) + (-1)(3) &amp; 0(2) + (-1)(4) \\ 2(1) + 3(3) &amp; 2(2) + 3(4) \end{pmatrix} = \begin{pmatrix} -3 &amp; -4 \\ 11 &amp; 16 \end{pmatrix}\)</span>. This follows from the definition of matrix <strong>multiplication</strong>, described in <strong>Section 16.1</strong>. We observe that <span class="math inline">\(AB \neq BA\)</span>. This illustrates that matrix multiplication is not <strong>commutative</strong>.</p></li>
<li><p><span class="math inline">\(A^T = \begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{pmatrix}\)</span>. This follows from the definition of matrix <strong>transpose</strong>, described in <strong>Section 16.1</strong>.</p></li>
<li><p><span class="math inline">\(B^T = \begin{pmatrix} 0 &amp; 2 \\ -1 &amp; 3 \end{pmatrix}\)</span>. This follows from the definition of matrix <strong>transpose</strong>, described in <strong>Section 16.1</strong>.</p></li>
<li><p><span class="math inline">\((AB)^T = \begin{pmatrix} 4 &amp; 8 \\ 5 &amp; 9 \end{pmatrix}\)</span>.</p></li>
<li><p><span class="math inline">\(B^T A^T =  \begin{pmatrix} 0 &amp; 2 \\ -1 &amp; 3 \end{pmatrix}\begin{pmatrix} 1 &amp; 3 \\ 2 &amp; 4 \end{pmatrix} = \begin{pmatrix} 4 &amp; 8 \\ 5 &amp; 9 \end{pmatrix}\)</span>. Thus <span class="math inline">\((AB)^T = B^T A^T\)</span>.</p></li>
</ul>
</section>
<section class="level3" id="sec-ch16solution2">
<h3 class="anchored" data-anchor-id="sec-ch16solution2">Solution 2</h3>
<p><a href="#sec-ch16exercise2">Exercise 2</a></p>
<p>Given <span class="math inline">\(x = \begin{pmatrix} 1 \\ -2 \\ 3 \end{pmatrix}\)</span> and <span class="math inline">\(y = \begin{pmatrix} 0 \\ 4 \\ -1 \end{pmatrix}\)</span>:</p>
<ul>
<li><p><span class="math inline">\(x^Ty = (1)(0) + (-2)(4) + (3)(-1) = 0 - 8 - 3 = -11\)</span>. This follows from the definition of <strong>inner product</strong>, described in <strong>Section 16.1</strong>.</p></li>
<li><p><span class="math inline">\(||x|| = \sqrt{(1)^2 + (-2)^2 + (3)^2} = \sqrt{1 + 4 + 9} = \sqrt{14}\)</span>. This follows from the definition of the <strong>Euclidean norm</strong>, described in <strong>Section 16.1</strong>.</p></li>
<li><p><span class="math inline">\(||y|| = \sqrt{(0)^2 + (4)^2 + (-1)^2} = \sqrt{0 + 16 + 1} = \sqrt{17}\)</span>. This follows from the definition of the <strong>Euclidean norm</strong>, described in <strong>Section 16.1</strong>.</p></li>
<li><p>Since <span class="math inline">\(x^Ty = -11 \neq 0\)</span>, <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are not orthogonal. This follows from the definition of <strong>orthogonal vectors</strong>, described in <strong>Section 16.1</strong>.</p></li>
</ul>
</section>
<section class="level3" id="sec-ch16solution3">
<h3 class="anchored" data-anchor-id="sec-ch16solution3">Solution 3</h3>
<p><a href="#sec-ch16exercise3">Exercise 3</a></p>
<p>Given <span class="math inline">\(D = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 5 \end{pmatrix}\)</span>, the inverse is <span class="math inline">\(D^{-1} = \begin{pmatrix} 1/2 &amp; 0 \\ 0 &amp; 1/5 \end{pmatrix}\)</span>. This follows from the properties of the <strong>inverse of a diagonal matrix</strong>, described in <strong>Section 16.1</strong>.</p>
<p>Verification: <span class="math inline">\(DD^{-1} = \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 5 \end{pmatrix}\begin{pmatrix} 1/2 &amp; 0 \\ 0 &amp; 1/5 \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = I\)</span>.</p>
</section>
<section class="level3" id="sec-ch16solution4">
<h3 class="anchored" data-anchor-id="sec-ch16solution4">Solution 4</h3>
<p><a href="#sec-ch16exercise4">Exercise 4</a></p>
<p>Given <span class="math inline">\(x_1 = \begin{pmatrix} 1 \\ 2 \end{pmatrix}\)</span>, <span class="math inline">\(x_2 = \begin{pmatrix} -1 \\ 1 \end{pmatrix}\)</span>, <span class="math inline">\(x_3 = \begin{pmatrix} 0 \\ 3 \end{pmatrix}\)</span>. We check for linear dependence by setting</p>
<p><span class="math display">\[
\alpha_1 x_1 + \alpha_2 x_2 + \alpha_3 x_3 = 0.
\]</span></p>
<p><span class="math display">\[
\alpha_1 \begin{pmatrix} 1 \\ 2 \end{pmatrix} + \alpha_2 \begin{pmatrix} -1 \\ 1 \end{pmatrix} + \alpha_3 \begin{pmatrix} 0 \\ 3 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix}
\]</span></p>
<p>This gives us the equations:</p>
<p><span class="math display">\[
\begin{cases}
\alpha_1 - \alpha_2 = 0 \\
2\alpha_1 + \alpha_2 + 3\alpha_3 = 0
\end{cases}
\]</span></p>
<p>From the first equation, <span class="math inline">\(\alpha_1 = \alpha_2\)</span>. Substituting into the second equation, <span class="math inline">\(2\alpha_1 + \alpha_1 + 3\alpha_3 = 0\)</span>, which simplifies to <span class="math inline">\(3\alpha_1 + 3\alpha_3 = 0\)</span>. We can choose, for instance, <span class="math inline">\(\alpha_1 = 1\)</span>, <span class="math inline">\(\alpha_2 = 1\)</span> and <span class="math inline">\(\alpha_3 = -1\)</span>. Since we found a non-trivial solution (not all <span class="math inline">\(\alpha_i\)</span> are zero), the vectors are <strong>linearly dependent</strong>, as defined in <strong>Definition 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution5">
<h3 class="anchored" data-anchor-id="sec-ch16solution5">Solution 5</h3>
<p><a href="#sec-ch16exercise5">Exercise 5</a></p>
<p>Given <span class="math inline">\(A = \begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 1 &amp; 4 \\ 0 &amp; 0 &amp; 0 \end{pmatrix}\)</span>. The rank of <span class="math inline">\(A\)</span> is the number of linearly independent rows (or columns). Since the last row is all zeros, and the first two rows are linearly independent, the rank of <span class="math inline">\(A\)</span> is 2. This is consistent with the <strong>Definition 16.3</strong> of <strong>rank</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution6">
<h3 class="anchored" data-anchor-id="sec-ch16solution6">Solution 6</h3>
<p><a href="#sec-ch16exercise6">Exercise 6</a></p>
<p>Given <span class="math inline">\(A = \begin{pmatrix} 1 &amp; -1 \\ 2 &amp; 3 \end{pmatrix}\)</span>.</p>
<ul>
<li><p>The <strong>trace</strong> of <span class="math inline">\(A\)</span>, <span class="math inline">\(\text{tr}(A) = 1 + 3 = 4\)</span>. This follows from the definition of <strong>trace</strong>, provided in <strong>Section 16.1</strong>.</p></li>
<li><p>The <strong>determinant</strong> of <span class="math inline">\(A\)</span>, <span class="math inline">\(\text{det}(A) = (1)(3) - (-1)(2) = 3 + 2 = 5\)</span>. This follows from the formula for the <strong>determinant of a <span class="math inline">\(2 \times 2\)</span> matrix</strong>, provided in <strong>Section 16.1</strong>.</p></li>
</ul>
</section>
<section class="level3" id="sec-ch16solution7">
<h3 class="anchored" data-anchor-id="sec-ch16solution7">Solution 7</h3>
<p><a href="#sec-ch16exercise7">Exercise 7</a></p>
<p>Let <span class="math inline">\(L = \{x \in \mathbb{R}^2 : x_1 + 2x_2 = 0 \}\)</span>. To show <span class="math inline">\(L\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^2\)</span>, we need to show that for any <span class="math inline">\(x, z \in L\)</span> and any <span class="math inline">\(\alpha, \beta \in \mathbb{R}\)</span>, <span class="math inline">\(\alpha x + \beta z \in L\)</span>. This follows the definition of a <strong>subspace</strong> given in <strong>Definition 16.4</strong>.</p>
<p>Let <span class="math inline">\(x = \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} \in L\)</span> and <span class="math inline">\(z = \begin{pmatrix} z_1 \\ z_2 \end{pmatrix} \in L\)</span>. Then <span class="math inline">\(x_1 + 2x_2 = 0\)</span> and <span class="math inline">\(z_1 + 2z_2 = 0\)</span>.</p>
<p>Now consider <span class="math inline">\(y = \alpha x + \beta z = \begin{pmatrix} \alpha x_1 + \beta z_1 \\ \alpha x_2 + \beta z_2 \end{pmatrix}\)</span>. We have:</p>
<p><span class="math display">\[
y_1 + 2y_2 = (\alpha x_1 + \beta z_1) + 2(\alpha x_2 + \beta z_2) = \alpha(x_1 + 2x_2) + \beta(z_1 + 2z_2) = \alpha(0) + \beta(0) = 0.
\]</span></p>
<p>Thus, <span class="math inline">\(y \in L\)</span>, and <span class="math inline">\(L\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^2\)</span>.</p>
</section>
<section class="level3" id="sec-ch16solution8">
<h3 class="anchored" data-anchor-id="sec-ch16solution8">Solution 8</h3>
<p><a href="#sec-ch16exercise8">Exercise 8</a></p>
<p>From Exercise 7, <span class="math inline">\(L = \{x \in \mathbb{R}^2 : x_1 + 2x_2 = 0 \}\)</span>. We can rewrite the condition as <span class="math inline">\(x_1 = -2x_2\)</span>. So, any vector in <span class="math inline">\(L\)</span> can be written as <span class="math inline">\(\begin{pmatrix} -2x_2 \\ x_2 \end{pmatrix} = x_2 \begin{pmatrix} -2 \\ 1 \end{pmatrix}\)</span>.</p>
<p>Thus, a basis for <span class="math inline">\(L\)</span> is the set <span class="math inline">\(\{ \begin{pmatrix} -2 \\ 1 \end{pmatrix} \}\)</span>. The dimension of <span class="math inline">\(L\)</span> is the number of vectors in the basis, which is 1. This follows the definition of <strong>basis</strong> and <strong>dimension</strong> given in <strong>Definition 16.7</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution9">
<h3 class="anchored" data-anchor-id="sec-ch16solution9">Solution 9</h3>
<p><a href="#sec-ch16exercise9">Exercise 9</a></p>
<p>Given <span class="math inline">\(A = \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span>. To find the eigenvalues, we solve <span class="math inline">\(\text{det}(A - \lambda I) = 0\)</span>.</p>
<p><span class="math display">\[
\text{det} \begin{pmatrix} 2-\lambda &amp; 1 \\ 1 &amp; 2-\lambda \end{pmatrix} = (2-\lambda)^2 - 1 = \lambda^2 - 4\lambda + 3 = (\lambda - 1)(\lambda - 3) = 0.
\]</span></p>
<p>The eigenvalues are <span class="math inline">\(\lambda_1 = 1\)</span> and <span class="math inline">\(\lambda_2 = 3\)</span>. This is consistent with the method for finding <strong>eigenvalues</strong>, described in <strong>Section 16.1.2</strong>.</p>
<p>For <span class="math inline">\(\lambda_1 = 1\)</span>, we solve <span class="math inline">\((A - I)u = 0\)</span>:</p>
<p><span class="math display">\[
\begin{pmatrix} 1 &amp; 1 \\ 1 &amp; 1 \end{pmatrix} \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \Rightarrow u_1 + u_2 = 0.
\]</span></p>
<p>An eigenvector is <span class="math inline">\(u_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\)</span>.</p>
<p>For <span class="math inline">\(\lambda_2 = 3\)</span>, we solve <span class="math inline">\((A - 3I)u = 0\)</span>:</p>
<p><span class="math display">\[
\begin{pmatrix} -1 &amp; 1 \\ 1 &amp; -1 \end{pmatrix} \begin{pmatrix} u_1 \\ u_2 \end{pmatrix} = \begin{pmatrix} 0 \\ 0 \end{pmatrix} \Rightarrow -u_1 + u_2 = 0.
\]</span></p>
<p>An eigenvector is <span class="math inline">\(u_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span>.</p>
</section>
<section class="level3" id="sec-ch16solution10">
<h3 class="anchored" data-anchor-id="sec-ch16solution10">Solution 10</h3>
<p><a href="#sec-ch16exercise10">Exercise 10</a></p>
<p>The characteristic polynomial of <span class="math inline">\(A = \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span> is <span class="math inline">\(p(\lambda) = \lambda^2 - 4\lambda + 3\)</span> (from Exercise 9). The Cayley-Hamilton theorem states that <span class="math inline">\(p(A) = 0\)</span>.</p>
<p><span class="math display">\[
p(A) = A^2 - 4A + 3I = \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix} - 4\begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix} + 3\begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix}
\]</span></p>
<p><span class="math display">\[
= \begin{pmatrix} 5 &amp; 4 \\ 4 &amp; 5 \end{pmatrix} - \begin{pmatrix} 8 &amp; 4 \\ 4 &amp; 8 \end{pmatrix} + \begin{pmatrix} 3 &amp; 0 \\ 0 &amp; 3 \end{pmatrix} = \begin{pmatrix} 0 &amp; 0 \\ 0 &amp; 0 \end{pmatrix}.
\]</span></p>
<p>Thus, the Cayley-Hamilton theorem holds. The statement of the theorem is given in <strong>Section 16.1.2</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution11">
<h3 class="anchored" data-anchor-id="sec-ch16solution11">Solution 11</h3>
<p><a href="#sec-ch16exercise11">Exercise 11</a></p>
<p>From Exercise 9, the eigenvectors are <span class="math inline">\(u_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\)</span> and <span class="math inline">\(u_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span>. Their inner product is <span class="math inline">\(u_1^T u_2 = (1)(1) + (-1)(1) = 0\)</span>. Thus, the eigenvectors are orthogonal. This is consistent with <strong>Theorem 16.1</strong>, which states that eigenvectors corresponding to distinct eigenvalues are orthogonal.</p>
</section>
<section class="level3" id="sec-ch16solution12">
<h3 class="anchored" data-anchor-id="sec-ch16solution12">Solution 12</h3>
<p><a href="#sec-ch16exercise12">Exercise 12</a></p>
<p>From Exercise 9, the eigenvalues of <span class="math inline">\(A\)</span> are <span class="math inline">\(\lambda_1 = 1\)</span> and <span class="math inline">\(\lambda_2 = 3\)</span>, and the corresponding eigenvectors are <span class="math inline">\(u_1 = \begin{pmatrix} 1 \\ -1 \end{pmatrix}\)</span> and <span class="math inline">\(u_2 = \begin{pmatrix} 1 \\ 1 \end{pmatrix}\)</span>. We need to normalize the eigenvectors to obtain an orthonormal matrix <span class="math inline">\(U\)</span>.</p>
<p><span class="math display">\[
||u_1|| = \sqrt{1^2 + (-1)^2} = \sqrt{2} \Rightarrow \hat{u}_1 = \begin{pmatrix} 1/\sqrt{2} \\ -1/\sqrt{2} \end{pmatrix}
\]</span></p>
<p><span class="math display">\[
||u_2|| = \sqrt{1^2 + 1^2} = \sqrt{2} \Rightarrow \hat{u}_2 = \begin{pmatrix} 1/\sqrt{2} \\ 1/\sqrt{2} \end{pmatrix}
\]</span></p>
<p><span class="math display">\[
U = \begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}, \quad \Lambda = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 3 \end{pmatrix}
\]</span></p>
<p>Then, <span class="math inline">\(A = U \Lambda U^T\)</span>. This decomposition is discussed in <strong>Theorem 16.2</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution13">
<h3 class="anchored" data-anchor-id="sec-ch16solution13">Solution 13</h3>
<p><a href="#sec-ch16exercise13">Exercise 13</a></p>
<p>Given <span class="math inline">\(A = \begin{pmatrix} 3 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span>. To determine the definiteness, we can check the eigenvalues.</p>
<p><span class="math display">\[
\text{det}(A - \lambda I) = \begin{vmatrix} 3-\lambda &amp; 1 \\ 1 &amp; 2-\lambda \end{vmatrix} = (3-\lambda)(2-\lambda) - 1 = \lambda^2 - 5\lambda + 5 = 0
\]</span></p>
<p>Using the quadratic formula:</p>
<p><span class="math display">\[
\lambda = \frac{-(-5) \pm \sqrt{(-5)^2 - 4(1)(5)}}{2(1)} = \frac{5 \pm \sqrt{5}}{2}
\]</span></p>
<p>Both eigenvalues are positive (<span class="math inline">\(\lambda_1 = \frac{5 + \sqrt{5}}{2} &gt; 0\)</span> and <span class="math inline">\(\lambda_2 = \frac{5 - \sqrt{5}}{2} &gt; 0\)</span>). Thus, the matrix is positive definite. This utilizes the relationship between eigenvalues and definiteness, described in <strong>Section 16.1 after Definition 16.10</strong> stating that a symmetric matrix is positive semi-definite if all eigenvalues are non-negative.</p>
</section>
<section class="level3" id="sec-ch16solution14">
<h3 class="anchored" data-anchor-id="sec-ch16solution14">Solution 14</h3>
<p><a href="#sec-ch16exercise14">Exercise 14</a></p>
<p>Given <span class="math inline">\(A = \begin{pmatrix} 4 &amp; 1 \\ 1 &amp; 4 \end{pmatrix}\)</span> and <span class="math inline">\(B= \begin{pmatrix} 2 &amp; 0 \\ 0 &amp; 2 \end{pmatrix}\)</span>. We check if <span class="math inline">\(A - B\)</span> is positive semi-definite. <span class="math inline">\(A - B = \begin{pmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{pmatrix}\)</span>. We check its eigenvalues. The matrix is psd if and only if all of its eigenvalues are non-negative. The eigenvalues were obtained in Exercise 9 and they are 1 and 3, which are both positive. Thus, <span class="math inline">\(A-B\)</span> is psd, and <span class="math inline">\(A \geq B\)</span>. This follows from <strong>Definition 16.11</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution15">
<h3 class="anchored" data-anchor-id="sec-ch16solution15">Solution 15</h3>
<p><a href="#sec-ch16exercise15">Exercise 15</a></p>
<p>Given <span class="math inline">\(\Sigma = \begin{pmatrix} 4 &amp; 1 \\ 1 &amp; 9 \end{pmatrix}\)</span>. We want to find <span class="math inline">\(A\)</span> such that <span class="math inline">\(A \Sigma A^T = I\)</span>. We can take <span class="math inline">\(A = \Sigma^{-1/2}\)</span>. We first find the eigendecomposition of <span class="math inline">\(\Sigma=U\Lambda U^T\)</span> where <span class="math inline">\(\Lambda\)</span> is a diagonal matrix with the eigenvalues and <span class="math inline">\(U\)</span> is an orthogonal matrix (<span class="math inline">\(U^TU = I\)</span>) formed by the eigenvectors. <span class="math inline">\(A\)</span> will be then <span class="math inline">\(U\Lambda^{-1/2}U^T\)</span>.</p>
<p>The characteristic polynomial is: <span class="math inline">\((4 - \lambda)(9 - \lambda) - 1 = \lambda^2 - 13\lambda + 35 = 0\)</span>.</p>
<p>The eigenvalues are:</p>
<p><span class="math inline">\(\lambda = \dfrac{13 \pm \sqrt{13^2 - 4 \cdot 35}}{2} = \dfrac{13 \pm \sqrt{29}}{2}\)</span>. <span class="math inline">\(\lambda_1 = \dfrac{13 + \sqrt{29}}{2}\)</span> and <span class="math inline">\(\lambda_2 = \dfrac{13 - \sqrt{29}}{2}\)</span> <span class="math inline">\(\Lambda = \begin{bmatrix} \dfrac{13 + \sqrt{29}}{2} &amp; 0 \\ 0 &amp; \dfrac{13 - \sqrt{29}}{2}  \end{bmatrix}\)</span></p>
<p>The eigenvectors need to be computed and the orthogonal matrix <span class="math inline">\(U\)</span> needs to be formed. The matrix <span class="math inline">\(\Sigma^{-1/2} = A = U \Lambda^{-1/2} U^T\)</span></p>
</section>
<section class="level3" id="sec-ch16solution16">
<h3 class="anchored" data-anchor-id="sec-ch16solution16">Solution 16</h3>
<p><a href="#sec-ch16exercise16">Exercise 16</a> The maximum value of <span class="math inline">\(x^T A x\)</span> subject to <span class="math inline">\(x^Tx=1\)</span> is the largest eigenvalue of <span class="math inline">\(A\)</span>. We can find the eigenvalues of <span class="math inline">\(A = \begin{bmatrix} 2 &amp; 1 \\ 1 &amp; 2 \end{bmatrix}\)</span> by solving the equation <span class="math inline">\(det(A-\lambda I) = 0\)</span> as follows:</p>
<p><span class="math inline">\(det(A-\lambda I) = det(\begin{bmatrix} 2-\lambda &amp; 1 \\ 1 &amp; 2-\lambda \end{bmatrix}) = (2-\lambda)^2 - 1 = 4 - 4\lambda + \lambda^2 - 1 =  \lambda^2-4\lambda + 3 = (\lambda - 1)(\lambda - 3) = 0\)</span>.</p>
<p>Thus, the eigenvalues are <span class="math inline">\(\lambda_1 = 1\)</span>, <span class="math inline">\(\lambda_2 = 3\)</span>. The largest eigenvalue is 3. This method follows directly from <strong>Theorem 16.3</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution17">
<h3 class="anchored" data-anchor-id="sec-ch16solution17">Solution 17</h3>
<p><a href="#sec-ch16exercise17">Exercise 17</a> From solution 12: <span class="math display">\[
U = \begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix}, \quad \Lambda = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 3 \end{pmatrix}
\]</span> <span class="math display">\[
\Lambda^{1/2} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{3} \end{pmatrix}
\]</span> <span class="math display">\[
A^{1/2} = U \Lambda^{1/2} U^T =  \begin{pmatrix} 1/\sqrt{2} &amp; 1/\sqrt{2} \\ -1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix} \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; \sqrt{3} \end{pmatrix} \begin{pmatrix} 1/\sqrt{2} &amp; -1/\sqrt{2} \\ 1/\sqrt{2} &amp; 1/\sqrt{2} \end{pmatrix} = \begin{pmatrix} \frac{1+\sqrt{3}}{2} &amp; \frac{-1+\sqrt{3}}{2} \\ \frac{-1+\sqrt{3}}{2} &amp; \frac{1+\sqrt{3}}{2} \end{pmatrix}
\]</span> The method for computing <span class="math inline">\(A^{1/2}\)</span> is shown in the paragraph preceding <strong>Theorem 16.4</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution18">
<h3 class="anchored" data-anchor-id="sec-ch16solution18">Solution 18</h3>
<p><a href="#sec-ch16exercise18">Exercise 18</a> We have <span class="math inline">\(A = \begin{pmatrix} 1 &amp; 2 \\ 3 &amp; 4 \end{pmatrix}\)</span>, and <span class="math inline">\(y = \begin{pmatrix} 5 \\ 11 \end{pmatrix}\)</span>. Since <span class="math inline">\(n=K=2\)</span> we have the unique solution <span class="math inline">\(x = A^{-1}y\)</span>. We calculate <span class="math inline">\(A^{-1}\)</span>. <span class="math inline">\(det(A) = (1)(4) - (2)(3) = -2\)</span> <span class="math inline">\(A^{-1} = \frac{1}{-2}\begin{pmatrix} 4 &amp; -2 \\ -3 &amp; 1 \end{pmatrix} = \begin{pmatrix} -2 &amp; 1 \\ 3/2 &amp; -1/2 \end{pmatrix}\)</span>. <span class="math inline">\(x = A^{-1}y = \begin{pmatrix} -2 &amp; 1 \\ 3/2 &amp; -1/2 \end{pmatrix} \begin{pmatrix} 5 \\ 11 \end{pmatrix} = \begin{pmatrix} -10 + 11 \\ 15/2 - 11/2 \end{pmatrix} =  \begin{pmatrix} 1 \\ 2 \end{pmatrix}\)</span>.</p>
</section>
<section class="level3" id="sec-ch16solution19">
<h3 class="anchored" data-anchor-id="sec-ch16solution19">Solution 19</h3>
<p><a href="#sec-ch16exercise19">Exercise 19</a> Let <span class="math inline">\(A\)</span> be a matrix. By definition of the transpose operation, <span class="math inline">\((A^T)^T = A\)</span>. Then, <span class="math inline">\((A^TA)^T = A^T(A^T)^T = A^TA\)</span>. So, <span class="math inline">\(A^TA\)</span> is symmetric. Similarly, <span class="math inline">\((AA^T)^T = (A^T)^TA^T = AA^T\)</span>. Therefore, <span class="math inline">\(AA^T\)</span> is also symmetric. This solution uses the definition of the <strong>transpose</strong> operation, described in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16solution20">
<h3 class="anchored" data-anchor-id="sec-ch16solution20">Solution 20</h3>
<p><a href="#sec-ch16exercise20">Exercise 20</a></p>
<p>The null space <span class="math inline">\(N(A)\)</span> of the matrix <span class="math inline">\(A\)</span> is the set of vectors <span class="math inline">\(x\)</span> such that <span class="math inline">\(Ax=0\)</span>. Let <span class="math inline">\(x = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}\)</span> Then <span class="math inline">\(Ax = \begin{bmatrix} 1 &amp; 2 \\ 2 &amp; 4 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = \begin{bmatrix} x_1 + 2x_2 \\ 2x_1 + 4x_2 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \end{bmatrix}\)</span> The system reduces to one equation <span class="math inline">\(x_1 + 2x_2 = 0\)</span> or <span class="math inline">\(x_1 = -2x_2\)</span>. Then, the solution is <span class="math inline">\(x = \begin{bmatrix} -2x_2 \\ x_2 \end{bmatrix} = x_2\begin{bmatrix} -2 \\ 1 \end{bmatrix}\)</span>. Thus a basis for the null space of <span class="math inline">\(A\)</span> is the set <span class="math inline">\(\{ \begin{bmatrix} -2 \\ 1 \end{bmatrix} \}\)</span> and the dimension of the null space is 1. This follows the definition of <strong>null space</strong> given in <strong>Definition 16.6</strong>, and the definition of <strong>basis</strong> and <strong>dimension</strong> in <strong>Definition 16.7</strong>.</p>
</section>
</section>
<section class="level2" id="r-script-examples">
<h2 class="anchored" data-anchor-id="r-script-examples">R Script Examples</h2>
<section class="level3" id="r-script-1-matrix-operations-and-linear-dependence">
<h3 class="anchored" data-anchor-id="r-script-1-matrix-operations-and-linear-dependence">R Script 1: Matrix Operations and Linear Dependence</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="co"># Define two matrices</span></span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb3-3"><a aria-hidden="true" href="#cb3-3" tabindex="-1"></a>B <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">0</span>, <span class="sc">-</span><span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb3-4"><a aria-hidden="true" href="#cb3-4" tabindex="-1"></a></span>
<span id="cb3-5"><a aria-hidden="true" href="#cb3-5" tabindex="-1"></a><span class="co"># Matrix addition</span></span>
<span id="cb3-6"><a aria-hidden="true" href="#cb3-6" tabindex="-1"></a>A_plus_B <span class="ot">&lt;-</span> A <span class="sc">+</span> B</span>
<span id="cb3-7"><a aria-hidden="true" href="#cb3-7" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"A + B:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "A + B:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a aria-hidden="true" href="#cb5-1" tabindex="-1"></a><span class="fu">print</span>(A_plus_B)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    1    1
[2,]    5    7</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a aria-hidden="true" href="#cb7-1" tabindex="-1"></a><span class="co"># Matrix multiplication</span></span>
<span id="cb7-2"><a aria-hidden="true" href="#cb7-2" tabindex="-1"></a>AB <span class="ot">&lt;-</span> A <span class="sc">%*%</span> B</span>
<span id="cb7-3"><a aria-hidden="true" href="#cb7-3" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"A * B:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "A * B:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a aria-hidden="true" href="#cb9-1" tabindex="-1"></a><span class="fu">print</span>(AB)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    4    5
[2,]    8    9</code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a aria-hidden="true" href="#cb11-1" tabindex="-1"></a><span class="co"># Matrix transpose</span></span>
<span id="cb11-2"><a aria-hidden="true" href="#cb11-2" tabindex="-1"></a>A_transpose <span class="ot">&lt;-</span> <span class="fu">t</span>(A)</span>
<span id="cb11-3"><a aria-hidden="true" href="#cb11-3" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"A Transpose:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "A Transpose:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a aria-hidden="true" href="#cb13-1" tabindex="-1"></a><span class="fu">print</span>(A_transpose)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    1    3
[2,]    2    4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a aria-hidden="true" href="#cb15-1" tabindex="-1"></a><span class="co"># Define vectors</span></span>
<span id="cb15-2"><a aria-hidden="true" href="#cb15-2" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span><span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb15-3"><a aria-hidden="true" href="#cb15-3" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">4</span>, <span class="sc">-</span><span class="dv">1</span>)</span>
<span id="cb15-4"><a aria-hidden="true" href="#cb15-4" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb15-5"><a aria-hidden="true" href="#cb15-5" tabindex="-1"></a></span>
<span id="cb15-6"><a aria-hidden="true" href="#cb15-6" tabindex="-1"></a><span class="co"># Inner product</span></span>
<span id="cb15-7"><a aria-hidden="true" href="#cb15-7" tabindex="-1"></a>inner_product <span class="ot">&lt;-</span> <span class="fu">t</span>(x) <span class="sc">%*%</span> y</span>
<span id="cb15-8"><a aria-hidden="true" href="#cb15-8" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Inner Product of x and y:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Inner Product of x and y:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a aria-hidden="true" href="#cb17-1" tabindex="-1"></a><span class="fu">print</span>(inner_product)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]
[1,]  -11</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a aria-hidden="true" href="#cb19-1" tabindex="-1"></a><span class="co"># Euclidean norm</span></span>
<span id="cb19-2"><a aria-hidden="true" href="#cb19-2" tabindex="-1"></a>norm_x <span class="ot">&lt;-</span> <span class="fu">norm</span>(x, <span class="at">type =</span> <span class="st">"2"</span>)  <span class="co"># "2" specifies Euclidean norm</span></span>
<span id="cb19-3"><a aria-hidden="true" href="#cb19-3" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Euclidean Norm of x:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Euclidean Norm of x:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a aria-hidden="true" href="#cb21-1" tabindex="-1"></a><span class="fu">print</span>(norm_x)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3.741657</code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a aria-hidden="true" href="#cb23-1" tabindex="-1"></a><span class="co"># Check for linear dependence</span></span>
<span id="cb23-2"><a aria-hidden="true" href="#cb23-2" tabindex="-1"></a><span class="co"># Define a matrix with the vectors as columns</span></span>
<span id="cb23-3"><a aria-hidden="true" href="#cb23-3" tabindex="-1"></a>vectors_matrix <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x, y, z)</span>
<span id="cb23-4"><a aria-hidden="true" href="#cb23-4" tabindex="-1"></a><span class="co">#Calculate the determinant</span></span>
<span id="cb23-5"><a aria-hidden="true" href="#cb23-5" tabindex="-1"></a>det_vectors_matrix <span class="ot">=</span> <span class="fu">det</span>(vectors_matrix)</span>
<span id="cb23-6"><a aria-hidden="true" href="#cb23-6" tabindex="-1"></a><span class="co"># Vectors are linearly dependent if the determinant of the matrix is 0.</span></span>
<span id="cb23-7"><a aria-hidden="true" href="#cb23-7" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Determinant of the matrix formed by the vectors"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Determinant of the matrix formed by the vectors"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a aria-hidden="true" href="#cb25-1" tabindex="-1"></a><span class="fu">print</span>(det_vectors_matrix)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a aria-hidden="true" href="#cb27-1" tabindex="-1"></a><span class="co">#Example of linearly dependent vectors</span></span>
<span id="cb27-2"><a aria-hidden="true" href="#cb27-2" tabindex="-1"></a>x1 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb27-3"><a aria-hidden="true" href="#cb27-3" tabindex="-1"></a>x2 <span class="ot">=</span> <span class="fu">c</span>(<span class="sc">-</span><span class="dv">3</span>, <span class="sc">-</span><span class="dv">3</span>)</span>
<span id="cb27-4"><a aria-hidden="true" href="#cb27-4" tabindex="-1"></a>x3 <span class="ot">=</span> <span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">0</span>)</span>
<span id="cb27-5"><a aria-hidden="true" href="#cb27-5" tabindex="-1"></a>linearly_dependent_matrix <span class="ot">=</span> <span class="fu">cbind</span>(x1,x2,x3)</span>
<span id="cb27-6"><a aria-hidden="true" href="#cb27-6" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Determinant of a matrix with linearly dependent columns"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Determinant of a matrix with linearly dependent columns"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a aria-hidden="true" href="#cb29-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">try</span>(<span class="fu">det</span>(linearly_dependent_matrix), <span class="cn">TRUE</span>))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Error in determinant.matrix(x, logarithm = TRUE, ...) : \n  'x' must be a square matrix\n"
attr(,"class")
[1] "try-error"
attr(,"condition")
&lt;simpleError in determinant.matrix(x, logarithm = TRUE, ...): 'x' must be a square matrix&gt;</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load Libraries:</strong> The <code>tidyverse</code> library is loaded for general data manipulation and visualization. It includes packages like <code>dplyr</code>, <code>ggplot2</code>, etc.</li>
<li><strong>Define Matrices:</strong> Two 2x2 matrices, <code>A</code> and <code>B</code>, are defined using the <code>matrix()</code> function. The <code>nrow</code> argument specifies the number of rows, and <code>byrow = TRUE</code> indicates that the matrix should be filled row-wise. This corresponds to the <strong>definition of a matrix</strong> in Section 16.1.</li>
<li><strong>Matrix Addition:</strong> <code>A + B</code> performs element-wise addition of the two matrices. This is consistent with the definition of <strong>matrix addition</strong> in Section 16.1.</li>
<li><strong>Matrix Multiplication:</strong> <code>A %*% B</code> performs matrix multiplication. The <code>%*%</code> operator is specifically for matrix multiplication in R. This follows the rules of <strong>matrix multiplication</strong> in Section 16.1.</li>
<li><strong>Matrix Transpose:</strong> <code>t(A)</code> calculates the transpose of matrix <code>A</code>. The rows become columns and vice versa. This corresponds to the <strong>definition of the transpose of a matrix</strong> provided in <strong>Section 16.1</strong>.</li>
<li><strong>Define Vectors:</strong> Two vectors, <code>x</code> and <code>y</code>, are defined using the <code>c()</code> function, which creates vectors. This corresponds to the <strong>definition of vectors</strong> in <strong>Section 16.1</strong>.</li>
<li><strong>Inner Product:</strong> <code>t(x) %*% y</code> computes the inner product (dot product) of <code>x</code> and <code>y</code>. We transpose <code>x</code> to make it a row vector, allowing for matrix multiplication with the column vector <code>y</code>. This is consistent with the <strong>definition of an inner product</strong> in <strong>Section 16.1</strong>.</li>
<li><strong>Euclidean Norm:</strong> <code>norm(x, type = "2")</code> calculates the Euclidean norm (or magnitude) of vector <code>x</code>. The <code>type = "2"</code> argument specifies the Euclidean norm (L2 norm). This relates to the definition of the <strong>Euclidean norm</strong> in <strong>Section 16.1</strong>.</li>
<li><strong>Check for linear dependence:</strong> We form a matrix whose columns are the vectors we defined. If the determinant of this matrix is zero, this would imply linear dependence. Then we show an example with linearly dependent vectors. This follows from <strong>Definitions 16.1, 16.2 and 16.3</strong>.</li>
</ol>
</section>
<section class="level3" id="r-script-2-eigenvalues-and-eigenvectors">
<h3 class="anchored" data-anchor-id="r-script-2-eigenvalues-and-eigenvectors">R Script 2: Eigenvalues and Eigenvectors</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb31-1"><a aria-hidden="true" href="#cb31-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb31-2"><a aria-hidden="true" href="#cb31-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb31-3"><a aria-hidden="true" href="#cb31-3" tabindex="-1"></a></span>
<span id="cb31-4"><a aria-hidden="true" href="#cb31-4" tabindex="-1"></a><span class="co"># Define a symmetric matrix</span></span>
<span id="cb31-5"><a aria-hidden="true" href="#cb31-5" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb31-6"><a aria-hidden="true" href="#cb31-6" tabindex="-1"></a></span>
<span id="cb31-7"><a aria-hidden="true" href="#cb31-7" tabindex="-1"></a><span class="co"># Calculate eigenvalues and eigenvectors</span></span>
<span id="cb31-8"><a aria-hidden="true" href="#cb31-8" tabindex="-1"></a>eigen_results <span class="ot">&lt;-</span> <span class="fu">eigen</span>(A)</span>
<span id="cb31-9"><a aria-hidden="true" href="#cb31-9" tabindex="-1"></a></span>
<span id="cb31-10"><a aria-hidden="true" href="#cb31-10" tabindex="-1"></a><span class="co"># Extract eigenvalues</span></span>
<span id="cb31-11"><a aria-hidden="true" href="#cb31-11" tabindex="-1"></a>eigenvalues <span class="ot">&lt;-</span> eigen_results<span class="sc">$</span>values</span>
<span id="cb31-12"><a aria-hidden="true" href="#cb31-12" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Eigenvalues:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Eigenvalues:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb33-1"><a aria-hidden="true" href="#cb33-1" tabindex="-1"></a><span class="fu">print</span>(eigenvalues)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 3 1</code></pre>
</div>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb35-1"><a aria-hidden="true" href="#cb35-1" tabindex="-1"></a><span class="co"># Extract eigenvectors</span></span>
<span id="cb35-2"><a aria-hidden="true" href="#cb35-2" tabindex="-1"></a>eigenvectors <span class="ot">&lt;-</span> eigen_results<span class="sc">$</span>vectors</span>
<span id="cb35-3"><a aria-hidden="true" href="#cb35-3" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Eigenvectors:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Eigenvectors:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb37-1"><a aria-hidden="true" href="#cb37-1" tabindex="-1"></a><span class="fu">print</span>(eigenvectors)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]       [,2]
[1,] 0.7071068 -0.7071068
[2,] 0.7071068  0.7071068</code></pre>
</div>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a aria-hidden="true" href="#cb39-1" tabindex="-1"></a><span class="co"># Verify that eigenvectors corresponding to distinct eigenvalues are orthogonal.</span></span>
<span id="cb39-2"><a aria-hidden="true" href="#cb39-2" tabindex="-1"></a><span class="co">#Check that eigenvectors are orthogonal (inner product is approximately zero)</span></span>
<span id="cb39-3"><a aria-hidden="true" href="#cb39-3" tabindex="-1"></a>orthogonal_check <span class="ot">=</span> <span class="fu">t</span>(eigenvectors[,<span class="dv">1</span>])<span class="sc">%*%</span>eigenvectors[,<span class="dv">2</span>]</span>
<span id="cb39-4"><a aria-hidden="true" href="#cb39-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Inner product of eigenvectors"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Inner product of eigenvectors"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb41"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb41-1"><a aria-hidden="true" href="#cb41-1" tabindex="-1"></a><span class="fu">print</span>(orthogonal_check)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]
[1,] 2.237114e-17</code></pre>
</div>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb43-1"><a aria-hidden="true" href="#cb43-1" tabindex="-1"></a><span class="co"># Verify the equation A * eigenvector = eigenvalue * eigenvector</span></span>
<span id="cb43-2"><a aria-hidden="true" href="#cb43-2" tabindex="-1"></a><span class="co"># For the first eigenvalue and eigenvector</span></span>
<span id="cb43-3"><a aria-hidden="true" href="#cb43-3" tabindex="-1"></a>verification1 <span class="ot">&lt;-</span> A <span class="sc">%*%</span> eigenvectors[, <span class="dv">1</span>]</span>
<span id="cb43-4"><a aria-hidden="true" href="#cb43-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"A * eigenvector1:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "A * eigenvector1:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a aria-hidden="true" href="#cb45-1" tabindex="-1"></a><span class="fu">print</span>(verification1)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        [,1]
[1,] 2.12132
[2,] 2.12132</code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a aria-hidden="true" href="#cb47-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"eigenvalue1 * eigenvector1"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "eigenvalue1 * eigenvector1"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a aria-hidden="true" href="#cb49-1" tabindex="-1"></a><span class="fu">print</span>(eigenvalues[<span class="dv">1</span>] <span class="sc">*</span> eigenvectors[, <span class="dv">1</span>])</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2.12132 2.12132</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Define a Symmetric Matrix:</strong> A 2x2 symmetric matrix <code>A</code> is defined.</li>
<li><strong>Calculate Eigenvalues and Eigenvectors:</strong> The <code>eigen()</code> function computes both the eigenvalues and eigenvectors of the matrix <code>A</code>. This directly implements the concepts described in <strong>Section 16.1.2</strong>.</li>
<li><strong>Extract Eigenvalues:</strong> <code>eigen_results$values</code> extracts the eigenvalues from the results of the <code>eigen()</code> function.</li>
<li><strong>Extract Eigenvectors:</strong> <code>eigen_results$vectors</code> extracts the eigenvectors. Each column of this matrix represents an eigenvector.</li>
<li><strong>Verify Orthogonality:</strong> The eigenvectors are checked if they are orthogonal, which follows from <strong>Theorem 16.1</strong>.</li>
<li><strong>Verify Eigenvalue-Eigenvector Relationship:</strong> We verify the fundamental relationship <span class="math inline">\(Au = \lambda u\)</span>, where <span class="math inline">\(u\)</span> is an eigenvector and <span class="math inline">\(\lambda\)</span> is the corresponding eigenvalue. We check this relationship by explicitly multiplying the eigenvector by the matrix <span class="math inline">\(A\)</span> and multiplying the eigenvector by the eigenvalue.</li>
</ol>
</section>
<section class="level3" id="r-script-3-eigendecomposition-and-positive-definiteness">
<h3 class="anchored" data-anchor-id="r-script-3-eigendecomposition-and-positive-definiteness">R Script 3: Eigendecomposition and Positive Definiteness</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a aria-hidden="true" href="#cb51-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb51-2"><a aria-hidden="true" href="#cb51-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb51-3"><a aria-hidden="true" href="#cb51-3" tabindex="-1"></a></span>
<span id="cb51-4"><a aria-hidden="true" href="#cb51-4" tabindex="-1"></a><span class="co"># Define a symmetric matrix</span></span>
<span id="cb51-5"><a aria-hidden="true" href="#cb51-5" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">2</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb51-6"><a aria-hidden="true" href="#cb51-6" tabindex="-1"></a></span>
<span id="cb51-7"><a aria-hidden="true" href="#cb51-7" tabindex="-1"></a><span class="co"># Calculate eigenvalues and eigenvectors</span></span>
<span id="cb51-8"><a aria-hidden="true" href="#cb51-8" tabindex="-1"></a>eigen_results <span class="ot">&lt;-</span> <span class="fu">eigen</span>(A)</span>
<span id="cb51-9"><a aria-hidden="true" href="#cb51-9" tabindex="-1"></a>eigenvalues <span class="ot">&lt;-</span> eigen_results<span class="sc">$</span>values</span>
<span id="cb51-10"><a aria-hidden="true" href="#cb51-10" tabindex="-1"></a>eigenvectors <span class="ot">&lt;-</span> eigen_results<span class="sc">$</span>vectors</span>
<span id="cb51-11"><a aria-hidden="true" href="#cb51-11" tabindex="-1"></a></span>
<span id="cb51-12"><a aria-hidden="true" href="#cb51-12" tabindex="-1"></a><span class="co"># Eigendecomposition: A = U * Lambda * U^T</span></span>
<span id="cb51-13"><a aria-hidden="true" href="#cb51-13" tabindex="-1"></a>U <span class="ot">&lt;-</span> eigenvectors</span>
<span id="cb51-14"><a aria-hidden="true" href="#cb51-14" tabindex="-1"></a>Lambda <span class="ot">&lt;-</span> <span class="fu">diag</span>(eigenvalues)  <span class="co"># Create a diagonal matrix from eigenvalues</span></span>
<span id="cb51-15"><a aria-hidden="true" href="#cb51-15" tabindex="-1"></a>U_transpose <span class="ot">&lt;-</span> <span class="fu">t</span>(U)</span>
<span id="cb51-16"><a aria-hidden="true" href="#cb51-16" tabindex="-1"></a></span>
<span id="cb51-17"><a aria-hidden="true" href="#cb51-17" tabindex="-1"></a><span class="co"># Reconstruct A</span></span>
<span id="cb51-18"><a aria-hidden="true" href="#cb51-18" tabindex="-1"></a>A_reconstructed <span class="ot">&lt;-</span> U <span class="sc">%*%</span> Lambda <span class="sc">%*%</span> U_transpose</span>
<span id="cb51-19"><a aria-hidden="true" href="#cb51-19" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Reconstructed A:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Reconstructed A:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a aria-hidden="true" href="#cb53-1" tabindex="-1"></a><span class="fu">print</span>(A_reconstructed)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    3    1
[2,]    1    2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb55-1"><a aria-hidden="true" href="#cb55-1" tabindex="-1"></a><span class="co"># Check for positive definiteness</span></span>
<span id="cb55-2"><a aria-hidden="true" href="#cb55-2" tabindex="-1"></a><span class="co"># A matrix is positive definite if all its eigenvalues are positive.</span></span>
<span id="cb55-3"><a aria-hidden="true" href="#cb55-3" tabindex="-1"></a>is_positive_definite <span class="ot">&lt;-</span> <span class="fu">all</span>(eigenvalues <span class="sc">&gt;</span> <span class="dv">0</span>)</span>
<span id="cb55-4"><a aria-hidden="true" href="#cb55-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Is A positive definite?"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Is A positive definite?"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a aria-hidden="true" href="#cb57-1" tabindex="-1"></a><span class="fu">print</span>(is_positive_definite)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] TRUE</code></pre>
</div>
<div class="sourceCode cell-code" id="cb59"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb59-1"><a aria-hidden="true" href="#cb59-1" tabindex="-1"></a><span class="co">#Calculate the square root of A</span></span>
<span id="cb59-2"><a aria-hidden="true" href="#cb59-2" tabindex="-1"></a>Lambda_sqrt <span class="ot">=</span> <span class="fu">diag</span>(<span class="fu">sqrt</span>(eigenvalues))</span>
<span id="cb59-3"><a aria-hidden="true" href="#cb59-3" tabindex="-1"></a>A_sqrt <span class="ot">=</span> U <span class="sc">%*%</span> Lambda_sqrt <span class="sc">%*%</span> U_transpose</span>
<span id="cb59-4"><a aria-hidden="true" href="#cb59-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Square root of A"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Square root of A"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb61"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb61-1"><a aria-hidden="true" href="#cb61-1" tabindex="-1"></a><span class="fu">print</span>(A_sqrt)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>          [,1]      [,2]
[1,] 1.7013016 0.3249197
[2,] 0.3249197 1.3763819</code></pre>
</div>
<div class="sourceCode cell-code" id="cb63"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb63-1"><a aria-hidden="true" href="#cb63-1" tabindex="-1"></a><span class="co">#Check that A_sqrt %*% A_sqrt equals approximately A</span></span>
<span id="cb63-2"><a aria-hidden="true" href="#cb63-2" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"A_sqrt %*% A_sqrt"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "A_sqrt %*% A_sqrt"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb65"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb65-1"><a aria-hidden="true" href="#cb65-1" tabindex="-1"></a><span class="fu">print</span>(A_sqrt <span class="sc">%*%</span> A_sqrt)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1] [,2]
[1,]    3    1
[2,]    1    2</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Define a Symmetric Matrix:</strong> A 2x2 symmetric matrix <code>A</code> is defined.</li>
<li><strong>Calculate Eigenvalues and Eigenvectors:</strong> The <code>eigen()</code> function is used to compute the eigenvalues and eigenvectors.</li>
<li><strong>Eigendecomposition:</strong> The matrix <code>A</code> is decomposed into <span class="math inline">\(U \Lambda U^T\)</span>, where:
<ul>
<li><code>U</code> is the matrix of eigenvectors.</li>
<li><code>Lambda</code> is a diagonal matrix with the eigenvalues on the diagonal (created using <code>diag(eigenvalues)</code>).</li>
<li><code>U_transpose</code> is the transpose of <code>U</code>. This follows <strong>Theorem 16.2</strong>.</li>
</ul></li>
<li><strong>Reconstruct A:</strong> The matrix <code>A</code> is reconstructed from its eigendecomposition to verify the decomposition is correct.</li>
<li><strong>Check for Positive Definiteness:</strong> The <code>all(eigenvalues &gt; 0)</code> checks if all eigenvalues are strictly positive. If true, the matrix is positive definite, as discussed after <strong>Definition 16.10</strong>.</li>
<li><strong>Calculate the square root of A:</strong> We take the square root of the eigenvalues and then reconstruct the matrix <span class="math inline">\(A^{1/2}\)</span>. This implements the method described in the text before <strong>Theorem 16.4</strong>.</li>
</ol>
</section>
<section class="level3" id="r-script-4-covariance-matrix-and-standardization">
<h3 class="anchored" data-anchor-id="r-script-4-covariance-matrix-and-standardization">R Script 4: Covariance Matrix and Standardization</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb67-1"><a aria-hidden="true" href="#cb67-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb67-2"><a aria-hidden="true" href="#cb67-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb67-3"><a aria-hidden="true" href="#cb67-3" tabindex="-1"></a></span>
<span id="cb67-4"><a aria-hidden="true" href="#cb67-4" tabindex="-1"></a><span class="co"># Simulate data</span></span>
<span id="cb67-5"><a aria-hidden="true" href="#cb67-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>) <span class="co"># for reproducibility</span></span>
<span id="cb67-6"><a aria-hidden="true" href="#cb67-6" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span> <span class="co"># number of observations</span></span>
<span id="cb67-7"><a aria-hidden="true" href="#cb67-7" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">2</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb67-8"><a aria-hidden="true" href="#cb67-8" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="sc">-</span><span class="dv">1</span>, <span class="at">sd =</span> <span class="dv">2</span>)</span>
<span id="cb67-9"><a aria-hidden="true" href="#cb67-9" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(x1, x2)</span>
<span id="cb67-10"><a aria-hidden="true" href="#cb67-10" tabindex="-1"></a></span>
<span id="cb67-11"><a aria-hidden="true" href="#cb67-11" tabindex="-1"></a><span class="co"># Calculate the sample covariance matrix</span></span>
<span id="cb67-12"><a aria-hidden="true" href="#cb67-12" tabindex="-1"></a>sample_covariance <span class="ot">&lt;-</span> <span class="fu">cov</span>(X)</span>
<span id="cb67-13"><a aria-hidden="true" href="#cb67-13" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Sample Covariance Matrix:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Sample Covariance Matrix:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb69-1"><a aria-hidden="true" href="#cb69-1" tabindex="-1"></a><span class="fu">print</span>(sample_covariance)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            x1          x2
x1  0.83323283 -0.08744214
x2 -0.08744214  3.74025239</code></pre>
</div>
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb71-1"><a aria-hidden="true" href="#cb71-1" tabindex="-1"></a><span class="co"># Calculate eigenvalues and eigenvectors</span></span>
<span id="cb71-2"><a aria-hidden="true" href="#cb71-2" tabindex="-1"></a>eigen_results <span class="ot">&lt;-</span> <span class="fu">eigen</span>(sample_covariance)</span>
<span id="cb71-3"><a aria-hidden="true" href="#cb71-3" tabindex="-1"></a>eigenvalues <span class="ot">&lt;-</span> eigen_results<span class="sc">$</span>values</span>
<span id="cb71-4"><a aria-hidden="true" href="#cb71-4" tabindex="-1"></a>eigenvectors <span class="ot">&lt;-</span> eigen_results<span class="sc">$</span>vectors</span>
<span id="cb71-5"><a aria-hidden="true" href="#cb71-5" tabindex="-1"></a></span>
<span id="cb71-6"><a aria-hidden="true" href="#cb71-6" tabindex="-1"></a><span class="co"># Calculate Sigma^(-1/2)</span></span>
<span id="cb71-7"><a aria-hidden="true" href="#cb71-7" tabindex="-1"></a>Sigma_inv_sqrt <span class="ot">&lt;-</span> eigenvectors <span class="sc">%*%</span> <span class="fu">diag</span>(<span class="dv">1</span><span class="sc">/</span><span class="fu">sqrt</span>(eigenvalues)) <span class="sc">%*%</span> <span class="fu">t</span>(eigenvectors)</span>
<span id="cb71-8"><a aria-hidden="true" href="#cb71-8" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Sigma^(-1/2):"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Sigma^(-1/2):"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb73-1"><a aria-hidden="true" href="#cb73-1" tabindex="-1"></a><span class="fu">print</span>(Sigma_inv_sqrt)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]       [,2]
[1,] 1.09671911 0.01742535
[2,] 0.01742535 0.51741237</code></pre>
</div>
<div class="sourceCode cell-code" id="cb75"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb75-1"><a aria-hidden="true" href="#cb75-1" tabindex="-1"></a><span class="co"># Standardize the data</span></span>
<span id="cb75-2"><a aria-hidden="true" href="#cb75-2" tabindex="-1"></a>X_centered <span class="ot">&lt;-</span> <span class="fu">scale</span>(X, <span class="at">center =</span> <span class="cn">TRUE</span>, <span class="at">scale =</span> <span class="cn">FALSE</span>)  <span class="co"># Center the data</span></span>
<span id="cb75-3"><a aria-hidden="true" href="#cb75-3" tabindex="-1"></a>Z <span class="ot">&lt;-</span> X_centered <span class="sc">%*%</span> Sigma_inv_sqrt</span>
<span id="cb75-4"><a aria-hidden="true" href="#cb75-4" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"First few rows of the standardized data"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "First few rows of the standardized data"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb77"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb77-1"><a aria-hidden="true" href="#cb77-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">head</span>(Z))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>            [,1]        [,2]
[1,] -0.73484432 -0.63519604
[2,] -0.33888928  0.37153543
[3,]  1.60546601 -0.11840509
[4,] -0.03018601 -0.24870031
[5,]  0.01322596 -0.87278882
[6,]  1.78397350  0.09300653</code></pre>
</div>
<div class="sourceCode cell-code" id="cb79"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb79-1"><a aria-hidden="true" href="#cb79-1" tabindex="-1"></a><span class="co"># Verify that the covariance matrix of Z is approximately the identity matrix</span></span>
<span id="cb79-2"><a aria-hidden="true" href="#cb79-2" tabindex="-1"></a>cov_Z <span class="ot">&lt;-</span> <span class="fu">cov</span>(Z)</span>
<span id="cb79-3"><a aria-hidden="true" href="#cb79-3" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Covariance Matrix of Z:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Covariance Matrix of Z:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb81-1"><a aria-hidden="true" href="#cb81-1" tabindex="-1"></a><span class="fu">print</span>(cov_Z)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              [,1]          [,2]
[1,]  1.000000e+00 -1.121437e-17
[2,] -1.121437e-17  1.000000e+00</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Simulate Data:</strong> Two random variables, <code>x1</code> and <code>x2</code>, are simulated using <code>rnorm()</code> (normally distributed). These are combined into a matrix <code>X</code>.</li>
<li><strong>Calculate Sample Covariance Matrix:</strong> The <code>cov(X)</code> function computes the sample covariance matrix of <code>X</code>. This relates to <strong>Definition 16.12</strong>.</li>
<li><strong>Calculate Eigenvalues and Eigenvectors:</strong> The <code>eigen()</code> function is used to find the eigenvalues and eigenvectors of the covariance matrix.</li>
<li><strong>Calculate <span class="math inline">\(\Sigma^{-1/2}\)</span>:</strong> The matrix <span class="math inline">\(\Sigma^{-1/2}\)</span> is calculated using the eigendecomposition. The square root of the inverse of each eigenvalue is taken (since we want <span class="math inline">\(\Sigma^{-1/2}\)</span>), and these are placed on the diagonal of a matrix. This follows the description of <span class="math inline">\(\Sigma^{-1/2}\)</span> and <strong>Theorem 16.5</strong>.</li>
<li><strong>Standardize the Data:</strong>
<ul>
<li><code>X_centered &lt;- scale(X, center = TRUE, scale = FALSE)</code> centers the data by subtracting the mean of each column.</li>
<li><code>Z &lt;- X_centered %*% Sigma_inv_sqrt</code> multiplies the centered data by <span class="math inline">\(\Sigma^{-1/2}\)</span> to obtain the standardized data <code>Z</code>. This implements the standardization process from <strong>Theorem 16.5</strong>.</li>
</ul></li>
<li><strong>Verify Covariance of Z:</strong> <code>cov(Z)</code> calculates the covariance matrix of the standardized data <code>Z</code>. This should be approximately equal to the identity matrix, as stated in <strong>Theorem 16.5</strong>.</li>
</ol>
</section>
<section class="level3" id="r-script-5-system-of-linear-equations-and-projection">
<h3 class="anchored" data-anchor-id="r-script-5-system-of-linear-equations-and-projection">R Script 5: System of Linear Equations and Projection</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb83"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb83-1"><a aria-hidden="true" href="#cb83-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb83-2"><a aria-hidden="true" href="#cb83-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb83-3"><a aria-hidden="true" href="#cb83-3" tabindex="-1"></a></span>
<span id="cb83-4"><a aria-hidden="true" href="#cb83-4" tabindex="-1"></a><span class="co"># Define matrix A and vector y</span></span>
<span id="cb83-5"><a aria-hidden="true" href="#cb83-5" tabindex="-1"></a>A <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb83-6"><a aria-hidden="true" href="#cb83-6" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">11</span>)</span>
<span id="cb83-7"><a aria-hidden="true" href="#cb83-7" tabindex="-1"></a></span>
<span id="cb83-8"><a aria-hidden="true" href="#cb83-8" tabindex="-1"></a><span class="co"># Solve the system Ax = y</span></span>
<span id="cb83-9"><a aria-hidden="true" href="#cb83-9" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">solve</span>(A, y)</span>
<span id="cb83-10"><a aria-hidden="true" href="#cb83-10" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Solution x:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Solution x:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb85"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb85-1"><a aria-hidden="true" href="#cb85-1" tabindex="-1"></a><span class="fu">print</span>(x)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb87"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb87-1"><a aria-hidden="true" href="#cb87-1" tabindex="-1"></a><span class="co"># Projection Example (n &gt; K)</span></span>
<span id="cb87-2"><a aria-hidden="true" href="#cb87-2" tabindex="-1"></a><span class="co"># Define matrix X and vector y (n=3, K=2)</span></span>
<span id="cb87-3"><a aria-hidden="true" href="#cb87-3" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">0</span>), <span class="at">nrow =</span> <span class="dv">3</span>, <span class="at">byrow=</span><span class="cn">FALSE</span>)</span>
<span id="cb87-4"><a aria-hidden="true" href="#cb87-4" tabindex="-1"></a>y_proj <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>, <span class="dv">1</span>)</span>
<span id="cb87-5"><a aria-hidden="true" href="#cb87-5" tabindex="-1"></a></span>
<span id="cb87-6"><a aria-hidden="true" href="#cb87-6" tabindex="-1"></a><span class="co"># Find projection using the formula:  x_hat = (X^T X)^(-1) X^T y</span></span>
<span id="cb87-7"><a aria-hidden="true" href="#cb87-7" tabindex="-1"></a>X_transpose <span class="ot">&lt;-</span> <span class="fu">t</span>(X)</span>
<span id="cb87-8"><a aria-hidden="true" href="#cb87-8" tabindex="-1"></a>x_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y_proj</span>
<span id="cb87-9"><a aria-hidden="true" href="#cb87-9" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"x_hat"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "x_hat"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb89"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb89-1"><a aria-hidden="true" href="#cb89-1" tabindex="-1"></a><span class="fu">print</span>(x_hat)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]
[1,]    1
[2,]    0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb91"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb91-1"><a aria-hidden="true" href="#cb91-1" tabindex="-1"></a><span class="co">#Calculate the projection y_hat = X %*% x_hat</span></span>
<span id="cb91-2"><a aria-hidden="true" href="#cb91-2" tabindex="-1"></a>y_hat <span class="ot">=</span> X <span class="sc">%*%</span> x_hat</span>
<span id="cb91-3"><a aria-hidden="true" href="#cb91-3" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Projection of y onto C(X):"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Projection of y onto C(X):"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb93"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb93-1"><a aria-hidden="true" href="#cb93-1" tabindex="-1"></a><span class="fu">print</span>(y_hat)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]
[1,]    1
[2,]    1
[3,]    0</code></pre>
</div>
<div class="sourceCode cell-code" id="cb95"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb95-1"><a aria-hidden="true" href="#cb95-1" tabindex="-1"></a><span class="co"># Check orthogonality: (y - y_hat) should be orthogonal to the columns of X</span></span>
<span id="cb95-2"><a aria-hidden="true" href="#cb95-2" tabindex="-1"></a>orthogonality_check <span class="ot">&lt;-</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> (y_proj <span class="sc">-</span> y_hat)</span>
<span id="cb95-3"><a aria-hidden="true" href="#cb95-3" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Orthogonality Check:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Orthogonality Check:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb97"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb97-1"><a aria-hidden="true" href="#cb97-1" tabindex="-1"></a><span class="fu">print</span>(orthogonality_check)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>     [,1]
[1,]    0
[2,]    0</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Define Matrix and Vector:</strong> A 2x2 matrix <code>A</code> and a 2x1 vector <code>y</code> are defined. This sets up a system of linear equations <span class="math inline">\(Ax = y\)</span>, as described in <strong>Section 16.2</strong>.</li>
<li><strong>Solve the System:</strong> The <code>solve(A, y)</code> function solves the system of equations. This corresponds to the case <span class="math inline">\(n = K\)</span> in <strong>Section 16.2</strong>, where a unique solution exists if <code>A</code> is invertible.</li>
<li><strong>Projection Example</strong> Another matrix X and vector y are defined, with the number of rows of matrix X being greater than the number of its columns.</li>
<li><strong>Find projection</strong>: We implement the formula <span class="math inline">\(x = (X^TX)^{-1}X^Ty\)</span> to find the projection of vector <span class="math inline">\(y\)</span> onto the column space of matrix <span class="math inline">\(X\)</span>.</li>
<li><strong>Calculate y_hat:</strong> <code>y_hat</code> is the projection of <code>y</code> onto the column space of <code>X</code>.</li>
<li><strong>Check Orthogonality:</strong> We check that the difference between <code>y</code> and its projection <code>y_hat</code> (<span class="math inline">\(y - \hat{y}\)</span>) is orthogonal to the columns of <span class="math inline">\(X\)</span>. This verification confirms the properties of the projection as stated in <strong>Theorem 16.8</strong>. The inner product of <span class="math inline">\((y - \hat{y})\)</span> and each column of <span class="math inline">\(X\)</span> should be approximately zero.</li>
</ol>
</section>
</section>
<section class="level2" id="youtube-videos-for-linear-algebra-concepts">
<h2 class="anchored" data-anchor-id="youtube-videos-for-linear-algebra-concepts">YouTube Videos for Linear Algebra Concepts</h2>
<p>Here are some YouTube videos that explain the concepts mentioned in the attached text. I have verified that all videos are currently available (as of October 26, 2023).</p>
<section class="level3" id="matrices-vectors-transpose-matrix-addition-and-multiplication">
<h3 class="anchored" data-anchor-id="matrices-vectors-transpose-matrix-addition-and-multiplication">1. Matrices, Vectors, Transpose, Matrix Addition, and Multiplication</h3>
<ul>
<li><p><strong>Title:</strong> Essence of linear algebra, Chapter 1</p></li>
<li><p><strong>Channel:</strong> 3Blue1Brown</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">https://www.youtube.com/watch?v=fNk_zzaMoSs&amp;list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab</a></p></li>
<li><p><strong>Relation to Text:</strong> This video provides a strong visual and intuitive introduction to vectors and matrices, covering the basics of what they represent geometrically. It gives context to operations like matrix addition, scalar multiplication which are defined in <strong>Section 16.1</strong>. It complements the text’s algebraic definitions with geometric interpretations.</p></li>
<li><p><strong>Title:</strong> Matrices - Intro (Part 1) | Don’t Memorise</p></li>
<li><p><strong>Channel:</strong> Infinity Learn NEET</p></li>
<li><p><strong>URL</strong>: <a href="https://www.youtube.com/watch?v=MxJz3AHjK8k">https://www.youtube.com/watch?v=MxJz3AHjK8k</a></p></li>
<li><p><strong>Relation to Text</strong>: This video presents an introduction to the very basics of matrix algebra. It covers the representation of a matrix, and the definition of basic operations such as addition, substraction, multiplication. It is a great complement of the formal definition presented in <strong>Section 16.1</strong>.</p></li>
<li><p><strong>Title:</strong> Matrix multiplication as composition | Essence of linear algebra, Chapter 4</p></li>
<li><p><strong>Channel:</strong> 3Blue1Brown</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=XkY2DOUCWMU">https://www.youtube.com/watch?v=XkY2DOUCWMU</a></p></li>
<li><p><strong>Relation to Text:</strong> This video focuses specifically on matrix multiplication, explaining it as a composition of linear transformations. This provides a deeper understanding of why matrix multiplication is defined the way it is (and why it’s not commutative), complementing the algebraic definition in <strong>Section 16.1</strong>.</p></li>
</ul>
</section>
<section class="level3" id="inner-product-norm-orthogonality">
<h3 class="anchored" data-anchor-id="inner-product-norm-orthogonality">2. Inner Product, Norm, Orthogonality</h3>
<ul>
<li><p><strong>Title:</strong> Dot products and duality | Essence of linear algebra, Chapter 9</p></li>
<li><p><strong>Channel:</strong> 3Blue1Brown</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=LyGKycYT2v0">https://www.youtube.com/watch?v=LyGKycYT2v0</a></p></li>
<li><p><strong>Relation to Text:</strong> This video explains the dot product (inner product) geometrically and connects it to the concept of duality. This complements the definition of the <strong>inner product</strong> in <strong>Section 16.1</strong> and the definition of <strong>orthogonal vectors</strong>. It also helps visualize the <strong>Euclidean norm</strong>, which is defined using the inner product.</p></li>
<li><p><strong>Title:</strong> Vector Dot Product and Vector Length</p></li>
<li><p><strong>Channel:</strong> Khan Academy</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=QykUMjWAWRw">https://www.youtube.com/watch?v=QykUMjWAWRw</a></p></li>
<li><p><strong>Relation to text:</strong> This video covers the concept of <strong>norm</strong> and <strong>inner product</strong>, which are discussed in <strong>Section 16.1</strong></p></li>
</ul>
</section>
<section class="level3" id="linear-dependence-independence-rank-span-basis">
<h3 class="anchored" data-anchor-id="linear-dependence-independence-rank-span-basis">3. Linear Dependence, Independence, Rank, Span, Basis</h3>
<ul>
<li><p><strong>Title:</strong> Linear combinations, span, and basis vectors | Essence of linear algebra, Chapter 2</p></li>
<li><p><strong>Channel:</strong> 3Blue1Brown</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=k7RM-ot2NWY">https://www.youtube.com/watch?v=k7RM-ot2NWY</a></p></li>
<li><p><strong>Relation to Text:</strong> This video introduces the concepts of <strong>linear combinations</strong>, <strong>span</strong>, and <strong>basis vectors</strong>, providing a visual and intuitive understanding. This directly relates to <strong>Definitions 16.1, 16.2, 16.5, and 16.7</strong>.</p></li>
<li><p><strong>Title:</strong> Linear independence and linear dependence, examples</p></li>
<li><p><strong>Channel:</strong> blackpenredpen</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=c_g4Z-M4-9U">https://www.youtube.com/watch?v=c_g4Z-M4-9U</a></p></li>
<li><p><strong>Relation to Text:</strong> This video uses concrete examples to describe <strong>linear dependence</strong> and <strong>independence</strong>. It complements <strong>Definition 16.1</strong> and <strong>Definition 16.2</strong>.</p></li>
<li><p><strong>Title:</strong> Introduction to the Rank of a Matrix</p></li>
<li><p><strong>Channel:</strong> Mathispower4u</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=85kLqWIS_dI">https://www.youtube.com/watch?v=85kLqWIS_dI</a></p></li>
<li><p><strong>Relation to text:</strong> The video provides a short and useful definition of matrix <strong>rank</strong> and its properties. It complements <strong>Definition 16.3</strong>.</p></li>
</ul>
</section>
<section class="level3" id="trace-and-determinant">
<h3 class="anchored" data-anchor-id="trace-and-determinant">4. Trace and Determinant</h3>
<ul>
<li><p><strong>Title:</strong> The Trace of a Matrix</p></li>
<li><p><strong>Channel:</strong> TheTrevTutor</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=-iz8kl_nagc">https://www.youtube.com/watch?v=-iz8kl_nagc</a></p></li>
<li><p><strong>Relation to Text:</strong> This video provides a good introduction to the concept of <strong>trace of a matrix</strong>, and complements the definition in <strong>Section 16.1</strong>.</p></li>
<li><p><strong>Title:</strong> Essence of linear algebra, Chapter 6</p></li>
<li><p><strong>Channel:</strong> 3Blue1Brown</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=Ip3X9LOh2dk">https://www.youtube.com/watch?v=Ip3X9LOh2dk</a></p></li>
<li><p><strong>Relation to Text:</strong> This video explains the <strong>determinant</strong> geometrically, as the factor by which a linear transformation changes areas or volumes. This provides intuition behind the determinant formula provided in <strong>Section 16.1</strong>.</p></li>
</ul>
</section>
<section class="level3" id="inverse-linear-spaces-null-space">
<h3 class="anchored" data-anchor-id="inverse-linear-spaces-null-space">5. Inverse, Linear Spaces, Null Space</h3>
<ul>
<li><strong>Title:</strong> Inverse matrices, column space and null space | Essence of linear algebra, Chapter 7</li>
<li><strong>Channel:</strong> 3Blue1Brown</li>
<li><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=uQhTuRlWMxw">https://www.youtube.com/watch?v=uQhTuRlWMxw</a></li>
<li><strong>Relation to Text:</strong> This video covers <strong>inverse matrices</strong>, <strong>column space</strong>, and <strong>null space</strong>, connecting them to solving systems of linear equations. This relates directly to the discussion of inverses in <strong>Section 16.1</strong>, <strong>Definition 16.6</strong> (null space), and the introduction of <strong>linear spaces</strong> in <strong>Section 16.1.1</strong>.</li>
</ul>
</section>
<section class="level3" id="eigenvectors-and-eigenvalues-1">
<h3 class="anchored" data-anchor-id="eigenvectors-and-eigenvalues-1">6. Eigenvectors and Eigenvalues</h3>
<ul>
<li><p><strong>Title:</strong> Essence of linear algebra, Chapter 14</p></li>
<li><p><strong>Channel:</strong> 3Blue1Brown</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=PFDu9oVAE-g">https://www.youtube.com/watch?v=PFDu9oVAE-g</a></p></li>
<li><p><strong>Relation to Text:</strong> This video provides a visual and intuitive explanation of <strong>eigenvectors</strong> and <strong>eigenvalues</strong>, explaining them as vectors that are only scaled (not rotated) by a linear transformation. This directly corresponds to <strong>Definition 16.9</strong> and the surrounding discussion in <strong>Section 16.1.2</strong>.</p></li>
<li><p><strong>Title:</strong> Eigenvectors and eigenvalues | MIT 18.06SC Linear Algebra, Fall 2011</p></li>
<li><p><strong>Channel:</strong> MIT OpenCourseWare</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=ue-1gfI6m4Y">https://www.youtube.com/watch?v=ue-1gfI6m4Y</a></p></li>
<li><p><strong>Relation to text:</strong> This video complements the information about <strong>eigenvalues</strong> and <strong>eigenvectors</strong> provided in <strong>Section 16.1.2</strong>.</p></li>
<li><p><strong>Title:</strong> The Applications of Eigenvectors &amp; Eigenvalues | How to find and apply them</p></li>
<li><p><strong>Channel:</strong> Zach Star</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=-Z2xO-a7zIQ">https://www.youtube.com/watch?v=-Z2xO-a7zIQ</a></p></li>
<li><p><strong>Relation to text:</strong> This video gives another perspective on the topic of <strong>eigenvalues</strong> and <strong>eigenvectors</strong>, and also discusses applications, which is aligned with <strong>Section 16.1.3</strong> in the text.</p></li>
</ul>
</section>
<section class="level3" id="eigendecomposition-positive-definite-matrices">
<h3 class="anchored" data-anchor-id="eigendecomposition-positive-definite-matrices">7. Eigendecomposition, Positive Definite Matrices</h3>
<ul>
<li><strong>Title:</strong> [Linear Algebra] Eigendecomposition</li>
<li><strong>Channel:</strong> Jacobo Sarin</li>
<li><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=EOtB7D681U8">https://www.youtube.com/watch?v=EOtB7D681U8</a></li>
<li><strong>Relation to Text</strong>: This video describes <strong>eigendecomposition</strong> and its properties, complementing <strong>Theorem 16.2</strong>.</li>
<li><strong>Title:</strong> Definite Matrices</li>
<li><strong>Channel</strong> : Dr. Trefor Bazett</li>
<li><strong>URL</strong>: <a href="https://www.youtube.com/watch?v=ZQtfmJHGQkI">https://www.youtube.com/watch?v=ZQtfmJHGQkI</a></li>
<li><strong>Relation to Text</strong>: This video presents a clear definition of <strong>positive definite matrices</strong> and some of their properties. This relates to <strong>Definition 16.10</strong>.</li>
</ul>
</section>
<section class="level3" id="systems-of-linear-equations-and-projections">
<h3 class="anchored" data-anchor-id="systems-of-linear-equations-and-projections">8. Systems of Linear Equations and Projections</h3>
<ul>
<li><p><strong>Title:</strong> Solving Ax = b | MIT 18.06SC Linear Algebra, Fall 2011</p></li>
<li><p><strong>Channel:</strong> MIT OpenCourseWare</p></li>
<li><p><strong>URL:</strong> <a href="https://www.youtube.com/watch?v=9s_0rAiwg78">https://www.youtube.com/watch?v=9s_0rAiwg78</a></p></li>
<li><p><strong>Relation to Text:</strong> This lecture discusses solving systems of linear equations, which is the primary topic of <strong>Section 16.2</strong>.</p></li>
<li><p><strong>Title:</strong> Visualizing Projections | MIT 18.06SC Linear Algebra, Fall 2011</p></li>
<li><p><strong>Channel</strong>: MIT OpenCourseWare</p></li>
<li><p><strong>URL:</strong><a href="https://www.youtube.com/watch?v=qczPZp7Z-b4">https://www.youtube.com/watch?v=qczPZp7Z-b4</a></p></li>
<li><p><strong>Relation to Text:</strong> This lecture explains projections visually, which is directly related to <strong>Theorem 16.8</strong> and the example following it.</p></li>
</ul>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch16mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch16mcsolution1">MC Solution 1</a></p>
<p>What is the transpose of the matrix <span class="math inline">\(\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}\)</span>?</p>
<ol type="a">
<li><span class="math inline">\(\begin{pmatrix} 1 &amp; 4 \\ 2 &amp; 5 \\ 3 &amp; 6 \end{pmatrix}\)</span></li>
<li><span class="math inline">\(\begin{pmatrix} 6 &amp; 5 &amp; 4 \\ 3 &amp; 2 &amp; 1 \end{pmatrix}\)</span></li>
<li><span class="math inline">\(\begin{pmatrix} 4 &amp; 5 &amp; 6 \\ 1 &amp; 2 &amp; 3 \end{pmatrix}\)</span></li>
<li><span class="math inline">\(\begin{pmatrix} 1 &amp; 2 &amp; 3 \\ 4 &amp; 5 &amp; 6 \end{pmatrix}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch16mcsolution2">MC Solution 2</a></p>
<p>If <span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times m\)</span> matrix and <span class="math inline">\(B\)</span> is a <span class="math inline">\(p \times q\)</span> matrix, under what condition can the matrix product <span class="math inline">\(AB\)</span> be defined?</p>
<ol type="a">
<li><span class="math inline">\(n = p\)</span></li>
<li><span class="math inline">\(m = p\)</span></li>
<li><span class="math inline">\(n = q\)</span></li>
<li><span class="math inline">\(m = q\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch16mcsolution3">MC Solution 3</a></p>
<p>Which of the following properties holds for the trace of matrices (assuming the matrices are conformable)?</p>
<ol type="a">
<li>tr<span class="math inline">\((AB)\)</span> = tr<span class="math inline">\((A)\)</span>tr<span class="math inline">\((B)\)</span></li>
<li>tr<span class="math inline">\((A+B)\)</span> = tr<span class="math inline">\((A)\)</span> + tr<span class="math inline">\((B)\)</span></li>
<li>tr<span class="math inline">\((A^T)\)</span> = 1/tr<span class="math inline">\((A)\)</span></li>
<li>tr<span class="math inline">\((AB)\)</span> = tr<span class="math inline">\((A)\)</span> / tr<span class="math inline">\((B)\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch16mcsolution4">MC Solution 4</a></p>
<p>What is the inner product of the vectors <span class="math inline">\(x = \begin{pmatrix} 1 \\ 2 \end{pmatrix}\)</span> and <span class="math inline">\(y = \begin{pmatrix} -1 \\ 3 \end{pmatrix}\)</span>?</p>
<ol type="a">
<li>4</li>
<li>5</li>
<li>6</li>
<li>7</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch16mcsolution5">MC Solution 5</a></p>
<p>What is the Euclidean norm of the vector <span class="math inline">\(x = \begin{pmatrix} 3 \\ -4 \end{pmatrix}\)</span>?</p>
<ol type="a">
<li>1</li>
<li>5</li>
<li>7</li>
<li>25</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch16mcsolution6">MC Solution 6</a></p>
<p>Two vectors <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> are orthogonal if:</p>
<ol type="a">
<li><span class="math inline">\(x^Ty = 1\)</span></li>
<li><span class="math inline">\(x^Ty = 0\)</span></li>
<li><span class="math inline">\(||x|| = ||y||\)</span></li>
<li><span class="math inline">\(x = y\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch16mcsolution7">MC Solution 7</a></p>
<p>A set of vectors is linearly dependent if:</p>
<ol type="a">
<li>All the vectors are identical.</li>
<li>The only linear combination that equals the zero vector is the trivial one (all coefficients are zero).</li>
<li>There exists a non-trivial linear combination of the vectors that equals the zero vector.</li>
<li>The vectors are all orthogonal to each other.</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch16mcsolution8">MC Solution 8</a></p>
<p>What is the determinant of the matrix <span class="math inline">\(\begin{pmatrix} 2 &amp; 1 \\ 3 &amp; 4 \end{pmatrix}\)</span>?</p>
<ol type="a">
<li>2</li>
<li>5</li>
<li>10</li>
<li>14</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch16mcsolution9">MC Solution 9</a></p>
<p>If <span class="math inline">\(A\)</span> is an <span class="math inline">\(n \times n\)</span> matrix, which of the following statements is true about the determinant of <span class="math inline">\(A\)</span>?</p>
<ol type="a">
<li>det<span class="math inline">\((A^T)\)</span> = 1/det<span class="math inline">\((A)\)</span></li>
<li>det<span class="math inline">\((A^T)\)</span> = -det<span class="math inline">\((A)\)</span></li>
<li>det<span class="math inline">\((A^T)\)</span> = det<span class="math inline">\((A)\)</span></li>
<li>det<span class="math inline">\((A^T)\)</span> = <span class="math inline">\(n \cdot\)</span> det<span class="math inline">\((A)\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch16mcsolution10">MC Solution 10</a></p>
<p>A square matrix <span class="math inline">\(A\)</span> is invertible (nonsingular) if:</p>
<ol type="a">
<li>Its determinant is zero.</li>
<li>Its rank is less than its dimension.</li>
<li>Its determinant is non-zero.</li>
<li>It is a diagonal matrix.</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch16mcsolution11">MC Solution 11</a></p>
<p>What is the inverse of the matrix <span class="math inline">\(\begin{pmatrix} a &amp; 0 \\ 0 &amp; b \end{pmatrix}\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are non-zero?</p>
<ol type="a">
<li><span class="math inline">\(\begin{pmatrix} b &amp; 0 \\ 0 &amp; a \end{pmatrix}\)</span></li>
<li><span class="math inline">\(\begin{pmatrix} 1/a &amp; 0 \\ 0 &amp; 1/b \end{pmatrix}\)</span></li>
<li><span class="math inline">\(\begin{pmatrix} -a &amp; 0 \\ 0 &amp; -b \end{pmatrix}\)</span></li>
<li><span class="math inline">\(\begin{pmatrix} 0 &amp; 1/b \\ 1/a &amp; 0 \end{pmatrix}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch16mcsolution12">MC Solution 12</a></p>
<p>An eigenvector <span class="math inline">\(u\)</span> of a matrix <span class="math inline">\(A\)</span> satisfies which of the following equations?</p>
<ol type="a">
<li><span class="math inline">\(Au = 0\)</span></li>
<li><span class="math inline">\(Au = u\)</span></li>
<li><span class="math inline">\(Au = \lambda u\)</span> for some scalar <span class="math inline">\(\lambda\)</span></li>
<li><span class="math inline">\(A^T u = Au\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch16mcsolution13">MC Solution 13</a></p>
<p>If a matrix <span class="math inline">\(A\)</span> has an eigenvalue <span class="math inline">\(\lambda\)</span>, what is the corresponding eigenvalue of <span class="math inline">\(A + 2I\)</span>, where <span class="math inline">\(I\)</span> is the identity matrix?</p>
<ol type="a">
<li><span class="math inline">\(\lambda\)</span></li>
<li><span class="math inline">\(\lambda + 2\)</span></li>
<li><span class="math inline">\(2\lambda\)</span></li>
<li><span class="math inline">\(\lambda^2\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch16mcsolution14">MC Solution 14</a></p>
<p>A real symmetric matrix is positive definite if:</p>
<ol type="a">
<li>All its entries are positive.</li>
<li>Its determinant is positive.</li>
<li>All its eigenvalues are positive.</li>
<li>All its eigenvalues are non-negative.</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch16mcsolution15">MC Solution 15</a></p>
<p>The rank of a matrix is:</p>
<ol type="a">
<li>The number of its rows.</li>
<li>The number of its columns.</li>
<li>The number of its linearly independent rows (or columns).</li>
<li>The sum of its diagonal elements.</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch16mcsolution16">MC Solution 16</a></p>
<p>Which of the following is a property of the eigenvalues of a real symmetric matrix?</p>
<ol type="a">
<li>They can be complex.</li>
<li>They are always real.</li>
<li>They are always positive.</li>
<li>They are always zero.</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch16mcsolution17">MC Solution 17</a></p>
<p>A set of vectors is called a basis for a vector space if: (a) The vectors are linearly dependent. (b) The vectors span the vector space and are linearly independent. (c) The vectors are orthogonal. (d) The vectors have unit length.</p>
</section>
<section class="level3" id="sec-ch16mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch16mcsolution18">MC Solution 18</a> If <span class="math inline">\(X\)</span> is a random variable with <span class="math inline">\(E(X)=\mu\)</span>, what does <span class="math inline">\(Cov(X)\)</span> represent? (a) <span class="math inline">\(E(X^2) - \mu\)</span> (b) <span class="math inline">\(E(XX^T)-\mu\mu^T\)</span> (c) <span class="math inline">\(E(X) - \mu^2\)</span> (d) <span class="math inline">\(E((X-\mu)^2)\)</span></p>
</section>
<section class="level3" id="sec-ch16mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch16mcsolution19">MC Solution 19</a> If a matrix <span class="math inline">\(A\)</span> is singular, it means that: (a) <span class="math inline">\(A\)</span> is not a square matrix. (b) <span class="math inline">\(A^{-1}\)</span> exists. (c) det(<span class="math inline">\(A\)</span>) = 0. (d) All elements of <span class="math inline">\(A\)</span> are zero.</p>
</section>
<section class="level3" id="sec-ch16mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch16mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch16mcsolution20">MC Solution 20</a></p>
<p>The dimension of the null space of a matrix <span class="math inline">\(A\)</span> is also known as the:</p>
<ol type="a">
<li>Rank of A.</li>
<li>Nullity of A.</li>
<li>Trace of A.</li>
<li>Determinant of A. ## Multiple Choice Solutions</li>
</ol>
</section>
<section class="level3" id="sec-ch16mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch16mcexercise1">MC Exercise 1</a></p>
<p>The correct answer is (a). The <strong>transpose</strong> of a matrix is obtained by interchanging its rows and columns. This is described in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch16mcexercise2">MC Exercise 2</a></p>
<p>The correct answer is (b). For the matrix product <span class="math inline">\(AB\)</span> to be defined, the number of columns of <span class="math inline">\(A\)</span> (which is <span class="math inline">\(m\)</span>) must be equal to the number of rows of <span class="math inline">\(B\)</span> (which is <span class="math inline">\(p\)</span>). This condition is stated in the definition of <strong>matrix multiplication</strong> in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch16mcexercise3">MC Exercise 3</a></p>
<p>The correct answer is (b). The trace of the sum of two matrices is the sum of their traces. This is one of the properties of the <strong>trace</strong> operation stated in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch16mcexercise4">MC Exercise 4</a></p>
<p>The correct answer is (b). The <strong>inner product</strong> is calculated as: <span class="math inline">\(x^Ty = (1)(-1) + (2)(3) = -1 + 6 = 5\)</span>. This calculation is shown in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch16mcexercise5">MC Exercise 5</a></p>
<p>The correct answer is (b). The <strong>Euclidean norm</strong> is calculated as: <span class="math inline">\(||x|| = \sqrt{(3)^2 + (-4)^2} = \sqrt{9 + 16} = \sqrt{25} = 5\)</span>. The formula for the <strong>norm</strong> is defined in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch16mcexercise6">MC Exercise 6</a></p>
<p>The correct answer is (b). Two vectors are <strong>orthogonal</strong> if their inner product is zero. This definition is provided in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch16mcexercise7">MC Exercise 7</a></p>
<p>The correct answer is (c). This is the definition of <strong>linear dependence</strong> given in <strong>Definition 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch16mcexercise8">MC Exercise 8</a></p>
<p>The correct answer is (b). The <strong>determinant</strong> is calculated as: <span class="math inline">\((2)(4) - (1)(3) = 8 - 3 = 5\)</span>. This follows the formula given in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch16mcexercise9">MC Exercise 9</a></p>
<p>The correct answer is (c). The determinant of the transpose of a matrix is equal to the determinant of the original matrix.</p>
</section>
<section class="level3" id="sec-ch16mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch16mcexercise10">MC Exercise 10</a></p>
<p>The correct answer is (c). A square matrix is <strong>invertible</strong> (nonsingular) if and only if its determinant is non-zero. This fact is mentioned in <strong>Section 16.1</strong> when discussing the inverse of a matrix.</p>
</section>
<section class="level3" id="sec-ch16mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch16mcexercise11">MC Exercise 11</a></p>
<p>The correct answer is (b). This is a special case of the <strong>inverse of a diagonal matrix</strong>, discussed in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch16mcexercise12">MC Exercise 12</a></p>
<p>The correct answer is (c). This is the definition of an <strong>eigenvector</strong> and its corresponding <strong>eigenvalue</strong>, as given in <strong>Definition 16.9</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch16mcexercise13">MC Exercise 13</a></p>
<p>The correct answer is (b). If <span class="math inline">\(Au = \lambda u\)</span>, then <span class="math inline">\((A + 2I)u = Au + 2Iu = \lambda u + 2u = (\lambda + 2)u\)</span>. This property is discussed in the text after <strong>Theorem 16.2</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch16mcexercise14">MC Exercise 14</a></p>
<p>The correct answer is (c). A real symmetric matrix is <strong>positive definite</strong> if all its eigenvalues are positive. This condition is explained in <strong>Section 16.1</strong> after <strong>Definition 16.10</strong>. Option (d) describes a <strong>positive semi-definite</strong> matrix.</p>
</section>
<section class="level3" id="sec-ch16mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch16mcexercise15">MC Exercise 15</a></p>
<p>The correct answer is (c). This is the definition of the <strong>rank</strong> of a matrix, given in <strong>Definition 16.3</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch16mcexercise16">MC Exercise 16</a></p>
<p>The correct answer is (b). The <strong>eigenvalues</strong> of a real symmetric matrix are always real. This is mentioned in <strong>Section 16.1.2</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch16mcexercise17">MC Exercise 17</a></p>
<p>The correct answer is (b). This is the definition of a <strong>basis</strong> for a vector space given in <strong>Definition 16.7</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch16mcexercise18">MC Exercise 18</a></p>
<p>The correct answer is (b). <span class="math inline">\(Cov(X)\)</span> is defined in <strong>Definition 16.12</strong> as <span class="math inline">\(E[(X - \mu)(X - \mu)^T] = E[XX^T] - \mu \mu^T\)</span>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch16mcexercise19">MC Exercise 19</a></p>
<p>The correct answer is (c). A matrix <span class="math inline">\(A\)</span> is <strong>singular</strong> if its determinant is 0. This fact is related to the discussion of <strong>inverse</strong> matrices in <strong>Section 16.1</strong>.</p>
</section>
<section class="level3" id="sec-ch16mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch16mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch16mcexercise20">MC Exercise 20</a> The correct answer is (b). The dimension of the null space of <span class="math inline">\(A\)</span> is called <strong>nullity</strong> of <span class="math inline">\(A\)</span>. The definition of the <strong>null space</strong> of a matrix appears in <strong>Definition 16.6</strong>.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>