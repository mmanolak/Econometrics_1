<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 19: Statistical Properties of the OLS Estimator – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap20.html" rel="next"/>
<link href="../chapters/chap18.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 19: Statistical Properties of the OLS Estimator</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level2" id="properties-of-ols">
<h2 class="anchored" data-anchor-id="properties-of-ols">19.1 Properties of OLS</h2>
<p>We investigate the statistical properties of the OLS estimator of <span class="math inline">\(\beta\)</span>. Specifically, we calculate its exact mean and variance and compare it with other possible estimators under assumptions A1 and A2.</p>
<section class="level3" id="definition-19.1">
<h3 class="anchored" data-anchor-id="definition-19.1">Definition 19.1</h3>
<p>The estimator <span class="math inline">\(\hat{\beta}\)</span> is <strong>linear</strong> in <span class="math inline">\(y\)</span>, i.e., there exists a matrix <span class="math inline">\(C\)</span> not depending on <span class="math inline">\(y\)</span> such that</p>
<p><span class="math inline">\(\qquad \hat{\beta} = (X^{T}X)^{-1}X^{T}y = Cy\)</span>.</p>
<p>This property simplifies calculations. In a time series context, where <span class="math inline">\(X\)</span> may contain lagged values of <span class="math inline">\(y\)</span>, this property is not so meaningful. We want to evaluate how <span class="math inline">\(\hat{\beta}\)</span> varies across hypothetical repeated samples under Assumption A.</p>
</section>
<section class="level3" id="theorem-19.1">
<h3 class="anchored" data-anchor-id="theorem-19.1">Theorem 19.1</h3>
<p>Suppose that assumption A1 holds. Then, we have</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\hat{\beta}|X] = (X^{T}X)^{-1}X^{T}\mathbb{E}[y|X] = (X^{T}X)^{-1}X^{T}X\beta = \beta\)</span>,</p>
<p>where this equality holds for all <span class="math inline">\(\beta\)</span>. We say that <span class="math inline">\(\hat{\beta}\)</span> is <strong>conditionally unbiased</strong>. It follows that it is unconditionally unbiased using the Law of Iterated Expectations.</p>
<p>Furthermore, we shall calculate the <span class="math inline">\(K \times K\)</span> conditional covariance matrix of <span class="math inline">\(\hat{\beta}\)</span>,</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta}|X) = \mathbb{E}\left[ (\hat{\beta} - \mathbb{E}[\hat{\beta}|X])(\hat{\beta} - \mathbb{E}[\hat{\beta}|X])^{T} | X \right] = \mathbb{E}\left[ (\hat{\beta} - \beta)(\hat{\beta} - \beta)^{T} | X \right]\)</span>.</p>
<p>This has diagonal elements <span class="math inline">\(\text{var}(\hat{\beta}_{j}|X)\)</span> and off-diagonals <span class="math inline">\(\text{cov}(\hat{\beta}_{j}, \hat{\beta}_{k}|X)\)</span>.</p>
<p><strong>Intuition:</strong> The conditional unbiasedness of the OLS estimator means that, on average, the estimator <span class="math inline">\(\hat{\beta}\)</span> will be equal to the true value <span class="math inline">\(\beta\)</span>, given the predictor matrix <span class="math inline">\(X\)</span>. It means that if we could repeatedly draw samples of the dependent variable <span class="math inline">\(y\)</span> for a fixed set of predictors <span class="math inline">\(X\)</span>, the average of the OLS estimates across these samples would converge to the true parameter value <span class="math inline">\(\beta\)</span>.</p>
</section>
<section class="level3" id="theorem-19.2">
<h3 class="anchored" data-anchor-id="theorem-19.2">Theorem 19.2</h3>
<p>Suppose that assumptions A1 and A2 hold. Then</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta}|X) = (X^{T}X)^{-1}X^{T}\mathbb{E}[\epsilon \epsilon^{T} | X]X(X^{T}X)^{-1} = (X^{T}X)^{-1}X^{T}\Sigma(X)X(X^{T}X)^{-1}\)</span>.</p>
<p>In the homoskedastic special case, <span class="math inline">\(\Sigma(X) = \sigma^{2}I_{n}\)</span>, this simplifies to</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta}|X) = \sigma^{2}(X^{T}X)^{-1}\)</span>.</p>
<p>The matrix <span class="math inline">\(X^{T}X\)</span> is generally not diagonal, except in special cases such as dummy variables.</p>
<p>We next consider the unconditional variance, which follows also from the law of iterated expectation</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\text{var}(\hat{\beta}) &amp;= \mathbb{E}[\text{var}(\hat{\beta}|X)] \\
&amp;= \mathbb{E}\left[ (X^{T}X)^{-1} X^{T} \Sigma(X) X (X^{T}X)^{-1} \right] \\
&amp;= \sigma^{2}\mathbb{E}\left[ (X^{T}X)^{-1} \right] \quad \text{in the homoskedastic case},
\end{aligned}\)</span></p>
<p>provided the moment exists. Under the assumption that <span class="math inline">\(X\)</span> is full rank with probability one we may always define <span class="math inline">\((X^{T}X)^{-1}\)</span>. However, this is not sufficient to guarantee the existence of the expectation of its inverse.</p>
<p><strong>Intuition:</strong></p>
<p>This theorem gives us the variance-covariance matrix of the OLS estimator, conditional on <span class="math inline">\(X\)</span>. The diagonal elements represent the variances of individual coefficient estimates (<span class="math inline">\(\hat{\beta_i}\)</span>), while the off-diagonal elements represent the covariances between different coefficient estimates. In the simple case of homoskedasticity (constant error variance) and no autocorrelation, the formula simplifies considerably.</p>
</section>
<section class="level3" id="example-19.1">
<h3 class="anchored" data-anchor-id="example-19.1">Example 19.1</h3>
<p>Suppose that <span class="math inline">\(n = K = 1\)</span> and <span class="math inline">\(x_{1} \sim N(0, 1)\)</span>, then <span class="math inline">\(\text{Pr}(x_{1}^{2} = 0) = 0\)</span> but <span class="math inline">\(\mathbb{E}[x_{1}^{-2}] = \infty\)</span>. However, when <span class="math inline">\(n \geq 2\)</span> this problem goes away because we have <span class="math inline">\(\mathbb{E}[(\sum_{i=1}^{n}x_{i}^{2})^{-1}] &lt; \infty\)</span>.</p>
<p>However, according to the ancillarity principle we should really just consider the conditional distribution.</p>
<p>The properties of an individual coefficient can be obtained from the partitioned regression formula <span class="math inline">\(\hat{\beta}_{1} = (X_{1}^{T}M_{2}X_{1})^{-1}X_{1}^{T}M_{2}y\)</span>. In general we have</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\text{var}[\hat{\beta}_{1}|X] &amp;= (X_{1}^{T}M_{2}X_{1})^{-1}X_{1}^{T}M_{2}\mathbb{E}[\epsilon \epsilon^{T}|X]M_{2}X_{1}(X_{1}^{T}M_{2}X_{1})^{-1} \\
&amp;= (X_{1}^{T}M_{2}X_{1})^{-1}X_{1}^{T}M_{2}\Sigma(X)M_{2}X_{1}(X_{1}^{T}M_{2}X_{1})^{-1} \\
&amp;= \sigma^{2}(X_{1}^{T}M_{2}X_{1})^{-1} \quad \text{in the homoskedastic case}.
\end{aligned}\)</span> In the special case that <span class="math inline">\(X_{2} = (1,..., 1)^{T}\)</span>, we have <span class="math inline">\(\qquad \text{var}(\hat{\beta}_{1}|X) = \dfrac{\sigma^{2}}{\sum_{i=1}^{n}(x_{i} - \bar{x})^{2}}\)</span>.</p>
<p>This is the well known variance of the least squares estimator in the single regressor plus intercept regression.</p>
<p><strong>Intuition:</strong> It states that the variance of a specific coefficient estimator <span class="math inline">\(\hat{\beta_1}\)</span> is influenced by:</p>
<ol type="1">
<li>The error variance (<span class="math inline">\(\sigma^2\)</span>): Higher error variance leads to higher estimator variance.</li>
<li>The variation in the corresponding predictor (<span class="math inline">\(X_1\)</span>) after accounting for the other predictors (<span class="math inline">\(X_2\)</span>): represented by <span class="math inline">\(X_1^T M_2 X_1\)</span>. Greater variation (after accounting for other predictors) leads to lower estimator variance. ### Theorem 19.3</li>
</ol>
<p>Suppose that condition A3 holds. Then, the distribution of <span class="math inline">\(\hat{\beta}\)</span> conditional on <span class="math inline">\(X\)</span> is the multivariate normal distribution</p>
<p><span class="math inline">\(\qquad \hat{\beta} \sim N(\beta, V(X)), \quad V(X) = (X^{T}X)^{-1}X^{T}\Sigma(X)X(X^{T}X)^{-1}.\)</span></p>
<p><strong>Proof:</strong> This follows because <span class="math inline">\(\hat{\beta} = \sum_{i=1}^{n} c_i y_i\)</span>, where <span class="math inline">\(c_i\)</span> depends only on the covariates, i.e., <span class="math inline">\(\hat{\beta}\)</span> is a linear combination of independent normals, and hence it is normal, conditionally. The unconditional distribution of <span class="math inline">\(\hat{\beta}\)</span> will not be normal – in fact, it will be a scale mixture of normals. However, it follows that</p>
<p><span class="math inline">\(\qquad V(X)^{-1/2}(\hat{\beta} - \beta) \sim N(0, I_{K})\)</span></p>
<p>conditional on <span class="math inline">\(X\)</span>, and because the right hand side distribution does not depend on <span class="math inline">\(X\)</span>, this result is unconditional too. Hence, the quadratic form</p>
<p><span class="math inline">\(\qquad \tau = (\hat{\beta} - \beta)^{T}V(X)^{-1}(\hat{\beta} - \beta) \sim \chi^{2}(K)\)</span></p>
<p>conditionally and unconditionally. The distribution of <span class="math inline">\(\tau\)</span> does not depend on unknown quantities, which will be helpful later when constructing hypothesis tests and confidence intervals.</p>
<p><strong>Intuition:</strong></p>
<p>If, in addition to the previous assumptions, we assume that the errors are normally distributed, then the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> also follows a normal distribution. This result is crucial for hypothesis testing and constructing confidence intervals, as we can use the known properties of the normal distribution to make inferences about the true parameters.</p>
<p>We are also interested in estimation of <span class="math inline">\(m = \mathbb{E}[y|X] = X\beta \in \mathbb{R}^{n}\)</span> and in the estimation of the function <span class="math inline">\(m(x) = \mathbb{E}[y_{i}|x_{i} = x]\)</span> for any <span class="math inline">\(x \in \mathbb{R}^{K}\)</span>. Let <span class="math inline">\(\hat{m} = X\hat{\beta}\)</span> and <span class="math inline">\(\hat{m}(x) = x^{T}\hat{\beta}\)</span>.</p>
</section>
<section class="level3" id="theorem-19.4">
<h3 class="anchored" data-anchor-id="theorem-19.4">Theorem 19.4</h3>
<p>Suppose that assumptions A1 and A2 hold. Then <span class="math inline">\(\hat{m}\)</span> is unbiased, i.e., <span class="math inline">\(\mathbb{E}[\hat{m}|X] = m\)</span>, with</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{m}|X) = X(X^{T}X)^{-1}X^{T}\Sigma(X)X(X^{T}X)^{-1}X^{T}\)</span>.</p>
<p>Likewise, <span class="math inline">\(\hat{m}(x)\)</span> is unbiased, i.e., <span class="math inline">\(\mathbb{E}[\hat{m}(x)|X] = m(x)\)</span>, with</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{m}(x)|X) = x(X^{T}X)^{-1}X^{T}\Sigma(X)X(X^{T}X)^{-1}x^{T}\)</span>.</p>
<p>Furthermore, if assumption A3 holds, then conditionally on <span class="math inline">\(X\)</span>,</p>
<p><span class="math inline">\(\qquad \hat{m} \sim N(m, X(X^{T}X)^{-1}X^{T}\Sigma(X)X(X^{T}X)^{-1}X^{T})\)</span>.</p>
<p>Note however, that <span class="math inline">\(\text{var}(\hat{m}|X)\)</span> is of rank <span class="math inline">\(K &lt; n\)</span>.</p>
<p>In the homoskedastic special case A4 we have conditionally</p>
<p><span class="math inline">\(\qquad \hat{\beta} \sim N(\beta, \sigma^{2}(X^{T}X)^{-1})\)</span></p>
<p><span class="math inline">\(\qquad \hat{m} \sim N(m, \sigma^{2}X(X^{T}X)^{-1}X^{T})\)</span>.</p>
<p>Suppose we assume condition A4, which is a complete specification of the conditional distribution of <span class="math inline">\(y\)</span>. Then the density function of the vector <span class="math inline">\(y\)</span> [conditional on <span class="math inline">\(X\)</span>] is</p>
<p><span class="math inline">\(\qquad f_{y|X}(y) = \dfrac{1}{(2\pi\sigma^{2})^{n/2}}\exp\left( -\dfrac{1}{2\sigma^{2}}(y - X\beta)^{T}(y - X\beta) \right)\)</span>.</p>
<p>The density function depends on the unknown parameters <span class="math inline">\(\beta, \sigma^{2}\)</span>. The log likelihood function for the observed data is</p>
<p><span class="math inline">\(\qquad l(b, \omega^{2}|y, X) = -\dfrac{n}{2}\log 2\pi - \dfrac{n}{2}\log \omega^{2} - \dfrac{1}{2\omega^{2}}(y - Xb)^{T}(y - Xb)\)</span>,</p>
<p>where <span class="math inline">\(b\)</span> and <span class="math inline">\(\omega\)</span> are unknown parameters. Perhaps we are being overly pedantic here by emphasizing the difference between the true values <span class="math inline">\(\beta, \sigma^{2}\)</span> and the arguments <span class="math inline">\(b\)</span> and <span class="math inline">\(\omega\)</span> of <span class="math inline">\(l\)</span>, and we shall not do this again. The maximum likelihood estimator <span class="math inline">\(\hat{\beta}_{mle}, \hat{\sigma}_{mle}^{2}\)</span> maximizes <span class="math inline">\(l(b, \omega^{2})\)</span> with respect to <span class="math inline">\(b\)</span> and <span class="math inline">\(\omega^{2}\)</span>. It is easy to see that</p>
<p><span class="math inline">\(\qquad \hat{\beta}_{mle} = \hat{\beta}; \quad \hat{\sigma}_{mle}^{2} = \dfrac{1}{n}(y - X\hat{\beta}_{mle})^{T}(y - X\hat{\beta}_{mle})\)</span>.</p>
<p>Basically, the criterion function is the least squares criterion apart from an affine transformation involving only <span class="math inline">\(\omega\)</span>. Note however, that if we had a different assumption about the errors than normality, e.g., they were from a t-distribution, then we would have a different likelihood and a different estimator than <span class="math inline">\(\hat{\beta}\)</span>. In particular, the estimator may not be explicitly defined and may be a nonlinear function of <span class="math inline">\(y\)</span>.</p>
<p>We next consider two widely used estimates of <span class="math inline">\(\sigma^{2}\)</span></p>
<p><span class="math inline">\(\qquad \hat{\sigma}_{mle}^{2} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n}\)</span></p>
<p><span class="math inline">\(\qquad s_{*}^{2} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n-K}\)</span>.</p>
<p>The first estimate is the maximum likelihood estimator of <span class="math inline">\(\sigma^{2}\)</span>. The second estimate is a modification of the MLE, which we can show is unbiased.</p>
</section>
<section class="level3" id="theorem-19.5">
<h3 class="anchored" data-anchor-id="theorem-19.5">Theorem 19.5</h3>
<p>Suppose that A1, A2 hold and that <span class="math inline">\(\Sigma(X) = \sigma^{2}I\)</span>. Then</p>
<p><span class="math inline">\(\qquad \mathbb{E}[s_{*}^{2}] = \sigma^{2}\)</span>.</p>
<p><strong>Proof:</strong></p>
<p>We have</p>
<p><span class="math inline">\(\qquad \hat{\epsilon} = M_{X}y = M_{X}X\beta + M_{X}\epsilon = M_{X}\epsilon\)</span></p>
<p>so that <span class="math inline">\(\hat{\epsilon}^{T}\hat{\epsilon}\)</span> is a quadratic form in normal random variables</p>
<p><span class="math inline">\(\qquad \hat{\epsilon}^{T}\hat{\epsilon} = y^{T}M_{X}M_{X}y = \epsilon^{T}M_{X}M_{X}\epsilon = \epsilon^{T}M_{X}\epsilon\)</span>.</p>
<p>Therefore, under A2</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\mathbb{E}[\hat{\epsilon}^{T}\hat{\epsilon}|X] &amp;= \mathbb{E}[\text{tr}(\epsilon^{T}M_{X}\epsilon)|X] \\
&amp;= \mathbb{E}[\text{tr}(\epsilon\epsilon^{T}M_{X})|X] \\
&amp;= \text{tr}(\mathbb{E}[\epsilon\epsilon^{T}|X]M_{X}) \\
&amp;= \text{tr}(\Sigma(X)M_{X}).
\end{aligned}\)</span></p>
<p>If <span class="math inline">\(\Sigma(X) = \sigma^{2}I\)</span>, then</p>
<p><span class="math inline">\(\qquad \text{tr}(\Sigma(X)M_{X}) = \sigma^{2}\text{tr}(M_{X}) = \sigma^{2}(n - K)\)</span>.</p>
<p><strong>Intuition:</strong></p>
<p>This theorem shows that the modified estimator of the error variance, <span class="math inline">\(s_*^2\)</span>, is unbiased. This means that, on average, this estimator will correctly estimate the true error variance. The correction factor of dividing by <span class="math inline">\((n-K)\)</span> instead of <span class="math inline">\(n\)</span> accounts for the degrees of freedom lost in estimating the <span class="math inline">\(K\)</span> parameters in <span class="math inline">\(\beta\)</span>.</p>
</section>
</section>
<section class="level2" id="alternative-estimators">
<h2 class="anchored" data-anchor-id="alternative-estimators">19.1.1 Alternative Estimators</h2>
<p>There are many possible alternative estimators to OLS that have been developed over the years. A major alternative is called the <strong>Least Absolute Deviations (LAD)</strong> procedure that minimizes the <span class="math inline">\(L_{1}\)</span> norm of the error term</p>
<p><span class="math inline">\(\qquad ||y - Xb||_{1},\)</span></p>
<p>where <span class="math inline">\(||u||_{1} = \sum_{i=1}^{n}|u_{i}|\)</span>. This procedure goes back to Laplace in the 18th century but was put on a firm footing by Koenker and Bassett (1978). The resulting estimator denoted <span class="math inline">\(\hat{\beta}_{LAD}\)</span> is a regression analogue of the sample median and has certain desirable properties in the face of outliers in the covariates. We discuss this procedure in more detail below.</p>
<p>Consider the scalar regression <span class="math inline">\(y_{i} = \beta x_{i} + \epsilon_{i}\)</span>. The OLS estimator is <span class="math inline">\(\hat{\beta} = \dfrac{\sum_{i=1}^{n} x_{i}y_{i}}{\sum_{i=1}^{n}x_{i}^{2}}\)</span>. Also plausible are <span class="math inline">\(\tilde{\beta} = \bar{y}/\bar{x}\)</span> and <span class="math inline">\(\breve{\beta} = \dfrac{\sum_{i=1}^{n} y_i}{\sum_{i=1}^n x_i}\)</span>. Another example that arises from accounting practice, is the so-called high-low method for determining average cost. The corresponding estimator is</p>
<p><span class="math inline">\(\qquad \hat{\beta}_{H-L} = \dfrac{y_{H} - y_{L}}{x_{H} - x_{L}}\)</span>,</p>
<p>where <span class="math inline">\(x_{H}, x_{L}\)</span> are the highest and lowest values achieved by the covariate respectively, and <span class="math inline">\(y_{H}, y_{L}\)</span> are the “concomitants”, that is, the corresponding values of the outcome variable. This estimator is also linear and unbiased under Assumption A.</p>
<p>A major class of estimators goes by the name of <strong>Instrumental variables</strong>. These are designed to outflank issues arising from endogeneity, which is of central importance in applied work. These are of the general form</p>
<p><span class="math inline">\(\qquad \tilde{\beta} = (Z^{T}X)^{-1}Z^{T}y\)</span>,</p>
<p>where <span class="math inline">\(Z\)</span> is a full rank <span class="math inline">\(n \times K\)</span> matrix of instruments. We will discuss this class of estimators somewhat more in the final chapter, as they are included as a special case of the Generalized Method of Moments.</p>
</section>
<section class="level2" id="optimality">
<h2 class="anchored" data-anchor-id="optimality">19.2 Optimality</h2>
<p>There are many estimators of <span class="math inline">\(\beta\)</span>. How do we choose between estimators? Computational convenience is an important issue, but the above estimators are all similar in their computational requirements. We now investigate statistical optimality.</p>
<section class="level3" id="definition-19.2">
<h3 class="anchored" data-anchor-id="definition-19.2">Definition 19.2</h3>
<p>The <strong>mean squared error (MSE)</strong> matrix of a generic estimator <span class="math inline">\(\tilde{\theta}\)</span> of a parameter <span class="math inline">\(\theta \in \mathbb{R}^{p}\)</span> is</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
M(\tilde{\theta}, \theta) &amp;= \mathbb{E}\left[ (\tilde{\theta} - \theta)(\tilde{\theta} - \theta)^{T} \right] \\
&amp;= \mathbb{E}\left[ (\tilde{\theta} - \mathbb{E}[\tilde{\theta}] + \mathbb{E}[\tilde{\theta}] - \theta)(\tilde{\theta} - \mathbb{E}[\tilde{\theta}] + \mathbb{E}[\tilde{\theta}] - \theta)^{T} \right] \\
&amp;= \mathbb{E}\left[ (\tilde{\theta} - \mathbb{E}[\tilde{\theta}])(\tilde{\theta} - \mathbb{E}[\tilde{\theta}])^{T} \right] + (\mathbb{E}[\tilde{\theta}] - \theta)(\mathbb{E}[\tilde{\theta}] - \theta)^{T}. \\
&amp; \quad \text{variance} \qquad\qquad \text{squared bias}
\end{aligned}\)</span></p>
<p>The expectation here can be conditional on <span class="math inline">\(X\)</span> or unconditional. The MSE matrix is generally a function of the true parameter <span class="math inline">\(\theta\)</span>. We would like a method that does well for all <span class="math inline">\(\theta\)</span>, not just a subset of parameter values – the estimator <span class="math inline">\(\hat{\theta} = 0\)</span> is an example of a procedure that will have MSE equal to zero at <span class="math inline">\(\theta = 0\)</span>, and hence will do well at this point, but as <span class="math inline">\(\theta\)</span> moves away, the MSE increases quadratically without limit. Note that no estimator can dominate uniformly across <span class="math inline">\(\theta\)</span> according to MSE because it would have to beat all constant estimators which have zero MSE at a single point. This is impossible unless there is no randomness. This is the same issue we discussed in the scalar case in an earlier chapter.</p>
<p>In the multivariate case, an additional issue arises even when comparing two estimators at a single point of the parameter space. MSE defines a complete ordering when <span class="math inline">\(p=1\)</span>, i.e., one can always rank any two estimators according to MSE. When <span class="math inline">\(p &gt; 1\)</span>, this is not so. In the general case we say that <span class="math inline">\(\tilde{\theta}\)</span> is better (according to MSE) than <span class="math inline">\(\hat{\theta}\)</span> (at some fixed point <span class="math inline">\(\theta\)</span>) if <span class="math inline">\(B \geq A\)</span> (i.e., <span class="math inline">\(B - A\)</span> is a positive semidefinite matrix), where <span class="math inline">\(B\)</span> is the MSE matrix of <span class="math inline">\(\tilde{\theta}\)</span> and <span class="math inline">\(A\)</span> is the MSE of <span class="math inline">\(\hat{\theta}\)</span>. For example, suppose that</p>
<p><span class="math inline">\(\qquad A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{bmatrix}, \quad B = \begin{bmatrix} 2 &amp; 0 \\ 0 &amp; 1/4 \end{bmatrix}\)</span>.</p>
<p>In this case, we can not rank the estimators. The problem is due to the multivariate nature of the optimality criterion. One solution is to take a scalar function of MSE such as the trace or determinant, which will result in a complete ordering. For example, for positive definite <span class="math inline">\(Q\)</span> let</p>
<p><span class="math inline">\(\qquad \text{tr}\left( \mathbb{E}\left[ (\tilde{\theta} - \theta)^{T}Q(\tilde{\theta} - \theta) \right] \right) = \text{tr}(MQ)\)</span>.</p>
<p>For example, consider linear combinations of the parameter <span class="math inline">\(c^{T}\theta\)</span>. In this case, the MSE of <span class="math inline">\(c^{T}(\tilde{\theta} - \theta)\)</span> is scalar and can be considered as the trace MSE with the particular <span class="math inline">\(Q = cc^{T}\)</span>. However, different scalar functions will rank estimators differently, e.g., what is good for some <span class="math inline">\(c\)</span> may be bad for some other <span class="math inline">\(c'\)</span>.</p>
</section>
<section class="level3" id="example-19.2">
<h3 class="anchored" data-anchor-id="example-19.2">Example 19.2</h3>
<p>For example, the estimation of <span class="math inline">\(m = X\beta\)</span> by <span class="math inline">\(\hat{m} = X\hat{\beta}\)</span>, yields</p>
<p><span class="math inline">\(\qquad \text{tr}\left( \mathbb{E}\left[ (\hat{m} - m)^{T}Q(\hat{m} - m) \right] \right) = \text{tr}\left( X(X^{T}X)^{-1}X^{T}\Sigma(X)X(X^{T}X)^{-1}X^{T}Q \right)\)</span>.</p>
<p>Under homoskedasticity and with <span class="math inline">\(Q = I_{n}\)</span>, this equals <span class="math inline">\(\sigma^{2}K\)</span>. For other <span class="math inline">\(Q\)</span> we obtain a different result.</p>
<p>The OLS estimator is <strong>admissible</strong> in the scalar case, meaning that no estimator uniformly dominates it according to mean squared error. However, in the multivariate case when the rank of <span class="math inline">\(Q\)</span> is greater than or equal to <span class="math inline">\(3\)</span> <strong>Stein (shrinkage) estimators</strong>, which are biased, improve on least squares according to MSE.</p>
<p>An alternative approach is to consider the minimax approach. For example, we might take the performance measure of the estimator <span class="math inline">\(\tilde{\theta}\)</span> to be</p>
<p><span class="math inline">\(\qquad R(\tilde{\theta}) = \max_{\theta \in \Theta} \text{tr}(M(\tilde{\theta}, \theta))\)</span>,</p>
<p>which takes the most pessimistic view. In this case, we might try and find the estimator that minimizes this criterion – this would be called a minimax estimator. The theory for this class of estimators is very complicated, and in any case it is not such a desirable criterion because it is so pessimistic about nature trying to do the worst to us.</p>
<p>Instead, we might reduce the class of allowable estimators. If we restrict attention to unbiased estimators then this rules out estimators like <span class="math inline">\(\tilde{\theta} = 0\)</span> because they will be biased. In this case there is some hope of an optimality theory for the class of unbiased estimators. We will now return to the linear regression model and make the further restriction that the estimators we consider are linear in <span class="math inline">\(y\)</span>. That is, we suppose that we have the set of all estimators <span class="math inline">\(\tilde{\beta}\)</span> that satisfy</p>
<p><span class="math inline">\(\qquad \tilde{\beta} = Ay\)</span></p>
<p>for some fixed matrix <span class="math inline">\(A\)</span> such that with probability one</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\tilde{\beta}|X] = \beta, \quad \forall \beta\)</span>.</p>
<p>This latter condition implies that <span class="math inline">\((AX - I)\beta = 0\)</span> for all <span class="math inline">\(\beta\)</span>, which is equivalent to <span class="math inline">\(AX = I\)</span>.</p>
</section>
<section class="level3" id="theorem-19.6-gauss-markov">
<h3 class="anchored" data-anchor-id="theorem-19.6-gauss-markov">Theorem 19.6 (Gauss Markov)</h3>
<p>Suppose that assumptions A1 and A2 hold and that <span class="math inline">\(\Sigma(X) = \sigma^{2}I_{n}\)</span>. Then the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> is <strong>Best Linear Unbiased (BLUE)</strong>, i.e., with probability one</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta}|X) \leq \text{var}(\tilde{\beta}|X)\)</span></p>
<p>for any other LUE.</p>
<p><strong>Proof:</strong></p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta}|X) = \sigma^{2}(X^{T}X)^{-1}; \quad \text{var}(\tilde{\beta}|X) = \sigma^{2}AA^{T}\)</span></p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\text{var}(\tilde{\beta}|X) - \text{var}(\hat{\beta}|X) &amp;= \sigma^{2}\left[ AA^{T} - (X^{T}X)^{-1} \right] \\
&amp;= \sigma^{2}\left[ AA^{T} - AX(X^{T}X)^{-1}X^{T}A^{T} \right] \\
&amp;= \sigma^{2}A\left[ I - X(X^{T}X)^{-1}X^{T} \right]A^{T} \\
&amp;= \sigma^{2}AM_{X}A^{T} \\
&amp;\geq 0.
\end{aligned}\)</span></p>
<p><strong>Remarks:</strong></p>
<ol type="1">
<li>No assumption is made about the distribution of the errors; it only assumes 0 mean and <span class="math inline">\(\sigma^{2}I\)</span> variance.</li>
<li>Result only compares <em>linear</em> estimators; it says nothing about for example <span class="math inline">\(\sum_{i=1}^{n}|y_{i} - \beta x_{i}|\)</span>.</li>
<li>Result only compares <em>unbiased</em> estimators [biased estimators can have 0 variances].</li>
<li>There are extensions to consider affine estimators <span class="math inline">\(\tilde{\beta} = a + Ay\)</span> for vectors <span class="math inline">\(a\)</span>. There are also equivalent results for the invariant quantity <span class="math inline">\(\hat{y}\)</span>.</li>
<li>This says that for any linear combination <span class="math inline">\(c^{T}\beta\)</span>, the OLS is BLUE when <span class="math inline">\(X\)</span> is of full rank. When <span class="math inline">\(X\)</span> is not of full rank, one can show that for an estimable linear combination <span class="math inline">\(c^{T}\beta\)</span>, the OLS is BLUE.</li>
</ol>
<p>If we dispense with the unbiasedness assumption and add the model assumption of error normality we get the well-known result.</p>
</section>
<section class="level3" id="theorem-19.7-cramér-rao">
<h3 class="anchored" data-anchor-id="theorem-19.7-cramér-rao">Theorem 19.7 (Cramér-Rao)</h3>
<p>Suppose that Assumption A4 holds. Then, <span class="math inline">\(\hat{\beta}\)</span> is <strong>Best Unbiased</strong>.</p>
<p>By making the stronger assumption A4, we get a much stronger conclusion. This allows us to compare say LAD estimation with OLS. The log-likelihood is given in equation (19.3), and it can be shown that the information matrix is</p>
<p><span class="math inline">\(\qquad I(\beta, \sigma^{2}) = \begin{bmatrix} \frac{1}{\sigma^{2}}X^{T}X &amp; 0 \\ 0 &amp; \frac{n}{2\sigma^{4}} \end{bmatrix}\)</span>.</p>
<p>This structure is block diagonal, which has some statistical implications. The inverse information is also block diagonal</p>
<p><span class="math inline">\(\qquad I(\beta, \sigma^{2})^{-1} = \begin{bmatrix} \sigma^{2}(X^{T}X)^{-1} &amp; 0 \\ 0 &amp; \frac{2\sigma^{4}}{n} \end{bmatrix}\)</span></p>
<p>and represents the lower bound for the variance of unbiased estimators. The MLE of <span class="math inline">\(\beta\)</span> is unbiased and achieves the bound, whereas the MLE of <span class="math inline">\(\sigma^{2}\)</span> is biased and does not achieve the bound in finite samples. Suppose that the parameter <span class="math inline">\(\beta\)</span> were known, the information with regard to the unknown parameter <span class="math inline">\(\sigma^{2}\)</span> is <span class="math inline">\(n/2\sigma^{4}\)</span>. On the other hand suppose that the parameter <span class="math inline">\(\sigma^{2}\)</span> is known, the information matrix with regard to the unknown parameter vector <span class="math inline">\(\beta\)</span> is <span class="math inline">\(X^{T}X/\sigma^{2}\)</span>. This says that knowledge of the other parameter provides no additional information about the unknown parameter.</p>
</section>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch19exercise1">
<h3 class="anchored" data-anchor-id="sec-ch19exercise1">Exercise 1</h3>
<p><a href="#sec-ch19solution1">Solution 1</a></p>
<p>Show that the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> is conditionally unbiased under Assumption A1, without using the matrix notation.</p>
</section>
<section class="level3" id="sec-ch19exercise2">
<h3 class="anchored" data-anchor-id="sec-ch19exercise2">Exercise 2</h3>
<p><a href="#sec-ch19solution2">Solution 2</a></p>
<p>Explain intuitively what it means for an estimator to be “linear” in the context of OLS. Give an example of a non-linear estimator.</p>
</section>
<section class="level3" id="sec-ch19exercise3">
<h3 class="anchored" data-anchor-id="sec-ch19exercise3">Exercise 3</h3>
<p><a href="#sec-ch19solution3">Solution 3</a></p>
<p>Derive the conditional variance-covariance matrix of the OLS estimator, <span class="math inline">\(\text{var}(\hat{\beta}|X)\)</span>, under the assumption of homoskedasticity, starting from the general formula.</p>
</section>
<section class="level3" id="sec-ch19exercise4">
<h3 class="anchored" data-anchor-id="sec-ch19exercise4">Exercise 4</h3>
<p><a href="#sec-ch19solution4">Solution 4</a></p>
<p>Explain the difference between conditional and unconditional unbiasedness. Does conditional unbiasedness imply unconditional unbiasedness?</p>
</section>
<section class="level3" id="sec-ch19exercise5">
<h3 class="anchored" data-anchor-id="sec-ch19exercise5">Exercise 5</h3>
<p><a href="#sec-ch19solution5">Solution 5</a></p>
<p>What is the role of the Law of Iterated Expectations in the context of proving the unbiasedness of the OLS estimator?</p>
</section>
<section class="level3" id="sec-ch19exercise6">
<h3 class="anchored" data-anchor-id="sec-ch19exercise6">Exercise 6</h3>
<p><a href="#sec-ch19solution6">Solution 6</a></p>
<p>Under what conditions does the variance of the OLS estimator, <span class="math inline">\(\text{var}(\hat{\beta})\)</span>, simplify to <span class="math inline">\(\sigma^{2}\mathbb{E}[(X^{T}X)^{-1}]\)</span>? Explain.</p>
</section>
<section class="level3" id="sec-ch19exercise7">
<h3 class="anchored" data-anchor-id="sec-ch19exercise7">Exercise 7</h3>
<p><a href="#sec-ch19solution7">Solution 7</a></p>
<p>Consider a simple linear regression model: <span class="math inline">\(y_i = \beta x_i + \epsilon_i\)</span>, where <span class="math inline">\(\epsilon_i \sim i.i.d. N(0, \sigma^2)\)</span>. If you have a sample of only two observations (<span class="math inline">\(n=2\)</span>), is the OLS estimator for <span class="math inline">\(\beta\)</span> still unbiased? Explain.</p>
</section>
<section class="level3" id="sec-ch19exercise8">
<h3 class="anchored" data-anchor-id="sec-ch19exercise8">Exercise 8</h3>
<p><a href="#sec-ch19solution8">Solution 8</a></p>
<p>Given the partitioned regression formula <span class="math inline">\(\hat{\beta}_1 = (X_1^T M_2 X_1)^{-1} X_1^T M_2 y\)</span>, explain the meaning of <span class="math inline">\(M_2\)</span> and its role in the formula.</p>
</section>
<section class="level3" id="sec-ch19exercise9">
<h3 class="anchored" data-anchor-id="sec-ch19exercise9">Exercise 9</h3>
<p><a href="#sec-ch19solution9">Solution 9</a></p>
<p>Explain in intuitive terms why, under the assumption of normality of errors, the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> also follows a normal distribution.</p>
</section>
<section class="level3" id="sec-ch19exercise10">
<h3 class="anchored" data-anchor-id="sec-ch19exercise10">Exercise 10</h3>
<p><a href="#sec-ch19solution10">Solution 10</a>(#sec-ch19solution10}</p>
<p>What is the significance of the result <span class="math inline">\(\tau = (\hat{\beta} - \beta)^{T}V(X)^{-1}(\hat{\beta} - \beta) \sim \chi^{2}(K)\)</span>? How is it used in practice?</p>
</section>
<section class="level3" id="sec-ch19exercise11">
<h3 class="anchored" data-anchor-id="sec-ch19exercise11">Exercise 11</h3>
<p><a href="#sec-ch19solution11">Solution 11</a>(#sec-ch19solution11}</p>
<p>Explain the difference between <span class="math inline">\(\hat{m}\)</span> and <span class="math inline">\(\hat{m}(x)\)</span> in the context of Theorem 19.4.</p>
</section>
<section class="level3" id="sec-ch19exercise12">
<h3 class="anchored" data-anchor-id="sec-ch19exercise12">Exercise 12</h3>
<p><a href="#sec-ch19solution12">Solution 12</a>(#sec-ch19solution12}</p>
<p>Derive the expression for the Maximum Likelihood Estimator (MLE) of <span class="math inline">\(\sigma^{2}\)</span> in the linear regression model under the assumption of normality.</p>
</section>
<section class="level3" id="sec-ch19exercise13">
<h3 class="anchored" data-anchor-id="sec-ch19exercise13">Exercise 13</h3>
<p><a href="#sec-ch19solution13">Solution 13</a>(#sec-ch19solution13}</p>
<p>Why is <span class="math inline">\(s_{*}^{2} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n-K}\)</span> an unbiased estimator of <span class="math inline">\(\sigma^{2}\)</span>, while <span class="math inline">\(\hat{\sigma}_{mle}^{2} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n}\)</span> is not?</p>
</section>
<section class="level3" id="sec-ch19exercise14">
<h3 class="anchored" data-anchor-id="sec-ch19exercise14">Exercise 14</h3>
<p><a href="#sec-ch19solution14">Solution 14</a>(#sec-ch19solution14}</p>
<p>Explain the concept of “degrees of freedom” in the context of estimating the error variance <span class="math inline">\(\sigma^2\)</span>.</p>
</section>
<section class="level3" id="sec-ch19exercise15">
<h3 class="anchored" data-anchor-id="sec-ch19exercise15">Exercise 15</h3>
<p><a href="#sec-ch19solution15">Solution 15</a>(#sec-ch19solution15}</p>
<p>What does it mean for an estimator to be “Best Linear Unbiased Estimator” (BLUE)?</p>
</section>
<section class="level3" id="sec-ch19exercise16">
<h3 class="anchored" data-anchor-id="sec-ch19exercise16">Exercise 16</h3>
<p><a href="#sec-ch19solution16">Solution 16</a>(#sec-ch19solution16}</p>
<p>Explain the Gauss-Markov Theorem in your own words, without using mathematical formulas.</p>
</section>
<section class="level3" id="sec-ch19exercise17">
<h3 class="anchored" data-anchor-id="sec-ch19exercise17">Exercise 17</h3>
<p><a href="#sec-ch19solution17">Solution 17</a>(#sec-ch19solution17}</p>
<p>List the key assumptions required for the Gauss-Markov Theorem to hold.</p>
</section>
<section class="level3" id="sec-ch19exercise18">
<h3 class="anchored" data-anchor-id="sec-ch19exercise18">Exercise 18</h3>
<p><a href="#sec-ch19solution18">Solution 18</a>(#sec-ch19solution18}</p>
<p>What is the Cramér-Rao Lower Bound, and what is its relevance to the OLS estimator?</p>
</section>
<section class="level3" id="sec-ch19exercise19">
<h3 class="anchored" data-anchor-id="sec-ch19exercise19">Exercise 19</h3>
<p><a href="#sec-ch19solution19">Solution 19</a>(#sec-ch19solution19}</p>
<p>Why is the information matrix for <span class="math inline">\((\beta, \sigma^2)\)</span> block diagonal in the standard linear regression model with normal errors? What are the implications of this block diagonality?</p>
</section>
<section class="level3" id="sec-ch19exercise20">
<h3 class="anchored" data-anchor-id="sec-ch19exercise20">Exercise 20</h3>
<p><a href="#sec-ch19solution20">Solution 20</a>(#sec-ch19solution20}</p>
<p>Consider the linear regression model <span class="math inline">\(y = X\beta + \epsilon\)</span> with heteroskedastic errors, such that <span class="math inline">\(\text{Var}(\epsilon|X) = \Sigma(X)\)</span>, where <span class="math inline">\(\Sigma(X)\)</span> is a diagonal matrix with elements <span class="math inline">\(\sigma_i^2\)</span>. Show that the variance of the OLS estimator is given by the formula provided in theorem 19.2.</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch19solution1">
<h3 class="anchored" data-anchor-id="sec-ch19solution1">Solution 1</h3>
<p><a href="#sec-ch19exercise1">Exercise 1</a></p>
<p>The linear regression model can be expressed as:</p>
<p><span class="math inline">\(\qquad y_i = x_i^T \beta + \epsilon_i\)</span>, for <span class="math inline">\(i = 1, ..., n\)</span>.</p>
<p>where <span class="math inline">\(y_i\)</span> is the dependent variable, <span class="math inline">\(x_i\)</span> is a <span class="math inline">\(K \times 1\)</span> vector of independent variables, <span class="math inline">\(\beta\)</span> is a <span class="math inline">\(K \times 1\)</span> vector of coefficients, and <span class="math inline">\(\epsilon_i\)</span> is the error term.</p>
<p>The OLS estimator for <span class="math inline">\(\beta\)</span> is obtained by minimizing the sum of squared errors:</p>
<p><span class="math inline">\(\qquad \sum_{i=1}^{n} (y_i - x_i^T \hat{\beta})^2\)</span>.</p>
<p>The solution to this minimization problem yields the OLS estimator:</p>
<p><span class="math inline">\(\qquad \hat{\beta} = (\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i y_i)\)</span>.</p>
<p>Substituting <span class="math inline">\(y_i = x_i^T \beta + \epsilon_i\)</span> into the OLS estimator formula: <span class="math inline">\(\qquad
\begin{aligned}
\hat{\beta} &amp;= (\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i (x_i^T \beta + \epsilon_i)) \\
&amp;= (\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i x_i^T \beta + \sum_{i=1}^{n} x_i \epsilon_i) \\
&amp;= (\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i x_i^T) \beta + (\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i \epsilon_i) \\
&amp;= \beta + (\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i \epsilon_i).
\end{aligned}\)</span> Now, we take the conditional expectation given <span class="math inline">\(X\)</span>: <span class="math inline">\(\qquad
\begin{aligned}
\mathbb{E}[\hat{\beta}|X] &amp;= \mathbb{E}[\beta|X] + \mathbb{E}[(\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i \epsilon_i)|X] \\
&amp;= \beta + (\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i \mathbb{E}[\epsilon_i|X]).
\end{aligned}\)</span></p>
<p>Assumption A1 states that <span class="math inline">\(\mathbb{E}[\epsilon|X] = 0\)</span>. This means that <span class="math inline">\(\mathbb{E}[\epsilon_i|X] = 0\)</span> for all <span class="math inline">\(i\)</span>. Thus,</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\hat{\beta}|X] = \beta + (\sum_{i=1}^{n} x_i x_i^T)^{-1} (\sum_{i=1}^{n} x_i \cdot 0) = \beta + 0 = \beta\)</span>.</p>
<p>Therefore, the OLS estimator is conditionally unbiased.</p>
</section>
<section class="level3" id="sec-ch19solution2">
<h3 class="anchored" data-anchor-id="sec-ch19solution2">Solution 2</h3>
<p><a href="#sec-ch19exercise2">Exercise 2</a></p>
<p>An estimator is <strong>linear</strong> if it can be expressed as a weighted sum of the observed dependent variable values (<span class="math inline">\(y_i\)</span>). In the context of OLS, this means the estimator <span class="math inline">\(\hat{\beta}\)</span> can be written in the form <span class="math inline">\(\hat{\beta} = Cy\)</span>, where <span class="math inline">\(C\)</span> is a matrix that depends only on the independent variables (<span class="math inline">\(X\)</span>) and not on the dependent variable (<span class="math inline">\(y\)</span>). This linearity simplifies many calculations and is a key property used in deriving the statistical properties of the OLS estimator. The linearity arises from the normal equations and the fact that we are solving a linear system of equations to find the OLS estimator.</p>
<p>An example of a <strong>non-linear estimator</strong> would be an estimator that involves a non-linear transformation of the dependent variable <span class="math inline">\(y\)</span> before estimating the parameters. For example, consider estimating the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> of a model: <span class="math inline">\(y_i=ae^{bx_i}\epsilon_i\)</span>. After a log transform, the parameters <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> can be estimated using linear regression methods, but we have to resort to nonlinear methods if we do not transform the data.</p>
</section>
<section class="level3" id="sec-ch19solution3">
<h3 class="anchored" data-anchor-id="sec-ch19solution3">Solution 3</h3>
<p><a href="#sec-ch19exercise3">Exercise 3</a></p>
<p>The general formula for the conditional variance-covariance matrix of the OLS estimator is:</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta}|X) = (X^{T}X)^{-1}X^{T}\mathbb{E}[\epsilon \epsilon^{T}|X]X(X^{T}X)^{-1}\)</span>.</p>
<p>Under the assumption of homoskedasticity, the conditional variance of each error term is constant, and the error terms are uncorrelated. This means <span class="math inline">\(\mathbb{E}[\epsilon_i^2|X] = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>, and <span class="math inline">\(\mathbb{E}[\epsilon_i \epsilon_j|X] = 0\)</span> for <span class="math inline">\(i \neq j\)</span>. This can be summarized as:</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\epsilon \epsilon^{T}|X] = \sigma^{2}I_{n}\)</span>,</p>
<p>where <span class="math inline">\(I_{n}\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix. Substituting this into the general formula:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\text{var}(\hat{\beta}|X) &amp;= (X^{T}X)^{-1}X^{T}(\sigma^{2}I_{n})X(X^{T}X)^{-1} \\
&amp;= \sigma^{2}(X^{T}X)^{-1}X^{T}I_{n}X(X^{T}X)^{-1} \\
&amp;= \sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1} \\
&amp;= \sigma^{2}(X^{T}X)^{-1}(X^{T}X)(X^{T}X)^{-1} \\
&amp;= \sigma^{2}(X^{T}X)^{-1}.
\end{aligned}\)</span></p>
<p>Thus, under homoskedasticity, the conditional variance-covariance matrix simplifies to <span class="math inline">\(\sigma^{2}(X^{T}X)^{-1}\)</span>.</p>
</section>
<section class="level3" id="sec-ch19solution4">
<h3 class="anchored" data-anchor-id="sec-ch19solution4">Solution 4</h3>
<p><a href="#sec-ch19exercise4">Exercise 4</a></p>
<p><strong>Conditional unbiasedness</strong> means that the expected value of the estimator, <em>given</em> the values of the independent variables <span class="math inline">\(X\)</span>, is equal to the true parameter value. Formally, <span class="math inline">\(\mathbb{E}[\hat{\beta}|X] = \beta\)</span>. This implies that for any specific set of <span class="math inline">\(X\)</span> values, if we were to repeatedly sample and calculate the estimator, the average of those estimates would converge to the true value.</p>
<p><strong>Unconditional unbiasedness</strong> means that the expected value of the estimator, averaging over <em>all possible</em> values of the independent variables <span class="math inline">\(X\)</span>, is equal to the true parameter value. Formally, <span class="math inline">\(\mathbb{E}[\hat{\beta}] = \beta\)</span>.</p>
<p>Yes, conditional unbiasedness <em>does</em> imply unconditional unbiasedness. This can be shown using the <strong>Law of Iterated Expectations</strong>:</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\hat{\beta}] = \mathbb{E}[\mathbb{E}[\hat{\beta}|X]]\)</span>.</p>
<p>If <span class="math inline">\(\mathbb{E}[\hat{\beta}|X] = \beta\)</span>, then</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\hat{\beta}] = \mathbb{E}[\beta] = \beta\)</span>.</p>
<p>The intuition here is that if the estimator is unbiased for every possible value of <span class="math inline">\(X\)</span>, then it must also be unbiased on average across all possible values of <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="sec-ch19solution5">
<h3 class="anchored" data-anchor-id="sec-ch19solution5">Solution 5</h3>
<p><a href="#sec-ch19exercise5">Exercise 5</a></p>
<p>The Law of Iterated Expectations (LIE) is crucial for demonstrating that conditional unbiasedness implies unconditional unbiasedness. The LIE states that:</p>
<p><span class="math inline">\(\qquad \mathbb{E}[Y] = \mathbb{E}[\mathbb{E}[Y|X]]\)</span>.</p>
<p>In the context of the OLS estimator, we first show that the estimator is conditionally unbiased: <span class="math inline">\(\mathbb{E}[\hat{\beta}|X] = \beta\)</span>. Then, to show unconditional unbiasedness, we apply the LIE:</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\hat{\beta}] = \mathbb{E}[\mathbb{E}[\hat{\beta}|X]]\)</span>.</p>
<p>Since we know <span class="math inline">\(\mathbb{E}[\hat{\beta}|X] = \beta\)</span>, we substitute this in:</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\hat{\beta}] = \mathbb{E}[\beta]\)</span>.</p>
<p>Since <span class="math inline">\(\beta\)</span> is a constant (the true parameter value), its expectation is simply itself:</p>
<p><span class="math inline">\(\qquad \mathbb{E}[\hat{\beta}] = \beta\)</span>.</p>
<p>Thus, the LIE allows us to move from the conditional statement (unbiased for a given <span class="math inline">\(X\)</span>) to the unconditional statement (unbiased overall).</p>
</section>
<section class="level3" id="sec-ch19solution6">
<h3 class="anchored" data-anchor-id="sec-ch19solution6">Solution 6</h3>
<p><a href="#sec-ch19exercise6">Exercise 6</a></p>
<p>The variance of the OLS estimator is generally given by:</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta}) = \mathbb{E}[\text{var}(\hat{\beta}|X)] = \mathbb{E}\left[ (X^{T}X)^{-1} X^{T} \Sigma(X) X (X^{T}X)^{-1} \right]\)</span>.</p>
<p>This simplifies to <span class="math inline">\(\sigma^{2}\mathbb{E}[(X^{T}X)^{-1}]\)</span> under the following conditions:</p>
<ol type="1">
<li><strong>Homoskedasticity:</strong> The variance of the error term is constant for all observations: <span class="math inline">\(\text{Var}(\epsilon_i|X) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>.</li>
<li><strong>No Autocorrelation:</strong> The error terms are uncorrelated with each other: <span class="math inline">\(\text{Cov}(\epsilon_i, \epsilon_j|X) = 0\)</span> for all <span class="math inline">\(i \neq j\)</span>.</li>
</ol>
<p>These two conditions together imply that <span class="math inline">\(\Sigma(X) = \mathbb{E}[\epsilon\epsilon^T|X] = \sigma^2 I_n\)</span>, where <span class="math inline">\(I_n\)</span> is the <span class="math inline">\(n \times n\)</span> identity matrix. Substituting this into the general formula:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\text{var}(\hat{\beta}) &amp;= \mathbb{E}\left[(X^{T}X)^{-1}X^{T}(\sigma^{2}I_{n})X(X^{T}X)^{-1}\right] \\
&amp;= \mathbb{E}\left[\sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-1}\right] \\
&amp;= \sigma^{2}\mathbb{E}\left[(X^{T}X)^{-1}\right].
\end{aligned}\)</span></p>
<p>Therefore, homoskedasticity and no autocorrelation are required for the simplification.</p>
</section>
<section class="level3" id="sec-ch19solution7">
<h3 class="anchored" data-anchor-id="sec-ch19solution7">Solution 7</h3>
<p><a href="#sec-ch19exercise7">Exercise 7</a></p>
<p>Yes, the OLS estimator for <span class="math inline">\(\beta\)</span> is still unbiased, even with only two observations. The unbiasedness property of the OLS estimator relies on Assumption A1 (<span class="math inline">\(\mathbb{E}[\epsilon|X] = 0\)</span>), which does not depend on the sample size, <span class="math inline">\(n\)</span>.</p>
<p>Let’s derive the OLS estimator for this specific case:</p>
<p><span class="math inline">\(\qquad y_1 = \beta x_1 + \epsilon_1\)</span> <span class="math inline">\(\qquad y_2 = \beta x_2 + \epsilon_2\)</span></p>
<p>In matrix form:</p>
<p><span class="math inline">\(\qquad
\begin{bmatrix}
y_1 \\
y_2
\end{bmatrix} =
\begin{bmatrix}
x_1 \\
x_2
\end{bmatrix} \beta +
\begin{bmatrix}
\epsilon_1 \\
\epsilon_2
\end{bmatrix}\)</span></p>
<p><span class="math inline">\(y = X\beta + \epsilon\)</span></p>
<p>The OLS estimator is given by <span class="math inline">\(\hat{\beta} = (X^{T}X)^{-1}X^{T}y\)</span>. In this case:</p>
<p><span class="math inline">\(\qquad X^{T}X = \begin{bmatrix} x_1 &amp; x_2 \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \end{bmatrix} = x_1^2 + x_2^2\)</span> <span class="math inline">\(\qquad X^{T}y = \begin{bmatrix} x_1 &amp; x_2 \end{bmatrix} \begin{bmatrix} y_1 \\ y_2 \end{bmatrix} = x_1 y_1 + x_2 y_2\)</span></p>
<p>So, <span class="math inline">\(\hat{\beta} = \dfrac{x_1 y_1 + x_2 y_2}{x_1^2 + x_2^2}\)</span>.</p>
<p>Now, let’s take the conditional expectation:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\mathbb{E}[\hat{\beta}|X] &amp;= \mathbb{E}\left[ \dfrac{x_1 y_1 + x_2 y_2}{x_1^2 + x_2^2} | x_1, x_2 \right] \\
&amp;=  \dfrac{1}{x_1^2 + x_2^2} \mathbb{E}\left[ x_1 y_1 + x_2 y_2 | x_1, x_2 \right] \\
&amp;=  \dfrac{1}{x_1^2 + x_2^2} \left( x_1 \mathbb{E}[y_1|x_1] + x_2 \mathbb{E}[y_2|x_2] \right)
\end{aligned}\)</span></p>
<p>Substitute <span class="math inline">\(y_i = \beta x_i + \epsilon_i\)</span>:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\mathbb{E}[\hat{\beta}|X] &amp;= \dfrac{1}{x_1^2 + x_2^2} \left( x_1 \mathbb{E}[\beta x_1 + \epsilon_1|x_1] + x_2 \mathbb{E}[\beta x_2 + \epsilon_2|x_2] \right) \\
&amp;= \dfrac{1}{x_1^2 + x_2^2} \left( x_1 (\beta x_1 + \mathbb{E}[\epsilon_1|x_1]) + x_2 (\beta x_2 + \mathbb{E}[\epsilon_2|x_2]) \right)
\end{aligned}\)</span></p>
<p>Using Assumption A1, <span class="math inline">\(\mathbb{E}[\epsilon_i|x_i] = 0\)</span>:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\mathbb{E}[\hat{\beta}|X] &amp;= \dfrac{1}{x_1^2 + x_2^2} (x_1^2 \beta + x_2^2 \beta) \\
&amp;= \dfrac{\beta(x_1^2 + x_2^2)}{x_1^2 + x_2^2} \\
&amp;= \beta
\end{aligned}\)</span></p>
<p>Thus, <span class="math inline">\(\hat{\beta}\)</span> is unbiased even with <span class="math inline">\(n=2\)</span>. However, while it’s unbiased, the <em>variance</em> of the estimator will be very large with only two data points.</p>
</section>
<section class="level3" id="sec-ch19solution8">
<h3 class="anchored" data-anchor-id="sec-ch19solution8">Solution 8</h3>
<p><a href="#sec-ch19exercise8">Exercise 8</a></p>
<p>The matrix <span class="math inline">\(M_2\)</span> is a <strong>residual maker matrix</strong> or <strong>annihilator matrix</strong> associated with the variables in <span class="math inline">\(X_2\)</span>. It is defined as:</p>
<p><span class="math inline">\(\qquad M_2 = I - X_2(X_2^T X_2)^{-1}X_2^T\)</span>.</p>
<p>Here’s its role and meaning:</p>
<ol type="1">
<li><strong>Projection:</strong> <span class="math inline">\(X_2(X_2^T X_2)^{-1}X_2^T\)</span> is the projection matrix onto the column space of <span class="math inline">\(X_2\)</span>. When this matrix multiplies a vector, it projects that vector onto the space spanned by the columns of <span class="math inline">\(X_2\)</span>.</li>
<li><strong>Residuals:</strong> <span class="math inline">\(M_2\)</span> does the opposite. When <span class="math inline">\(M_2\)</span> multiplies a vector, it produces the <em>residuals</em> of a regression of that vector on <span class="math inline">\(X_2\)</span>. That is, <span class="math inline">\(M_2 y\)</span> gives the residuals of a regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(X_2\)</span>.</li>
<li><strong>Orthogonality:</strong> <span class="math inline">\(M_2\)</span> is symmetric (<span class="math inline">\(M_2 = M_2^T\)</span>) and idempotent (<span class="math inline">\(M_2 M_2 = M_2\)</span>). Furthermore, <span class="math inline">\(M_2 X_2 = 0\)</span>. This means that <span class="math inline">\(M_2\)</span> projects any vector onto the space <em>orthogonal</em> to the column space of <span class="math inline">\(X_2\)</span>.</li>
</ol>
<p>In the partitioned regression formula, <span class="math inline">\(M_2\)</span> is used to remove the effect of <span class="math inline">\(X_2\)</span> from both <span class="math inline">\(y\)</span> and <span class="math inline">\(X_1\)</span>. Specifically:</p>
<ul>
<li><span class="math inline">\(M_2 y\)</span>: Represents the part of <span class="math inline">\(y\)</span> that is <em>not</em> explained by <span class="math inline">\(X_2\)</span>.</li>
<li><span class="math inline">\(M_2 X_1\)</span>: Represents the part of <span class="math inline">\(X_1\)</span> that is <em>not</em> explained by <span class="math inline">\(X_2\)</span>.</li>
</ul>
<p>The formula then essentially regresses the unexplained part of <span class="math inline">\(y\)</span> on the unexplained part of <span class="math inline">\(X_1\)</span>. This isolates the effect of <span class="math inline">\(X_1\)</span> on <span class="math inline">\(y\)</span>, after controlling for the effects of <span class="math inline">\(X_2\)</span>. This is a key concept in multiple regression, showing how we can isolate the effect of one variable while controlling for the effects of others.</p>
</section>
<section class="level3" id="sec-ch19solution9">
<h3 class="anchored" data-anchor-id="sec-ch19solution9">Solution 9</h3>
<p><a href="#sec-ch19exercise9">Exercise 9</a></p>
<p>The OLS estimator is a linear combination of the dependent variable <span class="math inline">\(y\)</span>:</p>
<p><span class="math inline">\(\qquad \hat{\beta} = (X^{T}X)^{-1}X^{T}y\)</span>.</p>
<p>Under the assumption of normality of errors, we have:</p>
<p><span class="math inline">\(\qquad \epsilon \sim N(0, \sigma^{2}I_{n})\)</span>.</p>
<p>Since <span class="math inline">\(y = X\beta + \epsilon\)</span>, and <span class="math inline">\(X\beta\)</span> is a constant (conditional on <span class="math inline">\(X\)</span>), <span class="math inline">\(y\)</span> is also normally distributed:</p>
<p><span class="math inline">\(\qquad y|X \sim N(X\beta, \sigma^{2}I_{n})\)</span>.</p>
<p>Now, <span class="math inline">\(\hat{\beta}\)</span> is a linear combination of <span class="math inline">\(y\)</span>, and a linear combination of normally distributed variables is also normally distributed. Specifically, if <span class="math inline">\(Y \sim N(\mu, \Sigma)\)</span> and <span class="math inline">\(A\)</span> is a constant matrix, then <span class="math inline">\(AY \sim N(A\mu, A\Sigma A^{T})\)</span>. Applying this to the OLS estimator:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\hat{\beta}|X &amp;= (X^{T}X)^{-1}X^{T}y \\
&amp;\sim N((X^{T}X)^{-1}X^{T}X\beta, (X^{T}X)^{-1}X^{T}(\sigma^{2}I_{n})((X^{T}X)^{-1}X^{T})^{T}) \\
&amp;\sim N(\beta, \sigma^{2}(X^{T}X)^{-1}X^{T}X(X^{T}X)^{-T}) \\
&amp;\sim N(\beta, \sigma^{2}(X^{T}X)^{-1}).
\end{aligned}\)</span></p>
<p>Therefore, <span class="math inline">\(\hat{\beta}\)</span> follows a normal distribution because it is a linear transformation of <span class="math inline">\(y\)</span>, which is itself normally distributed due to the normality of the error terms.</p>
</section>
<section class="level3" id="sec-ch19solution10">
<h3 class="anchored" data-anchor-id="sec-ch19solution10">Solution 10</h3>
<p><a href="#sec-ch19exercise10">Exercise 10</a></p>
<p>The result <span class="math inline">\(\tau = (\hat{\beta} - \beta)^{T}V(X)^{-1}(\hat{\beta} - \beta) \sim \chi^{2}(K)\)</span> is significant because it provides a statistic that follows a known distribution (the chi-squared distribution with <span class="math inline">\(K\)</span> degrees of freedom) <em>and</em> does not depend on any unknown parameters (under the assumptions of the model). This allows us to perform hypothesis tests and construct confidence intervals for <span class="math inline">\(\beta\)</span>. Here’s how it is used:</p>
<ol type="1">
<li><p><strong>Hypothesis Testing:</strong> We can test hypotheses about the coefficients <span class="math inline">\(\beta\)</span>. For example, to test the null hypothesis <span class="math inline">\(H_0: R\beta = r\)</span>, where <span class="math inline">\(R\)</span> is a <span class="math inline">\(q \times K\)</span> matrix and <span class="math inline">\(r\)</span> is a <span class="math inline">\(q \times 1\)</span> vector, we can use the fact that, under <span class="math inline">\(H_0\)</span>:</p>
<p><span class="math inline">\(\qquad (R\hat{\beta} - r)^{T}(RV(X)R^{T})^{-1}(R\hat{\beta} - r) \sim \chi^{2}(q)\)</span>.</p>
<p>We can then compare the calculated value of this statistic to the critical value from the chi-squared distribution with <span class="math inline">\(q\)</span> degrees of freedom to determine whether to reject the null hypothesis. This generalizes the individual t-tests to allow for testing multiple linear restrictions on the coefficients simultaneously.</p></li>
<li><p><strong>Confidence Regions:</strong> While we usually construct confidence <em>intervals</em> for individual coefficients, the chi-squared result allows us to construct confidence <em>regions</em> (ellipsoids) for the entire vector <span class="math inline">\(\beta\)</span>. A <span class="math inline">\((1-\alpha)\)</span> confidence region for <span class="math inline">\(\beta\)</span> is the set of all <span class="math inline">\(\beta\)</span> values that satisfy:</p>
<p><span class="math inline">\(\qquad (\hat{\beta} - \beta)^{T}V(X)^{-1}(\hat{\beta} - \beta) \leq \chi_{K,1-\alpha}^{2}\)</span>,</p>
<p>where <span class="math inline">\(\chi_{K,1-\alpha}^{2}\)</span> is the <span class="math inline">\((1-\alpha)\)</span> quantile of the chi-squared distribution with <span class="math inline">\(K\)</span> degrees of freedom.</p></li>
</ol>
<p>In summary, this result provides a foundation for inference in the linear regression model, moving beyond simple t-tests for individual coefficients.</p>
</section>
<section class="level3" id="sec-ch19solution11">
<h3 class="anchored" data-anchor-id="sec-ch19solution11">Solution 11</h3>
<p><a href="#sec-ch19exercise11">Exercise 11</a></p>
<p>In Theorem 19.4, <span class="math inline">\(\hat{m}\)</span> and <span class="math inline">\(\hat{m}(x)\)</span> represent different, but related, quantities:</p>
<ul>
<li><strong><span class="math inline">\(\hat{m}\)</span>:</strong> This is an estimator of the <em>conditional mean vector</em> <span class="math inline">\(m = \mathbb{E}[y|X] = X\beta\)</span>. <span class="math inline">\(\hat{m}\)</span> is an <span class="math inline">\(n \times 1\)</span> vector, where each element <span class="math inline">\(\hat{m}_i\)</span> is an estimate of the expected value of <span class="math inline">\(y_i\)</span> <em>given the corresponding row of the design matrix</em>, <span class="math inline">\(x_i^T\)</span>. That is, <span class="math inline">\(\hat{m}_i = x_i^T \hat{\beta}\)</span>. So, <span class="math inline">\(\hat{m} = X\hat{\beta}\)</span>.</li>
<li><strong><span class="math inline">\(\hat{m}(x)\)</span>:</strong> This is an estimator of the <em>conditional mean function</em> evaluated at a <em>specific</em> point <span class="math inline">\(x\)</span>. <span class="math inline">\(x\)</span> is a <span class="math inline">\(K \times 1\)</span> vector of values for the independent variables. <span class="math inline">\(\hat{m}(x)\)</span> is a <em>scalar</em>, representing the estimated expected value of <span class="math inline">\(y\)</span> when the independent variables take on the values in the vector <span class="math inline">\(x\)</span>. That is, <span class="math inline">\(\hat{m}(x) = x^T \hat{\beta}\)</span>.</li>
</ul>
<p>In simpler terms:</p>
<ul>
<li><span class="math inline">\(\hat{m}\)</span> gives you the predicted values for <span class="math inline">\(y\)</span> for each observation in your <em>existing</em> dataset (<span class="math inline">\(X\)</span>).</li>
<li><span class="math inline">\(\hat{m}(x)\)</span> gives you the predicted value for <span class="math inline">\(y\)</span> for a <em>new</em> set of independent variable values, <span class="math inline">\(x\)</span> (which might not be in your original dataset).</li>
</ul>
<p>The theorem provides the properties (unbiasedness, variance, and distribution) of both of these estimators. They are related because you can think of <span class="math inline">\(\hat{m}\)</span> as being constructed by stacking up <span class="math inline">\(\hat{m}(x_i)\)</span> for each row <span class="math inline">\(x_i^T\)</span> of the design matrix <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="sec-ch19solution12">
<h3 class="anchored" data-anchor-id="sec-ch19solution12">Solution 12</h3>
<p><a href="#sec-ch19exercise12">Exercise 12</a></p>
<p>Under the assumption of normality, the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(X\)</span> is:</p>
<p><span class="math inline">\(\qquad f_{y|X}(y) = \dfrac{1}{(2\pi\sigma^{2})^{n/2}}\exp\left( -\dfrac{1}{2\sigma^{2}}(y - X\beta)^{T}(y - X\beta) \right)\)</span>.</p>
<p>The log-likelihood function is the logarithm of this density:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
l(\beta, \sigma^{2}|y, X) &amp;= \log\left(\dfrac{1}{(2\pi\sigma^{2})^{n/2}}\exp\left( -\dfrac{1}{2\sigma^{2}}(y - X\beta)^{T}(y - X\beta) \right)\right) \\
&amp;= -\dfrac{n}{2}\log(2\pi) - \dfrac{n}{2}\log(\sigma^{2}) - \dfrac{1}{2\sigma^{2}}(y - X\beta)^{T}(y - X\beta).
\end{aligned}\)</span></p>
<p>To find the MLE, we maximize this log-likelihood with respect to <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span>. We already know that the MLE for <span class="math inline">\(\beta\)</span> is the OLS estimator, <span class="math inline">\(\hat{\beta} = (X^TX)^{-1}X^Ty\)</span>. Now, we need to find the MLE for <span class="math inline">\(\sigma^2\)</span>. We take the derivative of the log-likelihood with respect to <span class="math inline">\(\sigma^2\)</span> and set it equal to zero:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\dfrac{\partial l}{\partial \sigma^{2}} &amp;= -\dfrac{n}{2\sigma^{2}} + \dfrac{1}{2(\sigma^{2})^{2}}(y - X\beta)^{T}(y - X\beta) = 0 \\
\dfrac{n}{2\sigma^{2}} &amp;= \dfrac{1}{2(\sigma^{2})^{2}}(y - X\beta)^{T}(y - X\beta) \\
n &amp;= \dfrac{1}{\sigma^{2}}(y - X\beta)^{T}(y - X\beta) \\
\hat{\sigma}_{mle}^{2} &amp;= \dfrac{1}{n}(y - X\hat{\beta})^{T}(y - X\hat{\beta}).
\end{aligned}\)</span></p>
<p>Substituting <span class="math inline">\(\hat{\beta}\)</span> for <span class="math inline">\(\beta\)</span> (since we’re finding the joint MLE), we get:</p>
<p><span class="math inline">\(\qquad \hat{\sigma}_{mle}^{2} = \dfrac{1}{n}(y - X\hat{\beta})^{T}(y - X\hat{\beta}) = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n}\)</span>.</p>
<p>This is the MLE of <span class="math inline">\(\sigma^2\)</span>. It is the sum of squared residuals divided by the number of observations, <span class="math inline">\(n\)</span>.</p>
</section>
<section class="level3" id="sec-ch19solution13">
<h3 class="anchored" data-anchor-id="sec-ch19solution13">Solution 13</h3>
<p><a href="#sec-ch19exercise13">Exercise 13</a></p>
<p>The key difference lies in the <strong>degrees of freedom</strong>.</p>
<ul>
<li><span class="math inline">\(\hat{\sigma}_{mle}^{2} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n}\)</span> divides the sum of squared residuals by <span class="math inline">\(n\)</span>, the number of observations. This estimator is the Maximum Likelihood Estimator (MLE) and is biased downwards. It tends to underestimate the true variance <span class="math inline">\(\sigma^2\)</span> because it doesn’t account for the fact that we’ve already used up some degrees of freedom in estimating the <span class="math inline">\(\beta\)</span> coefficients.</li>
<li><span class="math inline">\(s_{*}^{2} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n-K}\)</span> divides the sum of squared residuals by <span class="math inline">\(n-K\)</span>, where <span class="math inline">\(K\)</span> is the number of parameters in the <span class="math inline">\(\beta\)</span> vector (including the intercept). This is the unbiased estimator.</li>
</ul>
<p>Here’s why the <span class="math inline">\(n-K\)</span> correction makes <span class="math inline">\(s_{*}^{2}\)</span> unbiased:</p>
<ol type="1">
<li><p><strong>Degrees of Freedom Lost:</strong> When we estimate <span class="math inline">\(\beta\)</span> using OLS, we are essentially imposing <span class="math inline">\(K\)</span> constraints on our data (the normal equations). This means we lose <span class="math inline">\(K\)</span> degrees of freedom.</p></li>
<li><p><strong>Expected Value of SSR:</strong> The expected value of the sum of squared residuals (<span class="math inline">\(\hat{\epsilon}^{T}\hat{\epsilon}\)</span>) is equal to <span class="math inline">\((n-K)\sigma^2\)</span>. This is a standard result in linear regression (and is related to the proof of Theorem 19.5).</p></li>
<li><p><strong>Unbiasedness:</strong> To obtain an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, we need to divide the sum of squared residuals by a quantity that, when we take the expectation, will give us <span class="math inline">\(\sigma^2\)</span>. Since <span class="math inline">\(\mathbb{E}[\hat{\epsilon}^{T}\hat{\epsilon}] = (n-K)\sigma^2\)</span>, dividing by <span class="math inline">\((n-K)\)</span> achieves this:</p>
<p><span class="math inline">\(\qquad \mathbb{E}[s_{*}^{2}] = \mathbb{E}\left[\dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n-K}\right] = \dfrac{\mathbb{E}[\hat{\epsilon}^{T}\hat{\epsilon}]}{n-K} = \dfrac{(n-K)\sigma^{2}}{n-K} = \sigma^{2}\)</span>.</p></li>
</ol>
<p>In summary, <span class="math inline">\(s_{*}^{2}\)</span> corrects for the degrees of freedom lost in estimating the regression coefficients, leading to an unbiased estimate of the error variance. The MLE, <span class="math inline">\(\hat{\sigma}_{mle}^{2}\)</span>, does not make this correction and is therefore biased.</p>
</section>
<section class="level3" id="sec-ch19solution14">
<h3 class="anchored" data-anchor-id="sec-ch19solution14">Solution 14</h3>
<p><a href="#sec-ch19exercise14">Exercise 14</a></p>
<p><strong>Degrees of freedom</strong> in the context of estimating the error variance <span class="math inline">\(\sigma^2\)</span> refer to the number of independent pieces of information available to estimate the variance <em>after</em> accounting for the parameters that have already been estimated from the data.</p>
<p>Think of it this way:</p>
<ol type="1">
<li><strong>Initial Information:</strong> You start with <span class="math inline">\(n\)</span> observations, which represent <span class="math inline">\(n\)</span> independent pieces of information.</li>
<li><strong>Estimating <span class="math inline">\(\beta\)</span>:</strong> When you estimate the <span class="math inline">\(K\)</span> parameters in the <span class="math inline">\(\beta\)</span> vector (including the intercept) using OLS, you are essentially imposing <span class="math inline">\(K\)</span> constraints on your data. These constraints are derived from the normal equations, which ensure that the residuals are orthogonal to the predictors.</li>
<li><strong>Remaining Information:</strong> After estimating <span class="math inline">\(\beta\)</span>, you are left with <span class="math inline">\(n-K\)</span> independent pieces of information to estimate the variance. This is because <span class="math inline">\(K\)</span> degrees of freedom have been “used up” in estimating <span class="math inline">\(\beta\)</span>.</li>
</ol>
<p>Therefore, the degrees of freedom for estimating <span class="math inline">\(\sigma^2\)</span> are <span class="math inline">\(n-K\)</span>. This is why the unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, <span class="math inline">\(s_{*}^{2}\)</span>, divides the sum of squared residuals by <span class="math inline">\(n-K\)</span>, not <span class="math inline">\(n\)</span>. Dividing by <span class="math inline">\(n-K\)</span> correctly accounts for the number of independent pieces of information remaining to estimate the variance.</p>
</section>
<section class="level3" id="sec-ch19solution15">
<h3 class="anchored" data-anchor-id="sec-ch19solution15">Solution 15</h3>
<p><a href="#sec-ch19exercise15">Exercise 15</a></p>
<p>An estimator is considered the <strong>Best Linear Unbiased Estimator (BLUE)</strong> if it satisfies the following three properties:</p>
<ol type="1">
<li><strong>Linear:</strong> The estimator is a linear function of the observed dependent variable values (<span class="math inline">\(y\)</span>). This means it can be expressed in the form <span class="math inline">\(\tilde{\beta} = Ay\)</span>, where <span class="math inline">\(A\)</span> is a matrix that does not depend on <span class="math inline">\(y\)</span> (though it can depend on <span class="math inline">\(X\)</span>).</li>
<li><strong>Unbiased:</strong> The expected value of the estimator is equal to the true parameter value. That is, <span class="math inline">\(\mathbb{E}[\tilde{\beta}] = \beta\)</span> (unconditionally) or <span class="math inline">\(\mathbb{E}[\tilde{\beta}|X] = \beta\)</span> (conditionally).</li>
<li><strong>Best (Minimum Variance):</strong> Among all <em>linear unbiased</em> estimators, the estimator has the smallest variance. This means that for any other linear unbiased estimator <span class="math inline">\(\tilde{\beta}\)</span>, <span class="math inline">\(\text{Var}(\hat{\beta}) \leq \text{var}(\tilde{\beta})\)</span> in the matrix sense (i.e., <span class="math inline">\(\text{var}(\tilde{\beta}) - \text{var}(\hat{\beta})\)</span> is positive semi-definite). Equivalently, for any linear combination of the parameters, <span class="math inline">\(c^T\beta\)</span>, the variance of <span class="math inline">\(c^T\hat{\beta}\)</span> is less than or equal to the variance of <span class="math inline">\(c^T\tilde{\beta}\)</span>.</li>
</ol>
<p>In summary, BLUE means the estimator is a weighted average of the <span class="math inline">\(y\)</span> values, it’s correct on average, and it’s the most precise (smallest variance) among all such estimators. The Gauss-Markov theorem states that the OLS estimator is BLUE under certain assumptions.</p>
</section>
<section class="level3" id="sec-ch19solution16">
<h3 class="anchored" data-anchor-id="sec-ch19solution16">Solution 16</h3>
<p><a href="#sec-ch19exercise16">Exercise 16</a></p>
<p>The <strong>Gauss-Markov Theorem</strong> is a fundamental result in linear regression. It states that, under certain assumptions about the error terms in the linear regression model, the Ordinary Least Squares (OLS) estimator is the “best” we can do among all linear and unbiased estimators.</p>
<p>“Best” in this context means <em>minimum variance</em>. Imagine you have many different ways to estimate the coefficients of your regression model, but you restrict yourself to methods that are both linear (the estimate is a weighted average of the <span class="math inline">\(y\)</span> values) and unbiased (on average, the estimate equals the true value). The Gauss-Markov theorem tells you that, out of all these possible linear and unbiased estimation methods, the OLS estimator will give you estimates that are, on average, closest to the true values. The spread (variance) of the OLS estimates around the true values will be the smallest possible.</p>
<p>It’s important to remember that this “best” property only holds <em>if</em> the assumptions of the theorem are met. If those assumptions are violated (e.g., if the errors are heteroskedastic or autocorrelated), then OLS might not be the best estimator anymore, and other estimators (like Generalized Least Squares) might be better.</p>
</section>
<section class="level3" id="sec-ch19solution17">
<h3 class="anchored" data-anchor-id="sec-ch19solution17">Solution 17</h3>
<p><a href="#sec-ch19exercise17">Exercise 17</a></p>
<p>The key assumptions required for the Gauss-Markov Theorem to hold are related to the error term (<span class="math inline">\(\epsilon\)</span>) in the linear regression model <span class="math inline">\(y = X\beta + \epsilon\)</span>:</p>
<ol type="1">
<li><strong>Linearity:</strong> The model is linear in parameters. The dependent variable is a linear function of the independent variables and the error term.</li>
<li><strong>Strict Exogeneity:</strong> The expected value of the error term, conditional on the independent variables, is zero: <span class="math inline">\(\mathbb{E}[\epsilon|X] = 0\)</span>. This means that the independent variables are not correlated with the error term.</li>
<li><strong>Homoskedasticity:</strong> The variance of the error term is constant for all observations: <span class="math inline">\(\text{Var}(\epsilon_i|X) = \sigma^2\)</span> for all <span class="math inline">\(i\)</span>. This means the error term has the same variance for all values of the independent variables.</li>
<li><strong>No Autocorrelation:</strong> The error terms are uncorrelated with each other: <span class="math inline">\(\text{Cov}(\epsilon_i, \epsilon_j|X) = 0\)</span> for all <span class="math inline">\(i \neq j\)</span>. This means there is no systematic relationship between the error terms of different observations.</li>
<li><strong>Full Rank of X:</strong> The matrix <span class="math inline">\(X\)</span> has full column rank. This means that there is no perfect multicollinearity among the independent variables (none of the independent variables can be written as a perfect linear combination of the others).</li>
</ol>
<p>It’s important to note that the Gauss-Markov theorem <em>does not</em> require the error term to be normally distributed. Normality is needed for certain hypothesis tests and confidence intervals, but not for the BLUE property of OLS. If any of the assumptions 2, 3 or 4 are not met, the Gauss-Markov theorem no longer holds.</p>
</section>
<section class="level3" id="sec-ch19solution18">
<h3 class="anchored" data-anchor-id="sec-ch19solution18">Solution 18</h3>
<p><a href="#sec-ch19exercise18">Exercise 18</a></p>
<p>The <strong>Cramér-Rao Lower Bound (CRLB)</strong> provides a theoretical lower bound on the variance of <em>any</em> unbiased estimator. It states that the variance of any unbiased estimator <span class="math inline">\(\tilde{\theta}\)</span> of a parameter <span class="math inline">\(\theta\)</span> is always greater than or equal to the inverse of the Fisher information, <span class="math inline">\(I(\theta)\)</span>:</p>
<p><span class="math inline">\(\qquad \text{Var}(\tilde{\theta}) \geq I(\theta)^{-1}\)</span>.</p>
<p>The Fisher information, <span class="math inline">\(I(\theta)\)</span>, measures the amount of information that the data provides about the unknown parameter <span class="math inline">\(\theta\)</span>. It is defined as the variance of the score function (the derivative of the log-likelihood function with respect to <span class="math inline">\(\theta\)</span>):</p>
<p><span class="math inline">\(\qquad I(\theta) = \mathbb{E}\left[ \left( \dfrac{\partial l(\theta|y)}{\partial \theta} \right)^2 \right] = -\mathbb{E}\left[ \dfrac{\partial^2 l(\theta|y)}{\partial \theta^2} \right]\)</span>,</p>
<p>where <span class="math inline">\(l(\theta|y)\)</span> is the log-likelihood function.</p>
<p>The relevance to the OLS estimator is as follows:</p>
<ol type="1">
<li><strong>Best Unbiased:</strong> Under the assumption of normally distributed errors (Assumption A4), the OLS estimator achieves the Cramér-Rao Lower Bound. This means that the OLS estimator is not just the best <em>linear</em> unbiased estimator (as stated by the Gauss-Markov theorem); it is the best unbiased estimator <em>among all unbiased estimators</em>, including non-linear ones.</li>
<li><strong>Efficiency:</strong> An estimator that achieves the CRLB is said to be <em>efficient</em>. So, under normality, the OLS estimator is efficient.</li>
<li><strong>Comparison:</strong> The CRLB provides a benchmark against which to compare the performance of other estimators. If an estimator’s variance is close to the CRLB, it’s doing well.</li>
</ol>
<p>It’s crucial to remember that the CRLB applies to <em>unbiased</em> estimators. There may be biased estimators with lower Mean Squared Error (MSE) than the CRLB (since MSE considers both bias and variance).</p>
</section>
<section class="level3" id="sec-ch19solution19">
<h3 class="anchored" data-anchor-id="sec-ch19solution19">Solution 19</h3>
<p><a href="#sec-ch19exercise19">Exercise 19</a></p>
<p>The information matrix for <span class="math inline">\((\beta, \sigma^2)\)</span> in the standard linear regression model with normal errors is block diagonal:</p>
<p><span class="math inline">\(\qquad I(\beta, \sigma^{2}) = \begin{bmatrix} \frac{1}{\sigma^{2}}X^{T}X &amp; 0 \\ 0 &amp; \frac{n}{2\sigma^{4}} \end{bmatrix}\)</span>.</p>
<p>This block diagonality arises from the structure of the log-likelihood function under the assumption of normality and homoskedasticity. Recall the log-likelihood:</p>
<p><span class="math inline">\(\qquad l(\beta, \sigma^{2}|y, X) = -\dfrac{n}{2}\log(2\pi) - \dfrac{n}{2}\log(\sigma^{2}) - \dfrac{1}{2\sigma^{2}}(y - X\beta)^{T}(y - X\beta)\)</span>.</p>
<p>The key is to examine the second derivatives (Hessian matrix):</p>
<ol type="1">
<li><strong><span class="math inline">\(\dfrac{\partial^2 l}{\partial \beta \partial \beta^T}\)</span>:</strong> The second derivative with respect to <span class="math inline">\(\beta\)</span> only involves the last term of the log-likelihood. After taking the derivative twice and taking expectations, we get <span class="math inline">\(\frac{1}{\sigma^2}X^TX\)</span>.</li>
<li><strong><span class="math inline">\(\dfrac{\partial^2 l}{\partial (\sigma^2)^2}\)</span>:</strong> The second derivative with respect to <span class="math inline">\(\sigma^2\)</span> involves the second and third terms. After taking derivatives twice, and taking expectations (using the fact that <span class="math inline">\(\mathbb{E}[(y-X\beta)^T(y-X\beta)] = n\sigma^2\)</span>), we get <span class="math inline">\(\frac{n}{2\sigma^4}\)</span>.</li>
<li><strong><span class="math inline">\(\dfrac{\partial^2 l}{\partial \beta \partial \sigma^2}\)</span>:</strong> This is the crucial part. Taking the derivative first with respect to <span class="math inline">\(\beta\)</span> gives: <span class="math inline">\(\dfrac{1}{\sigma^2}X^T(y-X\beta)\)</span>. Then taking the derivative with respect to <span class="math inline">\(\sigma^2\)</span> gives: <span class="math inline">\(-\dfrac{1}{\sigma^4}X^T(y-X\beta)\)</span>. Taking the expectation, and using the fact that <span class="math inline">\(\mathbb{E}[y-X\beta|X] = 0\)</span>, this cross-partial derivative is <em>zero</em>.</li>
</ol>
<p>The fact that the cross-partial derivative is zero is what leads to the block diagonality. This means that the Fisher Information matrix is block diagonal.</p>
<p><strong>Implications of Block Diagonality:</strong></p>
<ol type="1">
<li><strong>Independence of MLEs (Asymptotically):</strong> The block diagonality implies that the Maximum Likelihood Estimators (MLEs) of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> are asymptotically independent. This means that knowing the true value of <span class="math inline">\(\sigma^2\)</span> would not help in estimating <span class="math inline">\(\beta\)</span>, and vice-versa (at least in large samples).</li>
<li><strong>Separate Estimation:</strong> We can estimate <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> separately without loss of information. The estimation of one parameter doesn’t affect the efficiency of the estimation of the other.</li>
<li><strong>Variance Calculation:</strong> The inverse of the block diagonal information matrix is also block diagonal, making the calculation of the variances of the MLEs straightforward. The variance of <span class="math inline">\(\hat{\beta}\)</span> depends only on <span class="math inline">\(X\)</span> and <span class="math inline">\(\sigma^2\)</span>, and the variance of <span class="math inline">\(\hat{\sigma}^2\)</span> depends only on <span class="math inline">\(n\)</span> and <span class="math inline">\(\sigma^2\)</span>.</li>
</ol>
</section>
<section class="level3" id="sec-ch19solution20">
<h3 class="anchored" data-anchor-id="sec-ch19solution20">Solution 20</h3>
<p><a href="#sec-ch19exercise20">Exercise 20</a></p>
<p>The OLS estimator is given by: <span class="math inline">\(\hat{\beta} = (X^TX)^{-1}X^Ty\)</span>. We want to find the variance of <span class="math inline">\(\hat{\beta}\)</span> conditional on <span class="math inline">\(X\)</span>, given that <span class="math inline">\(\text{Var}(\epsilon|X) = \Sigma(X)\)</span>, where <span class="math inline">\(\Sigma(X)\)</span> is a diagonal matrix with elements <span class="math inline">\(\sigma_i^2\)</span>.</p>
<p>First, substitute the model equation into the estimator:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\hat{\beta} &amp;= (X^TX)^{-1}X^T(X\beta + \epsilon) \\
&amp;= (X^TX)^{-1}X^TX\beta + (X^TX)^{-1}X^T\epsilon \\
&amp;= \beta + (X^TX)^{-1}X^T\epsilon.
\end{aligned}\)</span></p>
<p>Now, subtract <span class="math inline">\(\beta\)</span> from both sides:</p>
<p><span class="math inline">\(\qquad \hat{\beta} - \beta = (X^TX)^{-1}X^T\epsilon\)</span>.</p>
<p>To find the variance, we use the definition of the variance-covariance matrix:</p>
<p><span class="math inline">\(\qquad \text{Var}(\hat{\beta}|X) = \mathbb{E}[(\hat{\beta}-\mathbb{E}[\hat{\beta}|X])(\hat{\beta}-\mathbb{E}[\hat{\beta}|X])^T|X]\)</span>.</p>
<p>Since <span class="math inline">\(\mathbb{E}[\hat{\beta}|X]=\beta\)</span>:</p>
<p><span class="math inline">\(\qquad
\begin{aligned}
\text{Var}(\hat{\beta}|X) &amp;= \mathbb{E}[(\hat{\beta} - \beta)(\hat{\beta} - \beta)^T|X] \\
&amp;= \mathbb{E}[((X^TX)^{-1}X^T\epsilon)((X^TX)^{-1}X^T\epsilon)^T|X] \\
&amp;= \mathbb{E}[(X^TX)^{-1}X^T\epsilon\epsilon^TX(X^TX)^{-1}|X] \\
&amp;= (X^TX)^{-1}X^T\mathbb{E}[\epsilon\epsilon^T|X]X(X^TX)^{-1}.
\end{aligned}\)</span></p>
<p>We are given that <span class="math inline">\(\text{Var}(\epsilon|X) = \Sigma(X) = \mathbb{E}[\epsilon\epsilon^T|X]\)</span>. Therefore:</p>
<p><span class="math inline">\(\qquad \text{Var}(\hat{\beta}|X) = (X^TX)^{-1}X^T\Sigma(X)X(X^TX)^{-1}\)</span>.</p>
<p>This is the formula provided in Theorem 19.2, showing that the variance of the OLS estimator under heteroskedasticity depends on the specific form of the heteroskedasticity, <span class="math inline">\(\Sigma(X)\)</span>. This demonstrates that the derivation in Theorem 19.2 holds generally for any covariance structure of the errors, not just homoskedastic errors.</p>
</section>
</section>
<section class="level2" id="r-scripts">
<h2 class="anchored" data-anchor-id="r-scripts">R Scripts</h2>
<section class="level3" id="r-script-1-demonstrating-unbiasedness-of-ols">
<h3 class="anchored" data-anchor-id="r-script-1-demonstrating-unbiasedness-of-ols">R Script 1: Demonstrating Unbiasedness of OLS</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="co"># Set the seed for reproducibility</span></span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb3-3"><a aria-hidden="true" href="#cb3-3" tabindex="-1"></a></span>
<span id="cb3-4"><a aria-hidden="true" href="#cb3-4" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb3-5"><a aria-hidden="true" href="#cb3-5" tabindex="-1"></a>n_simulations <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb3-6"><a aria-hidden="true" href="#cb3-6" tabindex="-1"></a>sample_size <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb3-7"><a aria-hidden="true" href="#cb3-7" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="fl">0.5</span>) <span class="co"># True intercept and slope</span></span>
<span id="cb3-8"><a aria-hidden="true" href="#cb3-8" tabindex="-1"></a>sigma_true <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co"># True error standard deviation</span></span>
<span id="cb3-9"><a aria-hidden="true" href="#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a aria-hidden="true" href="#cb3-10" tabindex="-1"></a><span class="co"># Create an empty data frame to store the results</span></span>
<span id="cb3-11"><a aria-hidden="true" href="#cb3-11" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>()</span>
<span id="cb3-12"><a aria-hidden="true" href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a aria-hidden="true" href="#cb3-13" tabindex="-1"></a><span class="co"># Run simulations</span></span>
<span id="cb3-14"><a aria-hidden="true" href="#cb3-14" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_simulations) {</span>
<span id="cb3-15"><a aria-hidden="true" href="#cb3-15" tabindex="-1"></a>  <span class="co"># Generate independent variable x</span></span>
<span id="cb3-16"><a aria-hidden="true" href="#cb3-16" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">runif</span>(sample_size, <span class="at">min =</span> <span class="sc">-</span><span class="dv">5</span>, <span class="at">max =</span> <span class="dv">5</span>)</span>
<span id="cb3-17"><a aria-hidden="true" href="#cb3-17" tabindex="-1"></a>  X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x) <span class="co"># Design matrix with intercept</span></span>
<span id="cb3-18"><a aria-hidden="true" href="#cb3-18" tabindex="-1"></a></span>
<span id="cb3-19"><a aria-hidden="true" href="#cb3-19" tabindex="-1"></a>  <span class="co"># Generate errors from a normal distribution</span></span>
<span id="cb3-20"><a aria-hidden="true" href="#cb3-20" tabindex="-1"></a>  epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(sample_size, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_true)</span>
<span id="cb3-21"><a aria-hidden="true" href="#cb3-21" tabindex="-1"></a></span>
<span id="cb3-22"><a aria-hidden="true" href="#cb3-22" tabindex="-1"></a>  <span class="co"># Generate dependent variable y</span></span>
<span id="cb3-23"><a aria-hidden="true" href="#cb3-23" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> epsilon</span>
<span id="cb3-24"><a aria-hidden="true" href="#cb3-24" tabindex="-1"></a></span>
<span id="cb3-25"><a aria-hidden="true" href="#cb3-25" tabindex="-1"></a>  <span class="co"># Calculate OLS estimates</span></span>
<span id="cb3-26"><a aria-hidden="true" href="#cb3-26" tabindex="-1"></a>  beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb3-27"><a aria-hidden="true" href="#cb3-27" tabindex="-1"></a></span>
<span id="cb3-28"><a aria-hidden="true" href="#cb3-28" tabindex="-1"></a>  <span class="co"># Store the results.  Use `rbind` with a named list for</span></span>
<span id="cb3-29"><a aria-hidden="true" href="#cb3-29" tabindex="-1"></a>  <span class="co"># easier column management.</span></span>
<span id="cb3-30"><a aria-hidden="true" href="#cb3-30" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, </span>
<span id="cb3-31"><a aria-hidden="true" href="#cb3-31" tabindex="-1"></a>                   <span class="fu">data.frame</span>(<span class="at">simulation =</span> i,</span>
<span id="cb3-32"><a aria-hidden="true" href="#cb3-32" tabindex="-1"></a>                              <span class="at">beta0_hat =</span> beta_hat[<span class="dv">1</span>, <span class="dv">1</span>],</span>
<span id="cb3-33"><a aria-hidden="true" href="#cb3-33" tabindex="-1"></a>                              <span class="at">beta1_hat =</span> beta_hat[<span class="dv">2</span>, <span class="dv">1</span>]))</span>
<span id="cb3-34"><a aria-hidden="true" href="#cb3-34" tabindex="-1"></a>}</span>
<span id="cb3-35"><a aria-hidden="true" href="#cb3-35" tabindex="-1"></a></span>
<span id="cb3-36"><a aria-hidden="true" href="#cb3-36" tabindex="-1"></a><span class="co"># Calculate the average of the estimated coefficients</span></span>
<span id="cb3-37"><a aria-hidden="true" href="#cb3-37" tabindex="-1"></a>average_beta_hat <span class="ot">&lt;-</span> results <span class="sc">%&gt;%</span></span>
<span id="cb3-38"><a aria-hidden="true" href="#cb3-38" tabindex="-1"></a>  <span class="fu">summarize</span>(<span class="at">avg_beta0_hat =</span> <span class="fu">mean</span>(beta0_hat),</span>
<span id="cb3-39"><a aria-hidden="true" href="#cb3-39" tabindex="-1"></a>            <span class="at">avg_beta1_hat =</span> <span class="fu">mean</span>(beta1_hat))</span>
<span id="cb3-40"><a aria-hidden="true" href="#cb3-40" tabindex="-1"></a></span>
<span id="cb3-41"><a aria-hidden="true" href="#cb3-41" tabindex="-1"></a><span class="co"># Print the results</span></span>
<span id="cb3-42"><a aria-hidden="true" href="#cb3-42" tabindex="-1"></a><span class="fu">print</span>(average_beta_hat)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>  avg_beta0_hat avg_beta1_hat
1      2.006845     0.4987579</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a aria-hidden="true" href="#cb5-1" tabindex="-1"></a><span class="co"># Visualize the distribution of the estimated coefficients</span></span>
<span id="cb5-2"><a aria-hidden="true" href="#cb5-2" tabindex="-1"></a>results <span class="sc">%&gt;%</span></span>
<span id="cb5-3"><a aria-hidden="true" href="#cb5-3" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(beta0_hat, beta1_hat),</span>
<span id="cb5-4"><a aria-hidden="true" href="#cb5-4" tabindex="-1"></a>               <span class="at">names_to =</span> <span class="st">"parameter"</span>,</span>
<span id="cb5-5"><a aria-hidden="true" href="#cb5-5" tabindex="-1"></a>               <span class="at">values_to =</span> <span class="st">"estimate"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb5-6"><a aria-hidden="true" href="#cb5-6" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> estimate)) <span class="sc">+</span></span>
<span id="cb5-7"><a aria-hidden="true" href="#cb5-7" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="at">bins =</span> <span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb5-8"><a aria-hidden="true" href="#cb5-8" tabindex="-1"></a>  <span class="fu">facet_wrap</span>(<span class="sc">~</span>parameter) <span class="sc">+</span></span>
<span id="cb5-9"><a aria-hidden="true" href="#cb5-9" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">parameter =</span> <span class="fu">c</span>(<span class="st">"beta0_hat"</span>, <span class="st">"beta1_hat"</span>),</span>
<span id="cb5-10"><a aria-hidden="true" href="#cb5-10" tabindex="-1"></a>                              <span class="at">true_value =</span> beta_true),</span>
<span id="cb5-11"><a aria-hidden="true" href="#cb5-11" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">xintercept =</span> true_value), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb5-12"><a aria-hidden="true" href="#cb5-12" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Distribution of OLS Estimates"</span>,</span>
<span id="cb5-13"><a aria-hidden="true" href="#cb5-13" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Estimated Value"</span>,</span>
<span id="cb5-14"><a aria-hidden="true" href="#cb5-14" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Frequency"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap19_files/figure-html/unnamed-chunk-1-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong>
<ul>
<li>We load the <code>tidyverse</code> library for data manipulation and visualization.</li>
<li><code>set.seed(123)</code> ensures reproducibility of the random number generation.</li>
<li>We define simulation parameters: <code>n_simulations</code> (number of times we repeat the experiment), <code>sample_size</code> (number of observations in each sample), <code>beta_true</code> (the true values of the intercept and slope), and <code>sigma_true</code> (the true standard deviation of the error term).</li>
</ul></li>
<li><strong>Simulation Loop:</strong>
<ul>
<li>We loop <code>n_simulations</code> times, generating a new dataset and calculating OLS estimates in each iteration.</li>
<li><code>x &lt;- runif(sample_size, min = -5, max = 5)</code>: We generate the independent variable <span class="math inline">\(x\)</span> from a uniform distribution.</li>
<li><code>X &lt;- cbind(1, x)</code>: We create the design matrix <span class="math inline">\(X\)</span>, including a column of ones for the intercept.</li>
<li><code>epsilon &lt;- rnorm(sample_size, mean = 0, sd = sigma_true)</code>: We generate the error terms <span class="math inline">\(\epsilon\)</span> from a normal distribution with mean 0 and standard deviation <code>sigma_true</code>. This satisfies Assumption A3 (normality) and, consequently, Assumptions A1 and A2.</li>
<li><code>y &lt;- X %*% beta_true + epsilon</code>: We generate the dependent variable <span class="math inline">\(y\)</span> according to the linear model <span class="math inline">\(y = X\beta + \epsilon\)</span>.</li>
<li><code>beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y</code>: This is the core OLS calculation. It implements the formula <span class="math inline">\(\hat{\beta} = (X^{T}X)^{-1}X^{T}y\)</span>. <code>solve(t(X) %*% X)</code> calculates <span class="math inline">\((X^{T}X)^{-1}\)</span>, and then we multiply by <span class="math inline">\(X^{T}y\)</span>.</li>
<li>The estimated intercept (<code>beta_hat[1, 1]</code>) and slope (<code>beta_hat[2, 1]</code>) are stored in the <code>results</code> data frame.</li>
</ul></li>
<li><strong>Analysis:</strong>
<ul>
<li><code>average_beta_hat &lt;- ...</code>: After the loop, we calculate the average of the estimated coefficients (<code>beta0_hat</code> and <code>beta1_hat</code>) across all simulations.</li>
<li><code>print(average_beta_hat)</code>: We print the average estimated coefficients. These should be close to the true values (2 and 0.5).</li>
</ul></li>
<li><strong>Visualization:</strong>
<ul>
<li>The code creates a histogram of the estimated coefficients for both <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</li>
<li><code>geom_vline(...)</code>: Red vertical lines are added to the histograms, indicating the true values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</li>
</ul></li>
</ol>
<p><strong>Connection to Concepts:</strong></p>
<ul>
<li><strong>Unbiasedness (Theorem 19.1):</strong> The script demonstrates the unbiasedness of the OLS estimator. The average of the estimated coefficients across many simulations converges to the true parameter values. This illustrates that <span class="math inline">\(\mathbb{E}[\hat{\beta}] = \beta\)</span>.</li>
<li><strong>Linear Estimator (Definition 19.1):</strong> The line <code>beta_hat &lt;- solve(t(X) %*% X) %*% t(X) %*% y</code> shows that the OLS estimator is a linear function of <span class="math inline">\(y\)</span> (it can be written as <span class="math inline">\(Cy\)</span>, where <span class="math inline">\(C = (X^{T}X)^{-1}X^{T}\)</span>).</li>
</ul>
</section>
<section class="level3" id="r-script-2-conditional-variance-of-ols">
<h3 class="anchored" data-anchor-id="r-script-2-conditional-variance-of-ols">R Script 2: Conditional Variance of OLS</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb6-1"><a aria-hidden="true" href="#cb6-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb6-2"><a aria-hidden="true" href="#cb6-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb6-3"><a aria-hidden="true" href="#cb6-3" tabindex="-1"></a></span>
<span id="cb6-4"><a aria-hidden="true" href="#cb6-4" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb6-5"><a aria-hidden="true" href="#cb6-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># Sample size</span></span>
<span id="cb6-6"><a aria-hidden="true" href="#cb6-6" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>)  <span class="co"># True intercept and slope</span></span>
<span id="cb6-7"><a aria-hidden="true" href="#cb6-7" tabindex="-1"></a>sigma_true <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="co"># True error standard deviation</span></span>
<span id="cb6-8"><a aria-hidden="true" href="#cb6-8" tabindex="-1"></a></span>
<span id="cb6-9"><a aria-hidden="true" href="#cb6-9" tabindex="-1"></a><span class="co"># Generate a fixed X (for conditional variance)</span></span>
<span id="cb6-10"><a aria-hidden="true" href="#cb6-10" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="at">min =</span> <span class="sc">-</span><span class="dv">2</span>, <span class="at">max =</span> <span class="dv">2</span>)</span>
<span id="cb6-11"><a aria-hidden="true" href="#cb6-11" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb6-12"><a aria-hidden="true" href="#cb6-12" tabindex="-1"></a></span>
<span id="cb6-13"><a aria-hidden="true" href="#cb6-13" tabindex="-1"></a><span class="co"># Number of simulations for estimating the *conditional* variance</span></span>
<span id="cb6-14"><a aria-hidden="true" href="#cb6-14" tabindex="-1"></a>n_sim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb6-15"><a aria-hidden="true" href="#cb6-15" tabindex="-1"></a>beta_hats <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n_sim, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb6-16"><a aria-hidden="true" href="#cb6-16" tabindex="-1"></a></span>
<span id="cb6-17"><a aria-hidden="true" href="#cb6-17" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sim) {</span>
<span id="cb6-18"><a aria-hidden="true" href="#cb6-18" tabindex="-1"></a>    <span class="co"># Generate errors (note: X is fixed, only epsilon changes)</span></span>
<span id="cb6-19"><a aria-hidden="true" href="#cb6-19" tabindex="-1"></a>    epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_true)</span>
<span id="cb6-20"><a aria-hidden="true" href="#cb6-20" tabindex="-1"></a></span>
<span id="cb6-21"><a aria-hidden="true" href="#cb6-21" tabindex="-1"></a>    <span class="co"># Generate y</span></span>
<span id="cb6-22"><a aria-hidden="true" href="#cb6-22" tabindex="-1"></a>    y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> epsilon</span>
<span id="cb6-23"><a aria-hidden="true" href="#cb6-23" tabindex="-1"></a></span>
<span id="cb6-24"><a aria-hidden="true" href="#cb6-24" tabindex="-1"></a>    <span class="co"># Calculate OLS estimates</span></span>
<span id="cb6-25"><a aria-hidden="true" href="#cb6-25" tabindex="-1"></a>    beta_hats[i, ] <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb6-26"><a aria-hidden="true" href="#cb6-26" tabindex="-1"></a>}</span>
<span id="cb6-27"><a aria-hidden="true" href="#cb6-27" tabindex="-1"></a></span>
<span id="cb6-28"><a aria-hidden="true" href="#cb6-28" tabindex="-1"></a><span class="co"># Calculate the *empirical* conditional variance-covariance matrix</span></span>
<span id="cb6-29"><a aria-hidden="true" href="#cb6-29" tabindex="-1"></a>empirical_conditional_var <span class="ot">&lt;-</span> <span class="fu">var</span>(beta_hats)</span>
<span id="cb6-30"><a aria-hidden="true" href="#cb6-30" tabindex="-1"></a></span>
<span id="cb6-31"><a aria-hidden="true" href="#cb6-31" tabindex="-1"></a><span class="co"># Calculate the *theoretical* conditional variance-covariance matrix</span></span>
<span id="cb6-32"><a aria-hidden="true" href="#cb6-32" tabindex="-1"></a>theoretical_conditional_var <span class="ot">&lt;-</span> sigma_true<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X)</span>
<span id="cb6-33"><a aria-hidden="true" href="#cb6-33" tabindex="-1"></a></span>
<span id="cb6-34"><a aria-hidden="true" href="#cb6-34" tabindex="-1"></a><span class="co"># Print both</span></span>
<span id="cb6-35"><a aria-hidden="true" href="#cb6-35" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Empirical Conditional Variance-Covariance Matrix:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Empirical Conditional Variance-Covariance Matrix:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb8-1"><a aria-hidden="true" href="#cb8-1" tabindex="-1"></a><span class="fu">print</span>(empirical_conditional_var)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>             [,1]         [,2]
[1,]  0.091842070 -0.008237466
[2,] -0.008237466  0.074014967</code></pre>
</div>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb10-1"><a aria-hidden="true" href="#cb10-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Theoretical Conditional Variance-Covariance Matrix:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Theoretical Conditional Variance-Covariance Matrix:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a aria-hidden="true" href="#cb12-1" tabindex="-1"></a><span class="fu">print</span>(theoretical_conditional_var)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>                        x
   0.09159406 -0.01062313
x -0.01062313  0.07079451</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a aria-hidden="true" href="#cb14-1" tabindex="-1"></a><span class="co"># Visualize the sampling distribution</span></span>
<span id="cb14-2"><a aria-hidden="true" href="#cb14-2" tabindex="-1"></a>beta_hats_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(beta_hats)</span>
<span id="cb14-3"><a aria-hidden="true" href="#cb14-3" tabindex="-1"></a><span class="fu">colnames</span>(beta_hats_df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"beta0_hat"</span>, <span class="st">"beta1_hat"</span>)</span>
<span id="cb14-4"><a aria-hidden="true" href="#cb14-4" tabindex="-1"></a></span>
<span id="cb14-5"><a aria-hidden="true" href="#cb14-5" tabindex="-1"></a>beta_hats_df <span class="sc">%&gt;%</span></span>
<span id="cb14-6"><a aria-hidden="true" href="#cb14-6" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> beta0_hat, <span class="at">y =</span> beta1_hat)) <span class="sc">+</span></span>
<span id="cb14-7"><a aria-hidden="true" href="#cb14-7" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.2</span>) <span class="sc">+</span></span>
<span id="cb14-8"><a aria-hidden="true" href="#cb14-8" tabindex="-1"></a>    <span class="fu">geom_vline</span>(<span class="at">xintercept=</span>beta_true[<span class="dv">1</span>], <span class="at">color=</span><span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb14-9"><a aria-hidden="true" href="#cb14-9" tabindex="-1"></a>    <span class="fu">geom_hline</span>(<span class="at">yintercept=</span>beta_true[<span class="dv">2</span>], <span class="at">color=</span><span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb14-10"><a aria-hidden="true" href="#cb14-10" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Sampling Distribution of OLS Estimator (Conditional on X)"</span>,</span>
<span id="cb14-11"><a aria-hidden="true" href="#cb14-11" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"beta0_hat"</span>,</span>
<span id="cb14-12"><a aria-hidden="true" href="#cb14-12" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"beta1_hat"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap19_files/figure-html/unnamed-chunk-2-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong>
<ul>
<li>We set the seed for reproducibility.</li>
<li>We define <code>n</code> (sample size), <code>beta_true</code>, and <code>sigma_true</code>.</li>
<li>Crucially, we generate <code>x</code> <em>once</em> and keep it fixed throughout the simulations. This is because we are interested in the <em>conditional</em> variance, given a specific <span class="math inline">\(X\)</span>.</li>
</ul></li>
<li><strong>Simulation Loop:</strong>
<ul>
<li>We loop <code>n_sim</code> times. In each iteration:
<ul>
<li>We generate new error terms <code>epsilon</code> from a normal distribution.</li>
<li>We generate <code>y</code> using the <em>fixed</em> <code>X</code> and the new <code>epsilon</code>.</li>
<li>We calculate the OLS estimates <code>beta_hat</code> and store them.</li>
</ul></li>
</ul></li>
<li><strong>Variance Calculation:</strong>
<ul>
<li><code>empirical_conditional_var &lt;- var(beta_hats)</code>: We calculate the <em>sample</em> variance-covariance matrix of the <code>beta_hats</code> obtained from the simulations. This is our empirical estimate of <span class="math inline">\(\text{Var}(\hat{\beta}|X)\)</span>.</li>
<li><code>theoretical_conditional_var &lt;- sigma_true^2 * solve(t(X) %*% X)</code>: We calculate the <em>theoretical</em> conditional variance-covariance matrix using the formula <span class="math inline">\(\sigma^2(X^TX)^{-1}\)</span>, which holds under homoskedasticity.</li>
</ul></li>
<li><strong>Output and Visualization:</strong>
<ul>
<li>We print both the empirical and theoretical variance-covariance matrices. They should be close to each other.</li>
<li>We generate a scatterplot of the simulated <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> values. The spread of these points visually represents the conditional variance.</li>
</ul></li>
</ol>
<p><strong>Connection to Concepts:</strong></p>
<ul>
<li><strong>Conditional Variance (Theorem 19.2):</strong> The script directly demonstrates the concept of the conditional variance of the OLS estimator, <span class="math inline">\(\text{Var}(\hat{\beta}|X)\)</span>. We see how the estimator varies <em>for a fixed <span class="math inline">\(X\)</span></em>, and we compare the empirical variance to the theoretical formula <span class="math inline">\(\sigma^2(X^TX)^{-1}\)</span>.</li>
<li><strong>Homoskedasticity:</strong> The simulation assumes homoskedasticity (constant error variance), which is why we can use the simplified formula for the conditional variance.</li>
</ul>
</section>
<section class="level3" id="r-script-3-demonstrating-the-chi-squared-distribution-of-the-wald-statistic">
<h3 class="anchored" data-anchor-id="r-script-3-demonstrating-the-chi-squared-distribution-of-the-wald-statistic">R Script 3: Demonstrating the Chi-Squared Distribution of the Wald Statistic</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a aria-hidden="true" href="#cb15-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb15-2"><a aria-hidden="true" href="#cb15-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb15-3"><a aria-hidden="true" href="#cb15-3" tabindex="-1"></a></span>
<span id="cb15-4"><a aria-hidden="true" href="#cb15-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb15-5"><a aria-hidden="true" href="#cb15-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb15-6"><a aria-hidden="true" href="#cb15-6" tabindex="-1"></a></span>
<span id="cb15-7"><a aria-hidden="true" href="#cb15-7" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb15-8"><a aria-hidden="true" href="#cb15-8" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># Sample size</span></span>
<span id="cb15-9"><a aria-hidden="true" href="#cb15-9" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="sc">-</span><span class="dv">1</span>)  <span class="co"># True intercept and slopes</span></span>
<span id="cb15-10"><a aria-hidden="true" href="#cb15-10" tabindex="-1"></a>sigma_true <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co"># True error standard deviation</span></span>
<span id="cb15-11"><a aria-hidden="true" href="#cb15-11" tabindex="-1"></a></span>
<span id="cb15-12"><a aria-hidden="true" href="#cb15-12" tabindex="-1"></a><span class="co"># Generate X</span></span>
<span id="cb15-13"><a aria-hidden="true" href="#cb15-13" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="at">min =</span> <span class="sc">-</span><span class="dv">2</span>, <span class="at">max =</span> <span class="dv">2</span>)</span>
<span id="cb15-14"><a aria-hidden="true" href="#cb15-14" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> <span class="dv">1</span>)</span>
<span id="cb15-15"><a aria-hidden="true" href="#cb15-15" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x1, x2)</span>
<span id="cb15-16"><a aria-hidden="true" href="#cb15-16" tabindex="-1"></a>K <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X) <span class="co"># Number of parameters</span></span>
<span id="cb15-17"><a aria-hidden="true" href="#cb15-17" tabindex="-1"></a></span>
<span id="cb15-18"><a aria-hidden="true" href="#cb15-18" tabindex="-1"></a><span class="co"># Number of simulations</span></span>
<span id="cb15-19"><a aria-hidden="true" href="#cb15-19" tabindex="-1"></a>n_sim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb15-20"><a aria-hidden="true" href="#cb15-20" tabindex="-1"></a>wald_stats <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_sim)</span>
<span id="cb15-21"><a aria-hidden="true" href="#cb15-21" tabindex="-1"></a></span>
<span id="cb15-22"><a aria-hidden="true" href="#cb15-22" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sim) {</span>
<span id="cb15-23"><a aria-hidden="true" href="#cb15-23" tabindex="-1"></a>  <span class="co"># Generate errors</span></span>
<span id="cb15-24"><a aria-hidden="true" href="#cb15-24" tabindex="-1"></a>  epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> <span class="dv">0</span>, <span class="at">sd =</span> sigma_true)</span>
<span id="cb15-25"><a aria-hidden="true" href="#cb15-25" tabindex="-1"></a></span>
<span id="cb15-26"><a aria-hidden="true" href="#cb15-26" tabindex="-1"></a>  <span class="co"># Generate y</span></span>
<span id="cb15-27"><a aria-hidden="true" href="#cb15-27" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> epsilon</span>
<span id="cb15-28"><a aria-hidden="true" href="#cb15-28" tabindex="-1"></a></span>
<span id="cb15-29"><a aria-hidden="true" href="#cb15-29" tabindex="-1"></a>  <span class="co"># Calculate OLS estimates</span></span>
<span id="cb15-30"><a aria-hidden="true" href="#cb15-30" tabindex="-1"></a>  beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb15-31"><a aria-hidden="true" href="#cb15-31" tabindex="-1"></a></span>
<span id="cb15-32"><a aria-hidden="true" href="#cb15-32" tabindex="-1"></a>  <span class="co"># Calculate estimated variance-covariance matrix</span></span>
<span id="cb15-33"><a aria-hidden="true" href="#cb15-33" tabindex="-1"></a>  V_hat <span class="ot">&lt;-</span> sigma_true<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="co"># Use true sigma for simplicity</span></span>
<span id="cb15-34"><a aria-hidden="true" href="#cb15-34" tabindex="-1"></a>  <span class="co">#  In practice, use:  s2 &lt;- sum((y - X%*%beta_hat)^2) / (n-K)</span></span>
<span id="cb15-35"><a aria-hidden="true" href="#cb15-35" tabindex="-1"></a>  <span class="co">#                      V_hat  &lt;- s2 * solve(t(X)%*%X)</span></span>
<span id="cb15-36"><a aria-hidden="true" href="#cb15-36" tabindex="-1"></a></span>
<span id="cb15-37"><a aria-hidden="true" href="#cb15-37" tabindex="-1"></a>  <span class="co"># Calculate the Wald statistic (testing H0: beta = beta_true)</span></span>
<span id="cb15-38"><a aria-hidden="true" href="#cb15-38" tabindex="-1"></a>  wald_stats[i] <span class="ot">&lt;-</span> <span class="fu">t</span>(beta_hat <span class="sc">-</span> beta_true) <span class="sc">%*%</span> <span class="fu">solve</span>(V_hat) <span class="sc">%*%</span> (beta_hat <span class="sc">-</span> beta_true)</span>
<span id="cb15-39"><a aria-hidden="true" href="#cb15-39" tabindex="-1"></a>}</span>
<span id="cb15-40"><a aria-hidden="true" href="#cb15-40" tabindex="-1"></a></span>
<span id="cb15-41"><a aria-hidden="true" href="#cb15-41" tabindex="-1"></a><span class="co"># Visualize the distribution of the Wald statistic</span></span>
<span id="cb15-42"><a aria-hidden="true" href="#cb15-42" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">wald_stat =</span> wald_stats) <span class="sc">%&gt;%</span></span>
<span id="cb15-43"><a aria-hidden="true" href="#cb15-43" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> wald_stat)) <span class="sc">+</span></span>
<span id="cb15-44"><a aria-hidden="true" href="#cb15-44" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y =</span> <span class="fu">after_stat</span>(density)), <span class="at">bins =</span> <span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb15-45"><a aria-hidden="true" href="#cb15-45" tabindex="-1"></a>  <span class="fu">stat_function</span>(<span class="at">fun =</span> dchisq, <span class="at">args =</span> <span class="fu">list</span>(<span class="at">df =</span> K), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb15-46"><a aria-hidden="true" href="#cb15-46" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Distribution of the Wald Statistic"</span>,</span>
<span id="cb15-47"><a aria-hidden="true" href="#cb15-47" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Wald Statistic"</span>,</span>
<span id="cb15-48"><a aria-hidden="true" href="#cb15-48" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb15-49"><a aria-hidden="true" href="#cb15-49" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>,<span class="dv">15</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 2 rows containing missing values or values outside the scale range
(`geom_bar()`).</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap19_files/figure-html/unnamed-chunk-3-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong>
<ul>
<li>Set seed, define <code>n</code>, <code>beta_true</code>, and <code>sigma_true</code>.</li>
<li>Generate the independent variables <code>x1</code> and <code>x2</code> and create the design matrix <code>X</code>.</li>
<li><code>K &lt;- ncol(X)</code> determines the number of parameters.</li>
</ul></li>
<li><strong>Simulation Loop:</strong>
<ul>
<li>Loop <code>n_sim</code> times.</li>
<li>Generate errors and <code>y</code> in each iteration.</li>
<li>Calculate OLS estimates <code>beta_hat</code>.</li>
<li><code>V_hat &lt;- sigma_true^2 * solve(t(X) %*% X)</code>: Calculate the <em>estimated</em> variance-covariance matrix. For simplicity, we use the <em>true</em> <span class="math inline">\(\sigma^2\)</span> here. In practice, you would use the estimated error variance (<span class="math inline">\(s^2\)</span>).</li>
<li><code>wald_stats[i] &lt;- ...</code>: Calculate the Wald statistic for the null hypothesis <span class="math inline">\(H_0: \beta = \beta_{true}\)</span>. This implements the formula <span class="math inline">\((\hat{\beta} - \beta)^T V(X)^{-1} (\hat{\beta} - \beta)\)</span>.</li>
</ul></li>
<li><strong>Visualization:</strong>
<ul>
<li>We create a histogram of the calculated Wald statistics.</li>
<li><code>stat_function(...)</code>: We overlay the theoretical chi-squared distribution with <span class="math inline">\(K\)</span> degrees of freedom (using <code>dchisq</code>). The histogram should closely follow the chi-squared distribution.</li>
</ul></li>
</ol>
<p><strong>Connection to Concepts:</strong></p>
<ul>
<li><strong>Chi-Squared Distribution (Theorem 19.3):</strong> The script demonstrates the result that the quadratic form <span class="math inline">\((\hat{\beta} - \beta)^{T}V(X)^{-1}(\hat{\beta} - \beta)\)</span> follows a chi-squared distribution with <span class="math inline">\(K\)</span> degrees of freedom under the null hypothesis. This is a key result for constructing hypothesis tests and confidence regions.</li>
</ul>
</section>
<section class="level3" id="r-script-4-illustrating-s2-as-an-unbiased-estimator-of-sigma2">
<h3 class="anchored" data-anchor-id="r-script-4-illustrating-s2-as-an-unbiased-estimator-of-sigma2">R Script 4: Illustrating <span class="math inline">\(s^2\)</span> as an Unbiased Estimator of <span class="math inline">\(\sigma^2\)</span></h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a aria-hidden="true" href="#cb17-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb17-2"><a aria-hidden="true" href="#cb17-2" tabindex="-1"></a></span>
<span id="cb17-3"><a aria-hidden="true" href="#cb17-3" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb17-4"><a aria-hidden="true" href="#cb17-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">101112</span>)</span>
<span id="cb17-5"><a aria-hidden="true" href="#cb17-5" tabindex="-1"></a></span>
<span id="cb17-6"><a aria-hidden="true" href="#cb17-6" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb17-7"><a aria-hidden="true" href="#cb17-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span>  <span class="co"># Sample size</span></span>
<span id="cb17-8"><a aria-hidden="true" href="#cb17-8" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">3</span>, <span class="sc">-</span><span class="dv">1</span>)  <span class="co"># True intercept and slope</span></span>
<span id="cb17-9"><a aria-hidden="true" href="#cb17-9" tabindex="-1"></a>sigma_true <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co"># True error standard deviation</span></span>
<span id="cb17-10"><a aria-hidden="true" href="#cb17-10" tabindex="-1"></a></span>
<span id="cb17-11"><a aria-hidden="true" href="#cb17-11" tabindex="-1"></a><span class="co"># Generate X</span></span>
<span id="cb17-12"><a aria-hidden="true" href="#cb17-12" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">runif</span>(n, <span class="dv">0</span>, <span class="dv">10</span>)</span>
<span id="cb17-13"><a aria-hidden="true" href="#cb17-13" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb17-14"><a aria-hidden="true" href="#cb17-14" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">ncol</span>(X)</span>
<span id="cb17-15"><a aria-hidden="true" href="#cb17-15" tabindex="-1"></a></span>
<span id="cb17-16"><a aria-hidden="true" href="#cb17-16" tabindex="-1"></a><span class="co"># Number of simulations</span></span>
<span id="cb17-17"><a aria-hidden="true" href="#cb17-17" tabindex="-1"></a>n_sim <span class="ot">&lt;-</span> <span class="dv">5000</span></span>
<span id="cb17-18"><a aria-hidden="true" href="#cb17-18" tabindex="-1"></a></span>
<span id="cb17-19"><a aria-hidden="true" href="#cb17-19" tabindex="-1"></a><span class="co"># Store results</span></span>
<span id="cb17-20"><a aria-hidden="true" href="#cb17-20" tabindex="-1"></a>s2_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_sim)</span>
<span id="cb17-21"><a aria-hidden="true" href="#cb17-21" tabindex="-1"></a>sigma2_hat_values <span class="ot">&lt;-</span> <span class="fu">numeric</span>(n_sim)</span>
<span id="cb17-22"><a aria-hidden="true" href="#cb17-22" tabindex="-1"></a></span>
<span id="cb17-23"><a aria-hidden="true" href="#cb17-23" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sim) {</span>
<span id="cb17-24"><a aria-hidden="true" href="#cb17-24" tabindex="-1"></a>  epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma_true)</span>
<span id="cb17-25"><a aria-hidden="true" href="#cb17-25" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> epsilon</span>
<span id="cb17-26"><a aria-hidden="true" href="#cb17-26" tabindex="-1"></a>  beta_hat <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb17-27"><a aria-hidden="true" href="#cb17-27" tabindex="-1"></a>  y_hat <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_hat</span>
<span id="cb17-28"><a aria-hidden="true" href="#cb17-28" tabindex="-1"></a>  residuals <span class="ot">&lt;-</span> y <span class="sc">-</span> y_hat</span>
<span id="cb17-29"><a aria-hidden="true" href="#cb17-29" tabindex="-1"></a></span>
<span id="cb17-30"><a aria-hidden="true" href="#cb17-30" tabindex="-1"></a>  s2 <span class="ot">&lt;-</span> <span class="fu">sum</span>(residuals<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> (n <span class="sc">-</span> k)       <span class="co"># Unbiased estimator</span></span>
<span id="cb17-31"><a aria-hidden="true" href="#cb17-31" tabindex="-1"></a>  sigma2_hat <span class="ot">&lt;-</span> <span class="fu">sum</span>(residuals<span class="sc">^</span><span class="dv">2</span>) <span class="sc">/</span> n  <span class="co"># MLE (biased estimator)</span></span>
<span id="cb17-32"><a aria-hidden="true" href="#cb17-32" tabindex="-1"></a></span>
<span id="cb17-33"><a aria-hidden="true" href="#cb17-33" tabindex="-1"></a>  s2_values[i] <span class="ot">&lt;-</span> s2</span>
<span id="cb17-34"><a aria-hidden="true" href="#cb17-34" tabindex="-1"></a>  sigma2_hat_values[i] <span class="ot">&lt;-</span> sigma2_hat</span>
<span id="cb17-35"><a aria-hidden="true" href="#cb17-35" tabindex="-1"></a>}</span>
<span id="cb17-36"><a aria-hidden="true" href="#cb17-36" tabindex="-1"></a></span>
<span id="cb17-37"><a aria-hidden="true" href="#cb17-37" tabindex="-1"></a><span class="co"># Calculate means</span></span>
<span id="cb17-38"><a aria-hidden="true" href="#cb17-38" tabindex="-1"></a>mean_s2 <span class="ot">&lt;-</span> <span class="fu">mean</span>(s2_values)</span>
<span id="cb17-39"><a aria-hidden="true" href="#cb17-39" tabindex="-1"></a>mean_sigma2_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(sigma2_hat_values)</span>
<span id="cb17-40"><a aria-hidden="true" href="#cb17-40" tabindex="-1"></a></span>
<span id="cb17-41"><a aria-hidden="true" href="#cb17-41" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mean of s2:"</span>, mean_s2, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean of s2: 4.011738 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a aria-hidden="true" href="#cb19-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Mean of sigma2_hat:"</span>, mean_sigma2_hat, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Mean of sigma2_hat: 3.851268 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a aria-hidden="true" href="#cb21-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"True sigma^2:"</span>, sigma_true<span class="sc">^</span><span class="dv">2</span>, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>True sigma^2: 4 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a aria-hidden="true" href="#cb23-1" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb23-2"><a aria-hidden="true" href="#cb23-2" tabindex="-1"></a><span class="fu">data.frame</span>(<span class="at">s2 =</span> s2_values, <span class="at">sigma2_hat =</span> sigma2_hat_values) <span class="sc">%&gt;%</span></span>
<span id="cb23-3"><a aria-hidden="true" href="#cb23-3" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">everything</span>(), <span class="at">names_to =</span> <span class="st">"Estimator"</span>, <span class="at">values_to =</span> <span class="st">"Estimate"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb23-4"><a aria-hidden="true" href="#cb23-4" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> Estimate, <span class="at">fill =</span> Estimator)) <span class="sc">+</span></span>
<span id="cb23-5"><a aria-hidden="true" href="#cb23-5" tabindex="-1"></a>  <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span><span class="fu">after_stat</span>(density)), <span class="at">position=</span><span class="st">"identity"</span>, <span class="at">alpha=</span><span class="fl">0.5</span>, <span class="at">bins=</span><span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb23-6"><a aria-hidden="true" href="#cb23-6" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept =</span> sigma_true<span class="sc">^</span><span class="dv">2</span>, <span class="at">color=</span><span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb23-7"><a aria-hidden="true" href="#cb23-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title=</span><span class="st">"Comparison of s2 and sigma2_hat"</span>, <span class="at">x=</span><span class="st">"Estimate"</span>, <span class="at">y=</span><span class="st">"Density"</span>) <span class="sc">+</span></span>
<span id="cb23-8"><a aria-hidden="true" href="#cb23-8" tabindex="-1"></a>  <span class="fu">xlim</span>(<span class="dv">0</span>, <span class="dv">10</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 4 rows containing missing values or values outside the scale range
(`geom_bar()`).</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap19_files/figure-html/unnamed-chunk-4-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><p><strong>Setup:</strong> We define simulation parameters and generate the independent variable <code>x</code>.</p></li>
<li><p><strong>Simulation Loop:</strong></p>
<ul>
<li>We generate errors and <code>y</code> for each simulation.</li>
<li>We calculate the OLS estimator <span class="math inline">\(\hat{\beta}\)</span>.</li>
<li><code>residuals &lt;- y - y_hat</code>: We calculate the residuals.</li>
<li><code>s2 &lt;- sum(residuals^2) / (n - k)</code>: We calculate the <em>unbiased</em> estimator of <span class="math inline">\(\sigma^2\)</span>, dividing by the degrees of freedom (<span class="math inline">\(n-k\)</span>).</li>
<li><code>sigma2_hat &lt;- sum(residuals^2) / n</code>: We calculate the MLE of <span class="math inline">\(\sigma^2\)</span> (which is biased), dividing by <span class="math inline">\(n\)</span>.</li>
</ul></li>
<li><p><strong>Analysis and Visualization:</strong></p>
<ul>
<li>We calculate and print the mean of <code>s2_values</code> and <code>sigma2_hat_values</code> across all simulations. The mean of <code>s2</code> should be close to the true <span class="math inline">\(\sigma^2\)</span> (4), while the mean of <code>sigma2_hat</code> will be smaller.</li>
<li>The histogram visually compares the distributions of the two estimators. The <code>s2</code> distribution should be centered around the true value, while the <code>sigma2_hat</code> distribution will be shifted to the left.</li>
</ul></li>
</ol>
<p><strong>Connection to Concepts:</strong></p>
<ul>
<li><strong>Unbiased Estimator of <span class="math inline">\(\sigma^2\)</span> (Theorem 19.5):</strong> The script directly demonstrates that <span class="math inline">\(s^2 = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n-K}\)</span> is an unbiased estimator of <span class="math inline">\(\sigma^2\)</span>, while the MLE <span class="math inline">\(\hat{\sigma}^2_{MLE} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n}\)</span> is biased downwards. The simulation shows that, on average, <span class="math inline">\(s^2\)</span> correctly estimates <span class="math inline">\(\sigma^2\)</span>, while the MLE consistently underestimates it.</li>
<li><strong>Degrees of freedom:</strong> The script shows the importance of using <span class="math inline">\(n-K\)</span> and not <span class="math inline">\(n\)</span> in the denominator of the estimator.</li>
</ul>
</section>
<section class="level3" id="r-script-5-gauss-markov-theorem-illustration-comparison-with-a-different-linear-unbiased-estimator">
<h3 class="anchored" data-anchor-id="r-script-5-gauss-markov-theorem-illustration-comparison-with-a-different-linear-unbiased-estimator">R Script 5: Gauss-Markov Theorem Illustration (Comparison with a Different Linear Unbiased Estimator)</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a aria-hidden="true" href="#cb25-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb25-2"><a aria-hidden="true" href="#cb25-2" tabindex="-1"></a></span>
<span id="cb25-3"><a aria-hidden="true" href="#cb25-3" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb25-4"><a aria-hidden="true" href="#cb25-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">131415</span>)</span>
<span id="cb25-5"><a aria-hidden="true" href="#cb25-5" tabindex="-1"></a></span>
<span id="cb25-6"><a aria-hidden="true" href="#cb25-6" tabindex="-1"></a><span class="co"># Parameters</span></span>
<span id="cb25-7"><a aria-hidden="true" href="#cb25-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">50</span></span>
<span id="cb25-8"><a aria-hidden="true" href="#cb25-8" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb25-9"><a aria-hidden="true" href="#cb25-9" tabindex="-1"></a>sigma_true <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb25-10"><a aria-hidden="true" href="#cb25-10" tabindex="-1"></a></span>
<span id="cb25-11"><a aria-hidden="true" href="#cb25-11" tabindex="-1"></a><span class="co"># Generate X</span></span>
<span id="cb25-12"><a aria-hidden="true" href="#cb25-12" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb25-13"><a aria-hidden="true" href="#cb25-13" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">cbind</span>(<span class="dv">1</span>, x)</span>
<span id="cb25-14"><a aria-hidden="true" href="#cb25-14" tabindex="-1"></a></span>
<span id="cb25-15"><a aria-hidden="true" href="#cb25-15" tabindex="-1"></a><span class="co"># Number of simulations</span></span>
<span id="cb25-16"><a aria-hidden="true" href="#cb25-16" tabindex="-1"></a>n_sim <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb25-17"><a aria-hidden="true" href="#cb25-17" tabindex="-1"></a></span>
<span id="cb25-18"><a aria-hidden="true" href="#cb25-18" tabindex="-1"></a><span class="co"># Store results</span></span>
<span id="cb25-19"><a aria-hidden="true" href="#cb25-19" tabindex="-1"></a>ols_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n_sim, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb25-20"><a aria-hidden="true" href="#cb25-20" tabindex="-1"></a>alternative_estimates <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="cn">NA</span>, <span class="at">nrow =</span> n_sim, <span class="at">ncol =</span> <span class="dv">2</span>)</span>
<span id="cb25-21"><a aria-hidden="true" href="#cb25-21" tabindex="-1"></a></span>
<span id="cb25-22"><a aria-hidden="true" href="#cb25-22" tabindex="-1"></a></span>
<span id="cb25-23"><a aria-hidden="true" href="#cb25-23" tabindex="-1"></a><span class="co"># Alternative linear unbiased estimator: average of y/x_i for each i</span></span>
<span id="cb25-24"><a aria-hidden="true" href="#cb25-24" tabindex="-1"></a>alternative_estimator <span class="ot">&lt;-</span> <span class="cf">function</span>(X, y) {</span>
<span id="cb25-25"><a aria-hidden="true" href="#cb25-25" tabindex="-1"></a>  <span class="co"># This estimator is only for the simple linear regression case with intercept.</span></span>
<span id="cb25-26"><a aria-hidden="true" href="#cb25-26" tabindex="-1"></a>  <span class="co">#  It is designed to be linear and unbiased, but inefficient.</span></span>
<span id="cb25-27"><a aria-hidden="true" href="#cb25-27" tabindex="-1"></a>  <span class="co"># It takes the average of the two estimators: y_bar, and mean(y/x).</span></span>
<span id="cb25-28"><a aria-hidden="true" href="#cb25-28" tabindex="-1"></a></span>
<span id="cb25-29"><a aria-hidden="true" href="#cb25-29" tabindex="-1"></a>  beta1_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(y <span class="sc">/</span> X[,<span class="dv">2</span>])  <span class="co">#  mean(y/x), biased for the intercept.</span></span>
<span id="cb25-30"><a aria-hidden="true" href="#cb25-30" tabindex="-1"></a>  beta0_hat <span class="ot">&lt;-</span> <span class="fu">mean</span>(y) <span class="sc">-</span> beta1_hat<span class="sc">*</span><span class="fu">mean</span>(X[,<span class="dv">2</span>]) <span class="co"># Correct for bias in intercept</span></span>
<span id="cb25-31"><a aria-hidden="true" href="#cb25-31" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">c</span>(beta0_hat, beta1_hat))</span>
<span id="cb25-32"><a aria-hidden="true" href="#cb25-32" tabindex="-1"></a>}</span>
<span id="cb25-33"><a aria-hidden="true" href="#cb25-33" tabindex="-1"></a></span>
<span id="cb25-34"><a aria-hidden="true" href="#cb25-34" tabindex="-1"></a></span>
<span id="cb25-35"><a aria-hidden="true" href="#cb25-35" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">1</span><span class="sc">:</span>n_sim) {</span>
<span id="cb25-36"><a aria-hidden="true" href="#cb25-36" tabindex="-1"></a>  epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma_true)</span>
<span id="cb25-37"><a aria-hidden="true" href="#cb25-37" tabindex="-1"></a>  y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta_true <span class="sc">+</span> epsilon</span>
<span id="cb25-38"><a aria-hidden="true" href="#cb25-38" tabindex="-1"></a></span>
<span id="cb25-39"><a aria-hidden="true" href="#cb25-39" tabindex="-1"></a>  <span class="co"># OLS estimates</span></span>
<span id="cb25-40"><a aria-hidden="true" href="#cb25-40" tabindex="-1"></a>  ols_estimates[i, ] <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X) <span class="sc">%*%</span> X) <span class="sc">%*%</span> <span class="fu">t</span>(X) <span class="sc">%*%</span> y</span>
<span id="cb25-41"><a aria-hidden="true" href="#cb25-41" tabindex="-1"></a></span>
<span id="cb25-42"><a aria-hidden="true" href="#cb25-42" tabindex="-1"></a>  <span class="co"># Alternative estimator</span></span>
<span id="cb25-43"><a aria-hidden="true" href="#cb25-43" tabindex="-1"></a>  alternative_estimates[i, ] <span class="ot">&lt;-</span> <span class="fu">alternative_estimator</span>(X, y)</span>
<span id="cb25-44"><a aria-hidden="true" href="#cb25-44" tabindex="-1"></a>}</span>
<span id="cb25-45"><a aria-hidden="true" href="#cb25-45" tabindex="-1"></a></span>
<span id="cb25-46"><a aria-hidden="true" href="#cb25-46" tabindex="-1"></a><span class="co"># Calculate variances</span></span>
<span id="cb25-47"><a aria-hidden="true" href="#cb25-47" tabindex="-1"></a>ols_variances <span class="ot">&lt;-</span> <span class="fu">apply</span>(ols_estimates, <span class="dv">2</span>, var)</span>
<span id="cb25-48"><a aria-hidden="true" href="#cb25-48" tabindex="-1"></a>alternative_variances <span class="ot">&lt;-</span> <span class="fu">apply</span>(alternative_estimates, <span class="dv">2</span>, var)</span>
<span id="cb25-49"><a aria-hidden="true" href="#cb25-49" tabindex="-1"></a></span>
<span id="cb25-50"><a aria-hidden="true" href="#cb25-50" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"OLS Variances:"</span>, ols_variances, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>OLS Variances: 0.07026925 0.06798292 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a aria-hidden="true" href="#cb27-1" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">"Alternative Variances:"</span>, alternative_variances, <span class="st">"</span><span class="sc">\n</span><span class="st">"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Alternative Variances: 0.08862799 1.168848 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a aria-hidden="true" href="#cb29-1" tabindex="-1"></a><span class="co"># Visualization</span></span>
<span id="cb29-2"><a aria-hidden="true" href="#cb29-2" tabindex="-1"></a>ols_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(ols_estimates)</span>
<span id="cb29-3"><a aria-hidden="true" href="#cb29-3" tabindex="-1"></a><span class="fu">colnames</span>(ols_df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"beta0_hat_ols"</span>, <span class="st">"beta1_hat_ols"</span>)</span>
<span id="cb29-4"><a aria-hidden="true" href="#cb29-4" tabindex="-1"></a>alt_df <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(alternative_estimates)</span>
<span id="cb29-5"><a aria-hidden="true" href="#cb29-5" tabindex="-1"></a><span class="fu">colnames</span>(alt_df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"beta0_hat_alt"</span>, <span class="st">"beta1_hat_alt"</span>)</span>
<span id="cb29-6"><a aria-hidden="true" href="#cb29-6" tabindex="-1"></a>combined_df <span class="ot">&lt;-</span> <span class="fu">bind_rows</span>(ols_df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">estimator =</span> <span class="st">"OLS"</span>),</span>
<span id="cb29-7"><a aria-hidden="true" href="#cb29-7" tabindex="-1"></a>                         alt_df <span class="sc">%&gt;%</span> <span class="fu">mutate</span>(<span class="at">estimator =</span> <span class="st">"Alternative"</span>))</span>
<span id="cb29-8"><a aria-hidden="true" href="#cb29-8" tabindex="-1"></a></span>
<span id="cb29-9"><a aria-hidden="true" href="#cb29-9" tabindex="-1"></a>combined_df <span class="sc">%&gt;%</span></span>
<span id="cb29-10"><a aria-hidden="true" href="#cb29-10" tabindex="-1"></a>    <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(<span class="sc">-</span>estimator), <span class="at">names_to =</span> <span class="st">"parameter"</span>, <span class="at">values_to=</span><span class="st">"estimate"</span>) <span class="sc">%&gt;%</span></span>
<span id="cb29-11"><a aria-hidden="true" href="#cb29-11" tabindex="-1"></a>    <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> estimate, <span class="at">fill =</span> estimator)) <span class="sc">+</span></span>
<span id="cb29-12"><a aria-hidden="true" href="#cb29-12" tabindex="-1"></a>    <span class="fu">geom_histogram</span>(<span class="fu">aes</span>(<span class="at">y=</span><span class="fu">after_stat</span>(density)), <span class="at">position=</span><span class="st">"identity"</span>, <span class="at">alpha=</span><span class="fl">0.5</span>, <span class="at">bins=</span><span class="dv">30</span>) <span class="sc">+</span></span>
<span id="cb29-13"><a aria-hidden="true" href="#cb29-13" tabindex="-1"></a>    <span class="fu">facet_wrap</span>(<span class="sc">~</span>parameter, <span class="at">scales =</span> <span class="st">"free"</span>) <span class="sc">+</span></span>
<span id="cb29-14"><a aria-hidden="true" href="#cb29-14" tabindex="-1"></a>     <span class="fu">geom_vline</span>(<span class="at">data =</span> <span class="fu">data.frame</span>(<span class="at">parameter =</span> <span class="fu">c</span>(<span class="st">"beta0_hat_ols"</span>, <span class="st">"beta1_hat_ols"</span>),</span>
<span id="cb29-15"><a aria-hidden="true" href="#cb29-15" tabindex="-1"></a>                              <span class="at">true_value =</span> beta_true),</span>
<span id="cb29-16"><a aria-hidden="true" href="#cb29-16" tabindex="-1"></a>             <span class="fu">aes</span>(<span class="at">xintercept =</span> true_value), <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb29-17"><a aria-hidden="true" href="#cb29-17" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title=</span><span class="st">"Comparison of OLS and Alternative Estimator"</span>, <span class="at">x=</span><span class="st">"Estimate"</span>, <span class="at">y=</span><span class="st">"Density"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Removed 4000 rows containing non-finite outside the scale range
(`stat_bin()`).</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap19_files/figure-html/unnamed-chunk-5-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong>
<ul>
<li>We set the seed, define parameters, and generate <code>x</code>.</li>
<li><code>alternative_estimator</code>: We define a function that implements a <em>different</em> linear unbiased estimator. This estimator is constructed to be linear in <span class="math inline">\(y\)</span> and unbiased (we can verify this mathematically), but it is <em>not</em> efficient. For the case of a simple linear regression <span class="math inline">\(y_i = \beta_0 + \beta_1x_i + u_i\)</span>, this alternative estimator is given by taking <span class="math inline">\(\hat{\beta_1} = \dfrac{1}{n}\sum_{i=1}^n \dfrac{y_i}{x_i}\)</span> and setting the intercept to satisfy the condition that the regression line passes through the sample mean, that is, <span class="math inline">\(\hat{\beta_0} = \bar{y} - \hat{\beta_1}*\bar{x}\)</span>.</li>
</ul></li>
<li><strong>Simulation Loop:</strong>
<ul>
<li>We generate errors and <code>y</code>.</li>
<li>We calculate both the OLS estimates <em>and</em> the estimates from our <code>alternative_estimator</code>.</li>
</ul></li>
<li><strong>Analysis and Visualization:</strong>
<ul>
<li><code>ols_variances</code> and <code>alternative_variances</code>: We calculate the <em>sample variances</em> of the OLS estimates and the alternative estimates (across the simulations). The variances of the OLS estimates should be <em>smaller</em> than the variances of the alternative estimates.</li>
<li>The histograms visually compare the distributions of the OLS and alternative estimators. The OLS estimator’s distribution should be more concentrated around the true values (less spread out), demonstrating its lower variance.</li>
</ul></li>
</ol>
<p><strong>Connection to Concepts:</strong></p>
<ul>
<li><strong>Gauss-Markov Theorem (Theorem 19.6):</strong> The script illustrates the Gauss-Markov theorem. It shows that, among linear unbiased estimators, the OLS estimator has the smallest variance. We create an alternative linear unbiased estimator and show (through simulation) that its variance is larger than that of the OLS estimator.</li>
<li><strong>BLUE (Best Linear Unbiased Estimator):</strong> This example reinforces the meaning of “best” in BLUE – it refers to minimum variance within the class of linear unbiased estimators.</li>
</ul>
<p>These five R scripts cover key concepts related to the statistical properties of the OLS estimator, illustrating unbiasedness, conditional variance, the chi-squared distribution of the Wald statistic, the unbiased estimation of the error variance, and the Gauss-Markov theorem. The simulations and visualizations help to solidify the understanding of these theoretical concepts. The code is well-commented, uses tidyverse style where appropriate, and provides clear explanations.</p>
</section>
</section>
<section class="level2" id="youtube-videos-related-to-ols-properties">
<h2 class="anchored" data-anchor-id="youtube-videos-related-to-ols-properties">YouTube Videos Related to OLS Properties</h2>
<p>Here are some YouTube videos that explain concepts mentioned in the attached text, along with explanations of their relevance. I have verified that these videos are currently available on YouTube (as of October 26, 2023).</p>
<section class="level3" id="unbiasedness-of-ols">
<h3 class="anchored" data-anchor-id="unbiasedness-of-ols">1. Unbiasedness of OLS</h3>
<ul>
<li><strong>Video Title:</strong> “Econometrics // Lecture 3: OLS and Goodness-Of-Fit (R-Squared)” by <em>Ben Lambert</em></li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=4xa0F54h-pM">https://www.youtube.com/watch?v=4xa0F54h-pM</a></li>
<li><strong>Relevance to Text:</strong> While this video covers multiple topics, a significant portion (starting around 2:50) is dedicated to demonstrating the <strong>unbiasedness of the OLS estimator</strong>. Lambert walks through the derivation, showing that <span class="math inline">\(\mathbb{E}[\hat{\beta}] = \beta\)</span>, which is directly related to <strong>Theorem 19.1</strong> in the text. He uses a clear, step-by-step approach, explaining the assumptions involved. He visually shows how OLS works with an intuitive explanation. This complements the more formal proof in the text.</li>
</ul>
</section>
<section class="level3" id="variance-of-ols-and-gauss-markov-theorem">
<h3 class="anchored" data-anchor-id="variance-of-ols-and-gauss-markov-theorem">2. Variance of OLS and Gauss-Markov Theorem</h3>
<ul>
<li><strong>Video Title:</strong> “6. Properties of OLS: unbiasedness and BLUE theorem” by <em>Apoorva Javadekar</em></li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=jI8INnEa58Q">https://www.youtube.com/watch?v=jI8INnEa58Q</a></li>
<li><strong>Relevance to Text:</strong> This video discusses the variance of the OLS estimator and introduces the <strong>Gauss-Markov Theorem</strong>.
<ul>
<li>It covers the derivation of <span class="math inline">\(\text{Var}(\hat{\beta}|X) = \sigma^2(X^TX)^{-1}\)</span> under homoskedasticity, corresponding to <strong>Theorem 19.2</strong>.</li>
<li>It explains the <strong>Gauss-Markov Theorem (Theorem 19.6)</strong>, stating that OLS is <strong>BLUE</strong> (Best Linear Unbiased Estimator). It presents the theorem’s assumptions and its implications.</li>
<li>The presenter explains all the derivations step by step, making them simple to understand.</li>
</ul></li>
</ul>
</section>
<section class="level3" id="derivation-of-ols-estimator-and-its-properties">
<h3 class="anchored" data-anchor-id="derivation-of-ols-estimator-and-its-properties">3. Derivation of OLS Estimator and its Properties</h3>
<ul>
<li><strong>Video Title:</strong> “2.2 Properties of OLS: derivation of the OLS estimator, unbiasedness, variance, and efficiency” by <em>Quantitative Economics with R</em></li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=9-X1qWnWKtM">https://www.youtube.com/watch?v=9-X1qWnWKtM</a></li>
<li><strong>Relevance:</strong> This video provides a comprehensive overview that closely aligns with several parts of the text. It includes:
<ul>
<li><strong>Derivation of the OLS Estimator:</strong> It shows how to obtain <span class="math inline">\(\hat{\beta} = (X^TX)^{-1}X^Ty\)</span> by minimizing the sum of squared residuals. This relates to <strong>Definition 19.1</strong>.</li>
<li><strong>Unbiasedness:</strong> It proves <span class="math inline">\(\mathbb{E}[\hat{\beta}] = \beta\)</span> (<strong>Theorem 19.1</strong>).</li>
<li><strong>Variance:</strong> It derives the variance-covariance matrix <span class="math inline">\(\text{Var}(\hat{\beta}|X) = \sigma^2(X^TX)^{-1}\)</span> (<strong>Theorem 19.2</strong>, under homoskedasticity).</li>
<li><strong>Gauss-Markov:</strong> It mentions the Gauss-Markov theorem and the BLUE property of OLS (<strong>Theorem 19.6</strong>), albeit briefly.</li>
<li>The video covers the derivations step by step.</li>
</ul></li>
</ul>
</section>
<section class="level3" id="chi-squared-distribution-and-hypothesis-testing">
<h3 class="anchored" data-anchor-id="chi-squared-distribution-and-hypothesis-testing">4. Chi-squared Distribution and Hypothesis Testing</h3>
<ul>
<li><strong>Video Title:</strong> “Hypothesis Testing and Confidence Intervals with the OLS Estimator (Part 1)” by <em>BurkeyAcademy</em></li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=-1o_MupV6_k">https://www.youtube.com/watch?v=-1o_MupV6_k</a></li>
<li><strong>Relevance to Text:</strong> While this video doesn’t directly derive the chi-squared result for the Wald statistic, it explains the general principles of hypothesis testing in the context of OLS, which is a crucial application of <strong>Theorem 19.3</strong>. It covers:
<ul>
<li>How to build t-tests and F-tests to perform hypothesis testing.</li>
<li>The assumptions required for constructing tests to draw inferences.</li>
<li>Concepts like p-values and confidence intervals, which rely on the distributional results of the OLS estimator (including the normality assumption and the resulting t and chi-squared distributions).</li>
</ul></li>
</ul>
</section>
<section class="level3" id="understanding-the-ols-estimator-and-its-properties">
<h3 class="anchored" data-anchor-id="understanding-the-ols-estimator-and-its-properties">5. Understanding the OLS estimator and its properties</h3>
<ul>
<li><strong>Video Title:</strong> “OLS estimator properties” by <em>Jochumzen</em></li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=M9eRzPjeLpY">https://www.youtube.com/watch?v=M9eRzPjeLpY</a></li>
<li><strong>Relevance to Text:</strong> This video serves as a good general overview and summary of the key properties of the OLS estimator:
<ul>
<li><strong>Linearity:</strong> It highlights that OLS is a linear estimator (<strong>Definition 19.1</strong>).</li>
<li><strong>Unbiasedness:</strong> It explains the concept of unbiasedness (<strong>Theorem 19.1</strong>).</li>
<li><strong>Variance:</strong> It discusses the variance of the OLS estimator and its dependence on <span class="math inline">\(\sigma^2\)</span> and <span class="math inline">\(X\)</span> (<strong>Theorem 19.2</strong>).</li>
<li><strong>Gauss-Markov Theorem:</strong> It briefly mentions the Gauss-Markov Theorem (<strong>Theorem 19.6</strong>).</li>
</ul></li>
</ul>
<p>These videos provide a good visual and auditory complement to the text, covering many of the core concepts with varying levels of detail and mathematical rigor. They can be particularly helpful for grasping the intuition behind the derivations and understanding the practical implications of the theorems.</p>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch19mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch19mcsolution1">MC Solution 1</a></p>
<p>The OLS estimator, <span class="math inline">\(\hat{\beta}\)</span>, is considered “linear” because:</p>
<ol type="a">
<li>It is a linear function of the parameters <span class="math inline">\(\beta\)</span>.</li>
<li>It is a linear function of the independent variables <span class="math inline">\(X\)</span>.</li>
<li>It is a linear function of the dependent variable <span class="math inline">\(y\)</span>.</li>
<li>The relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(X\)</span> is linear.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch19mcsolution2">MC Solution 2</a></p>
<p>Under Assumption A1 (<span class="math inline">\(\mathbb{E}[\epsilon|X] = 0\)</span>), the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> is:</p>
<ol type="a">
<li>Always biased.</li>
<li>Conditionally unbiased.</li>
<li>Unconditionally biased, but conditionally unbiased.</li>
<li>Only unbiased if the errors are normally distributed.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch19mcsolution3">MC Solution 3</a></p>
<p>The conditional variance of the OLS estimator, <span class="math inline">\(\text{Var}(\hat{\beta}|X)\)</span>, under homoskedasticity is given by:</p>
<ol type="a">
<li><span class="math inline">\((X^{T}X)^{-1}X^{T}\Sigma(X)X(X^{T}X)^{-1}\)</span></li>
<li><span class="math inline">\(\sigma^{2}(X^{T}X)^{-1}\)</span></li>
<li><span class="math inline">\(\sigma^{2}X^{T}X\)</span></li>
<li><span class="math inline">\(\mathbb{E}[(X^{T}X)^{-1}]\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch19mcsolution4">MC Solution 4</a>(#sec-ch19mcsolution4}</p>
<p>Which of the following statements is TRUE regarding conditional and unconditional unbiasedness?</p>
<ol type="a">
<li>Unconditional unbiasedness implies conditional unbiasedness.</li>
<li>Conditional unbiasedness implies unconditional unbiasedness.</li>
<li>Conditional and unconditional unbiasedness are unrelated concepts.</li>
<li>An estimator is either conditionally unbiased or unconditionally unbiased, but not both.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch19mcsolution5">MC Solution 5</a></p>
<p>The Law of Iterated Expectations is used to show that:</p>
<ol type="a">
<li><span class="math inline">\(\text{Var}(\hat{\beta}|X) = \sigma^{2}(X^{T}X)^{-1}\)</span></li>
<li>Conditional unbiasedness implies unconditional unbiasedness.</li>
<li>The OLS estimator is BLUE.</li>
<li>The Wald statistic follows a chi-squared distribution.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch19mcsolution6">MC Solution 6</a></p>
<p>The expression <span class="math inline">\(\text{Var}(\hat{\beta}) = \sigma^{2}\mathbb{E}[(X^{T}X)^{-1}]\)</span> holds true under:</p>
<ol type="a">
<li>Only heteroskedasticity.</li>
<li>Only homoskedasticity and no autocorrelation.</li>
<li>Any distribution of the error term.</li>
<li>Only when the sample size is large.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch19mcsolution7">MC Solution 7</a></p>
<p>In a simple linear regression <span class="math inline">\(y_i = \beta x_i + \epsilon_i\)</span>, if <span class="math inline">\(\mathbb{E}[\epsilon_i|x_i] = 0\)</span>, the OLS estimator of <span class="math inline">\(\beta\)</span> is: (a) Biased (b) Unbiased (c) Only unbiased if n &gt; 30 (d) Not defined</p>
</section>
<section class="level3" id="sec-ch19mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch19mcsolution8">MC Solution 8</a>(#sec-ch19mcsolution8}</p>
<p>In the partitioned regression formula <span class="math inline">\(\hat{\beta}_1 = (X_1^T M_2 X_1)^{-1} X_1^T M_2 y\)</span>, the matrix <span class="math inline">\(M_2\)</span> projects vectors onto the space:</p>
<ol type="a">
<li>Spanned by <span class="math inline">\(X_1\)</span>.</li>
<li>Spanned by <span class="math inline">\(X_2\)</span>.</li>
<li>Orthogonal to the space spanned by <span class="math inline">\(X_1\)</span>.</li>
<li>Orthogonal to the space spanned by <span class="math inline">\(X_2\)</span>.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch19mcsolution9">MC Solution 9</a></p>
<p>Under the assumption of normality of errors, the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> follows a:</p>
<ol type="a">
<li>t-distribution.</li>
<li>Chi-squared distribution.</li>
<li>Normal distribution.</li>
<li>F-distribution.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch19mcsolution10">MC Solution 10</a>(#sec-ch19mcsolution10}</p>
<p>The statistic <span class="math inline">\(\tau = (\hat{\beta} - \beta)^{T}V(X)^{-1}(\hat{\beta} - \beta)\)</span> follows a chi-squared distribution with degrees of freedom equal to:</p>
<ol type="a">
<li><span class="math inline">\(n\)</span> (sample size).</li>
<li><span class="math inline">\(n-K\)</span> (sample size minus the number of parameters).</li>
<li><span class="math inline">\(K\)</span> (number of parameters).</li>
<li><span class="math inline">\(n-1\)</span>.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch19mcsolution11">MC Solution 11</a>(#sec-ch19mcsolution11}</p>
<p><span class="math inline">\(\hat{m}(x)\)</span> in Theorem 19.4 represents:</p>
<ol type="a">
<li>The vector of predicted values for the original sample.</li>
<li>The predicted value for a specific set of independent variable values <span class="math inline">\(x\)</span>.</li>
<li>The matrix <span class="math inline">\(X\)</span>.</li>
<li>The error term.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch19mcsolution12">MC Solution 12</a>(#sec-ch19mcsolution12}</p>
<p>The Maximum Likelihood Estimator (MLE) of <span class="math inline">\(\sigma^{2}\)</span> in the linear regression model with normal errors is:</p>
<ol type="a">
<li><span class="math inline">\(\dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n-K}\)</span></li>
<li><span class="math inline">\(\dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n}\)</span></li>
<li><span class="math inline">\(\dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{K}\)</span></li>
<li><span class="math inline">\((X^{T}X)^{-1}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch19mcsolution13">MC Solution 13</a>(#sec-ch19mcsolution13}</p>
<p>The estimator <span class="math inline">\(s_{*}^{2} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n-K}\)</span> is preferred over <span class="math inline">\(\hat{\sigma}_{mle}^{2} = \dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n}\)</span> because:</p>
<ol type="a">
<li><span class="math inline">\(s_{*}^{2}\)</span> is the MLE.</li>
<li><span class="math inline">\(s_{*}^{2}\)</span> is unbiased.</li>
<li><span class="math inline">\(s_{*}^{2}\)</span> has a larger variance.</li>
<li><span class="math inline">\(s_{*}^{2}\)</span> is always smaller.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch19mcsolution14">MC Solution 14</a>(#sec-ch19mcsolution14}</p>
<p>The “degrees of freedom” in the context of estimating <span class="math inline">\(\sigma^2\)</span> are:</p>
<ol type="a">
<li><span class="math inline">\(n\)</span></li>
<li><span class="math inline">\(K\)</span></li>
<li><span class="math inline">\(n-1\)</span></li>
<li><span class="math inline">\(n-K\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch19mcsolution15">MC Solution 15</a>(#sec-ch19mcsolution15}</p>
<p>A BLUE estimator is:</p>
<ol type="a">
<li>Best Linear Unconditional Estimator.</li>
<li>Best Linear Unbiased Estimator.</li>
<li>Biased Linear Unconditional Estimator.</li>
<li>Best Logarithmic Unbiased Estimator.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch19mcsolution16">MC Solution 16</a>(#sec-ch19mcsolution16}</p>
<p>The Gauss-Markov Theorem states that, under certain assumptions, the OLS estimator is:</p>
<ol type="a">
<li>Always the best estimator, even among non-linear estimators.</li>
<li>BLUE (Best Linear Unbiased Estimator).</li>
<li>Biased but consistent.</li>
<li>Only unbiased if the errors are normally distributed.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch19mcsolution17">MC Solution 17</a>(#sec-ch19mcsolution17}</p>
<p>Which of the following is <em>NOT</em> an assumption of the Gauss-Markov Theorem?</p>
<ol type="a">
<li><span class="math inline">\(\mathbb{E}[\epsilon|X] = 0\)</span></li>
<li>Homoskedasticity</li>
<li>No autocorrelation</li>
<li>Normality of the error terms</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch19mcsolution18">MC Solution 18</a>(#sec-ch19mcsolution18}</p>
<p>The Cramér-Rao Lower Bound (CRLB) provides a lower bound on the:</p>
<ol type="a">
<li>Bias of any estimator.</li>
<li>Variance of any unbiased estimator.</li>
<li>Mean Squared Error of any estimator.</li>
<li>Variance of any linear estimator.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch19mcsolution19">MC Solution 19</a>(#sec-ch19mcsolution19}</p>
<p>The block diagonality of the information matrix for <span class="math inline">\((\beta, \sigma^2)\)</span> implies:</p>
<ol type="a">
<li>The OLS estimator is biased.</li>
<li>The MLEs of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> are asymptotically independent.</li>
<li>The errors are heteroskedastic.</li>
<li>The model is non-linear.</li>
</ol>
</section>
<section class="level3" id="sec-ch19mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch19mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch19mcsolution20">MC Solution 20</a>(#sec-ch19mcsolution20} If <span class="math inline">\(\text{Var}(\epsilon|X) = \Sigma(X)\)</span>, where <span class="math inline">\(\Sigma(X)\)</span> is not necessarily <span class="math inline">\(\sigma^2 I\)</span>, the variance of the OLS estimator, <span class="math inline">\(\text{Var}(\hat{\beta}|X)\)</span> is given by:</p>
<ol type="a">
<li><span class="math inline">\(\sigma^2 (X^T X)^{-1}\)</span></li>
<li><span class="math inline">\((X^TX)^{-1}X^T\Sigma(X)X(X^TX)^{-1}\)</span></li>
<li><span class="math inline">\((X^TX)^{-1}\)</span></li>
<li><span class="math inline">\(\Sigma(X)\)</span></li>
</ol>
</section>
</section>
<section class="level2" id="multiple-choice-solutions">
<h2 class="anchored" data-anchor-id="multiple-choice-solutions">Multiple Choice Solutions</h2>
<section class="level3" id="sec-ch19mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch19mcexercise1">MC Exercise 1</a></p>
<p><strong>(c) It is a linear function of the dependent variable <span class="math inline">\(y\)</span>.</strong></p>
<p><strong>Explanation:</strong></p>
<p><strong>Definition 19.1</strong> states that the OLS estimator is linear in <span class="math inline">\(y\)</span> because it can be expressed as <span class="math inline">\(\hat{\beta} = Cy\)</span>, where <span class="math inline">\(C = (X^TX)^{-1}X^T\)</span> is a matrix that depends only on <span class="math inline">\(X\)</span> (and not on <span class="math inline">\(y\)</span>). Options (a) and (d) describe the linearity of the <em>model</em>, not the estimator. Option (b) is incorrect; the estimator is not necessarily a linear function of <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="sec-ch19mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch19mcexercise2">MC Exercise 2</a></p>
<p><strong>(b) Conditionally unbiased.</strong></p>
<p><strong>Explanation:</strong></p>
<p><strong>Theorem 19.1</strong> states that under Assumption A1 (<span class="math inline">\(\mathbb{E}[\epsilon|X] = 0\)</span>), the OLS estimator is <em>conditionally</em> unbiased: <span class="math inline">\(\mathbb{E}[\hat{\beta}|X] = \beta\)</span>. This means that <em>given</em> the values of the independent variables, the expected value of the estimator equals the true parameter. Conditional unbiasedness, by the law of iterated expectations, also implies <em>unconditional</em> unbiasedness. The unbiasedness property does <em>not</em> require normality of the errors.</p>
</section>
<section class="level3" id="sec-ch19mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch19mcexercise3">MC Exercise 3</a></p>
<p><strong>(b) <span class="math inline">\(\sigma^{2}(X^{T}X)^{-1}\)</span></strong></p>
<p><strong>Explanation:</strong></p>
<p><strong>Theorem 19.2</strong> provides the general formula for the conditional variance: <span class="math inline">\((X^{T}X)^{-1}X^{T}\Sigma(X)X(X^{T}X)^{-1}\)</span>. Under <strong>homoskedasticity</strong>, <span class="math inline">\(\Sigma(X) = \sigma^2 I\)</span>, which simplifies the expression to <span class="math inline">\(\sigma^{2}(X^{T}X)^{-1}\)</span>. Option (a) is the general formula without assuming homoskedasticity. Options (c) and (d) are incorrect.</p>
</section>
<section class="level3" id="sec-ch19mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch19mcexercise4">MC Exercise 4</a></p>
<p><strong>(b) Conditional unbiasedness implies unconditional unbiasedness.</strong></p>
<p><strong>Explanation:</strong></p>
<p>This is a direct consequence of the <strong>Law of Iterated Expectations</strong>: <span class="math inline">\(\mathbb{E}[\hat{\beta}] = \mathbb{E}[\mathbb{E}[\hat{\beta}|X]]\)</span>. If <span class="math inline">\(\mathbb{E}[\hat{\beta}|X] = \beta\)</span> (conditional unbiasedness), then <span class="math inline">\(\mathbb{E}[\hat{\beta}] = \mathbb{E}[\beta] = \beta\)</span> (unconditional unbiasedness). The reverse is not necessarily true.</p>
</section>
<section class="level3" id="sec-ch19mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch19mcexercise5">MC Exercise 5</a></p>
<p><strong>(b) Conditional unbiasedness implies unconditional unbiasedness.</strong></p>
<p><strong>Explanation:</strong></p>
<p>As explained in Solution 4, the Law of Iterated Expectations is the key to showing that if an estimator is conditionally unbiased, it is also unconditionally unbiased.</p>
</section>
<section class="level3" id="sec-ch19mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch19mcexercise6">MC Exercise 6</a></p>
<p><strong>(b) Only homoskedasticity and no autocorrelation.</strong></p>
<p><strong>Explanation:</strong></p>
<p>The general formula for <span class="math inline">\(\text{Var}(\hat{\beta})\)</span> involves <span class="math inline">\(\Sigma(X)\)</span>, the variance-covariance matrix of the errors. The simplification to <span class="math inline">\(\sigma^{2}\mathbb{E}[(X^{T}X)^{-1}]\)</span> requires <span class="math inline">\(\Sigma(X) = \sigma^2 I\)</span>. This holds when the errors are <strong>homoskedastic</strong> (constant variance, <span class="math inline">\(\sigma^2\)</span>) and have <strong>no autocorrelation</strong> (errors are uncorrelated, so the off-diagonal elements of <span class="math inline">\(\Sigma(X)\)</span> are zero).</p>
</section>
<section class="level3" id="sec-ch19mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch19mcexercise7">MC Exercise 7</a></p>
<p><strong>(b) Unbiased</strong></p>
<p><strong>Explanation</strong> The condition <span class="math inline">\(\mathbb{E}[\epsilon_i|x_i] = 0\)</span> is a sufficient condition for the unbiasedness of OLS. It does not need a large n.</p>
</section>
<section class="level3" id="sec-ch19mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch19mcexercise8">MC Exercise 8</a></p>
<p><strong>(d) Orthogonal to the space spanned by <span class="math inline">\(X_2\)</span>.</strong></p>
<p><strong>Explanation:</strong></p>
<p>The matrix <span class="math inline">\(M_2 = I - X_2(X_2^TX_2)^{-1}X_2^T\)</span> is a <strong>residual maker</strong> or <strong>annihilator matrix</strong>. It projects vectors onto the space <em>orthogonal</em> to the column space of <span class="math inline">\(X_2\)</span>. This means that <span class="math inline">\(M_2y\)</span> gives the residuals of a regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(X_2\)</span>.</p>
</section>
<section class="level3" id="sec-ch19mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch19mcexercise9">MC Exercise 9</a></p>
<p><strong>(c) Normal distribution.</strong></p>
<p><strong>Explanation:</strong></p>
<p><strong>Theorem 19.3</strong> states that if the errors are normally distributed (Assumption A3), then the OLS estimator <span class="math inline">\(\hat{\beta}\)</span> is also normally distributed (conditional on <span class="math inline">\(X\)</span>). This is because <span class="math inline">\(\hat{\beta}\)</span> is a linear combination of the <span class="math inline">\(y\)</span> values, which are themselves linear combinations of the normally distributed errors.</p>
</section>
<section class="level3" id="sec-ch19mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch19mcexercise10">MC Exercise 10</a></p>
<p><strong>(c) <span class="math inline">\(K\)</span> (number of parameters).</strong></p>
<p><strong>Explanation:</strong></p>
<p><strong>Theorem 19.3</strong> states that this quadratic form follows a chi-squared distribution with degrees of freedom equal to the number of parameters in the model, <span class="math inline">\(K\)</span>. This is a fundamental result used for constructing Wald tests.</p>
</section>
<section class="level3" id="sec-ch19mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch19mcexercise11">MC Exercise 11</a></p>
<p><strong>(b) The predicted value for a specific set of independent variable values <span class="math inline">\(x\)</span>.</strong></p>
<p><strong>Explanation:</strong></p>
<p><span class="math inline">\(\hat{m}(x) = x^T\hat{\beta}\)</span> is the estimated expected value of <span class="math inline">\(y\)</span> <em>given</em> that the independent variables take on the specific values in the vector <span class="math inline">\(x\)</span>. It’s a scalar. <span class="math inline">\(\hat{m}\)</span>, on the other hand, is the vector of predicted values for the <em>original</em> sample (i.e., for each row of the <span class="math inline">\(X\)</span> matrix).</p>
</section>
<section class="level3" id="sec-ch19mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch19mcexercise12">MC Exercise 12</a></p>
<p><strong>(b) <span class="math inline">\(\dfrac{\hat{\epsilon}^{T}\hat{\epsilon}}{n}\)</span></strong></p>
<p><strong>Explanation:</strong></p>
<p>This is derived by maximizing the log-likelihood function of the normal distribution with respect to <span class="math inline">\(\sigma^2\)</span>. The resulting MLE is the sum of squared residuals divided by the number of observations, <span class="math inline">\(n\)</span>. It is, however, biased.</p>
</section>
<section class="level3" id="sec-ch19mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch19mcexercise13">MC Exercise 13</a></p>
<p><strong>(b) <span class="math inline">\(s_{*}^{2}\)</span> is unbiased.</strong></p>
<p><strong>Explanation:</strong></p>
<p><span class="math inline">\(s_{*}^{2}\)</span> divides the sum of squared residuals by <span class="math inline">\(n-K\)</span> (degrees of freedom), which corrects for the bias in the MLE. <strong>Theorem 19.5</strong> proves that <span class="math inline">\(\mathbb{E}[s_{*}^{2}] = \sigma^2\)</span>.</p>
</section>
<section class="level3" id="sec-ch19mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch19mcexercise14">MC Exercise 14</a></p>
<p><strong>(d) <span class="math inline">\(n-K\)</span></strong></p>
<p><strong>Explanation:</strong></p>
<p>The degrees of freedom represent the number of independent pieces of information remaining to estimate the variance <em>after</em> estimating the <span class="math inline">\(K\)</span> parameters in <span class="math inline">\(\beta\)</span>.</p>
</section>
<section class="level3" id="sec-ch19mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch19mcexercise15">MC Exercise 15</a></p>
<p><strong>(b) Best Linear Unbiased Estimator.</strong></p>
<p><strong>Explanation:</strong></p>
<p>BLUE stands for Best Linear Unbiased Estimator. “Best” means minimum variance within the class of linear unbiased estimators.</p>
</section>
<section class="level3" id="sec-ch19mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch19mcexercise16">MC Exercise 16</a></p>
<p><strong>(b) BLUE (Best Linear Unbiased Estimator).</strong></p>
<p><strong>Explanation:</strong></p>
<p>The <strong>Gauss-Markov Theorem (Theorem 19.6)</strong> states that under the assumptions of linearity, strict exogeneity, homoskedasticity, and no autocorrelation, the OLS estimator is the Best Linear Unbiased Estimator (BLUE). It does <em>not</em> claim that OLS is the best among <em>all</em> estimators (including non-linear ones) – that requires normality of the errors.</p>
</section>
<section class="level3" id="sec-ch19mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch19mcexercise17">MC Exercise 17</a></p>
<p><strong>(d) Normality of the error terms</strong></p>
<p><strong>Explanation:</strong></p>
<p>The Gauss-Markov Theorem <em>does not</em> require the assumption of normality of the error terms. Normality is needed for certain hypothesis tests and confidence interval constructions, but not for the OLS estimator to be BLUE. Assumptions (a), (b), and (c) <em>are</em> required for the Gauss-Markov Theorem.</p>
</section>
<section class="level3" id="sec-ch19mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch19mcexercise18">MC Exercise 18</a></p>
<p><strong>(b) Variance of any unbiased estimator.</strong></p>
<p><strong>Explanation:</strong></p>
<p>The CRLB provides a lower bound on the variance of <em>any unbiased</em> estimator. It doesn’t apply to biased estimators, and the Mean Squared Error (MSE) considers both bias and variance.</p>
</section>
<section class="level3" id="sec-ch19mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch19mcexercise19">MC Exercise 19</a></p>
<p><strong>(b) The MLEs of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> are asymptotically independent.</strong></p>
<p><strong>Explanation:</strong></p>
<p>The block diagonality of the information matrix implies that the MLEs of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> are asymptotically independent. This means that, in large samples, knowing the true value of one parameter doesn’t provide any additional information for estimating the other.</p>
</section>
<section class="level3" id="sec-ch19mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch19mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch19mcexercise20">MC Exercise 20</a></p>
<p><strong>(b) <span class="math inline">\((X^TX)^{-1}X^T\Sigma(X)X(X^TX)^{-1}\)</span></strong></p>
<p><strong>Explanation</strong> This is the general formula of the variance of the OLS estimator. It does not require constant variance of the errors.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>