<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 23: Generalized Method of Moments and Extremum Estimators – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap26.html" rel="next"/>
<link href="../chapters/chap22.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 23: Generalized Method of Moments and Extremum Estimators</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level2" id="generalized-method-moments">
<h2 class="anchored" data-anchor-id="generalized-method-moments">23.1 Generalized Method Moments</h2>
<p>We suppose that there is i.i.d. vector data <span class="math inline">\(\{Z_i\}_{i=1}^n\)</span> from some population. It is known that there exists a unique <span class="math inline">\(\theta_0 \in \mathbb{R}^p\)</span> such that</p>
<p><span class="math display">\[
\mathbb{E}[g(Z_i, \theta_0)] = 0
\qquad (23.1)
\]</span></p>
<p>for some vector of known functions <span class="math inline">\(g(\cdot)\)</span> [<span class="math inline">\(q \times 1\)</span>]. For example, the first order condition from some optimization problem for the representative agent, see below. This is a <strong>semiparametric model</strong>, because the distribution of <span class="math inline">\(Z_i\)</span> is unspecified apart from the <span class="math inline">\(q\)</span> moments, but we are only interested in the parameters <span class="math inline">\(\theta\)</span>. There are several cases:</p>
<ol type="1">
<li><span class="math inline">\(p &gt; q\)</span>: unidentified case</li>
<li><span class="math inline">\(p = q\)</span>: exactly identified case</li>
<li><span class="math inline">\(p &lt; q\)</span>: overidentified case.</li>
</ol>
<p>We next give some examples:</p>
<section class="level3" id="example-23.1.-simultaneous-equations-model.">
<h3 class="anchored" data-anchor-id="example-23.1.-simultaneous-equations-model.">Example 23.1. Simultaneous Equations Model.</h3>
<p>Suppose that we observe <span class="math inline">\(y_i \in \mathbb{R}^L\)</span> and <span class="math inline">\(x_i \in \mathbb{R}^K\)</span>, where</p>
<p><span class="math display">\[
B(\theta) y_i = C(\theta) x_i + u_i,
\]</span></p>
<p>where <span class="math inline">\(B(\theta)\)</span> is an <span class="math inline">\(L \times L\)</span> matrix and <span class="math inline">\(C(\theta)\)</span> is an <span class="math inline">\(L \times K\)</span> matrix of unknown quantities depending on unknown parameters <span class="math inline">\(\theta \in \mathbb{R}^p\)</span>, while the error term <span class="math inline">\(u_i \in \mathbb{R}^L\)</span> satisfies the conditional moment restriction</p>
<p><span class="math display">\[
\mathbb{E}(u_i | x_i) = 0.
\qquad (23.2)
\]</span></p>
<p>The parameters of interest are <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span>, which are not themselves identified unless the parameter vector <span class="math inline">\(\theta\)</span> encodes some restrictions (such as <span class="math inline">\(B_{ij} = 0\)</span>). Notice that <span class="math inline">\(\mathbb{E}(y_i|x_i) = B(\theta)^{-1} C(\theta) x_i = \Phi(\theta) x_i\)</span> provided <span class="math inline">\(B\)</span> is invertible, where <span class="math inline">\(\Phi(\theta)\)</span> is an <span class="math inline">\(L \times K\)</span> matrix, therefore, the parameters in <span class="math inline">\(\Phi\)</span> are identified provided <span class="math inline">\(\mathbb{E}(x_i x_i^\top)\)</span> is of full rank. However, it is the parameters in <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> that are fundamental to the economic interpretation and are therefore the quantities of interest. We will discuss the identification issue later. Let</p>
<p><span class="math display">\[
g(Z_i, \theta) = (B(\theta) y_i - C(\theta) x_i) \otimes h(x_i),
\]</span></p>
<p>where <span class="math inline">\(h(x_i)\)</span> is an <span class="math inline">\(M \times 1\)</span> vector of functions of <span class="math inline">\(x_i\)</span>; here, for vectors <span class="math inline">\(a \in \mathbb{R}^L\)</span> and <span class="math inline">\(b \in \mathbb{R}^M\)</span>, <span class="math inline">\(a \otimes b\)</span> means the <span class="math inline">\(L \times M\)</span> by <span class="math inline">\(1\)</span> vector <span class="math inline">\((a_1 b^\top, \dots, a_L b^\top)^\top\)</span> that contains all the cross products. This falls into the framework (23.1) with <span class="math inline">\(Z_i = (y_i^\top, x_i^\top)^\top\)</span> and <span class="math inline">\(q = L \times M\)</span>. The traditional approach here has been to assume the stronger condition that</p>
<p><span class="math display">\[
u_i \sim N(0, \Sigma(\theta))
\]</span></p>
<p>for some unknown covariance matrix <span class="math inline">\(\Sigma\)</span>, in which case, <span class="math inline">\(y_i | x_i \sim N(B(\theta)^{-1} C(\theta) x_i, B(\theta)^{-1} \Sigma(\theta) B(\theta)^{\top -1})\)</span>.</p>
</section>
<section class="level3" id="example-23.2.-hansen-and-singleton-1982">
<h3 class="anchored" data-anchor-id="example-23.2.-hansen-and-singleton-1982">Example 23.2. (Hansen and Singleton, 1982)</h3>
<p>One of the most influential econometric papers of the 1980s. Intertemporal consumption/Investment decision: <span class="math inline">\(c_i\)</span> consumption <span class="math inline">\(u(\cdot)\)</span> utility <span class="math inline">\(u_c &gt; 0\)</span>, <span class="math inline">\(u_{cc} &lt; 0\)</span>, <span class="math inline">\(1 + r_{j,i+1}\)</span>, <span class="math inline">\(j = 1, \dots, m\)</span> is gross return on asset <span class="math inline">\(j\)</span> at time <span class="math inline">\(i+1\)</span>. The representative agent solves the following optimization problem</p>
<p><span class="math display">\[
\max_{\{c_i, w_i\}_{i=0}^\infty} \sum_{t=0}^\infty \beta^t \mathbb{E}[u(c_{i+t})|I_i],
\]</span></p>
<p>where <span class="math inline">\(w_i\)</span> is a vector of portfolio weights and <span class="math inline">\(\beta\)</span> is the discount factor. This is a dynamic programming problem. We assume that there is a unique interior solution; this is characterized by the following condition</p>
<p><span class="math display">\[
u'(c_i) = \beta \mathbb{E}[(1 + r_{j,i+1}) u'(c_{i+1})|I_i], \quad j = 1, \dots, m.
\]</span></p>
<p>Now suppose that</p>
<p><span class="math display">\[
u(c_i) =
\begin{cases}
\dfrac{c_i^{1-\gamma}}{1-\gamma} &amp;\text{if } \gamma &gt; 0, \gamma \neq 1, \\
\log c_i &amp;\text{if } \gamma = 1.
\end{cases}
\]</span></p>
<p>Here, <span class="math inline">\(\gamma\)</span> is the coefficient of relative risk aversion. Then</p>
<p><span class="math display">\[
c_i^{-\gamma} = \beta \mathbb{E}[(1 + r_{j,i+1}) c_{i+1}^{-\gamma} | I_i].
\]</span> Rearranging we get <span class="math display">\[
\mathbb{E}\left[ \left\{ 1- \beta (1+r_{j, i+1}) \left( \frac{c_{i+1}}{c_i}\right)^{-\gamma} \right\} \Bigg| I_i^* \right] = 0, \; j=1,\dots,m
\]</span></p>
<p>where <span class="math inline">\(I_i^* \subset I_i\)</span> and <span class="math inline">\(I_i^*\)</span> is the econometrician’s information set. We want to estimate the parameters and test the theory given a dataset consisting of <span class="math inline">\(c_i,r_{j,i+1},I_i^*\)</span>. Let <span class="math inline">\(\theta_{p x 1}=(\beta, \gamma)'\)</span> and define: <span class="math display">\[
g(Z_i, \theta) = \begin{bmatrix}
\vdots \\
\left\{1-\beta(1+r_{j,i+1})\left(\frac{c_{i+1}}{c_i} \right)^{-\gamma} \right\} v_i \\
\vdots
\end{bmatrix}_{q \times 1}
\]</span> where <span class="math inline">\(v_i \in I_i^*\)</span> and <span class="math inline">\(Z_i=(v_i, c_i, c_{i+1}, r_{1,i+1}, \dots, r_{m, i+1})'\)</span>.</p>
<p>For any <span class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}^p\)</span>, let <span class="math display">\[
G_n(\theta)=\frac{1}{n} \sum_{i=1}^n g(Z_i, \theta) \in \mathbb{R}^q.
\]</span> Here, <span class="math inline">\(\Theta\)</span> is the parameter space or the set of allowable parameters. In the exactly identified case where <span class="math inline">\(q=p\)</span>, we can hopefully solve the <span class="math inline">\(p\)</span> equations in <span class="math inline">\(p\)</span>-unknowns: <span class="math inline">\(G_n(\theta)=0\)</span> exactly, possibly using some numerical methods. However, in the overidentified case when <span class="math inline">\(p&lt;q\)</span>, this will not be possible because we cannot simultaneously zero <span class="math inline">\(q\)</span> functions with <span class="math inline">\(p\)</span> controls. Define <span class="math display">\[
Q_n(\theta)=G_n(\theta)'W_n(\theta)G_n(\theta)=||G_n(\theta)||^2_{W_n}
\]</span> where <span class="math inline">\(W_n(\theta)\)</span> is a <span class="math inline">\(q \times q\)</span> positive definite weighting matrix, and <span class="math inline">\(||x||_A^2=x'Ax\)</span>. For example, <span class="math inline">\(W_n(\theta)=I_{q \times q}\)</span>. Then let <span class="math inline">\(\hat{\theta}_{GMM}\)</span> minimize <span class="math inline">\(Q_n(\theta)\)</span> over <span class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}^p\)</span>. This defines a large class of estimators, one for each weighting matrix <span class="math inline">\(W_n\)</span>. It is generally a nonlinear optimization problem like maximum likelihood; various techniques are available for finding the minimizer.</p>
<p>An alternative approach to using overidentifying information is to combine the estimating equations, that is let <span class="math display">\[
G_n^*(\theta)=A_n(\theta)G_n(\theta) \in \mathbb{R}^p, \qquad (23.3)
\]</span> where <span class="math inline">\(A_n(\theta)\)</span> is a full rank deterministic <span class="math inline">\(p \times q\)</span> matrix. Then find the value of <span class="math inline">\(\theta \in \Theta \subseteq \mathbb{R}^p\)</span> that solves <span class="math inline">\(G_n^*(\theta)=0\)</span>. The two approaches are broadly equivalent: for a given choice of <span class="math inline">\(W_n\)</span> there is an equivalent choice of <span class="math inline">\(A_n\)</span> and vice versa.</p>
</section>
</section>
<section class="level2" id="asymptotic-properties-of-extremum-estimators">
<h2 class="anchored" data-anchor-id="asymptotic-properties-of-extremum-estimators">23.2 Asymptotic Properties of Extremum Estimators</h2>
<p>We consider the asymptotic properties of a general class of estimators <span class="math inline">\(\hat{\theta}\)</span> that minimize <span class="math display">\[
Q_n(\theta) \text{ over } \theta \in \Theta \subseteq \mathbb{R}^p \qquad (23.4)
\]</span> for some general objective function <span class="math inline">\(Q_n(\theta)\)</span> that depends on the data. This includes GMM and Maximum Likelihood as special cases. The difficult part is consistency because in general the objective function is nonlinear in the parameters and does not have a closed form solution.</p>
<section class="level3" id="consistency">
<h3 class="anchored" data-anchor-id="consistency">23.2.1 Consistency</h3>
<p>There are many treatments of the asymptotic properties of extremum estimators, we give a standard version that rules out discontinuous criterion functions; there are versions that allow for discontinuity in <span class="math inline">\(\theta\)</span>, see below.</p>
<p>What we do is construct a sequence of functions, <span class="math inline">\(\{ Q_n(\theta) \}\)</span>, which for each finite <span class="math inline">\(n\)</span> have some distribution, but which “converge to” <span class="math inline">\(Q(\theta)\)</span> in some sense as <span class="math inline">\(n\)</span> grows large. If that sense is strong enough, then for <span class="math inline">\(n\)</span> sufficiently large, the value of <span class="math inline">\(\theta\)</span> that minimizes <span class="math inline">\(Q_n\)</span> will be the value that minimizes <span class="math inline">\(Q\)</span>. What do we need for this logic to make sense?</p>
<ol type="1">
<li><p><strong>Identification Condition.</strong> The first thing to note is that there must be only one “minimum” of <span class="math inline">\(Q(\theta)\)</span>. More formally what we will require is that for every <span class="math inline">\(\theta\)</span> different from <span class="math inline">\(\theta_0\)</span> (i.e., provided <span class="math inline">\(||\theta - \theta_0|| \geq \delta &gt; 0, Q(\theta) - Q(\theta_0) \geq \epsilon(\delta) &gt; 0\)</span>). If this were not true then there would be two distinct <span class="math inline">\(\theta\)</span>’s that would minimize the objective function, and we have no way of distinguishing which one is the true <span class="math inline">\(\theta_0\)</span> (alternatively there is no way of knowing whether the computational algorithm stops at the right one). Consequently this is a condition we will have to impose on the problem. We will refer to it as the <strong>identification condition</strong> for the nonlinear model. Of course, if there is another way to choose between different <span class="math inline">\(\theta\)</span>’s, then the model could be identified even if this condition is not satisfied for <span class="math inline">\(Q\)</span>.</p></li>
<li><p><strong>Convergence.</strong> The second point here is that for the logic to make sense the convergence must be uniform over <span class="math inline">\(\theta \in \Theta\)</span>, i.e., <span class="math inline">\(\sup_{\theta \in \Theta} |Q_n(\theta) - Q(\theta)| = O_p(1)\)</span>. If this were not the case then, even if the identification condition were met, we could go to some <span class="math inline">\(\theta\)</span> different from <span class="math inline">\(\theta_0\)</span>, say <span class="math inline">\(\theta_*\)</span> and find that <span class="math inline">\(Q_n(\theta)\)</span> hovers about <span class="math inline">\(Q_n(\theta_0)\)</span> as <span class="math inline">\(\theta\)</span> circles around that <span class="math inline">\(\theta_*\)</span> even though <span class="math inline">\(Q(\theta_*) - Q(\theta_0) &gt; \delta &gt; 0\)</span>. Since at any fixed <span class="math inline">\(\theta\)</span>, say <span class="math inline">\(\theta_1\)</span>, <span class="math inline">\(G_n\)</span> is just a sample mean of mean zero i.i.d. deviates, a standard LLN establishes that <span class="math inline">\(||Q_n(\theta_*) - Q(\theta_*)|| = o_p(1)\)</span>. What we need for consistency is the stronger property that <span class="math inline">\(\sup_{\theta \in \Theta} |Q_n(\theta) - Q(\theta)| = o_p(1)\)</span>, i.e., a “uniform law of large numbers”.</p></li>
</ol>
<p>These two properties, that is the ULLN and the identification condition seem to be the only properties used in the intuition underlying the consistency of the estimator. Indeed, as we show now they are more than enough to prove consistency. Note that neither of these properties have anything directly to do with smoothness of the objective function; i.e., of <span class="math inline">\(Q_n(\cdot)\)</span>. So using them certainly does not rule out estimators based on objective functions that are not differentiable.</p>
</section>
<section class="level3" id="theorem-23.1.-consistency.">
<h3 class="anchored" data-anchor-id="theorem-23.1.-consistency.">Theorem 23.1. (Consistency).</h3>
<p>Suppose that the following conditions hold:</p>
<ol type="A">
<li><p>The parameter space <span class="math inline">\(\Theta\)</span> is a compact subset of Euclidean <span class="math inline">\(p\)</span>-space.</p></li>
<li><p><span class="math inline">\(Q_n(\theta)\)</span> is continuous in <span class="math inline">\(\theta \in \Theta\)</span> for all possible samples and is a measurable function of the data for all <span class="math inline">\(\theta \in \Theta\)</span>.</p></li>
<li><p>There exists a nonstochastic function <span class="math inline">\(Q(\theta)\)</span> such that <span class="math display">\[
\sup_{\theta \in \Theta} |Q_n(\theta) - Q(\theta)| \overset{P}{\to} 0, \qquad (23.5)
\]</span> as <span class="math inline">\(n \to \infty\)</span>.</p></li>
<li><p>The limit <span class="math inline">\(Q(\theta)\)</span> achieves a unique global minimum at <span class="math inline">\(\theta = \theta_0\)</span>.</p></li>
</ol>
<p>Then, <span class="math display">\[
\hat{\theta} \overset{P}{\to} \theta_0.
\]</span></p>
<p><strong>Proof.</strong> Conditions (A) and (B) guarantee that there exists a minimizer <span class="math inline">\(\hat{\theta}\)</span>. From (D), if <span class="math inline">\(||\theta - \theta_0|| &gt; \delta\)</span>, then there is an <span class="math inline">\(\epsilon(\delta) &gt; 0\)</span> such that <span class="math inline">\(Q(\theta) - Q(\theta_0) \geq \epsilon(\delta)\)</span>. Consequently, <span class="math display">\[
\Pr(||\hat{\theta} - \theta_0|| &gt; \delta) \leq \Pr(Q(\hat{\theta}) - Q(\theta_0) \geq \epsilon(\delta)),
\]</span> and it is sufficient to prove that for any <span class="math inline">\(\epsilon(\delta) &gt; 0\)</span>, the latter probability goes to zero. By adding and subtracting terms we obtain <span class="math display">\[
\begin{aligned}
Q(\hat{\theta}) - Q(\theta_0) &amp;= Q(\hat{\theta}) - Q_n(\hat{\theta}) + Q_n(\hat{\theta}) - Q_n(\theta_0) + Q_n(\theta_0) - Q(\theta_0) \\
&amp;= I + II + III.
\end{aligned}
\]</span> By the fact that <span class="math inline">\(\hat{\theta} \in \Theta\)</span>, we can bound the first and third times in absolute value by the left hand side of (23.5), i.e., <span class="math display">\[
|I|, |III| \leq \sup_{\theta \in \Theta} |Q(\theta) - Q_n(\theta)| \overset{P}{\to} 0,
\]</span> where the convergence to zero is assumed in (C). Then by the definition of the estimator, <span class="math inline">\(II \leq 0\)</span>. Together this implies that <span class="math inline">\(\Pr(Q(\hat{\theta}) - Q(\theta_0) \geq \epsilon(\delta)) \to 0\)</span>. <span class="math inline">\(\square\)</span></p>
<p>Condition (A) that the parameter space is compact (closed and bounded such as an interval) is not needed in linear regression since we automatically, under full rank condition, have the existence of an estimator, but a general criterion may not have a minimizer over the whole of <span class="math inline">\(\mathbb{R}^p\)</span>.</p>
</section>
<section class="level3" id="uniformity">
<h3 class="anchored" data-anchor-id="uniformity">23.2.1.1 Uniformity</h3>
<p>The <strong>Uniform Law of Large Numbers (ULLN)</strong> condition in (C) is necessary for uncountable parameter spaces, as the following example illustrates.</p>
</section>
<section class="level3" id="example-23.3.">
<h3 class="anchored" data-anchor-id="example-23.3.">Example 23.3.</h3>
<p>Consider the following family of functions <span class="math inline">\(\{Q_n(\cdot), n=1,2,\dots \}\)</span>, where <span class="math display">\[
Q_n(\theta) = \begin{cases}
    \dfrac{\theta^2}{\theta^2 + (1-\theta)^2} &amp; 0 \leq \theta &lt; 1 \\
    1/2 &amp; \theta = 1.
\end{cases}
\]</span></p>
<p>Then, <span class="math inline">\(|Q_n(\theta)| \leq 1\)</span> and <span class="math display">\[
\lim_{n \to \infty} |Q_n(\theta) - Q(\theta)| = 0
\]</span> for all fixed <span class="math inline">\(\theta \in [0,1]\)</span>, where <span class="math inline">\(Q(\theta) = 0\)</span> for all <span class="math inline">\(\theta\)</span> with <span class="math inline">\(0 \leq \theta &lt; 1\)</span> and <span class="math inline">\(Q(1) = 1/2\)</span>. However, <span class="math display">\[
\lim_{n \to \infty} \sup_{0 \leq \theta \leq 1} |Q_n(\theta) - Q(\theta)| \neq 0.
\]</span> Furthermore, <span class="math display">\[
Q_n\left( \frac{1}{n} \right) = 1 \text{ for all } n.
\]</span> Thus the maximizing value of <span class="math inline">\(Q_n\)</span> [<span class="math inline">\(\theta_n = 1/n\)</span>] converges to <span class="math inline">\(0\)</span>, while the maximizing value of <span class="math inline">\(Q\)</span> is achieved at <span class="math inline">\(\theta=1\)</span>.</p>
<p>The ULLN condition is often satisfied because <span class="math inline">\(Q_n(\theta)\)</span> is typically a function of a sample average, i.e., of the form <span class="math inline">\(f\left( n^{-1} \sum_{i=1}^n q_i(\theta) \right)\)</span>, where <span class="math inline">\(q_i\)</span> depends only on the <span class="math inline">\(i\)</span>th observation. When the data are independent across <span class="math inline">\(i\)</span>, or at least only weakly dependent, many results can be applied to verify the required convergence. Andrews (1994) gives suitable conditions for uniform law of large numbers to hold for sample averages <span class="math inline">\(n^{-1} \sum_{i=1}^n q_i(\theta)\)</span>. Bernstein’s inequality or its variants is often a key tool in establishing these results. In some cases, simple arguments apply</p>
</section>
<section class="level3" id="example-23.4.-normal-linear-regression-model.">
<h3 class="anchored" data-anchor-id="example-23.4.-normal-linear-regression-model.">Example 23.4. [Normal linear regression model.]</h3>
<p>Suppose that <span class="math inline">\(Q_n(\beta) = -n^{-1} \sum_{i=1}^n (y_i - \beta^\top x_i)^2\)</span>. Then</p>
<p><span class="math display">\[
\begin{aligned}
-Q_n(\beta) &amp;= \frac{1}{n} \sum_{i=1}^n \{\epsilon_i - (\beta - \beta_0)^\top x_i \}^2 \\
&amp;= \frac{1}{n} \sum_{i=1}^n \epsilon_i^2 + (\beta - \beta_0)^\top \frac{1}{n} \sum_{i=1}^n x_i x_i^\top (\beta - \beta_0) + 2 (\beta - \beta_0)^\top \frac{1}{n} \sum_{i=1}^n \epsilon_i x_i.
\end{aligned}
\]</span></p>
<p>But <span class="math inline">\(n^{-1} \sum_{i=1}^n \epsilon_i x_i \overset{P}{\to} 0\)</span>, <span class="math inline">\(n^{-1} \sum_{i=1}^n x_i x_i^\top \overset{P}{\to} M = \mathbb{E} x_i x_i^\top\)</span>, and <span class="math inline">\(n^{-1} \sum_{i=1}^n \epsilon_i^2 \overset{P}{\to} \sigma^2\)</span>. Therefore,</p>
<p><span class="math display">\[
Q_n(\beta) \overset{P}{\to} \sigma^2 + (\beta - \beta_0)^\top M (\beta - \beta_0) = Q(\beta).
\]</span></p>
<p>The convergence is uniform over <span class="math inline">\(\beta \in B\)</span>, where <span class="math inline">\(B\)</span> is a compact set because for example (take <span class="math inline">\(K=1\)</span> and <span class="math inline">\(B=[-b,b]\)</span>)</p>
<p><span class="math display">\[
\sup_{\beta \in B} (\beta - \beta_0)^\top \frac{1}{n} \sum_{i=1}^n \epsilon_i x_i \leq \sup_{\beta \in B} |\beta - \beta_0| \times \left| \frac{1}{n} \sum_{i=1}^n \epsilon_i x_i \right|
\leq 2b \times \left| \frac{1}{n} \sum_{i=1}^n \epsilon_i x_i \right| \overset{P}{\to} 0.
\]</span></p>
<p>We next give a classic ULLN result.</p>
</section>
<section class="level3" id="theorem-23.2.-glivenko-1933-cantelli-1933">
<h3 class="anchored" data-anchor-id="theorem-23.2.-glivenko-1933-cantelli-1933">Theorem 23.2. (Glivenko, 1933; Cantelli, 1933)</h3>
<p><span class="math display">\[
\sup_{x \in \mathbb{R}} |F_n(x) - F(x)| \overset{P}{\to} 0.
\]</span></p>
<p><strong>Proof.</strong> We assume for simplicity that <span class="math inline">\(F\)</span> is continuous and strictly monotonic. Let <span class="math inline">\(x_{jk}\)</span> be the value of <span class="math inline">\(x\)</span> that satisfies <span class="math inline">\(F(x_{jk}) = j/k\)</span> for integer <span class="math inline">\(j, k\)</span> with <span class="math inline">\(j \leq k\)</span>. For any <span class="math inline">\(x\)</span> between <span class="math inline">\(x_{jk}\)</span> and <span class="math inline">\(x_{j+1,k}\)</span>,</p>
<p><span class="math display">\[
F(x_{jk}) \leq F(x) \leq F(x_{j+1,k}); \quad F_n(x_{jk}) \leq F_n(x) \leq F_n(x_{j+1,k}),
\]</span></p>
<p>while <span class="math inline">\(0 \leq F(x_{j+1,k}) - F(x_{jk}) \leq 1/k\)</span>, so that</p>
<p><span class="math display">\[
\begin{aligned}
F_n(x) - F(x) &amp;\leq F_n(x_{j+1,k}) - F(x_{jk}) \leq F_n(x_{j+1,k}) - F(x_{j+1,k}) + \frac{1}{k} \\
F_n(x) - F(x) &amp;\geq F_n(x_{j,k}) - F(x_{j+1,k}) \geq F_n(x_{j,k}) - F(x_{j,k}) - \frac{1}{k}.
\end{aligned}
\]</span></p>
<p>Therefore, for any <span class="math inline">\(x\)</span> and <span class="math inline">\(k\)</span>,</p>
<p><span class="math display">\[
|F_n(x) - F(x)| \leq \max_{1 \leq j \leq k} |F_n(x_{jk}) - F(x_{jk})| + \frac{1}{k} \qquad (23.6)
\]</span></p>
<p>Since the right hand side of (23.6) does not depend on <span class="math inline">\(x\)</span>, we can replace the left hand side by <span class="math inline">\(\sup_{-\infty &lt; x &lt; \infty} |F_n(x) - F(x)|\)</span>.</p>
<p>Let <span class="math inline">\(\epsilon &gt; 0\)</span> be fixed and take <span class="math inline">\(k = \lceil 1/2\epsilon \rceil\)</span>. It suffices to show that <span class="math inline">\(\max_{1 \leq j \leq k} |F_n(x_{jk}) - F(x_{jk})| \overset{P}{\to} 0\)</span>. Now let <span class="math inline">\(A_{jk}(\epsilon) = \{\omega : |F_n(x_{jk}) - F(x_{jk})| \geq \epsilon \}\)</span> and</p>
<p><span class="math display">\[
A_k(\epsilon) = \bigcup_{j=1}^k A_{jk}(\epsilon) = \left\{ \omega : \max_{1 \leq j \leq k} |F_n(x_{jk}) - F(x_{jk})| \geq \epsilon \right\}.
\]</span></p>
<p>We have for any <span class="math inline">\(\delta &gt; 0\)</span> there exists <span class="math inline">\(n_j\)</span> such that for all <span class="math inline">\(n \geq n_j\)</span>, <span class="math inline">\(\Pr(A_{jk}(\epsilon)) \leq \delta\)</span>. Therefore,</p>
<p><span class="math display">\[
\Pr \left( \max_{1 \leq j \leq k} |F_n(x_{jk}) - F(x_{jk})| \geq \epsilon \right) = \Pr(A_k(\epsilon)) \leq k \delta
\]</span></p>
<p>for all <span class="math inline">\(n \geq \max_{1 \leq j \leq k} n_j\)</span>. It follows that for any <span class="math inline">\(\delta'\)</span> we can find <span class="math inline">\(\delta\)</span> such that <span class="math inline">\(\delta' = k \delta\)</span> and <span class="math inline">\(n'\)</span> such that for all <span class="math inline">\(n &gt; n'\)</span></p>
<p><span class="math display">\[
\Pr \left( \max_{1 \leq j \leq k} |F_n(x_{jk}) - F(x_{jk})| \geq \epsilon \right) \leq \delta.
\]</span></p>
<p>The result follows.</p>
<p>An alternative proof. We take <span class="math inline">\(k = \log n\)</span>, which ensures that</p>
<p><span class="math display">\[
\sup_{x \in \mathbb{R}} |F_n(x) - F(x)| \leq \max_{1 \leq j \leq k} |F_n(x_{jk}) - F(x_{jk})| + o(1)
\]</span></p>
<p>as <span class="math inline">\(n \to \infty\)</span>. For any <span class="math inline">\(\epsilon &gt; 0\)</span>, let <span class="math inline">\(A_j = \{|F_n(x_{jk}) - F(x_{jk})| &gt; \epsilon - 1/k \}\)</span> and</p>
<p><span class="math display">\[
A = \bigcup_{j=1}^k A_j = \left\{ \max_{1 \leq j \leq k} |F_n(x_{jk}) - F(x_{jk})| &gt; \epsilon - \frac{1}{k} \right\}.
\]</span></p>
<p>Then for large enough <span class="math inline">\(n\)</span>, <span class="math inline">\(\epsilon - 1/k &gt; \epsilon/2\)</span> and</p>
<p><span class="math display">\[
\begin{aligned}
\Pr(A) &amp;\leq \sum_{j=1}^k \Pr(A_j) \\
&amp;\leq \sum_{j=1}^k \frac{4 \mathbb{E}(F_n(x_{jk}) - F(x_{jk}))^2}{\epsilon^2}
\end{aligned}
\]</span></p>
<p><span class="math display">\[
\begin{aligned}
&amp;= \sum_{j=1}^k \frac{4F(x_{jk})(1 - F(x_{jk}))}{\epsilon^2 n} \\
&amp;\leq \frac{\log n}{\epsilon^2 n},
\end{aligned}
\]</span></p>
<p>because <span class="math inline">\(F(x)(1 - F(x)) \leq 1/4\)</span> for all <span class="math inline">\(x\)</span>. The first inequality follows by the Bonferroni inequality, while the second one uses the Chebychev or Markov inequality. Now <span class="math inline">\(\log n / n \to 0\)</span> so that <span class="math inline">\(\Pr(A) \to 0\)</span> as <span class="math inline">\(n \to \infty\)</span>, which implies the result. <span class="math inline">\(\square\)</span></p>
<p>The asymptotic properties of the empirical c.d.f. have been established in 1933 in two separate papers by Glivenko and Cantelli; in fact they showed the strong law version (convergence with probability one). The only ‘condition’ in their result is that <span class="math inline">\(X_i\)</span> are i.i.d., although note that since <span class="math inline">\(F\)</span> is a distribution function it has at most a countable number of discontinuities, is bounded between zero and one and is right continuous. Note also that the supremum is over a non-compact set – in much subsequent work generalizing this theorem it has been necessary to restrict attention to compact sets. The proof of this theorem exploits some special structure: specifically that for each <span class="math inline">\(x\)</span>, <span class="math inline">\(\mathbb{1}(X_i \leq x)\)</span> is Bernoulli with probability <span class="math inline">\(F(x)\)</span>. This proof is very special and uses the structure of the empirical c.d.f. quite a lot. Much work has gone into establish ULLN’s for more general settings.</p>
</section>
<section class="level3" id="identification">
<h3 class="anchored" data-anchor-id="identification">23.2.1.2 Identification</h3>
<p>Often more difficult to establish is condition (D). For the MLE the following lemma is available.</p>
</section>
<section class="level3" id="lemma-23.1.">
<h3 class="anchored" data-anchor-id="lemma-23.1.">Lemma 23.1.</h3>
<p>Suppose that <span class="math inline">\(\theta_0\)</span> is identified, i.e., for any <span class="math inline">\(\theta \in \Theta\)</span> with <span class="math inline">\(\theta \neq \theta_0\)</span> we have with positive probability <span class="math inline">\(f(X|\theta) \neq f(X|\theta_0)\)</span>. Suppose also that for all <span class="math inline">\(\theta \in \Theta\)</span>, <span class="math inline">\(\mathbb{E}\{ |\ln f(X|\theta)| \} &lt; \infty\)</span>. Then, <span class="math inline">\(Q(\theta) = \mathbb{E} \{\ln f(X|\theta) \}\)</span> has a unique maximum at <span class="math inline">\(\theta_0\)</span>.</p>
<p><strong>Proof.</strong> For all <span class="math inline">\(\theta \neq \theta_0\)</span>,</p>
<p><span class="math display">\[
Q(\theta_0) - Q(\theta) = \mathbb{E} \left[ -\ln \frac{f(X|\theta)}{f(X|\theta_0)} \right] &gt; -\ln \left[ \mathbb{E} \frac{f(X|\theta)}{f(X|\theta_0)} \right] = 0,
\]</span></p>
<p>by Jensen’s inequality. <span class="math inline">\(\square\)</span></p>
</section>
<section class="level3" id="example-23.5.-normal-linear-regression-model.">
<h3 class="anchored" data-anchor-id="example-23.5.-normal-linear-regression-model.">Example 23.5. [Normal linear regression model.]</h3>
<p>Suppose that <span class="math inline">\(Q_n(\beta) = -n^{-1} \sum_{i=1}^n (y_i - \beta^\top x_i)^2\)</span>. Then we have</p>
<p><span class="math display">\[
Q(\beta) = \sigma^2 + (\beta - \beta_0)^\top M (\beta - \beta_0).
\]</span></p>
<p>Provided <span class="math inline">\(M &gt; 0\)</span>, the function <span class="math inline">\(Q(\beta)\)</span> is uniquely minimized at <span class="math inline">\(\beta = \beta_0\)</span>.</p>
</section>
<section class="level3" id="example-23.6.-the-simultaneous-equation-system.">
<h3 class="anchored" data-anchor-id="example-23.6.-the-simultaneous-equation-system.">Example 23.6. The simultaneous Equation system.</h3>
<p>The normalized negative likelihood function is</p>
<p><span class="math display">\[
Q_n(\theta) = \log \det(B(\theta)) + \frac{1}{2n} \sum_{i=1}^n (B(\theta)y_i - C(\theta)x_i)^\top \Sigma(\theta)^{-1} (B(\theta)y_i - C(\theta)x_i).
\]</span></p>
<p>Under the assumption that <span class="math inline">\(\mathbb{E}(y_i|x_i) = \Phi(\theta_0)x_i\)</span> and <span class="math inline">\(\text{var}(y_i|x_i) = B(\theta_0)^{-1} \Sigma(\theta_0) B(\theta_0)^{\top -1}\)</span> for some <span class="math inline">\(\theta_0 \in \Theta\)</span>, <span class="math inline">\(Q_n(\theta)\)</span> has the probability limit</p>
<p><span class="math display">\[
\begin{aligned}
Q(\theta) &amp;= \log \det(B(\theta)) + \frac{1}{2} \text{tr} \left( M (C(\theta_0) - C(\theta))^\top \Sigma(\theta)^{-1} (C(\theta_0) - C(\theta)) \right) \\
&amp;+ \frac{1}{2} \text{tr} \left( B(\theta_0)^{-1} \Sigma(\theta_0) B(\theta_0)^{\top -1} B(\theta)^\top \Sigma(\theta)^{-1} B(\theta) \right)
\end{aligned}
\]</span></p>
<p>for each <span class="math inline">\(\theta \in \Theta\)</span>. We have</p>
<p><span class="math display">\[
Q(\theta_0) = \log \det(B(\theta_0)) + \frac{L}{2}.
\]</span></p>
<p>The question is, whether there exists any other <span class="math inline">\(\theta \in \Theta\)</span> such that <span class="math inline">\(Q(\theta) = Q(\theta_0)\)</span>. In the case where <span class="math inline">\(\theta \in \mathbb{R}^{L^2 + KL + L(L+1)/2}\)</span> is unrestricted, then the answer is positive, meaning the model is not identified. This is because the triple <span class="math inline">\(B^* = FB\)</span>, <span class="math inline">\(C^* = FC\)</span>, and <span class="math inline">\(\Sigma^* = F \Sigma F^\top\)</span>, where <span class="math inline">\(F\)</span> is any nonsingular <span class="math inline">\(L \times L\)</span> matrix, will yield exactly the same <span class="math inline">\(Q(\theta)\)</span> because</p>
<p><span class="math display">\[
\begin{aligned}
B^{*-1} C^* &amp;= B^{-1} F^{-1} F C = B^{-1} C \\
B^{*-1} \Sigma^* B^{* \top -1} &amp;= B^{-1} F^{-1} F \Sigma F^\top F^{\top -1} B^{\top -1} = B^{-1} \Sigma B^{\top -1}.
\end{aligned}
\]</span></p>
</section>
<section class="level3" id="asymptotic-normality">
<h3 class="anchored" data-anchor-id="asymptotic-normality">23.2.2 Asymptotic Normality</h3>
<p>Once we have consistency we can confine ourselves to “local conditions” to prove subsequent limit properties of the estimator. That is if we now can prove that provided that any <span class="math inline">\(\hat{\theta}\)</span> that is eventually within a <span class="math inline">\(\delta\)</span> neighbourhood of <span class="math inline">\(\theta_0\)</span> will have a particular property, then our estimator will have that property with probability tending to one (since our estimator will be in that neighbourhood with probability tending to one). This allows us to focus in on conditions on <span class="math inline">\(Q_n\)</span> and <span class="math inline">\(Q\)</span> in a neighbourhood of <span class="math inline">\(\theta_0\)</span>, and ignore entirely the behaviour of these functions outside of this neighbourhood. This is in distinct contrast to consistency, which is generally thought of as a “global” property; it depends on the properties of <span class="math inline">\(Q_n\)</span> and <span class="math inline">\(Q\)</span> over all of <span class="math inline">\(\Theta\)</span>. That is the conditions we need for consistency are global, but once we have consistency, the additional conditions we need for asymptotic normality are local. The literature often goes one step further than this in its discussion of local properties.</p>
</section>
<section class="level3" id="theorem-23.3.">
<h3 class="anchored" data-anchor-id="theorem-23.3.">Theorem 23.3.</h3>
<p>Suppose that <span class="math inline">\(\hat{\theta} \overset{P}{\to} \theta_0\)</span>. Then there exists a sequence <span class="math inline">\(\{\delta_n\}\)</span> with <span class="math inline">\(\delta_n \to 0\)</span>, such that</p>
<p><span class="math display">\[
\lim_{n \to \infty} \Pr(||\hat{\theta} - \theta_0|| &gt; \delta_n) = 0.
\]</span></p>
<p><strong>Proof.</strong> Consistency implies that for all <span class="math inline">\(\epsilon &gt; 0\)</span> and for all positive integers <span class="math inline">\(J\)</span>, there exists a positive integer <span class="math inline">\(n_0(J)\)</span> such that for all <span class="math inline">\(n \geq n_0(J)\)</span></p>
<p><span class="math display">\[
\Pr(||\hat{\theta} - \theta_0|| &gt; 1/J) \leq \epsilon.
\]</span></p>
<p>For every <span class="math inline">\(J\)</span>, let <span class="math inline">\(n^*(J)\)</span> be the smallest value of <span class="math inline">\(n\)</span> that satisfies this condition, so that <span class="math inline">\(n^*(J)\)</span> is an increasing sequence. Then set</p>
<p><span class="math display">\[
\delta_n = 1/J \text{ for } n^*(J) &lt; n \leq n^*(J+1).
\]</span></p>
<p>Clearly, <span class="math inline">\(\lim_{n \to \infty} \delta_n = 0\)</span>. Therefore, by construction, for all <span class="math inline">\(n \geq n^*(1/\delta_n)\)</span></p>
<p><span class="math display">\[
\Pr(||\hat{\theta} - \theta_0|| &gt; \delta_n) \leq \epsilon. \qquad \square
\]</span></p>
<p>The sequence <span class="math inline">\(\delta_n\)</span> can go to zero arbitrarily slowly depending on the case. The discussion of subsequent properties can confine itself to conditions that need only hold in “shrinking neighbourhoods” of <span class="math inline">\(\theta_0\)</span>; i.e., neighbourhoods of <span class="math inline">\(\theta_0\)</span> that can get arbitrarily small as <span class="math inline">\(n\)</span> grows large, and still we know that our estimator will have that property with probability tending to one. Define the event <span class="math inline">\(\Theta_n = \{||\hat{\theta} - \theta_0|| \leq \delta_n \}\)</span>. Then, for any event <span class="math inline">\(A\)</span>, we have</p>
<p><span class="math display">\[
\Pr(A) = \Pr(A \cap \Theta_n) + \Pr(A \cap \Theta_n^c) \leq \Pr(A \cap \Theta_n) + \Pr(\Theta_n^c) \leq \Pr(A \cap \Theta_n) + \epsilon.
\]</span></p>
<p>Since <span class="math inline">\(\epsilon\)</span> is arbitrary, we can effectively assume that <span class="math inline">\(\Theta_n\)</span> is true.</p>
<p>We next consider the asymptotic distribution of the optimization estimator.</p>
</section>
<section class="level3" id="theorem-23.4.-asymptotic-normality.">
<h3 class="anchored" data-anchor-id="theorem-23.4.-asymptotic-normality.">Theorem 23.4. (Asymptotic Normality).</h3>
<p>Suppose that the following conditions hold:</p>
<ol type="A">
<li><p><span class="math inline">\(\hat{\theta} \overset{P}{\to} \theta_0\)</span>;</p></li>
<li><p><span class="math inline">\(\theta_0\)</span> is an interior point of <span class="math inline">\(\Theta\)</span></p></li>
<li><p><span class="math inline">\(\dfrac{\partial^2 Q_n}{\partial \theta \partial \theta^\top}(\theta)\)</span> exists and is continuous in an open convex neighbourhood of <span class="math inline">\(\theta_0\)</span>.</p></li>
<li><p>There exists a finite nonsingular matrix <span class="math inline">\(A\)</span>, such that</p></li>
</ol>
<p><span class="math display">\[
\sup_{|\theta - \theta_0| &lt; \delta_n} \left| \left| \frac{\partial^2 Q_n}{\partial \theta \partial \theta^\top} (\theta) - A \right| \right| \overset{P}{\to} 0
\]</span></p>
<p>for any sequence <span class="math inline">\(\delta_n \to 0\)</span>.</p>
<ol start="5" type="A">
<li><span class="math inline">\(n^{1/2} \dfrac{\partial Q_n}{\partial \theta}(\theta_0) \overset{D}{\to} N(0, B)\)</span> for some positive definite matrix <span class="math inline">\(B\)</span>.</li>
</ol>
<p>Then with <span class="math inline">\(V(\theta) = A(\theta)^{-1} B(\theta) A(\theta)^{-1}\)</span> and <span class="math inline">\(V = V(\theta_0)\)</span> we have</p>
<p><span class="math display">\[
n^{1/2} (\hat{\theta} - \theta_0) \overset{D}{\to} N(0, V). \qquad (23.7)
\]</span></p>
<p><strong>Proof.</strong> Conditions A and B ensure that with probability tending to one, <span class="math inline">\(\hat{\theta}\)</span> satisfies the first order condition. By the Mean Value Theorem</p>
<p><span class="math display">\[
0 = \frac{\partial Q_n}{\partial \theta}(\hat{\theta}) = n^{1/2} \frac{\partial Q_n}{\partial \theta} (\theta_0) + \frac{\partial^2 Q_n}{\partial \theta \partial \theta^\top} (\theta^*) n^{1/2} (\hat{\theta} - \theta_0),
\]</span></p>
<p>where <span class="math inline">\(\theta^*\)</span> is intermediate between <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\theta_0\)</span>, i.e., <span class="math inline">\(||\theta^* - \theta_0|| \leq ||\hat{\theta} - \theta_0||\)</span>. Actually, we need a different such <span class="math inline">\(\theta^*\)</span> for each row, but each will satisfy the contraction condition. By assumption D we can replace <span class="math inline">\(\dfrac{\partial^2 Q_n}{\partial \theta \partial \theta^\top}(\theta^*)\)</span> by the limiting matrix <span class="math inline">\(A\)</span> with probability tending to one, and obtain that</p>
<p><span class="math display">\[
n^{1/2} (\hat{\theta} - \theta_0) = -A^{-1} n^{1/2} \frac{\partial Q_n}{\partial \theta} (\theta_0) + R_n,
\]</span></p>
<p>where the remainder term <span class="math inline">\(R_n \overset{P}{\to} 0\)</span>. Then apply Assumption E and Slutsky’s theorem to conclude the result (23.7). <span class="math inline">\(\square\)</span></p>
<p>The uniformity condition (B) is needed, i.e. it is not generally sufficient that</p>
<p><span class="math display">\[
\frac{\partial^2 Q_n}{\partial \theta \partial \theta^\top} (\theta_0) \overset{P}{\to} A.
\]</span></p>
<p>In the linear regression case, the usual least squares objective function satisfies</p>
<p><span class="math display">\[
\frac{\partial^2 Q_n}{\partial \beta \partial \beta^\top} (\beta) = -X^\top X
\]</span></p>
<p>for all parameter values <span class="math inline">\(\beta\)</span> and so this condition is automatically satisfied.</p>
<p>Condition B is needed because otherwise <span class="math inline">\(\hat{\theta}\)</span> may not satisfy the first order condition.</p>
</section>
<section class="level3" id="example-23.7.">
<h3 class="anchored" data-anchor-id="example-23.7.">Example 23.7.</h3>
<p>Suppose that <span class="math inline">\(X \sim N(\mu, 1)\)</span>, where <span class="math inline">\(\mu \in \Theta = [\underline{\mu}, \overline{\mu}]\)</span>. The maximum likelihood estimator in this case can be shown to be</p>
<p><span class="math display">\[
\hat{\mu} = \begin{cases}
\overline{\mu} &amp;\text{if } \overline{X} &gt; \overline{\mu} \\
\overline{X} &amp;\text{if } \overline{X} \in [\underline{\mu}, \overline{\mu}] \\
\underline{\mu} &amp;\text{if } \overline{X} \leq \underline{\mu}.
\end{cases}
\]</span></p>
<p>This will satisfy</p>
<p><span class="math display">\[
\sum_{i=1}^n (X_i - \hat{\mu}) = 0 \qquad (23.8)
\]</span></p>
<p>only when <span class="math inline">\(\overline{X} \in (\underline{\mu}, \overline{\mu})\)</span>, i.e., the constraint is not binding. If the true parameter <span class="math inline">\(\mu_0 = \underline{\mu}\)</span>, then we can see that only 50% of the time will (23.8) occur. In fact, <span class="math inline">\(\hat{\mu}\)</span> is not asymptotically normal in this case. See the exercise.</p>
<p>The precise form of the limiting distribution depends on the details of <span class="math inline">\(Q_n\)</span>. We consider the main leading cases separately.</p>
<p><strong>Correctly Specified Likelihood.</strong> The score function and Hessian are sample averages of independent variables (under the i.i.d. sampling) and thus satisfy the appropriate CLTs and ULLNs under some regularity conditions. By virtue of the information matrix equality, <span class="math inline">\(A = B = \mathcal{I}\)</span>, the asymptotic variance has the simpler form <span class="math inline">\(\mathcal{I}^{-1}\)</span>.</p>
<p>A number of different methods exist for estimating the asymptotic covariance matrix using either the Hessian or outer product:</p>
<p><span class="math display">\[
\frac{1}{n} \frac{\partial^2 l}{\partial \theta \partial \theta^\top} (\hat{\theta}) \quad \text{or} \quad \frac{1}{n} \sum_{i=1}^n \frac{\partial l_i}{\partial \theta} \frac{\partial l_i}{\partial \theta^\top} (\hat{\theta}).
\]</span></p>
<p><strong>Misspecified Likelihood.</strong> Clearly, the meaning of the limit <span class="math inline">\(\theta_0\)</span> is not obvious when the model is not correct, since these parameters may have nothing to do with the true distribution of the data. However, under regularity conditions, <span class="math inline">\(\theta_0\)</span> is the parameter value that minimizes the Kullback-Liebler distance between the distribution of the data and the specified model. Under partial misspecification, one may retain consistency. For example, in a regression model if the mean is correctly specified but the distribution of the errors is not normal or the heteroskedasticity is ignored. In these cases the (pseudo or quasi) ML estimates of the mean parameters can be consistent. In the general case, the limiting variance is of the sandwich form <span class="math inline">\(A^{-1} B A^{-1}\)</span>. One can carry out robust inference by allowing for this more general structure when estimating the asymptotic covariance matrix. The leading example here is when the parametric model is linear regression with normal homoskedastic errors but the distribution of the data has heteroskedasticity. In this case, <span class="math inline">\(A = X^\top X\)</span> and <span class="math inline">\(B = X^\top \Sigma X\)</span>, which can be estimated by replacing the diagonal matrix <span class="math inline">\(\Sigma\)</span> by the diagonal matrix whose elements are the squared least squares residuals. For a general (pseudo) likelihood criterion, we estimate the covariance matrix robustly by</p>
<p><span class="math display">\[
\hat{V} = \left\{ \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 l_i}{\partial \theta \partial \theta^\top} (\hat{\theta}) \right\}^{-1} \left\{ \frac{1}{n} \sum_{i=1}^n \frac{\partial l_i}{\partial \theta} \frac{\partial l_i}{\partial \theta^\top} (\hat{\theta}) \right\} \left\{ \frac{1}{n} \sum_{i=1}^n \frac{\partial^2 l_i}{\partial \theta \partial \theta^\top} (\hat{\theta}) \right\}^{-1}.
\]</span></p>
<p>See White (1982) for a comprehensive discussion of this theory.</p>
<p><strong>Generalized Method of Moments.</strong> Suppose that <span class="math inline">\(W_n \overset{P}{\to} W &gt; 0\)</span> and that the moments are correctly specified. Then</p>
<p><span class="math display">\[
\sqrt{n} (\hat{\theta}_{GMM} - \theta_0) \overset{D}{\to} N(0, (\Gamma^\top W \Gamma)^{-1} \Gamma^\top W \Omega W \Gamma (\Gamma^\top W \Gamma)^{-1}),
\]</span></p>
<p><span class="math display">\[
\Omega = \text{var} \sqrt{n} G_n(\theta_0) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(g(Z_i, \theta_0) g(Z_i, \theta_0)^\top)
\]</span></p>
<p><span class="math display">\[
\Gamma = \mathbb{E} \left( \frac{\partial g(Z_i, \theta_0)}{\partial \theta} \right).
\]</span></p>
<p>If <span class="math inline">\(p=q\)</span>, then <span class="math inline">\((\Gamma^\top W \Gamma)^{-1} \Gamma^\top W \Omega W \Gamma (\Gamma^\top W \Gamma)^{-1} = \Gamma^{-1} \Omega \Gamma^{\top -1}\)</span>, and the asymptotic variance simplifies. We estimate <span class="math inline">\(\Gamma\)</span> and <span class="math inline">\(\Omega\)</span> by</p>
<p><span class="math display">\[
\hat{\Gamma} = \frac{\partial G_n(\hat{\theta})}{\partial \theta}; \quad \hat{\Omega} = \frac{1}{n} \sum_{i=1}^n g(Z_i, \hat{\theta}) g(Z_i, \hat{\theta})^\top
\]</span></p>
<p>and hence <span class="math inline">\(\hat{V} = (\hat{\Gamma}^\top W_n \hat{\Gamma})^{-1} \hat{\Gamma}^\top W_n \hat{\Omega} W_n \hat{\Gamma} (\hat{\Gamma}^\top W_n \hat{\Gamma})^{-1}\)</span> is used to estimate <span class="math inline">\(V = (\Gamma^\top W \Gamma)^{-1} \Gamma^\top W \Omega W \Gamma (\Gamma^\top W \Gamma)^{-1}\)</span>.</p>
</section>
<section class="level3" id="example-23.8.-a-simple-example-linear-regression-y-xbeta-epsilon.">
<h3 class="anchored" data-anchor-id="example-23.8.-a-simple-example-linear-regression-y-xbeta-epsilon.">Example 23.8. A simple example, linear regression <span class="math inline">\(y = X\beta + \epsilon\)</span>.</h3>
<p>In this case, the moment conditions are</p>
<p><span class="math display">\[
\mathbb{E}[X_i \epsilon_i(\beta_0)] = 0.
\]</span></p>
<p>Here there are <span class="math inline">\(K\)</span> conditions and <span class="math inline">\(K\)</span> parameters, it corresponds to the exactly identified case. In this case, there exists a unique <span class="math inline">\(\beta\)</span> that satisfies the empirical conditions provided <span class="math inline">\(X\)</span> is of full rank.</p>
</section>
<section class="level3" id="example-23.9.-instrumental-variables.">
<h3 class="anchored" data-anchor-id="example-23.9.-instrumental-variables.">Example 23.9. Instrumental variables.</h3>
<p>Suppose now <span class="math inline">\(\mathbb{E}[x_i \epsilon_i(\beta_0)] \neq 0\)</span> because of omitted variables/endogeneity. However, suppose that</p>
<p><span class="math display">\[
\mathbb{E}[z_i \epsilon_i(\beta_0)] = 0
\]</span></p>
<p>for instruments <span class="math inline">\(z_i \in \mathbb{R}^J\)</span>, with <span class="math inline">\(J &gt; K\)</span>. In this case, we can’t solve uniquely for <span class="math inline">\(\hat{\beta}_{IV}\)</span> because there are too many equations which can’t all be satisfied simultaneously. Take</p>
<p><span class="math display">\[
G_n(\theta) = \frac{1}{n} \sum_{i=1}^n g_i(\theta) g_i(\theta)^\top; \quad g_i(\theta) = z_i \epsilon_i(\beta_0) = z_i (y_i - x_i^\top \beta_0).
\]</span></p>
<p>If we take weighting matrix <span class="math inline">\(W_n = (Z^\top Z)^{-1}\)</span>, where <span class="math inline">\(Z\)</span> is the <span class="math inline">\(n \times J\)</span> matrix of instruments, then the objective function becomes</p>
<p><span class="math display">\[
Q_n(\beta) = (y - X\beta)^\top Z (Z^\top Z)^{-1} Z^\top (y - X\beta) = ||P_Z (y - X\beta)||^2
\]</span></p>
<p>where <span class="math inline">\(P_Z = Z(Z^\top Z)^{-1} Z^\top\)</span>. This has a closed form solution</p>
<p><span class="math display">\[
\hat{\beta}_{GMM} = ((P_Z X)^\top P_Z X)^{-1} (P_Z X)^\top (P_Z y) = (X^\top P_Z X)^{-1} X^\top P_Z y,
\]</span></p>
<p>i.e., it is an instrumental variables estimator with instruments <span class="math inline">\(X^* = P_Z X\)</span>. We require <span class="math inline">\(Z\)</span> to be full rank.</p>
</section>
</section>
<section class="level2" id="quantile-regression">
<h2 class="anchored" data-anchor-id="quantile-regression">23.3 Quantile Regression</h2>
<p>The quantile regression model is of the form</p>
<p><span class="math display">\[
y_i = x_i^\top \beta + u_i, \qquad (23.9)
\]</span></p>
<p>where the conditional <span class="math inline">\(\alpha\)</span>-quantile of <span class="math inline">\(u_i\)</span> given <span class="math inline">\(x_i\)</span> is zero. Therefore, the conditional quantile function satisfies <span class="math inline">\(Q_{y_i|x_i}(y_i|x_i; \alpha) = x_i^\top \beta\)</span>. The quantile restriction can be expressed as</p>
<p><span class="math display">\[
\mathbb{E}(\psi_\alpha(u_i)|x_i) = 0,
\]</span></p>
<p>where <span class="math inline">\(\psi_\alpha(x) = \text{sgn}(x) - (1 - 2\alpha)\)</span>, which is a conditional moment restriction. To estimate <span class="math inline">\(\beta\)</span> we may consider the objective function</p>
<p><span class="math display">\[
Q_n(\beta) = \frac{1}{n} \sum_{i=1}^n \rho_\alpha(y_i - x_i^\top \beta), \text{ where } \rho_\alpha(x) = x(\alpha - \mathbb{1}(x &lt; 0),
\]</span></p>
<p>or its first order condition</p>
<p><span class="math display">\[
G_n(\beta) = \frac{1}{n} \sum_{i=1}^n x_i \psi_\alpha(y_i - x_i^\top \beta), \text{ where }
\]</span></p>
<p>is the check function. The median corresponds to the case <span class="math inline">\(\rho_{1/2}(x) = |x|\)</span> and <span class="math inline">\(\psi_{1/2}(x) = \text{sgn}(x)\)</span>.</p>
<p>The solution is not generally unique. Note that the objective function is not differentiable (at one point) and the first order condition is not even continuous at the same point, so that typically different methods have to be employed to study the large sample properties. Nevertheless, the consistency theorem can be applied directly. In fact, conditions A and B are unnecessary, because one can show that the objective function is globally convex and so there exists a minimizing value <span class="math inline">\(\beta \in \mathbb{R}^K\)</span> although it won’t generally be unique.</p>
<p>We have the following result (an adaptation of Koenker (2005)).</p>
<section class="level3" id="assumptions-q.">
<h3 class="anchored" data-anchor-id="assumptions-q.">Assumptions Q.</h3>
<p><strong>Q1.</strong> Suppose that <span class="math inline">\(u_i, x_i\)</span> are i.i.d. and <span class="math inline">\(u_i|x_i\)</span> is continuous with density <span class="math inline">\(f_{u|x}\)</span> and <span class="math inline">\(\alpha\)</span>-quantile <span class="math inline">\(0\)</span> such that <span class="math inline">\(0 &lt; \inf_x f_{u|x}(0|x) \leq \sup_x f_{u|x}(0|x) &lt; \infty\)</span>.</p>
<p><strong>Q2.</strong> The matrices <span class="math inline">\(M = \mathbb{E}[x_i x_i^\top]\)</span> and <span class="math inline">\(M_\alpha = \mathbb{E}[x_i x_i^\top f_{u|x}(0|x_i)]\)</span> exist and are positive definite.</p>
</section>
<section class="level3" id="theorem-23.5.">
<h3 class="anchored" data-anchor-id="theorem-23.5.">Theorem 23.5.</h3>
<p>Suppose that assumptions Q1 and Q2 hold. Then</p>
<p><span class="math display">\[
\sqrt{n} (\hat{\beta} - \beta) \overset{D}{\to} N(0, \alpha(1-\alpha) M_\alpha^{-1} M M_\alpha^{-1}).
\]</span></p>
<p><strong>Proof.</strong> We just give some heuristics. We may show that for any <span class="math inline">\(\beta \in \mathbb{R}^K\)</span> that</p>
<p><span class="math display">\[
G_n(\beta) = \frac{1}{n} \sum_{i=1}^n x_i \psi_\alpha(y_i - x_i^\top \beta) \overset{P}{\to} G(\beta) = \mathbb{E}[x_i \psi_\alpha(u_i - x_i^\top (\beta - \beta_0))].
\]</span></p>
<p>There are special arguments that make use of the convexity of the objective function that guarantees that this convergence is uniform. We suppose for convenience of notation that <span class="math inline">\(x\)</span> is continuously distributed. We can write</p>
<p><span class="math display">\[
\begin{aligned}
G(\beta) &amp;= \int [x \psi_\alpha(u - x^\top (\beta - \beta_0))] f_{u|x}(u|x) f_x(x) du dx \\
&amp;= \int [x \text{sgn}(u - x^\top (\beta - \beta_0))] f_{u|x}(u|x) f_x(x) du dx - (1 - 2\alpha) \\
&amp;= \int [x \text{sgn}(v)] f_{u|x}(v + x^\top (\beta - \beta_0)|x) f_x(x) dv dx - (1 - 2\alpha) \\
&amp;= \int_0^\infty x f_{u|x}(v + x^\top (\beta - \beta_0)|x) f_x(x) dv dx - \int_{-\infty}^0 x f_{u|x}(v + x^\top (\beta - \beta_0)|x) f_x(x) dv dx - (1 - 2\alpha) \\
&amp;= \int x [1 - F_{u|x}(x^\top (\beta - \beta_0)|x)] f_x(x) dx - \int x F_{u|x}(x^\top (\beta - \beta_0)|x) f_x(x) dx - (1 - 2\alpha) \\
&amp;= \int x [1 - 2 F_{u|x}(x^\top (\beta - \beta_0)|x)] f_x(x) dx - (1 - 2\alpha),
\end{aligned}
\]</span></p>
<p>where we used the change of variable <span class="math inline">\(u \to v = u - x^\top (\beta - \beta_0)\)</span>. The function <span class="math inline">\(G(\beta)\)</span> satisfies <span class="math inline">\(G(\beta_0) = 0\)</span> and it is differentiable with</p>
<p><span class="math display">\[
\frac{\partial G}{\partial \beta} (\beta_0) = -2 \int x x^\top f_{u|x}(0|x) f_x(x) dx = -2 \mathbb{E}[x x^\top f_{u|x}(0|x)],
\]</span></p>
<p>which we will assume below is non-zero. This implies that at least in a neighbourhood of <span class="math inline">\(\beta_0\)</span> there can be no other zero of <span class="math inline">\(G(\beta)\)</span> and indeed <span class="math inline">\(Q(\beta_0) &lt; Q(\beta)\)</span> for all such <span class="math inline">\(\beta\)</span>, where <span class="math inline">\(Q(\beta) = \mathbb{E} Q_n(\beta)\)</span>. Consistency follows. Regarding the asymptotic normality, the tricky part is to show that the estimator satisfies the condition</p>
<p><span class="math display">\[
n^{1/2} (\hat{\beta} - \beta_0) = \left[ -\frac{\partial G}{\partial \beta} (\beta_0) \right]^{-1} n^{1/2} G_n(\beta_0) + R_n, \qquad (23.10)
\]</span></p>
<p>where <span class="math inline">\(R_n \overset{P}{\to} 0\)</span>. Note that we have <span class="math inline">\(\partial G / \partial \beta\)</span> not <span class="math inline">\(\mathbb{E} \partial G_n / \partial \beta\)</span>, which is not well defined due to the discontinuity of <span class="math inline">\(G\)</span>. Once this is established, the result follows by the Slutsky theorem etc. since</p>
<p><span class="math display">\[
n^{1/2} G_n(\beta_0) = \frac{1}{\sqrt{n}} \sum_{i=1}^n x_i \psi_\alpha(u_i)
\]</span></p>
<p>satisfies a CLT because by assumption <span class="math inline">\(\mathbb{E}(\psi_\alpha(u_i)|x_i) = 0\)</span>. <span class="math inline">\(\square\)</span></p>
<p>Quantile regression is robust in the sense that nowhere in the above theory have we required <span class="math inline">\(\mathbb{E}(u_i^2) &lt; \infty\)</span> or even <span class="math inline">\(\mathbb{E}(|u_i|) &lt; \infty\)</span>, which contrasts with the theory for OLS. The monotone equivariance property implies that</p>
<p><span class="math display">\[
Q_{\Lambda(y_i)|x_i}(\Lambda(y_i)|x_i; \alpha) = \Lambda(x_i^\top \beta)
\]</span></p>
<p>for any strictly increasing transformation <span class="math inline">\(\Lambda\)</span> such as the logarithm. This property says for example that we can infer the effect of <span class="math inline">\(x_i\)</span> on say <span class="math inline">\(\log(y_i)\)</span> from the quantile regression of <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_i\)</span>.</p>
</section>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch23exercise1">
<h3 class="anchored" data-anchor-id="sec-ch23exercise1">Exercise 1</h3>
<p><a href="#sec-ch23solution1">Solution 1</a></p>
<p>Consider the simultaneous equations model in Example 23.1. Suppose <span class="math inline">\(L=2\)</span>, <span class="math inline">\(K=1\)</span>, and we have the following structural equations:</p>
<p><span class="math display">\[
\begin{aligned}
y_{1i} &amp;= \gamma y_{2i} + \beta_1 x_i + u_{1i} \\
y_{2i} &amp;= \delta y_{1i} + \beta_2 x_i + u_{2i},
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbb{E}[u_{1i}|x_i] = \mathbb{E}[u_{2i}|x_i] = 0\)</span>. Express this model in the matrix form <span class="math inline">\(B(\theta)y_i = C(\theta)x_i + u_i\)</span>, and identify the matrices <span class="math inline">\(B(\theta)\)</span>, <span class="math inline">\(C(\theta)\)</span>, and the parameter vector <span class="math inline">\(\theta\)</span>.</p>
</section>
<section class="level3" id="sec-ch23exercise2">
<h3 class="anchored" data-anchor-id="sec-ch23exercise2">Exercise 2</h3>
<p><a href="#sec-ch23solution2">Solution 2</a></p>
<p>Referring to Exercise 1, write down the reduced form equations for <span class="math inline">\(y_{1i}\)</span> and <span class="math inline">\(y_{2i}\)</span> in terms of <span class="math inline">\(x_i\)</span> and the parameters. Explain how the reduced form relates to the matrix <span class="math inline">\(\Phi(\theta)\)</span> in Example 23.1. What condition on <span class="math inline">\(\gamma\)</span> and <span class="math inline">\(\delta\)</span> is required for <span class="math inline">\(B(\theta)\)</span> to be invertible?</p>
</section>
<section class="level3" id="sec-ch23exercise3">
<h3 class="anchored" data-anchor-id="sec-ch23exercise3">Exercise 3</h3>
<p><a href="#sec-ch23solution3">Solution 3</a></p>
<p>For the model in Exercise 1, derive an expression for <span class="math inline">\(g(Z_i, \theta)\)</span> using <span class="math inline">\(h(x_i) = x_i\)</span>. What is the dimension of <span class="math inline">\(g(Z_i, \theta)\)</span> in this case?</p>
</section>
<section class="level3" id="sec-ch23exercise4">
<h3 class="anchored" data-anchor-id="sec-ch23exercise4">Exercise 4</h3>
<p><a href="#sec-ch23solution4">Solution 4</a></p>
<p>In Example 23.2 (Hansen and Singleton, 1982), assume there is only one asset, <span class="math inline">\(m=1\)</span>, and <span class="math inline">\(r_{1,i+1} = r_{i+1}\)</span>. Let <span class="math inline">\(v_i = 1\)</span>. Derive the explicit form of <span class="math inline">\(g(Z_i, \theta)\)</span> and specify <span class="math inline">\(Z_i\)</span> and <span class="math inline">\(\theta\)</span>. What is the dimension <span class="math inline">\(q\)</span> of the moment condition?</p>
</section>
<section class="level3" id="sec-ch23exercise5">
<h3 class="anchored" data-anchor-id="sec-ch23exercise5">Exercise 5</h3>
<p><a href="#sec-ch23solution5">Solution 5</a></p>
<p>Explain the difference between the <strong>exactly identified</strong>, <strong>overidentified</strong>, and <strong>unidentified</strong> cases in the context of GMM. Provide an example of each case using a simple linear model with moment conditions.</p>
</section>
<section class="level3" id="sec-ch23exercise6">
<h3 class="anchored" data-anchor-id="sec-ch23exercise6">Exercise 6</h3>
<p><a href="#sec-ch23solution6">Solution 6</a></p>
<p>What is the role of the <strong>weighting matrix</strong> <span class="math inline">\(W_n(\theta)\)</span> in GMM? Explain how different choices of <span class="math inline">\(W_n(\theta)\)</span> lead to different estimators.</p>
</section>
<section class="level3" id="sec-ch23exercise7">
<h3 class="anchored" data-anchor-id="sec-ch23exercise7">Exercise 7</h3>
<p><a href="#sec-ch23solution7">Solution 7</a></p>
<p>Explain the concept of an <strong>extremum estimator</strong>. Give three examples of extremum estimators.</p>
</section>
<section class="level3" id="sec-ch23exercise8">
<h3 class="anchored" data-anchor-id="sec-ch23exercise8">Exercise 8</h3>
<p><a href="#sec-ch23solution8">Solution 8</a>(#sec-ch23solution8}</p>
<p>State the <strong>identification condition</strong> for consistency of an extremum estimator. Explain why this condition is necessary.</p>
</section>
<section class="level3" id="sec-ch23exercise9">
<h3 class="anchored" data-anchor-id="sec-ch23exercise9">Exercise 9</h3>
<p><a href="#sec-ch23solution9">Solution 9</a>(#sec-ch23solution9}</p>
<p>Explain the concept of <strong>uniform convergence</strong> in the context of extremum estimators and why it is important for consistency. Relate it to the Uniform Law of Large Numbers (ULLN).</p>
</section>
<section class="level3" id="sec-ch23exercise10">
<h3 class="anchored" data-anchor-id="sec-ch23exercise10">Exercise 10</h3>
<p><a href="#sec-ch23solution10">Solution 10</a>(#sec-ch23solution10}</p>
<p>In the proof of Theorem 23.1 (Consistency), explain the roles of terms I, II, and III in establishing that <span class="math inline">\(\Pr(||\hat{\theta} - \theta_0|| &gt; \delta)\)</span> goes to zero.</p>
</section>
<section class="level3" id="sec-ch23exercise11">
<h3 class="anchored" data-anchor-id="sec-ch23exercise11">Exercise 11</h3>
<p><a href="#sec-ch23solution11">Solution 11</a>(#sec-ch23solution11}</p>
<p>Give an example where the parameter space <span class="math inline">\(\Theta\)</span> is <em>not</em> compact, but consistency of an estimator can still be established. Explain how the proof of Theorem 23.1 might be adapted.</p>
</section>
<section class="level3" id="sec-ch23exercise12">
<h3 class="anchored" data-anchor-id="sec-ch23exercise12">Exercise 12</h3>
<p><a href="#sec-ch23solution12">Solution 12</a>(#sec-ch23solution12}</p>
<p>In Example 23.4 (Normal linear regression model), verify that <span class="math inline">\(n^{-1} \sum_{i=1}^n \epsilon_i x_i \overset{P}{\to} 0\)</span> under the assumption that <span class="math inline">\(\mathbb{E}[\epsilon_i | x_i] = 0\)</span> and <span class="math inline">\(\mathbb{E}[||x_i||^2] &lt; \infty\)</span>.</p>
</section>
<section class="level3" id="sec-ch23exercise13">
<h3 class="anchored" data-anchor-id="sec-ch23exercise13">Exercise 13</h3>
<p><a href="#sec-ch23solution13">Solution 13</a>(#sec-ch23solution13}</p>
<p>In Example 23.7, show that if the true parameter <span class="math inline">\(\mu_0 = \overline{\mu}\)</span>, then the MLE <span class="math inline">\(\hat{\mu}\)</span> is not asymptotically normal. Hint: Consider the probability <span class="math inline">\(\Pr(\overline{X} &gt; \overline{\mu})\)</span>.</p>
</section>
<section class="level3" id="sec-ch23exercise14">
<h3 class="anchored" data-anchor-id="sec-ch23exercise14">Exercise 14</h3>
<p><a href="#sec-ch23solution14">Solution 14</a>(#sec-ch23solution14}</p>
<p>Explain the difference between <strong>local</strong> and <strong>global conditions</strong> in the context of establishing the asymptotic properties of extremum estimators.</p>
</section>
<section class="level3" id="sec-ch23exercise15">
<h3 class="anchored" data-anchor-id="sec-ch23exercise15">Exercise 15</h3>
<p><a href="#sec-ch23solution15">Solution 15</a>(#sec-ch23solution15}</p>
<p>Explain why, under correct specification, the asymptotic variance of the MLE takes the simpler form <span class="math inline">\(\mathcal{I}^{-1}\)</span>, where <span class="math inline">\(\mathcal{I}\)</span> is the information matrix.</p>
</section>
<section class="level3" id="sec-ch23exercise16">
<h3 class="anchored" data-anchor-id="sec-ch23exercise16">Exercise 16</h3>
<p><a href="#sec-ch23solution16">Solution 16</a>(#sec-ch23solution16}</p>
<p>What is meant by a <strong>misspecified likelihood</strong>? Give an example where the MLE can still be consistent despite misspecification.</p>
</section>
<section class="level3" id="sec-ch23exercise17">
<h3 class="anchored" data-anchor-id="sec-ch23exercise17">Exercise 17</h3>
<p><a href="#sec-ch23solution17">Solution 17</a>(#sec-ch23solution17}</p>
<p>In the context of GMM, explain how the asymptotic variance simplifies when <span class="math inline">\(p=q\)</span> (the exactly identified case).</p>
</section>
<section class="level3" id="sec-ch23exercise18">
<h3 class="anchored" data-anchor-id="sec-ch23exercise18">Exercise 18</h3>
<p><a href="#sec-ch23solution18">Solution 18</a>(#sec-ch23solution18}</p>
<p>In Example 23.9 (Instrumental variables), explain why we require <span class="math inline">\(J &gt; K\)</span> (the number of instruments to be greater than the number of endogenous regressors). What happens if <span class="math inline">\(J=K\)</span>?</p>
</section>
<section class="level3" id="sec-ch23exercise19">
<h3 class="anchored" data-anchor-id="sec-ch23exercise19">Exercise 19</h3>
<p><a href="#sec-ch23solution19">Solution 19</a>(#sec-ch23exercise19}</p>
<p>What is the <strong>check function</strong> in quantile regression? Write down the check function for the median regression (<span class="math inline">\(\alpha = 0.5\)</span>).</p>
</section>
<section class="level3" id="sec-ch23exercise20">
<h3 class="anchored" data-anchor-id="sec-ch23exercise20">Exercise 20</h3>
<p><a href="#sec-ch23solution20">Solution 20</a>(#sec-ch23solution20}</p>
<p>Explain the <strong>monotone equivariance property</strong> of quantile regression. Provide an example of how this property can be useful.</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch23solution1">
<h3 class="anchored" data-anchor-id="sec-ch23solution1">Solution 1</h3>
<p><a href="#sec-ch23exercise1">Exercise 1</a></p>
<p>The given system of equations is:</p>
<p><span class="math display">\[
\begin{aligned}
y_{1i} &amp;= \gamma y_{2i} + \beta_1 x_i + u_{1i} \\
y_{2i} &amp;= \delta y_{1i} + \beta_2 x_i + u_{2i}
\end{aligned}
\]</span></p>
<p>We can rewrite this system in matrix form as follows:</p>
<p><span class="math display">\[
\begin{aligned}
\begin{bmatrix}
1 &amp; -\gamma \\
-\delta &amp; 1
\end{bmatrix}
\begin{bmatrix}
y_{1i} \\
y_{2i}
\end{bmatrix}
=
\begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix}
x_i
+
\begin{bmatrix}
u_{1i} \\
u_{2i}
\end{bmatrix}
\end{aligned}
\]</span></p>
<p>Comparing this to the general form <span class="math inline">\(B(\theta)y_i = C(\theta)x_i + u_i\)</span>, we can identify:</p>
<p><span class="math display">\[
B(\theta) = \begin{bmatrix}
1 &amp; -\gamma \\
-\delta &amp; 1
\end{bmatrix}, \quad
y_i = \begin{bmatrix}
y_{1i} \\
y_{2i}
\end{bmatrix}, \quad
C(\theta) = \begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix}, \quad
x_i = x_i, \quad
u_i = \begin{bmatrix}
u_{1i} \\
u_{2i}
\end{bmatrix}.
\]</span></p>
<p>The parameter vector <span class="math inline">\(\theta\)</span> consists of the unknown coefficients in <span class="math inline">\(B(\theta)\)</span> and <span class="math inline">\(C(\theta)\)</span>, so <span class="math inline">\(\theta = (\gamma, \delta, \beta_1, \beta_2)^\top\)</span>.</p>
</section>
<section class="level3" id="sec-ch23solution2">
<h3 class="anchored" data-anchor-id="sec-ch23solution2">Solution 2</h3>
<p><a href="#sec-ch23exercise2">Exercise 2</a></p>
<p>To find the reduced form, we need to solve for <span class="math inline">\(y_{1i}\)</span> and <span class="math inline">\(y_{2i}\)</span> in terms of <span class="math inline">\(x_i\)</span> and the parameters. We can do this by inverting <span class="math inline">\(B(\theta)\)</span>:</p>
<p><span class="math display">\[
B(\theta)^{-1} = \frac{1}{1 - \gamma\delta} \begin{bmatrix}
1 &amp; \gamma \\
\delta &amp; 1
\end{bmatrix},
\]</span></p>
<p>provided that <span class="math inline">\(1 - \gamma\delta \neq 0\)</span>. Multiplying both sides of the matrix equation by <span class="math inline">\(B(\theta)^{-1}\)</span>, we get:</p>
<p><span class="math display">\[
\begin{bmatrix}
y_{1i} \\
y_{2i}
\end{bmatrix}
=
\frac{1}{1 - \gamma\delta} \begin{bmatrix}
1 &amp; \gamma \\
\delta &amp; 1
\end{bmatrix}
\begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix}
x_i
+
\frac{1}{1 - \gamma\delta} \begin{bmatrix}
1 &amp; \gamma \\
\delta &amp; 1
\end{bmatrix}
\begin{bmatrix}
u_{1i} \\
u_{2i}
\end{bmatrix}.
\]</span></p>
<p>This simplifies to:</p>
<p><span class="math display">\[
\begin{aligned}
y_{1i} &amp;= \frac{\beta_1 + \gamma\beta_2}{1 - \gamma\delta} x_i + \frac{u_{1i} + \gamma u_{2i}}{1 - \gamma\delta} \\
y_{2i} &amp;= \frac{\delta\beta_1 + \beta_2}{1 - \gamma\delta} x_i + \frac{\delta u_{1i} + u_{2i}}{1 - \gamma\delta}.
\end{aligned}
\]</span></p>
<p>These are the reduced form equations. The matrix <span class="math inline">\(\Phi(\theta)\)</span> in Example 23.1 is the matrix that multiplies <span class="math inline">\(x_i\)</span> in the reduced form, i.e.,</p>
<p><span class="math display">\[
\Phi(\theta) = B(\theta)^{-1}C(\theta) = \frac{1}{1 - \gamma\delta} \begin{bmatrix}
1 &amp; \gamma \\
\delta &amp; 1
\end{bmatrix}
\begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix}
=
\frac{1}{1 - \gamma\delta}
\begin{bmatrix}
\beta_1 + \gamma\beta_2 \\
\delta\beta_1 + \beta_2
\end{bmatrix}.
\]</span></p>
<p>The condition required for <span class="math inline">\(B(\theta)\)</span> to be invertible is that its determinant is non-zero, which is <span class="math inline">\(1 - \gamma\delta \neq 0\)</span>.</p>
</section>
<section class="level3" id="sec-ch23solution3">
<h3 class="anchored" data-anchor-id="sec-ch23solution3">Solution 3</h3>
<p><a href="#sec-ch23exercise3">Exercise 3</a></p>
<p>From Exercise 1, we have:</p>
<p><span class="math display">\[
B(\theta) = \begin{bmatrix}
1 &amp; -\gamma \\
-\delta &amp; 1
\end{bmatrix}, \quad
C(\theta) = \begin{bmatrix}
\beta_1 \\
\beta_2
\end{bmatrix}, \quad
\theta = (\gamma, \delta, \beta_1, \beta_2)^\top.
\]</span></p>
<p>With <span class="math inline">\(h(x_i) = x_i\)</span>, we have:</p>
<p><span class="math display">\[
g(Z_i, \theta) = (B(\theta)y_i - C(\theta)x_i) \otimes h(x_i) = \begin{bmatrix}
y_{1i} - \gamma y_{2i} - \beta_1 x_i \\
y_{2i} - \delta y_{1i} - \beta_2 x_i
\end{bmatrix} \otimes x_i =
\begin{bmatrix}
(y_{1i} - \gamma y_{2i} - \beta_1 x_i)x_i \\
(y_{2i} - \delta y_{1i} - \beta_2 x_i)x_i
\end{bmatrix}.
\]</span></p>
<p>Since <span class="math inline">\(B(\theta)\)</span> is <span class="math inline">\(2 \times 2\)</span> and <span class="math inline">\(h(x_i)\)</span> is <span class="math inline">\(1 \times 1\)</span>, the dimension of <span class="math inline">\(g(Z_i, \theta)\)</span> is <span class="math inline">\(L \times M = 2 \times 1 = 2\)</span>, which is <span class="math inline">\(q\)</span>. So, <span class="math inline">\(g(Z_i, \theta)\)</span> is a <span class="math inline">\(2 \times 1\)</span> vector.</p>
</section>
<section class="level3" id="sec-ch23solution4">
<h3 class="anchored" data-anchor-id="sec-ch23solution4">Solution 4</h3>
<p><a href="#sec-ch23exercise4">Exercise 4</a></p>
<p>In Example 23.2, with <span class="math inline">\(m=1\)</span>, <span class="math inline">\(r_{1,i+1} = r_{i+1}\)</span>, and <span class="math inline">\(v_i = 1\)</span>, the moment condition is:</p>
<p><span class="math display">\[
\mathbb{E}\left[ \left\{ 1 - \beta (1 + r_{i+1}) \left( \frac{c_{i+1}}{c_i} \right)^{-\gamma} \right\} \Big| I_i^* \right] = 0.
\]</span></p>
<p>The function <span class="math inline">\(g(Z_i, \theta)\)</span> is given by:</p>
<p><span class="math display">\[
g(Z_i, \theta) = \left\{ 1 - \beta (1 + r_{i+1}) \left( \frac{c_{i+1}}{c_i} \right)^{-\gamma} \right\} v_i.
\]</span></p>
<p>With <span class="math inline">\(v_i = 1\)</span>, this becomes:</p>
<p><span class="math display">\[
g(Z_i, \theta) = 1 - \beta (1 + r_{i+1}) \left( \frac{c_{i+1}}{c_i} \right)^{-\gamma}.
\]</span></p>
<p>Here, <span class="math inline">\(Z_i = (c_i, c_{i+1}, r_{i+1})^\top\)</span>, and <span class="math inline">\(\theta = (\beta, \gamma)^\top\)</span>. Since there is only one moment condition, the dimension <span class="math inline">\(q\)</span> is 1.</p>
</section>
<section class="level3" id="sec-ch23solution5">
<h3 class="anchored" data-anchor-id="sec-ch23solution5">Solution 5</h3>
<p><a href="#sec-ch23exercise5">Exercise 5</a></p>
<ul>
<li><p><strong>Exactly identified (p=q):</strong> The number of moment conditions (<span class="math inline">\(q\)</span>) equals the number of parameters (<span class="math inline">\(p\)</span>). In this case, we can typically find a unique solution to the sample moment conditions <span class="math inline">\(G_n(\theta) = 0\)</span>.</p>
<p><em>Example:</em> <span class="math inline">\(y_i = \beta x_i + u_i\)</span>, with <span class="math inline">\(\mathbb{E}[u_i] = 0\)</span>. Here, <span class="math inline">\(p=q=1\)</span>, and the sample moment condition is <span class="math inline">\(\frac{1}{n}\sum_{i=1}^n (y_i - \beta x_i) = 0\)</span>.</p></li>
<li><p><strong>Overidentified (p&lt;q):</strong> The number of moment conditions (<span class="math inline">\(q\)</span>) is greater than the number of parameters (<span class="math inline">\(p\)</span>). In this case, we generally cannot find a solution that satisfies all sample moment conditions simultaneously. We use a weighting matrix to find the “best” solution.</p>
<p><em>Example:</em> <span class="math inline">\(y_i = \beta x_i + u_i\)</span>, with <span class="math inline">\(\mathbb{E}[u_i] = 0\)</span> and <span class="math inline">\(\mathbb{E}[x_i u_i] = 0\)</span>. Here, <span class="math inline">\(p=1\)</span> and <span class="math inline">\(q=2\)</span>. We have two moment conditions, but only one parameter.</p></li>
<li><p><strong>Unidentified (p&gt;q):</strong> The number of moment conditions (<span class="math inline">\(q\)</span>) is less than the number of parameters (<span class="math inline">\(p\)</span>). In this case, there are infinitely many solutions that satisfy the moment conditions, and we cannot uniquely estimate the parameters.</p>
<p><em>Example:</em> <span class="math inline">\(y_i = \beta_1 x_{1i} + \beta_2 x_{2i} + u_i\)</span>, with only one moment condition <span class="math inline">\(\mathbb{E}[u_i]=0\)</span>. Here p=2 and q=1. We have one moment condition and two parameters.</p></li>
</ul>
</section>
<section class="level3" id="sec-ch23solution6">
<h3 class="anchored" data-anchor-id="sec-ch23solution6">Solution 6</h3>
<p><a href="#sec-ch23exercise6">Exercise 6</a></p>
<p>The <strong>weighting matrix</strong> <span class="math inline">\(W_n(\theta)\)</span> in GMM is a <span class="math inline">\(q \times q\)</span> positive definite matrix that determines how the sample moment conditions <span class="math inline">\(G_n(\theta)\)</span> are combined to estimate the parameters. In the overidentified case (<span class="math inline">\(p&lt;q\)</span>), we cannot satisfy all moment conditions exactly, so <span class="math inline">\(W_n(\theta)\)</span> determines the relative importance given to each moment condition.</p>
<p>Different choices of <span class="math inline">\(W_n(\theta)\)</span> lead to different GMM estimators. For example:</p>
<ul>
<li><p><span class="math inline">\(W_n(\theta) = I\)</span>: This gives equal weight to all moment conditions.</p></li>
<li><p><span class="math inline">\(W_n(\theta) = \hat{\Omega}^{-1}\)</span>: This is the optimal weighting matrix, where <span class="math inline">\(\hat{\Omega}\)</span> is a consistent estimator of the variance-covariance matrix of the sample moment conditions. This choice leads to the most efficient GMM estimator.</p></li>
</ul>
<p>In general, any positive definite weighting matrix will lead to a consistent estimator, but the optimal weighting matrix leads to the most efficient estimator (smallest asymptotic variance).</p>
</section>
<section class="level3" id="sec-ch23solution7">
<h3 class="anchored" data-anchor-id="sec-ch23solution7">Solution 7</h3>
<p><a href="#sec-ch23exercise7">Exercise 7</a></p>
<p>An <strong>extremum estimator</strong> is an estimator that is obtained by minimizing (or maximizing) some objective function <span class="math inline">\(Q_n(\theta)\)</span> over the parameter space <span class="math inline">\(\Theta\)</span>.</p>
<p>Examples of extremum estimators:</p>
<ol type="1">
<li><strong>Ordinary Least Squares (OLS):</strong> Minimizes the sum of squared residuals: <span class="math inline">\(Q_n(\beta) = \sum_{i=1}^n (y_i - x_i^\top \beta)^2\)</span>.</li>
<li><strong>Maximum Likelihood Estimation (MLE):</strong> Maximizes the likelihood function (or equivalently, minimizes the negative log-likelihood function).</li>
<li><strong>Generalized Method of Moments (GMM):</strong> Minimizes a quadratic form of the sample moment conditions: <span class="math inline">\(Q_n(\theta) = G_n(\theta)^\top W_n(\theta) G_n(\theta)\)</span>.</li>
</ol>
</section>
<section class="level3" id="sec-ch23solution8">
<h3 class="anchored" data-anchor-id="sec-ch23solution8">Solution 8</h3>
<p><a href="#sec-ch23exercise8">Exercise 8</a></p>
<p>The <strong>identification condition</strong> for consistency of an extremum estimator states that the limit of the objective function, <span class="math inline">\(Q(\theta)\)</span>, must have a unique global minimum at the true parameter value <span class="math inline">\(\theta_0\)</span>. Formally, for every <span class="math inline">\(\theta \neq \theta_0\)</span>, we require <span class="math inline">\(Q(\theta) &gt; Q(\theta_0)\)</span>.</p>
<p>This condition is necessary because if there were another value <span class="math inline">\(\theta_1 \neq \theta_0\)</span> such that <span class="math inline">\(Q(\theta_1) = Q(\theta_0)\)</span>, then the objective function would have two global minima. In this case, we would have no way of distinguishing between <span class="math inline">\(\theta_0\)</span> and <span class="math inline">\(\theta_1\)</span> based on the objective function, and the estimator would not be consistent.</p>
</section>
<section class="level3" id="sec-ch23solution9">
<h3 class="anchored" data-anchor-id="sec-ch23solution9">Solution 9</h3>
<p><a href="#sec-ch23exercise9">Exercise 9</a></p>
<p><strong>Uniform convergence</strong> in the context of extremum estimators means that the objective function <span class="math inline">\(Q_n(\theta)\)</span> converges to its limit <span class="math inline">\(Q(\theta)\)</span> uniformly over the parameter space <span class="math inline">\(\Theta\)</span>. Formally, <span class="math inline">\(\sup_{\theta \in \Theta} |Q_n(\theta) - Q(\theta)| \overset{P}{\to} 0\)</span>.</p>
<p>This is important for consistency because it ensures that the minimizer of <span class="math inline">\(Q_n(\theta)\)</span> converges to the minimizer of <span class="math inline">\(Q(\theta)\)</span>. If the convergence were not uniform, then <span class="math inline">\(Q_n(\theta)\)</span> could be very different from <span class="math inline">\(Q(\theta)\)</span> for some values of <span class="math inline">\(\theta\)</span>, even for large <span class="math inline">\(n\)</span>, and the minimizer of <span class="math inline">\(Q_n(\theta)\)</span> might not be close to the true parameter value <span class="math inline">\(\theta_0\)</span>.</p>
<p>The Uniform Law of Large Numbers (ULLN) provides conditions under which sample averages converge uniformly to their expectations. Since many objective functions in extremum estimation involve sample averages (e.g., GMM, MLE), the ULLN is often used to establish uniform convergence.</p>
</section>
<section class="level3" id="sec-ch23solution10">
<h3 class="anchored" data-anchor-id="sec-ch23solution10">Solution 10</h3>
<p><a href="#sec-ch23exercise10">Exercise 10</a></p>
<p>In the proof of Theorem 23.1, we have:</p>
<p><span class="math display">\[
Q(\hat{\theta}) - Q(\theta_0) = \underbrace{Q(\hat{\theta}) - Q_n(\hat{\theta})}_{I} + \underbrace{Q_n(\hat{\theta}) - Q_n(\theta_0)}_{II} + \underbrace{Q_n(\theta_0) - Q(\theta_0)}_{III}.
\]</span></p>
<ul>
<li><p><strong>Term I:</strong> By uniform convergence (condition C), <span class="math inline">\(|I| = |Q(\hat{\theta}) - Q_n(\hat{\theta})| \leq \sup_{\theta \in \Theta} |Q_n(\theta) - Q(\theta)| \overset{P}{\to} 0\)</span>. This means that the difference between the limit objective function and the sample objective function evaluated at the estimator goes to zero in probability.</p></li>
<li><p><strong>Term II:</strong> Since <span class="math inline">\(\hat{\theta}\)</span> minimizes <span class="math inline">\(Q_n(\theta)\)</span>, we have <span class="math inline">\(Q_n(\hat{\theta}) \leq Q_n(\theta_0)\)</span>. Therefore, <span class="math inline">\(II = Q_n(\hat{\theta}) - Q_n(\theta_0) \leq 0\)</span>.</p></li>
<li><p><strong>Term III:</strong> Again, by uniform convergence, <span class="math inline">\(|III| = |Q_n(\theta_0) - Q(\theta_0)| \leq \sup_{\theta \in \Theta} |Q_n(\theta) - Q(\theta)| \overset{P}{\to} 0\)</span>. This means the difference between the sample objective function and the limit objective function evaluated at the true parameter goes to zero in probability.</p></li>
</ul>
<p>Combining these results, we have that <span class="math inline">\(Q(\hat{\theta}) - Q(\theta_0)\)</span> is bounded by terms that go to zero in probability, plus a non-positive term. This implies that <span class="math inline">\(Q(\hat{\theta}) - Q(\theta_0)\)</span> converges to a non-positive number. But from the identification condition <span class="math inline">\(Q(\hat{\theta}) - Q(\theta_0) \geq \epsilon(\delta)\)</span> when <span class="math inline">\(||\hat{\theta} - \theta_0||&gt;\delta\)</span>. This implies that <span class="math inline">\(\Pr(||\hat{\theta} - \theta_0|| &gt; \delta) \leq \Pr(Q(\hat{\theta}) - Q(\theta_0) \geq \epsilon(\delta)) \to 0\)</span></p>
</section>
<section class="level3" id="sec-ch23solution11">
<h3 class="anchored" data-anchor-id="sec-ch23solution11">Solution 11</h3>
<p><a href="#sec-ch23exercise11">Exercise 11</a></p>
<p>Consider the linear regression model <span class="math inline">\(y_i = \beta x_i + u_i\)</span>, where <span class="math inline">\(x_i\)</span> and <span class="math inline">\(u_i\)</span> are i.i.d. with <span class="math inline">\(\mathbb{E}[u_i] = 0\)</span> and <span class="math inline">\(\mathbb{E}[x_i u_i] = 0\)</span>. The parameter space for <span class="math inline">\(\beta\)</span> is often taken to be <span class="math inline">\(\Theta = \mathbb{R}\)</span>, which is <em>not</em> compact (it is not bounded).</p>
<p>Consistency of the OLS estimator <span class="math inline">\(\hat{\beta} = (\sum_{i=1}^n x_i^2)^{-1} \sum_{i=1}^n x_i y_i\)</span> can still be established under suitable assumptions (e.g., <span class="math inline">\(\mathbb{E}[x_i^2] &gt; 0\)</span>).</p>
<p>The proof of Theorem 23.1 relies on the existence of a minimizer <span class="math inline">\(\hat{\theta}\)</span>, which is guaranteed by the compactness of <span class="math inline">\(\Theta\)</span> and the continuity of <span class="math inline">\(Q_n(\theta)\)</span>. In the linear regression case, even though <span class="math inline">\(\Theta\)</span> is not compact, the OLS estimator exists and is unique (with probability 1) as long as <span class="math inline">\(\sum_{i=1}^n x_i^2 &gt; 0\)</span>. We can work directly with probability limits. The objective function <span class="math inline">\(Q_n(\beta) = \frac{1}{n}\sum(y_i - \beta x_i)^2\)</span>. We have already shown in Example 23.4 that <span class="math inline">\(Q_n(\beta) \overset{P}{\to} Q(\beta)=\sigma^2 + (\beta-\beta_0)^2\mathbb{E}(x_i^2)\)</span> which has a unique minimum at <span class="math inline">\(\beta = \beta_0\)</span>. We can modify the proof by restricting attention to a compact subset of <span class="math inline">\(\Theta\)</span>, say <span class="math inline">\(B_M = [-M, M]\)</span>, where <span class="math inline">\(M\)</span> is a large positive number. We can show that, with probability approaching 1, the minimizer of <span class="math inline">\(Q_n(\beta)\)</span> lies within <span class="math inline">\(B_M\)</span>. Then, we can apply Theorem 23.1 on <span class="math inline">\(B_M\)</span> to show consistency.</p>
</section>
<section class="level3" id="sec-ch23solution12">
<h3 class="anchored" data-anchor-id="sec-ch23solution12">Solution 12</h3>
<p><a href="#sec-ch23exercise12">Exercise 12</a></p>
<p>Let <span class="math inline">\(w_i = \epsilon_i x_i\)</span>. We are given that <span class="math inline">\(\mathbb{E}[\epsilon_i | x_i] = 0\)</span> and <span class="math inline">\(\mathbb{E}[||x_i||^2] &lt; \infty\)</span>. By the law of iterated expectations, <span class="math inline">\(\mathbb{E}[w_i] = \mathbb{E}[\mathbb{E}[\epsilon_i x_i | x_i]] = \mathbb{E}[x_i \mathbb{E}[\epsilon_i | x_i]] = \mathbb{E}[x_i \cdot 0] = 0\)</span>. Since <span class="math inline">\(x_i\)</span> and <span class="math inline">\(\epsilon_i\)</span> are i.i.d., <span class="math inline">\(w_i\)</span> are also i.i.d.</p>
<p>We need to show that <span class="math inline">\(n^{-1} \sum_{i=1}^n w_i \overset{P}{\to} 0\)</span>. By the Weak Law of Large Numbers (WLLN), if <span class="math inline">\(\mathbb{E}[w_i] = 0\)</span> and <span class="math inline">\(\text{Var}(w_i) &lt; \infty\)</span>, then <span class="math inline">\(n^{-1} \sum_{i=1}^n w_i \overset{P}{\to} 0\)</span>.</p>
<p>We have <span class="math inline">\(\mathbb{E}[w_i] = 0\)</span>. Now we need to check if <span class="math inline">\(\text{Var}(w_i) &lt; \infty\)</span>. Assuming that <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(x_i\)</span> have finite second moments, we consider: <span class="math display">\[\text{Var}(w_i)=\mathbb{E}[w_i^2] - (\mathbb{E}[w_i])^2= \mathbb{E}[w_i^2] = \mathbb{E}[\epsilon_i^2 x_i^2]\]</span> If we also assume that <span class="math inline">\(\mathbb{E}[\epsilon_i^2|x_i] = \sigma^2\)</span>, then <span class="math display">\[\mathbb{E}[\epsilon_i^2 x_i^2] = \mathbb{E}[\mathbb{E}[\epsilon_i^2 x_i^2 | x_i]] = \mathbb{E}[x_i^2 \mathbb{E}[\epsilon_i^2 | x_i] ] = \sigma^2 \mathbb{E}[x_i^2]\]</span> Since we are assuming <span class="math inline">\(\mathbb{E}[x_i^2] &lt; \infty\)</span> we conclude that <span class="math inline">\(\text{Var}(w_i) &lt; \infty\)</span>. By the WLLN, <span class="math inline">\(n^{-1} \sum_{i=1}^n \epsilon_i x_i \overset{P}{\to} 0\)</span>.</p>
</section>
<section class="level3" id="sec-ch23solution13">
<h3 class="anchored" data-anchor-id="sec-ch23solution13">Solution 13</h3>
<p><a href="#sec-ch23exercise13">Exercise 13</a></p>
<p>If <span class="math inline">\(\mu_0 = \overline{\mu}\)</span>, the MLE is</p>
<p><span class="math display">\[
\hat{\mu} = \begin{cases}
\overline{\mu} &amp;\text{if } \overline{X} &gt; \overline{\mu} \\
\overline{X} &amp;\text{if } \overline{X} \leq \overline{\mu}.
\end{cases}
\]</span></p>
<p>Let <span class="math inline">\(Z = \sqrt{n}(\overline{X} - \mu_0) = \sqrt{n}(\overline{X} - \overline{\mu})\)</span>. Since <span class="math inline">\(X_i \sim N(\mu_0, 1)\)</span>, we have <span class="math inline">\(\overline{X} \sim N(\mu_0, 1/n)\)</span>, and thus <span class="math inline">\(Z \sim N(0, 1)\)</span>.</p>
<p>Now, consider <span class="math inline">\(\sqrt{n}(\hat{\mu} - \overline{\mu})\)</span>. If <span class="math inline">\(\overline{X} &gt; \overline{\mu}\)</span>, then <span class="math inline">\(\hat{\mu} = \overline{\mu}\)</span>, so <span class="math inline">\(\sqrt{n}(\hat{\mu} - \overline{\mu}) = 0\)</span>. If <span class="math inline">\(\overline{X} \leq \overline{\mu}\)</span>, then <span class="math inline">\(\hat{\mu} = \overline{X}\)</span>, so <span class="math inline">\(\sqrt{n}(\hat{\mu} - \overline{\mu}) = \sqrt{n}(\overline{X} - \overline{\mu}) = Z\)</span>.</p>
<p>Therefore,</p>
<p><span class="math display">\[
\sqrt{n}(\hat{\mu} - \overline{\mu}) = \begin{cases}
0 &amp;\text{if } Z &gt; 0 \\
Z &amp;\text{if } Z \leq 0.
\end{cases}
\]</span></p>
<p>This means that <span class="math inline">\(\sqrt{n}(\hat{\mu} - \overline{\mu})\)</span> converges in distribution to a random variable that is 0 with probability 1/2 (since <span class="math inline">\(Z \sim N(0,1)\)</span>) and equal to <span class="math inline">\(Z\)</span> with probability 1/2. This is a mixed distribution, and it is not a normal distribution. Therefore, <span class="math inline">\(\hat{\mu}\)</span> is not asymptotically normal when <span class="math inline">\(\mu_0 = \overline{\mu}\)</span>.</p>
</section>
<section class="level3" id="sec-ch23solution14">
<h3 class="anchored" data-anchor-id="sec-ch23solution14">Solution 14</h3>
<p><a href="#sec-ch23exercise14">Exercise 14</a></p>
<ul>
<li><p><strong>Global conditions:</strong> These conditions relate to the behavior of the objective function <span class="math inline">\(Q_n(\theta)\)</span> and its limit <span class="math inline">\(Q(\theta)\)</span> over the <em>entire</em> parameter space <span class="math inline">\(\Theta\)</span>. Consistency typically requires global conditions (e.g., the identification condition and uniform convergence).</p></li>
<li><p><strong>Local conditions:</strong> These conditions relate to the behavior of <span class="math inline">\(Q_n(\theta)\)</span> and <span class="math inline">\(Q(\theta)\)</span> in a <em>neighborhood</em> of the true parameter value <span class="math inline">\(\theta_0\)</span>. Asymptotic normality typically requires local conditions (e.g., smoothness of <span class="math inline">\(Q_n(\theta)\)</span> around <span class="math inline">\(\theta_0\)</span>, and convergence of the second derivative).</p></li>
</ul>
<p>The idea is that once we have established consistency (<span class="math inline">\(\hat{\theta} \overset{P}{\to} \theta_0\)</span>), we know that the estimator will be close to the true parameter value with high probability for large <span class="math inline">\(n\)</span>. Therefore, we only need to examine the behavior of the objective function <em>locally</em> around <span class="math inline">\(\theta_0\)</span> to derive the asymptotic distribution.</p>
</section>
<section class="level3" id="sec-ch23solution15">
<h3 class="anchored" data-anchor-id="sec-ch23solution15">Solution 15</h3>
<p><a href="#sec-ch23exercise15">Exercise 15</a></p>
<p>Under correct specification, the score function <span class="math inline">\(s(Z_i, \theta) = \frac{\partial l_i}{\partial \theta}\)</span> has mean zero at the true parameter value: <span class="math inline">\(\mathbb{E}[s(Z_i, \theta_0)] = 0\)</span>. The information matrix is defined as</p>
<p><span class="math display">\[
\mathcal{I} = \mathbb{E}\left[ s(Z_i, \theta_0) s(Z_i, \theta_0)^\top \right] = -\mathbb{E}\left[ \frac{\partial^2 l_i}{\partial \theta \partial \theta^\top} (\theta_0) \right].
\]</span></p>
<p>This is the information matrix equality. In the context of Theorem 23.4, this means that <span class="math inline">\(A = - \mathbb{E} \left[ \frac{1}{n}\sum_{i=1}^n\frac{\partial^2 l_i}{\partial \theta \partial \theta^\top}(\theta_0)\right]\)</span> and <span class="math inline">\(B = \text{Var}\left(\frac{1}{\sqrt{n}}\sum_{i=1}^n \frac{\partial l_i}{\partial \theta} (\theta_0) \right) = \mathbb{E}\left[ s(Z_i, \theta_0) s(Z_i, \theta_0)^\top \right]\)</span>. Thus, under the information matrix equality, <span class="math inline">\(A=B=\mathcal{I}\)</span>.</p>
<p>The asymptotic variance in Theorem 23.4 is given by <span class="math inline">\(V = A^{-1} B A^{-1}\)</span>. Under correct specification and the information matrix equality, we have <span class="math inline">\(A = B = \mathcal{I}\)</span>, so <span class="math inline">\(V = \mathcal{I}^{-1} \mathcal{I} \mathcal{I}^{-1} = \mathcal{I}^{-1}\)</span>.</p>
</section>
<section class="level3" id="sec-ch23solution16">
<h3 class="anchored" data-anchor-id="sec-ch23solution16">Solution 16</h3>
<p><a href="#sec-ch23exercise16">Exercise 16</a></p>
<p>A <strong>misspecified likelihood</strong> means that the assumed parametric model for the data is incorrect. That is, the true data generating process does not belong to the family of distributions assumed by the likelihood function.</p>
<p><em>Example:</em> Suppose we observe data <span class="math inline">\(y_i\)</span> and <span class="math inline">\(x_i\)</span>, and we assume a linear model with normal errors: <span class="math inline">\(y_i = \beta x_i + u_i\)</span>, where <span class="math inline">\(u_i \sim N(0, \sigma^2)\)</span>. We use maximum likelihood to estimate <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> under this assumption.</p>
<p>However, suppose the true model is <span class="math inline">\(y_i = \beta x_i + u_i\)</span>, where <span class="math inline">\(u_i\)</span> follows a t-distribution with 3 degrees of freedom (which has heavier tails than the normal distribution). In this case, the likelihood is misspecified.</p>
<p>Despite the misspecification, the MLE for <span class="math inline">\(\beta\)</span> (which is equivalent to the OLS estimator in this case) can still be consistent for the true <span class="math inline">\(\beta\)</span> if <span class="math inline">\(\mathbb{E}[u_i | x_i] = 0\)</span>. This is because the OLS estimator only relies on the conditional mean being correctly specified. However, the MLE for <span class="math inline">\(\sigma^2\)</span> will generally be inconsistent, and the standard errors for <span class="math inline">\(\hat{\beta}\)</span> based on the normality assumption will be incorrect.</p>
</section>
<section class="level3" id="sec-ch23solution17">
<h3 class="anchored" data-anchor-id="sec-ch23solution17">Solution 17</h3>
<p><a href="#sec-ch23exercise17">Solution 17</a></p>
<p>In the exactly identified case (<span class="math inline">\(p=q\)</span>), the number of moment conditions equals the number of parameters. The asymptotic variance of the GMM estimator is given by:</p>
<p><span class="math display">\[
V = (\Gamma^\top W \Gamma)^{-1} \Gamma^\top W \Omega W \Gamma (\Gamma^\top W \Gamma)^{-1},
\]</span></p>
<p>where <span class="math inline">\(\Gamma = \mathbb{E} \left[ \frac{\partial g(Z_i, \theta_0)}{\partial \theta} \right]\)</span> and <span class="math inline">\(\Omega = \text{Var} \sqrt{n} G_n(\theta_0) = \frac{1}{n} \sum_{i=1}^n \mathbb{E}(g(Z_i, \theta_0) g(Z_i, \theta_0)^\top)\)</span>.</p>
<p>Since <span class="math inline">\(p=q\)</span>, <span class="math inline">\(\Gamma\)</span> is a square matrix (assuming it’s full rank). Therefore, we can simplify:</p>
<p><span class="math display">\[
\begin{aligned}
V &amp;= (\Gamma^\top W \Gamma)^{-1} \Gamma^\top W \Omega W \Gamma (\Gamma^\top W \Gamma)^{-1} \\
&amp;= \Gamma^{-1} W^{-1} (\Gamma^\top)^{-1} \Gamma^\top W \Omega W \Gamma \Gamma^{-1} W^{-1} (\Gamma^\top)^{-1} \\
&amp;= \Gamma^{-1} W^{-1} \Omega W \Gamma^{-1} \\
&amp;= \Gamma^{-1} \Omega \Gamma^{\top -1}.
\end{aligned}
\]</span> So, the asymptotic variance simplifies to <span class="math inline">\(\Gamma^{-1} \Omega \Gamma^{\top -1}\)</span>.</p>
</section>
<section class="level3" id="sec-ch23solution18">
<h3 class="anchored" data-anchor-id="sec-ch23solution18">Solution 18</h3>
<p><a href="#sec-ch23exercise18">Exercise 18</a></p>
<p>In the instrumental variables (IV) setting, we have endogenous regressors (correlated with the error term), and we use instruments to address this endogeneity.</p>
<p>We require <span class="math inline">\(J &gt; K\)</span> (overidentification) because:</p>
<ol type="1">
<li><p><strong>Identification:</strong> If <span class="math inline">\(J &lt; K\)</span> (underidentification), we have fewer instruments than endogenous regressors. In this case, we cannot uniquely identify the parameters of interest. We have more parameters to estimate than moment conditions to use. The parameters are not identified.</p></li>
<li><p><strong>Estimation:</strong> If <span class="math inline">\(J = K\)</span> (exact identification), we can solve the sample moment conditions exactly, and the IV estimator is equivalent to setting the sample moments to zero. This is analogous to the method of moments.</p></li>
</ol>
<p>If <span class="math inline">\(J &gt; K\)</span>, we have more moment conditions than parameters. This allows us to use the extra moment conditions to improve the efficiency of the estimator and to test the validity of the instruments (overidentifying restrictions test). With more information (instruments), we can get a “better” estimate by minimizing the weighted distance of sample moments to zero.</p>
</section>
<section class="level3" id="sec-ch23solution19">
<h3 class="anchored" data-anchor-id="sec-ch23solution19">Solution 19</h3>
<p><a href="#sec-ch23exercise19">Exercise 19</a></p>
<p>The <strong>check function</strong> in quantile regression, denoted by <span class="math inline">\(\rho_\alpha(u)\)</span>, is defined as:</p>
<p><span class="math display">\[
\rho_\alpha(u) = u(\alpha - \mathbb{1}(u &lt; 0)),
\]</span></p>
<p>where <span class="math inline">\(\alpha\)</span> is the quantile of interest (<span class="math inline">\(0 &lt; \alpha &lt; 1\)</span>), and <span class="math inline">\(\mathbb{1}(u &lt; 0)\)</span> is the indicator function, which equals 1 if <span class="math inline">\(u &lt; 0\)</span> and 0 otherwise.</p>
<p>For the median regression, <span class="math inline">\(\alpha = 0.5\)</span>. Substituting this into the check function, we get:</p>
<p><span class="math display">\[
\rho_{0.5}(u) = u(0.5 - \mathbb{1}(u &lt; 0)).
\]</span></p>
<p>This can be simplified to:</p>
<p><span class="math display">\[
\rho_{0.5}(u) = \begin{cases}
0.5u &amp;\text{if } u \geq 0 \\
0.5u - u = -0.5u &amp;\text{if } u &lt; 0.
\end{cases}
\]</span></p>
<p>This is equivalent to <span class="math inline">\(\rho_{0.5}(u)=0.5|u|\)</span>. Thus the check function that we minimize is equivalent to minimizing the sum of absolute values of the residuals.</p>
</section>
<section class="level3" id="sec-ch23solution20">
<h3 class="anchored" data-anchor-id="sec-ch23solution20">Solution 20</h3>
<p><a href="#sec-ch23exercise20">Exercise 20</a></p>
<p>The <strong>monotone equivariance property</strong> of quantile regression states that if we apply a strictly increasing transformation <span class="math inline">\(\Lambda(\cdot)\)</span> to the response variable <span class="math inline">\(y\)</span>, the conditional quantiles of the transformed variable <span class="math inline">\(\Lambda(y)\)</span> are equal to the transformed conditional quantiles of <span class="math inline">\(y\)</span>. Formally:</p>
<p><span class="math display">\[
Q_{\Lambda(y_i)|x_i}(\Lambda(y_i)|x_i; \alpha) = \Lambda(Q_{y_i|x_i}(y_i|x_i; \alpha)) = \Lambda(x_i^\top \beta).
\]</span></p>
<p><em>Example:</em> Suppose we are interested in the effect of education (<span class="math inline">\(x\)</span>) on wages (<span class="math inline">\(y\)</span>). Instead of modeling the conditional quantiles of wages directly, we might be interested in the conditional quantiles of <em>log</em> wages, <span class="math inline">\(\log(y)\)</span>.</p>
<p>The monotone equivariance property tells us that if we estimate the <span class="math inline">\(\alpha\)</span>-quantile regression of <span class="math inline">\(y\)</span> on <span class="math inline">\(x\)</span>:</p>
<p><span class="math display">\[
Q_{y_i|x_i}(y_i|x_i; \alpha) = x_i^\top \beta,
\]</span></p>
<p>then the <span class="math inline">\(\alpha\)</span>-quantile regression of <span class="math inline">\(\log(y)\)</span> on <span class="math inline">\(x\)</span> is simply:</p>
<p><span class="math display">\[
Q_{\log(y_i)|x_i}(\log(y_i)|x_i; \alpha) = \log(x_i^\top \beta).
\]</span> That is the coefficient of the log quantile regression can be interpreted as the effect of the covariates on the log of the quantiles of <span class="math inline">\(y\)</span>.</p>
<p>This is useful because it allows us to model the conditional quantiles of a transformed variable without having to re-estimate the model. It also implies that the quantile regression model is robust to monotonic transformations of the response variable.</p>
</section>
</section>
<section class="level2" id="r-scripts">
<h2 class="anchored" data-anchor-id="r-scripts">R Scripts</h2>
<section class="level3" id="r-script-1-simultaneous-equations-model-and-identification">
<h3 class="anchored" data-anchor-id="r-script-1-simultaneous-equations-model-and-identification">R Script 1: Simultaneous Equations Model and Identification</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="fu">library</span>(matlib) <span class="co"># For matrix operations</span></span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2" tabindex="-1"></a></span>
<span id="cb3-3"><a aria-hidden="true" href="#cb3-3" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb3-4"><a aria-hidden="true" href="#cb3-4" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb3-5"><a aria-hidden="true" href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a aria-hidden="true" href="#cb3-6" tabindex="-1"></a><span class="co"># Simulate data for a simultaneous equations model (Example 23.1)</span></span>
<span id="cb3-7"><a aria-hidden="true" href="#cb3-7" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span>  <span class="co"># Sample size</span></span>
<span id="cb3-8"><a aria-hidden="true" href="#cb3-8" tabindex="-1"></a>gamma <span class="ot">&lt;-</span> <span class="fl">0.6</span> <span class="co"># Parameter</span></span>
<span id="cb3-9"><a aria-hidden="true" href="#cb3-9" tabindex="-1"></a>delta <span class="ot">&lt;-</span> <span class="fl">0.2</span> <span class="co"># Parameter</span></span>
<span id="cb3-10"><a aria-hidden="true" href="#cb3-10" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">1</span>  <span class="co"># Parameter</span></span>
<span id="cb3-11"><a aria-hidden="true" href="#cb3-11" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="sc">-</span><span class="fl">0.5</span> <span class="co"># Parameter</span></span>
<span id="cb3-12"><a aria-hidden="true" href="#cb3-12" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n) <span class="co"># Exogenous variable</span></span>
<span id="cb3-13"><a aria-hidden="true" href="#cb3-13" tabindex="-1"></a>u1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n) <span class="co"># Error term 1</span></span>
<span id="cb3-14"><a aria-hidden="true" href="#cb3-14" tabindex="-1"></a>u2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n) <span class="co"># Error term 2</span></span>
<span id="cb3-15"><a aria-hidden="true" href="#cb3-15" tabindex="-1"></a></span>
<span id="cb3-16"><a aria-hidden="true" href="#cb3-16" tabindex="-1"></a><span class="co"># Structural equations</span></span>
<span id="cb3-17"><a aria-hidden="true" href="#cb3-17" tabindex="-1"></a><span class="co"># y1 = gamma * y2 + beta1 * x + u1</span></span>
<span id="cb3-18"><a aria-hidden="true" href="#cb3-18" tabindex="-1"></a><span class="co"># y2 = delta * y1 + beta2 * x + u2</span></span>
<span id="cb3-19"><a aria-hidden="true" href="#cb3-19" tabindex="-1"></a></span>
<span id="cb3-20"><a aria-hidden="true" href="#cb3-20" tabindex="-1"></a><span class="co"># Reduced form equations (see Exercise 2 Solution)</span></span>
<span id="cb3-21"><a aria-hidden="true" href="#cb3-21" tabindex="-1"></a><span class="co"># y1 = (beta1 + gamma*beta2)/(1 - gamma*delta) * x + (u1 + gamma*u2)/(1 - gamma*delta)</span></span>
<span id="cb3-22"><a aria-hidden="true" href="#cb3-22" tabindex="-1"></a><span class="co"># y2 = (delta*beta1 + beta2)/(1 - gamma*delta) * x + (delta*u1 + u2)/(1 - gamma*delta)</span></span>
<span id="cb3-23"><a aria-hidden="true" href="#cb3-23" tabindex="-1"></a>y1 <span class="ot">&lt;-</span> (beta1 <span class="sc">+</span> gamma<span class="sc">*</span>beta2)<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">-</span> gamma<span class="sc">*</span>delta) <span class="sc">*</span> x <span class="sc">+</span> (u1 <span class="sc">+</span> gamma<span class="sc">*</span>u2)<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">-</span> gamma<span class="sc">*</span>delta)</span>
<span id="cb3-24"><a aria-hidden="true" href="#cb3-24" tabindex="-1"></a>y2 <span class="ot">&lt;-</span> (delta<span class="sc">*</span>beta1 <span class="sc">+</span> beta2)<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">-</span> gamma<span class="sc">*</span>delta) <span class="sc">*</span> x <span class="sc">+</span> (delta<span class="sc">*</span>u1 <span class="sc">+</span> u2)<span class="sc">/</span>(<span class="dv">1</span> <span class="sc">-</span> gamma<span class="sc">*</span>delta)</span>
<span id="cb3-25"><a aria-hidden="true" href="#cb3-25" tabindex="-1"></a></span>
<span id="cb3-26"><a aria-hidden="true" href="#cb3-26" tabindex="-1"></a><span class="co"># Create a data frame</span></span>
<span id="cb3-27"><a aria-hidden="true" href="#cb3-27" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y1, y2, x)</span>
<span id="cb3-28"><a aria-hidden="true" href="#cb3-28" tabindex="-1"></a></span>
<span id="cb3-29"><a aria-hidden="true" href="#cb3-29" tabindex="-1"></a><span class="co"># Define B(theta) and C(theta) (see Exercise 1 Solution)</span></span>
<span id="cb3-30"><a aria-hidden="true" href="#cb3-30" tabindex="-1"></a>B_theta <span class="ot">&lt;-</span> <span class="cf">function</span>(gamma, delta) {</span>
<span id="cb3-31"><a aria-hidden="true" href="#cb3-31" tabindex="-1"></a>  <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="sc">-</span>gamma, <span class="sc">-</span>delta, <span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">2</span>, <span class="at">byrow =</span> <span class="cn">TRUE</span>)</span>
<span id="cb3-32"><a aria-hidden="true" href="#cb3-32" tabindex="-1"></a>}</span>
<span id="cb3-33"><a aria-hidden="true" href="#cb3-33" tabindex="-1"></a></span>
<span id="cb3-34"><a aria-hidden="true" href="#cb3-34" tabindex="-1"></a>C_theta <span class="ot">&lt;-</span> <span class="cf">function</span>(beta1, beta2) {</span>
<span id="cb3-35"><a aria-hidden="true" href="#cb3-35" tabindex="-1"></a>  <span class="fu">matrix</span>(<span class="fu">c</span>(beta1, beta2), <span class="at">nrow =</span> <span class="dv">2</span>)</span>
<span id="cb3-36"><a aria-hidden="true" href="#cb3-36" tabindex="-1"></a>}</span>
<span id="cb3-37"><a aria-hidden="true" href="#cb3-37" tabindex="-1"></a></span>
<span id="cb3-38"><a aria-hidden="true" href="#cb3-38" tabindex="-1"></a><span class="co"># Check invertibility of B(theta) (determinant should be non-zero)</span></span>
<span id="cb3-39"><a aria-hidden="true" href="#cb3-39" tabindex="-1"></a><span class="fu">det</span>(<span class="fu">B_theta</span>(gamma, delta))  <span class="co"># 1 - gamma*delta</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.88</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a aria-hidden="true" href="#cb5-1" tabindex="-1"></a><span class="co"># Calculate Phi(theta) = B(theta)^(-1) * C(theta) (Reduced form coefficients)</span></span>
<span id="cb5-2"><a aria-hidden="true" href="#cb5-2" tabindex="-1"></a>Phi_theta <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">B_theta</span>(gamma, delta)) <span class="sc">%*%</span> <span class="fu">C_theta</span>(beta1, beta2)</span>
<span id="cb5-3"><a aria-hidden="true" href="#cb5-3" tabindex="-1"></a>Phi_theta</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]
[1,]  0.7954545
[2,] -0.3409091</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a aria-hidden="true" href="#cb7-1" tabindex="-1"></a><span class="co"># Estimate reduced form by OLS</span></span>
<span id="cb7-2"><a aria-hidden="true" href="#cb7-2" tabindex="-1"></a>lm_y1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y1 <span class="sc">~</span> x, <span class="at">data =</span> df)</span>
<span id="cb7-3"><a aria-hidden="true" href="#cb7-3" tabindex="-1"></a>lm_y2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(y2 <span class="sc">~</span> x, <span class="at">data =</span> df)</span>
<span id="cb7-4"><a aria-hidden="true" href="#cb7-4" tabindex="-1"></a></span>
<span id="cb7-5"><a aria-hidden="true" href="#cb7-5" tabindex="-1"></a><span class="co"># Compare estimated reduced form coefficients with true Phi(theta)</span></span>
<span id="cb7-6"><a aria-hidden="true" href="#cb7-6" tabindex="-1"></a><span class="fu">coef</span>(lm_y1)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)           x 
 0.03313892  0.88250631 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a aria-hidden="true" href="#cb9-1" tabindex="-1"></a><span class="fu">coef</span>(lm_y2)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)           x 
-0.01317719 -0.34256831 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb11-1"><a aria-hidden="true" href="#cb11-1" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"True Phi(theta):"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "True Phi(theta):"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb13-1"><a aria-hidden="true" href="#cb13-1" tabindex="-1"></a>Phi_theta</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>           [,1]
[1,]  0.7954545
[2,] -0.3409091</code></pre>
</div>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb15-1"><a aria-hidden="true" href="#cb15-1" tabindex="-1"></a><span class="co"># Visualize the relationship between y1 and x, and y2 and x</span></span>
<span id="cb15-2"><a aria-hidden="true" href="#cb15-2" tabindex="-1"></a><span class="fu">ggplot</span>(df, <span class="fu">aes</span>(<span class="at">x =</span> x)) <span class="sc">+</span></span>
<span id="cb15-3"><a aria-hidden="true" href="#cb15-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> y1, <span class="at">color =</span> <span class="st">"y1"</span>), <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb15-4"><a aria-hidden="true" href="#cb15-4" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">y =</span> y2, <span class="at">color =</span> <span class="st">"y2"</span>), <span class="at">alpha =</span> <span class="fl">0.6</span>) <span class="sc">+</span></span>
<span id="cb15-5"><a aria-hidden="true" href="#cb15-5" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="fu">aes</span>(<span class="at">y =</span> y1, <span class="at">color =</span> <span class="st">"y1"</span>), <span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb15-6"><a aria-hidden="true" href="#cb15-6" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="fu">aes</span>(<span class="at">y =</span> y2, <span class="at">color =</span> <span class="st">"y2"</span>), <span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>) <span class="sc">+</span></span>
<span id="cb15-7"><a aria-hidden="true" href="#cb15-7" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Simultaneous Equations Model: Reduced Form"</span>,</span>
<span id="cb15-8"><a aria-hidden="true" href="#cb15-8" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"x"</span>,</span>
<span id="cb15-9"><a aria-hidden="true" href="#cb15-9" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"y1 and y2"</span>,</span>
<span id="cb15-10"><a aria-hidden="true" href="#cb15-10" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"Variable"</span>) <span class="sc">+</span></span>
<span id="cb15-11"><a aria-hidden="true" href="#cb15-11" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'
`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap23_files/figure-html/unnamed-chunk-1-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load Packages:</strong> We load <code>tidyverse</code> for data manipulation and visualization, and <code>matlib</code> for matrix operations like <code>solve()</code> (matrix inverse).</li>
<li><strong>Simulate Data:</strong> We simulate data from a simultaneous equations model with two endogenous variables (<span class="math inline">\(y_1\)</span>, <span class="math inline">\(y_2\)</span>) and one exogenous variable (<span class="math inline">\(x\)</span>). The parameters <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\delta\)</span>, <span class="math inline">\(\beta_1\)</span>, and <span class="math inline">\(\beta_2\)</span> determine the relationships between the variables. We generate the data using the <em>reduced form</em> equations, derived from the structural equations.</li>
<li><strong>Create Data Frame:</strong> We combine the simulated variables into a data frame <code>df</code>.</li>
<li><strong>Define <code>B_theta</code> and <code>C_theta</code>:</strong> These functions represent the matrices <span class="math inline">\(B(\theta)\)</span> and <span class="math inline">\(C(\theta)\)</span> from the simultaneous equations model, as described in Example 23.1 and Exercise 1.</li>
<li><strong>Check Invertibility:</strong> We calculate the determinant of <span class="math inline">\(B(\theta)\)</span>. For the reduced form to exist (and for the model to be identified in a certain sense), <span class="math inline">\(B(\theta)\)</span> must be invertible, meaning its determinant must be non-zero.</li>
<li><strong>Calculate <code>Phi_theta</code>:</strong> We calculate the matrix <span class="math inline">\(\Phi(\theta) = B(\theta)^{-1}C(\theta)\)</span>, which represents the coefficients of the reduced form equations.</li>
<li><strong>Estimate Reduced Form (OLS):</strong> We estimate the reduced form equations using ordinary least squares (OLS) regression. This gives us estimates of the reduced form coefficients.</li>
<li><strong>Compare Coefficients:</strong> We compare the OLS estimates of the reduced form coefficients with the true <span class="math inline">\(\Phi(\theta)\)</span> calculated from the known parameters. They should be close, demonstrating that OLS can consistently estimate the reduced form parameters.</li>
<li><strong>Visualize:</strong> The <code>ggplot2</code> code creates a scatter plot of <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span> against <span class="math inline">\(x\)</span>, along with the OLS regression lines. This visualizes the reduced form relationships. The plot illustrates how the exogenous variable <span class="math inline">\(x\)</span> affects both endogenous variables <span class="math inline">\(y_1\)</span> and <span class="math inline">\(y_2\)</span>.</li>
</ol>
<p><strong>Connection to Text:</strong></p>
<ul>
<li><strong>Example 23.1:</strong> This script directly implements the simultaneous equations model described in the example. It shows how the structural equations are related to the reduced form.</li>
<li><strong>Identification:</strong> The condition <code>det(B_theta(gamma, delta)) != 0</code> relates to the identification of the structural parameters. If this determinant were zero, we couldn’t uniquely recover the structural parameters from the reduced form estimates.</li>
<li><strong>Reduced form:</strong> The estimation of reduced form using OLS is discussed in the text.</li>
</ul>
</section>
<section class="level3" id="r-script-2-generalized-method-of-moments-gmm-estimation">
<h3 class="anchored" data-anchor-id="r-script-2-generalized-method-of-moments-gmm-estimation">R Script 2: Generalized Method of Moments (GMM) Estimation</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb17-1"><a aria-hidden="true" href="#cb17-1" tabindex="-1"></a><span class="co"># Load necessary packages (if not already loaded)</span></span>
<span id="cb17-2"><a aria-hidden="true" href="#cb17-2" tabindex="-1"></a><span class="co"># library(tidyverse)</span></span>
<span id="cb17-3"><a aria-hidden="true" href="#cb17-3" tabindex="-1"></a><span class="fu">library</span>(gmm) <span class="co"># For GMM estimation</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: sandwich</code></pre>
</div>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb19-1"><a aria-hidden="true" href="#cb19-1" tabindex="-1"></a><span class="co"># Simulate data for a simple linear model with endogeneity (Example 23.9)</span></span>
<span id="cb19-2"><a aria-hidden="true" href="#cb19-2" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb19-3"><a aria-hidden="true" href="#cb19-3" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb19-4"><a aria-hidden="true" href="#cb19-4" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n) <span class="co"># Endogenous regressor</span></span>
<span id="cb19-5"><a aria-hidden="true" href="#cb19-5" tabindex="-1"></a>z <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n) <span class="co"># Instrument</span></span>
<span id="cb19-6"><a aria-hidden="true" href="#cb19-6" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fl">0.5</span> <span class="sc">*</span> z <span class="sc">+</span> <span class="fu">rnorm</span>(n) <span class="co"># Error term correlated with x (endogeneity)</span></span>
<span id="cb19-7"><a aria-hidden="true" href="#cb19-7" tabindex="-1"></a>y <span class="ot">&lt;-</span> beta_true <span class="sc">*</span> x <span class="sc">+</span> u</span>
<span id="cb19-8"><a aria-hidden="true" href="#cb19-8" tabindex="-1"></a></span>
<span id="cb19-9"><a aria-hidden="true" href="#cb19-9" tabindex="-1"></a><span class="co"># Create a data frame</span></span>
<span id="cb19-10"><a aria-hidden="true" href="#cb19-10" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x, z)</span>
<span id="cb19-11"><a aria-hidden="true" href="#cb19-11" tabindex="-1"></a></span>
<span id="cb19-12"><a aria-hidden="true" href="#cb19-12" tabindex="-1"></a><span class="co"># Define the moment function g(Z_i, theta) (see Example 23.9)</span></span>
<span id="cb19-13"><a aria-hidden="true" href="#cb19-13" tabindex="-1"></a>g_func <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, data) {</span>
<span id="cb19-14"><a aria-hidden="true" href="#cb19-14" tabindex="-1"></a>  y <span class="ot">&lt;-</span> data[, <span class="st">"y"</span>]</span>
<span id="cb19-15"><a aria-hidden="true" href="#cb19-15" tabindex="-1"></a>  x <span class="ot">&lt;-</span> data[, <span class="st">"x"</span>]</span>
<span id="cb19-16"><a aria-hidden="true" href="#cb19-16" tabindex="-1"></a>  z <span class="ot">&lt;-</span> data[, <span class="st">"z"</span>]</span>
<span id="cb19-17"><a aria-hidden="true" href="#cb19-17" tabindex="-1"></a>  <span class="fu">return</span>(z <span class="sc">*</span> (y <span class="sc">-</span> theta <span class="sc">*</span> x)) <span class="co"># Moment condition: E[z * (y - theta * x)] = 0</span></span>
<span id="cb19-18"><a aria-hidden="true" href="#cb19-18" tabindex="-1"></a>}</span>
<span id="cb19-19"><a aria-hidden="true" href="#cb19-19" tabindex="-1"></a></span>
<span id="cb19-20"><a aria-hidden="true" href="#cb19-20" tabindex="-1"></a><span class="co"># GMM estimation (exactly identified case: 1 parameter, 1 instrument)</span></span>
<span id="cb19-21"><a aria-hidden="true" href="#cb19-21" tabindex="-1"></a>gmm_result <span class="ot">&lt;-</span> <span class="fu">gmm</span>(<span class="at">g =</span> g_func, <span class="at">x =</span> df, <span class="at">t0 =</span> <span class="dv">0</span>) <span class="co"># t0 is the initial value for theta</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in (function (par, fn, gr = NULL, ..., method = c("Nelder-Mead", : one-dimensional optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly</code></pre>
</div>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb21-1"><a aria-hidden="true" href="#cb21-1" tabindex="-1"></a><span class="co"># Summary of GMM results</span></span>
<span id="cb21-2"><a aria-hidden="true" href="#cb21-2" tabindex="-1"></a><span class="fu">summary</span>(gmm_result)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
gmm(g = g_func, x = df, t0 = 0)


Method:  twoStep 

Kernel:  Quadratic Spectral

Coefficients:
          Estimate     Std. Error   t value      Pr(&gt;|t|)   
Theta[1]  -4.9974e+03   1.5777e+06  -3.1675e-03   9.9747e-01

J-Test: degrees of freedom is 0 
                J-test               P-value            
Test E(g)=0:    1.0794189986549e-06  *******            

#############
Information related to the numerical optimization
Convergence code =  0 
Function eval. =  54 
Gradian eval. =  NA </code></pre>
</div>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb23-1"><a aria-hidden="true" href="#cb23-1" tabindex="-1"></a><span class="co"># Extract the estimated coefficient</span></span>
<span id="cb23-2"><a aria-hidden="true" href="#cb23-2" tabindex="-1"></a>beta_hat_gmm <span class="ot">&lt;-</span> <span class="fu">coef</span>(gmm_result)</span>
<span id="cb23-3"><a aria-hidden="true" href="#cb23-3" tabindex="-1"></a>beta_hat_gmm</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Theta[1] 
 -4997.4 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb25-1"><a aria-hidden="true" href="#cb25-1" tabindex="-1"></a><span class="co"># Compare with true beta</span></span>
<span id="cb25-2"><a aria-hidden="true" href="#cb25-2" tabindex="-1"></a>beta_true</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a aria-hidden="true" href="#cb27-1" tabindex="-1"></a><span class="co"># Compare with OLS (which is biased due to endogeneity)</span></span>
<span id="cb27-2"><a aria-hidden="true" href="#cb27-2" tabindex="-1"></a>lm_result <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x, <span class="at">data =</span> df)</span>
<span id="cb27-3"><a aria-hidden="true" href="#cb27-3" tabindex="-1"></a><span class="fu">coef</span>(lm_result)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>(Intercept)           x 
 0.01879416  2.04357828 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a aria-hidden="true" href="#cb29-1" tabindex="-1"></a><span class="co"># Overidentified case (add another instrument)</span></span>
<span id="cb29-2"><a aria-hidden="true" href="#cb29-2" tabindex="-1"></a>z2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb29-3"><a aria-hidden="true" href="#cb29-3" tabindex="-1"></a>df<span class="sc">$</span>z2 <span class="ot">&lt;-</span> z2</span>
<span id="cb29-4"><a aria-hidden="true" href="#cb29-4" tabindex="-1"></a></span>
<span id="cb29-5"><a aria-hidden="true" href="#cb29-5" tabindex="-1"></a>g_func_overid <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, data) {</span>
<span id="cb29-6"><a aria-hidden="true" href="#cb29-6" tabindex="-1"></a>  y <span class="ot">&lt;-</span> data[, <span class="st">"y"</span>]</span>
<span id="cb29-7"><a aria-hidden="true" href="#cb29-7" tabindex="-1"></a>  x <span class="ot">&lt;-</span> data[, <span class="st">"x"</span>]</span>
<span id="cb29-8"><a aria-hidden="true" href="#cb29-8" tabindex="-1"></a>  z <span class="ot">&lt;-</span> data[, <span class="fu">c</span>(<span class="st">"z"</span>, <span class="st">"z2"</span>)] <span class="co"># Now using two instruments</span></span>
<span id="cb29-9"><a aria-hidden="true" href="#cb29-9" tabindex="-1"></a>  <span class="fu">return</span>(<span class="fu">cbind</span>(z[,<span class="dv">1</span>] <span class="sc">*</span> (y <span class="sc">-</span> theta <span class="sc">*</span> x), z[,<span class="dv">2</span>] <span class="sc">*</span> (y <span class="sc">-</span> theta <span class="sc">*</span> x)))</span>
<span id="cb29-10"><a aria-hidden="true" href="#cb29-10" tabindex="-1"></a>}</span>
<span id="cb29-11"><a aria-hidden="true" href="#cb29-11" tabindex="-1"></a></span>
<span id="cb29-12"><a aria-hidden="true" href="#cb29-12" tabindex="-1"></a>gmm_result_overid <span class="ot">&lt;-</span> <span class="fu">gmm</span>(<span class="at">g =</span> g_func_overid, <span class="at">x =</span> df, <span class="at">t0 =</span> <span class="dv">0</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in (function (par, fn, gr = NULL, ..., method = c("Nelder-Mead", : one-dimensional optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning in (function (par, fn, gr = NULL, ..., method = c("Nelder-Mead", : one-dimensional optimization by Nelder-Mead is unreliable:
use "Brent" or optimize() directly</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a aria-hidden="true" href="#cb32-1" tabindex="-1"></a><span class="fu">summary</span>(gmm_result_overid)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
gmm(g = g_func_overid, x = df, t0 = 0)


Method:  twoStep 

Kernel:  Quadratic Spectral(with bw =  0.46692 )

Coefficients:
          Estimate  Std. Error  t value  Pr(&gt;|t|)
Theta[1]  0.45625   3.08024     0.14812  0.88225 

J-Test: degrees of freedom is 1 
                J-test     P-value  
Test E(g)=0:    9.110e+01  1.366e-21

Initial values of the coefficients
Theta[1] 
    2.95 

#############
Information related to the numerical optimization
Convergence code =  0 
Function eval. =  16 
Gradian eval. =  NA </code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a aria-hidden="true" href="#cb34-1" tabindex="-1"></a><span class="co"># J-test for overidentifying restrictions (test of instrument validity)</span></span>
<span id="cb34-2"><a aria-hidden="true" href="#cb34-2" tabindex="-1"></a><span class="co"># The null hypothesis is that all instruments are valid</span></span>
<span id="cb34-3"><a aria-hidden="true" href="#cb34-3" tabindex="-1"></a>j_test <span class="ot">&lt;-</span> <span class="fu">summary</span>(gmm_result_overid)<span class="sc">$</span>stest</span>
<span id="cb34-4"><a aria-hidden="true" href="#cb34-4" tabindex="-1"></a><span class="fu">print</span>(j_test)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
 ##  J-Test: degrees of freedom is 1  ## 

                J-test     P-value  
Test E(g)=0:    9.110e+01  1.366e-21</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load Packages:</strong> Load <code>gmm</code> for GMM estimation.</li>
<li><strong>Simulate Data:</strong> Simulate data from a simple linear model <span class="math inline">\(y = \beta x + u\)</span>, where <span class="math inline">\(x\)</span> is endogenous (correlated with <span class="math inline">\(u\)</span>). We create an instrument <span class="math inline">\(z\)</span> that is correlated with <span class="math inline">\(x\)</span> but uncorrelated with <span class="math inline">\(u\)</span>.</li>
<li><strong>Create Data Frame:</strong> Combine the variables into a data frame.</li>
<li><strong>Define Moment Function:</strong> Define the function <code>g_func</code> that calculates the sample moment conditions. This function corresponds to <span class="math inline">\(g(Z_i, \theta)\)</span> in the text. The moment condition is based on the assumption that the instrument <span class="math inline">\(z\)</span> is uncorrelated with the error term <span class="math inline">\(u\)</span>: <span class="math inline">\(\mathbb{E}[z(y - \theta x)] = 0\)</span>.</li>
<li><strong>GMM Estimation (Exactly Identified):</strong> Use the <code>gmm()</code> function to estimate the parameter <span class="math inline">\(\beta\)</span> using GMM. Since we have one parameter and one instrument, this is the exactly identified case. <code>t0</code> is the starting value for the optimization.</li>
<li><strong>Summary and Coefficient:</strong> We print a summary of the GMM results and extract the estimated coefficient <span class="math inline">\(\hat{\beta}_{GMM}\)</span>.</li>
<li><strong>Comparison:</strong> We compare <span class="math inline">\(\hat{\beta}_{GMM}\)</span> with the true <span class="math inline">\(\beta\)</span> and with the OLS estimate (which is biased due to endogeneity).</li>
<li><strong>Overidentified Case:</strong> We add a second instrument (<code>z2</code>) and repeat the GMM estimation. Now we have one parameter and two instruments (overidentified).</li>
<li><strong>J-test:</strong> We perform the J-test (test for overidentifying restrictions). This tests the null hypothesis that <em>all</em> instruments are valid (uncorrelated with the error term). A small p-value suggests that at least one instrument is invalid.</li>
</ol>
<p><strong>Connection to Text:</strong></p>
<ul>
<li><strong>Example 23.9:</strong> This script directly implements the instrumental variables example.</li>
<li><strong>Moment Function:</strong> The <code>g_func</code> corresponds to the moment function <span class="math inline">\(g(Z_i, \theta)\)</span> discussed in the text.</li>
<li><strong>Exactly Identified and Overidentified:</strong> The script demonstrates both the exactly identified and overidentified cases.</li>
<li><strong>GMM Estimation:</strong> The <code>gmm()</code> function performs the GMM estimation, minimizing the quadratic form of the sample moment conditions.</li>
<li><strong>J-test:</strong> The J-test is mentioned in the text in the context of overidentified models (page 326).</li>
</ul>
</section>
<section class="level3" id="r-script-3-maximum-likelihood-estimation-mle-and-misspecification">
<h3 class="anchored" data-anchor-id="r-script-3-maximum-likelihood-estimation-mle-and-misspecification">R Script 3: Maximum Likelihood Estimation (MLE) and Misspecification</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a aria-hidden="true" href="#cb36-1" tabindex="-1"></a><span class="co"># Load necessary packages (if not already loaded)</span></span>
<span id="cb36-2"><a aria-hidden="true" href="#cb36-2" tabindex="-1"></a><span class="co"># library(tidyverse)</span></span>
<span id="cb36-3"><a aria-hidden="true" href="#cb36-3" tabindex="-1"></a></span>
<span id="cb36-4"><a aria-hidden="true" href="#cb36-4" tabindex="-1"></a><span class="co"># Simulate data from a normal distribution</span></span>
<span id="cb36-5"><a aria-hidden="true" href="#cb36-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb36-6"><a aria-hidden="true" href="#cb36-6" tabindex="-1"></a>mu_true <span class="ot">&lt;-</span> <span class="dv">5</span></span>
<span id="cb36-7"><a aria-hidden="true" href="#cb36-7" tabindex="-1"></a>sigma_true <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb36-8"><a aria-hidden="true" href="#cb36-8" tabindex="-1"></a>y <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="at">mean =</span> mu_true, <span class="at">sd =</span> sigma_true)</span>
<span id="cb36-9"><a aria-hidden="true" href="#cb36-9" tabindex="-1"></a></span>
<span id="cb36-10"><a aria-hidden="true" href="#cb36-10" tabindex="-1"></a><span class="co"># Define the negative log-likelihood function for the normal distribution</span></span>
<span id="cb36-11"><a aria-hidden="true" href="#cb36-11" tabindex="-1"></a>neg_loglik_normal <span class="ot">&lt;-</span> <span class="cf">function</span>(theta, data) {</span>
<span id="cb36-12"><a aria-hidden="true" href="#cb36-12" tabindex="-1"></a>  mu <span class="ot">&lt;-</span> theta[<span class="dv">1</span>]</span>
<span id="cb36-13"><a aria-hidden="true" href="#cb36-13" tabindex="-1"></a>  sigma <span class="ot">&lt;-</span> theta[<span class="dv">2</span>]</span>
<span id="cb36-14"><a aria-hidden="true" href="#cb36-14" tabindex="-1"></a>  <span class="cf">if</span> (sigma <span class="sc">&lt;=</span> <span class="dv">0</span>) {  <span class="co"># Ensure sigma is positive</span></span>
<span id="cb36-15"><a aria-hidden="true" href="#cb36-15" tabindex="-1"></a>    <span class="fu">return</span>(<span class="cn">Inf</span>)</span>
<span id="cb36-16"><a aria-hidden="true" href="#cb36-16" tabindex="-1"></a>  }</span>
<span id="cb36-17"><a aria-hidden="true" href="#cb36-17" tabindex="-1"></a>  <span class="sc">-</span><span class="fu">sum</span>(<span class="fu">dnorm</span>(data, <span class="at">mean =</span> mu, <span class="at">sd =</span> sigma, <span class="at">log =</span> <span class="cn">TRUE</span>))</span>
<span id="cb36-18"><a aria-hidden="true" href="#cb36-18" tabindex="-1"></a>}</span>
<span id="cb36-19"><a aria-hidden="true" href="#cb36-19" tabindex="-1"></a></span>
<span id="cb36-20"><a aria-hidden="true" href="#cb36-20" tabindex="-1"></a><span class="co"># MLE estimation (correctly specified)</span></span>
<span id="cb36-21"><a aria-hidden="true" href="#cb36-21" tabindex="-1"></a>mle_result_correct <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="at">par =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">fn =</span> neg_loglik_normal, <span class="at">data =</span> y,</span>
<span id="cb36-22"><a aria-hidden="true" href="#cb36-22" tabindex="-1"></a>                            <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>, <span class="at">lower =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="cn">Inf</span>, <span class="fl">0.001</span>)) <span class="co"># Constrain sigma &gt; 0</span></span>
<span id="cb36-23"><a aria-hidden="true" href="#cb36-23" tabindex="-1"></a></span>
<span id="cb36-24"><a aria-hidden="true" href="#cb36-24" tabindex="-1"></a><span class="co"># Estimated parameters</span></span>
<span id="cb36-25"><a aria-hidden="true" href="#cb36-25" tabindex="-1"></a>mle_result_correct<span class="sc">$</span>par</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5.010923 1.998936</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a aria-hidden="true" href="#cb38-1" tabindex="-1"></a><span class="co"># True parameters</span></span>
<span id="cb38-2"><a aria-hidden="true" href="#cb38-2" tabindex="-1"></a><span class="fu">c</span>(mu_true, sigma_true)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb40-1"><a aria-hidden="true" href="#cb40-1" tabindex="-1"></a><span class="co"># Simulate data from a t-distribution (misspecified case)</span></span>
<span id="cb40-2"><a aria-hidden="true" href="#cb40-2" tabindex="-1"></a>y_t <span class="ot">&lt;-</span> <span class="fu">rt</span>(n, <span class="at">df =</span> <span class="dv">3</span>) <span class="sc">*</span> sigma_true <span class="sc">+</span> mu_true <span class="co"># t-distribution with 3 df</span></span>
<span id="cb40-3"><a aria-hidden="true" href="#cb40-3" tabindex="-1"></a></span>
<span id="cb40-4"><a aria-hidden="true" href="#cb40-4" tabindex="-1"></a><span class="co"># MLE estimation (misspecified) assuming normal distribution</span></span>
<span id="cb40-5"><a aria-hidden="true" href="#cb40-5" tabindex="-1"></a>mle_result_misspec <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="at">par =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>), <span class="at">fn =</span> neg_loglik_normal, <span class="at">data =</span> y_t,</span>
<span id="cb40-6"><a aria-hidden="true" href="#cb40-6" tabindex="-1"></a>                          <span class="at">method =</span> <span class="st">"L-BFGS-B"</span>, <span class="at">lower =</span> <span class="fu">c</span>(<span class="sc">-</span><span class="cn">Inf</span>, <span class="fl">0.001</span>))</span>
<span id="cb40-7"><a aria-hidden="true" href="#cb40-7" tabindex="-1"></a></span>
<span id="cb40-8"><a aria-hidden="true" href="#cb40-8" tabindex="-1"></a><span class="co"># Estimated parameters under misspecification</span></span>
<span id="cb40-9"><a aria-hidden="true" href="#cb40-9" tabindex="-1"></a>mle_result_misspec<span class="sc">$</span>par</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 4.869772 3.157655</code></pre>
</div>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb42-1"><a aria-hidden="true" href="#cb42-1" tabindex="-1"></a><span class="co"># True parameters (for comparison, although the "true" sigma doesn't exist in the same way)</span></span>
<span id="cb42-2"><a aria-hidden="true" href="#cb42-2" tabindex="-1"></a><span class="fu">c</span>(mu_true, sigma_true)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 5 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a aria-hidden="true" href="#cb44-1" tabindex="-1"></a><span class="co"># Illustrative Plot:</span></span>
<span id="cb44-2"><a aria-hidden="true" href="#cb44-2" tabindex="-1"></a><span class="fu">hist</span>(y_t, <span class="at">breaks=</span><span class="dv">30</span>, <span class="at">probability=</span><span class="cn">TRUE</span>, <span class="at">main=</span><span class="st">"Histogram of t-distributed Data and Fitted Normal"</span>)</span>
<span id="cb44-3"><a aria-hidden="true" href="#cb44-3" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="at">mean=</span>mle_result_misspec<span class="sc">$</span>par[<span class="dv">1</span>], <span class="at">sd=</span>mle_result_misspec<span class="sc">$</span>par[<span class="dv">2</span>]),</span>
<span id="cb44-4"><a aria-hidden="true" href="#cb44-4" tabindex="-1"></a>      <span class="at">col=</span><span class="st">"red"</span>, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb44-5"><a aria-hidden="true" href="#cb44-5" tabindex="-1"></a><span class="fu">curve</span>(<span class="fu">dnorm</span>(x, <span class="at">mean=</span>mu_true, <span class="at">sd=</span>sigma_true<span class="sc">*</span><span class="fu">sqrt</span>(<span class="dv">3</span>)<span class="sc">/</span><span class="fu">sqrt</span>(<span class="dv">1</span>) ), <span class="at">col=</span><span class="st">"blue"</span>, <span class="at">add=</span><span class="cn">TRUE</span>, <span class="at">lty=</span><span class="dv">2</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span>
<span id="cb44-6"><a aria-hidden="true" href="#cb44-6" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">"topright"</span>, <span class="at">legend=</span><span class="fu">c</span>(<span class="st">"Fitted Normal"</span>, <span class="st">"True Normal(same variance)"</span>), <span class="at">col=</span><span class="fu">c</span>(<span class="st">"red"</span>, <span class="st">"blue"</span>), <span class="at">lty=</span><span class="dv">1</span><span class="sc">:</span><span class="dv">2</span>, <span class="at">lwd=</span><span class="dv">2</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap23_files/figure-html/unnamed-chunk-3-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load Packages:</strong> We might not need additional packages beyond what’s already loaded for basic MLE.</li>
<li><strong>Simulate Data (Normal):</strong> Simulate data from a normal distribution with known mean (<code>mu_true</code>) and standard deviation (<code>sigma_true</code>).</li>
<li><strong>Define Negative Log-Likelihood:</strong> Define the negative log-likelihood function for the normal distribution. We minimize the <em>negative</em> log-likelihood because <code>optim()</code> is a minimization function. The <code>if (sigma &lt;= 0)</code> part is important: it ensures that the optimization doesn’t try invalid parameter values (standard deviation must be positive).</li>
<li><strong>MLE (Correctly Specified):</strong> Use <code>optim()</code> to find the MLE estimates of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span> assuming a normal distribution. We use the “L-BFGS-B” method, which allows for box constraints (we constrain <span class="math inline">\(\sigma &gt; 0\)</span>).</li>
<li><strong>Estimated Parameters:</strong> Print the estimated parameters from the correctly specified model.</li>
<li><strong>Simulate Data (t-distribution):</strong> Simulate data from a t-distribution with 3 degrees of freedom. This represents the case where the true data generating process is <em>not</em> normal. We scale and shift the t-distribution so it has, approximately, mean of <code>mu_true</code> and standard deviation of <code>sigma_true</code>.</li>
<li><strong>MLE (Misspecified):</strong> Use <code>optim()</code> again, but this time apply the <em>normal</em> log-likelihood function to the <em>t-distributed</em> data. This is the misspecified case.</li>
<li><strong>Estimated Parameters (Misspecified):</strong> Print the estimated parameters under the misspecified model. The estimated mean will likely still be close to <code>mu_true</code>, demonstrating consistency of the mean estimator, but the estimated standard deviation will be biased.</li>
<li><strong>Illustrative plot</strong>: This plots the histogram of the t-distributed data, showing the data’s heavier tails. It also shows the fitted normal density from the misspecified MLE, along with a normal curve that has the same variance as the true t-distribution (which is <span class="math inline">\(\frac{df}{df-2}\sigma^2\)</span> for <span class="math inline">\(df&gt;2\)</span>).</li>
</ol>
<p><strong>Connection to Text:</strong></p>
<ul>
<li><strong>Maximum Likelihood Estimation:</strong> This script demonstrates MLE, which is a special case of an extremum estimator.</li>
<li><strong>Correctly Specified Likelihood:</strong> The first part of the script shows MLE when the model is correctly specified.</li>
<li><strong>Misspecified Likelihood:</strong> The second part shows what happens when the likelihood is misspecified (assuming normality when the data is t-distributed). The example demonstrates that the MLE for the mean can still be consistent even with misspecification of the distributional form, as discussed on page 325.</li>
<li><strong>Optimization:</strong> The script uses the R function <code>optim()</code>. This is a numerical method for finding the values that minimizes the objective function (the negative log-likelihood in this case)</li>
</ul>
</section>
<section class="level3" id="r-script-4-quantile-regression">
<h3 class="anchored" data-anchor-id="r-script-4-quantile-regression">R Script 4: Quantile Regression</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb45"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb45-1"><a aria-hidden="true" href="#cb45-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb45-2"><a aria-hidden="true" href="#cb45-2" tabindex="-1"></a><span class="fu">library</span>(quantreg) <span class="co"># For quantile regression</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: SparseM</code></pre>
</div>
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb47-1"><a aria-hidden="true" href="#cb47-1" tabindex="-1"></a><span class="co"># library(tidyverse) # If not already loaded</span></span>
<span id="cb47-2"><a aria-hidden="true" href="#cb47-2" tabindex="-1"></a></span>
<span id="cb47-3"><a aria-hidden="true" href="#cb47-3" tabindex="-1"></a><span class="co"># Simulate data for quantile regression (Example 23.3)</span></span>
<span id="cb47-4"><a aria-hidden="true" href="#cb47-4" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">1000</span></span>
<span id="cb47-5"><a aria-hidden="true" href="#cb47-5" tabindex="-1"></a>beta0 <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb47-6"><a aria-hidden="true" href="#cb47-6" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb47-7"><a aria-hidden="true" href="#cb47-7" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb47-8"><a aria-hidden="true" href="#cb47-8" tabindex="-1"></a>u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n) <span class="co"># Homoscedastic errors</span></span>
<span id="cb47-9"><a aria-hidden="true" href="#cb47-9" tabindex="-1"></a>y <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> x <span class="sc">+</span> u</span>
<span id="cb47-10"><a aria-hidden="true" href="#cb47-10" tabindex="-1"></a></span>
<span id="cb47-11"><a aria-hidden="true" href="#cb47-11" tabindex="-1"></a><span class="co"># Quantile regression for different quantiles</span></span>
<span id="cb47-12"><a aria-hidden="true" href="#cb47-12" tabindex="-1"></a>tau_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>, <span class="fl">0.9</span>) <span class="co"># Quantiles of interest</span></span>
<span id="cb47-13"><a aria-hidden="true" href="#cb47-13" tabindex="-1"></a></span>
<span id="cb47-14"><a aria-hidden="true" href="#cb47-14" tabindex="-1"></a><span class="co"># Store results</span></span>
<span id="cb47-15"><a aria-hidden="true" href="#cb47-15" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb47-16"><a aria-hidden="true" href="#cb47-16" tabindex="-1"></a></span>
<span id="cb47-17"><a aria-hidden="true" href="#cb47-17" tabindex="-1"></a><span class="cf">for</span> (tau <span class="cf">in</span> tau_values) {</span>
<span id="cb47-18"><a aria-hidden="true" href="#cb47-18" tabindex="-1"></a>  <span class="co"># rq() performs quantile regression</span></span>
<span id="cb47-19"><a aria-hidden="true" href="#cb47-19" tabindex="-1"></a>  rq_fit <span class="ot">&lt;-</span> <span class="fu">rq</span>(y <span class="sc">~</span> x, <span class="at">tau =</span> tau)</span>
<span id="cb47-20"><a aria-hidden="true" href="#cb47-20" tabindex="-1"></a>  results[[<span class="fu">as.character</span>(tau)]] <span class="ot">&lt;-</span> <span class="fu">coef</span>(rq_fit)</span>
<span id="cb47-21"><a aria-hidden="true" href="#cb47-21" tabindex="-1"></a>}</span>
<span id="cb47-22"><a aria-hidden="true" href="#cb47-22" tabindex="-1"></a></span>
<span id="cb47-23"><a aria-hidden="true" href="#cb47-23" tabindex="-1"></a><span class="co"># Print the estimated coefficients for each quantile</span></span>
<span id="cb47-24"><a aria-hidden="true" href="#cb47-24" tabindex="-1"></a><span class="fu">print</span>(results)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$`0.1`
(Intercept)           x 
 -0.2879848   2.0140112 

$`0.25`
(Intercept)           x 
  0.3539796   1.9972320 

$`0.5`
(Intercept)           x 
  0.9712894   2.0134440 

$`0.75`
(Intercept)           x 
   1.664000    2.028558 

$`0.9`
(Intercept)           x 
   2.299093    2.081870 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a aria-hidden="true" href="#cb49-1" tabindex="-1"></a><span class="co"># True coefficients (for comparison - in this case, they are the same for all quantiles)</span></span>
<span id="cb49-2"><a aria-hidden="true" href="#cb49-2" tabindex="-1"></a><span class="fu">c</span>(beta0, beta1)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 1 2</code></pre>
</div>
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb51-1"><a aria-hidden="true" href="#cb51-1" tabindex="-1"></a><span class="co"># Heteroscedastic errors</span></span>
<span id="cb51-2"><a aria-hidden="true" href="#cb51-2" tabindex="-1"></a>u_het <span class="ot">&lt;-</span> (<span class="dv">1</span> <span class="sc">+</span> <span class="fl">0.5</span> <span class="sc">*</span> <span class="fu">abs</span>(x)) <span class="sc">*</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb51-3"><a aria-hidden="true" href="#cb51-3" tabindex="-1"></a>y_het <span class="ot">&lt;-</span> beta0 <span class="sc">+</span> beta1 <span class="sc">*</span> x <span class="sc">+</span> u_het</span>
<span id="cb51-4"><a aria-hidden="true" href="#cb51-4" tabindex="-1"></a></span>
<span id="cb51-5"><a aria-hidden="true" href="#cb51-5" tabindex="-1"></a><span class="co"># Quantile regression with heteroscedastic errors</span></span>
<span id="cb51-6"><a aria-hidden="true" href="#cb51-6" tabindex="-1"></a>results_het <span class="ot">&lt;-</span> <span class="fu">list</span>()</span>
<span id="cb51-7"><a aria-hidden="true" href="#cb51-7" tabindex="-1"></a></span>
<span id="cb51-8"><a aria-hidden="true" href="#cb51-8" tabindex="-1"></a><span class="cf">for</span> (tau <span class="cf">in</span> tau_values) {</span>
<span id="cb51-9"><a aria-hidden="true" href="#cb51-9" tabindex="-1"></a>  rq_fit_het <span class="ot">&lt;-</span> <span class="fu">rq</span>(y_het <span class="sc">~</span> x, <span class="at">tau =</span> tau)</span>
<span id="cb51-10"><a aria-hidden="true" href="#cb51-10" tabindex="-1"></a>  results_het[[<span class="fu">as.character</span>(tau)]] <span class="ot">&lt;-</span> <span class="fu">coef</span>(rq_fit_het)</span>
<span id="cb51-11"><a aria-hidden="true" href="#cb51-11" tabindex="-1"></a>}</span>
<span id="cb51-12"><a aria-hidden="true" href="#cb51-12" tabindex="-1"></a></span>
<span id="cb51-13"><a aria-hidden="true" href="#cb51-13" tabindex="-1"></a><span class="fu">print</span>(results_het)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>$`0.1`
(Intercept)           x 
 -0.9632439   1.9441107 

$`0.25`
(Intercept)           x 
 0.05275925  1.96584052 

$`0.5`
(Intercept)           x 
  0.9845508   1.9546655 

$`0.75`
(Intercept)           x 
   2.030928    1.876352 

$`0.9`
(Intercept)           x 
   2.925622    1.934705 </code></pre>
</div>
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb53-1"><a aria-hidden="true" href="#cb53-1" tabindex="-1"></a><span class="co"># Create a plot to visualize quantile regression</span></span>
<span id="cb53-2"><a aria-hidden="true" href="#cb53-2" tabindex="-1"></a>df_plot <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">x=</span>x, <span class="at">y=</span>y_het)</span>
<span id="cb53-3"><a aria-hidden="true" href="#cb53-3" tabindex="-1"></a></span>
<span id="cb53-4"><a aria-hidden="true" href="#cb53-4" tabindex="-1"></a><span class="fu">ggplot</span>(df_plot, <span class="fu">aes</span>(<span class="at">x =</span> x, <span class="at">y =</span> y_het)) <span class="sc">+</span></span>
<span id="cb53-5"><a aria-hidden="true" href="#cb53-5" tabindex="-1"></a>    <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb53-6"><a aria-hidden="true" href="#cb53-6" tabindex="-1"></a>    <span class="fu">geom_quantile</span>(<span class="at">quantiles =</span> tau_values, <span class="at">color =</span> <span class="st">"red"</span>, <span class="at">size=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb53-7"><a aria-hidden="true" href="#cb53-7" tabindex="-1"></a>    <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Quantile Regression with Heteroscedastic Errors"</span>,</span>
<span id="cb53-8"><a aria-hidden="true" href="#cb53-8" tabindex="-1"></a>         <span class="at">x =</span> <span class="st">"x"</span>, <span class="at">y =</span> <span class="st">"y"</span>) <span class="sc">+</span></span>
<span id="cb53-9"><a aria-hidden="true" href="#cb53-9" tabindex="-1"></a>    <span class="fu">theme_bw</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.
ℹ Please use `linewidth` instead.</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Smoothing formula not specified. Using: y ~ x</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap23_files/figure-html/unnamed-chunk-4-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load Packages:</strong> Load <code>quantreg</code> for quantile regression.</li>
<li><strong>Simulate Data (Homoscedastic):</strong> Simulate data from a linear model <span class="math inline">\(y = \beta_0 + \beta_1 x + u\)</span>, where the errors <span class="math inline">\(u\)</span> are homoscedastic (constant variance).</li>
<li><strong>Quantile Regression:</strong> Perform quantile regression for different quantiles (<code>tau_values</code>) using the <code>rq()</code> function. We store the estimated coefficients for each quantile.</li>
<li><strong>Print Results (Homoscedastic):</strong> Print the estimated coefficients for each quantile. With homoscedastic errors, the slope coefficients should be similar across quantiles.</li>
<li><strong>Simulate Data (Heteroscedastic):</strong> Simulate data with heteroscedastic errors, where the variance of <span class="math inline">\(u\)</span> depends on <span class="math inline">\(x\)</span>.</li>
<li><strong>Quantile Regression (Heteroscedastic):</strong> Perform quantile regression again with the heteroscedastic data.</li>
<li><strong>Print Results (Heteroscedastic):</strong> Print the estimated coefficients. With heteroscedastic errors, the slope coefficients will generally be different across quantiles.</li>
<li><strong>Visualize</strong>: The <code>ggplot2</code> code creates a scatter plot and overlays the estimated quantile regression lines. With heteroscedasticity, you’ll see that the lines are not parallel, reflecting the changing conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span>.</li>
</ol>
<p><strong>Connection to Text:</strong></p>
<ul>
<li><strong>Quantile Regression:</strong> This script directly implements quantile regression as described in Section 23.3.</li>
<li><strong><code>rq()</code> Function:</strong> The <code>rq()</code> function from the <code>quantreg</code> package performs the quantile regression estimation.</li>
<li><strong>Heteroscedasticity:</strong> The script demonstrates how quantile regression can be used to model the conditional distribution of <span class="math inline">\(y\)</span> given <span class="math inline">\(x\)</span> even when the errors are heteroscedastic. The coefficients obtained for the different quantiles will be different. This is discussed in the context of mispecification. The conditional mean model may be correct but not the assumptions about the error term.</li>
<li><strong>Visualization:</strong> The visualization helps to see the difference between OLS regression and quantile regression. Quantile regression captures how the <em>entire conditional distribution</em> changes with <span class="math inline">\(x\)</span>, not just the conditional mean.</li>
</ul>
</section>
<section class="level3" id="r-script-5-consistency-of-extremum-estimators">
<h3 class="anchored" data-anchor-id="r-script-5-consistency-of-extremum-estimators">R Script 5: Consistency of Extremum Estimators</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb56-1"><a aria-hidden="true" href="#cb56-1" tabindex="-1"></a><span class="co"># Load necessary packages</span></span>
<span id="cb56-2"><a aria-hidden="true" href="#cb56-2" tabindex="-1"></a><span class="co"># library(tidyverse)</span></span>
<span id="cb56-3"><a aria-hidden="true" href="#cb56-3" tabindex="-1"></a></span>
<span id="cb56-4"><a aria-hidden="true" href="#cb56-4" tabindex="-1"></a><span class="co"># Simulate data from a simple linear model</span></span>
<span id="cb56-5"><a aria-hidden="true" href="#cb56-5" tabindex="-1"></a>n_values <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">10</span>, <span class="dv">50</span>, <span class="dv">100</span>, <span class="dv">500</span>, <span class="dv">1000</span>, <span class="dv">5000</span>)  <span class="co"># Different sample sizes</span></span>
<span id="cb56-6"><a aria-hidden="true" href="#cb56-6" tabindex="-1"></a>beta_true <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb56-7"><a aria-hidden="true" href="#cb56-7" tabindex="-1"></a>results_ols <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(n_values))</span>
<span id="cb56-8"><a aria-hidden="true" href="#cb56-8" tabindex="-1"></a></span>
<span id="cb56-9"><a aria-hidden="true" href="#cb56-9" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(n_values)) {</span>
<span id="cb56-10"><a aria-hidden="true" href="#cb56-10" tabindex="-1"></a>  n <span class="ot">&lt;-</span> n_values[i]</span>
<span id="cb56-11"><a aria-hidden="true" href="#cb56-11" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb56-12"><a aria-hidden="true" href="#cb56-12" tabindex="-1"></a>  u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb56-13"><a aria-hidden="true" href="#cb56-13" tabindex="-1"></a>  y <span class="ot">&lt;-</span> beta_true <span class="sc">*</span> x <span class="sc">+</span> u</span>
<span id="cb56-14"><a aria-hidden="true" href="#cb56-14" tabindex="-1"></a></span>
<span id="cb56-15"><a aria-hidden="true" href="#cb56-15" tabindex="-1"></a>  <span class="co"># OLS estimation (an extremum estimator)</span></span>
<span id="cb56-16"><a aria-hidden="true" href="#cb56-16" tabindex="-1"></a>  lm_fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x)</span>
<span id="cb56-17"><a aria-hidden="true" href="#cb56-17" tabindex="-1"></a>  results_ols[i] <span class="ot">&lt;-</span> <span class="fu">coef</span>(lm_fit)[<span class="dv">2</span>] <span class="co"># Store the estimated slope coefficient</span></span>
<span id="cb56-18"><a aria-hidden="true" href="#cb56-18" tabindex="-1"></a>}</span>
<span id="cb56-19"><a aria-hidden="true" href="#cb56-19" tabindex="-1"></a></span>
<span id="cb56-20"><a aria-hidden="true" href="#cb56-20" tabindex="-1"></a><span class="co"># Create a data frame for plotting</span></span>
<span id="cb56-21"><a aria-hidden="true" href="#cb56-21" tabindex="-1"></a>df_results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">n =</span> n_values, <span class="at">beta_hat =</span> results_ols)</span>
<span id="cb56-22"><a aria-hidden="true" href="#cb56-22" tabindex="-1"></a></span>
<span id="cb56-23"><a aria-hidden="true" href="#cb56-23" tabindex="-1"></a><span class="co"># Plot the estimated coefficients against sample size</span></span>
<span id="cb56-24"><a aria-hidden="true" href="#cb56-24" tabindex="-1"></a><span class="fu">ggplot</span>(df_results, <span class="fu">aes</span>(<span class="at">x =</span> n, <span class="at">y =</span> beta_hat)) <span class="sc">+</span></span>
<span id="cb56-25"><a aria-hidden="true" href="#cb56-25" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb56-26"><a aria-hidden="true" href="#cb56-26" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> beta_true, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb56-27"><a aria-hidden="true" href="#cb56-27" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Consistency of OLS Estimator"</span>,</span>
<span id="cb56-28"><a aria-hidden="true" href="#cb56-28" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Sample Size (n)"</span>,</span>
<span id="cb56-29"><a aria-hidden="true" href="#cb56-29" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Estimated Slope Coefficient (beta_hat)"</span>) <span class="sc">+</span></span>
<span id="cb56-30"><a aria-hidden="true" href="#cb56-30" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">breaks=</span>n_values) <span class="sc">+</span></span>
<span id="cb56-31"><a aria-hidden="true" href="#cb56-31" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap23_files/figure-html/unnamed-chunk-5-1.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb57-1"><a aria-hidden="true" href="#cb57-1" tabindex="-1"></a><span class="co"># Example with a non-linear objective function.</span></span>
<span id="cb57-2"><a aria-hidden="true" href="#cb57-2" tabindex="-1"></a><span class="co"># We will use an estimator that minimizes the sum of 4th powers of errors.</span></span>
<span id="cb57-3"><a aria-hidden="true" href="#cb57-3" tabindex="-1"></a>results_m <span class="ot">&lt;-</span> <span class="fu">numeric</span>(<span class="fu">length</span>(n_values))</span>
<span id="cb57-4"><a aria-hidden="true" href="#cb57-4" tabindex="-1"></a>obj_fun <span class="ot">&lt;-</span> <span class="cf">function</span>(beta, x, y) {</span>
<span id="cb57-5"><a aria-hidden="true" href="#cb57-5" tabindex="-1"></a>    <span class="fu">sum</span>((y<span class="sc">-</span>beta<span class="sc">*</span>x)<span class="sc">^</span><span class="dv">4</span>)</span>
<span id="cb57-6"><a aria-hidden="true" href="#cb57-6" tabindex="-1"></a>}</span>
<span id="cb57-7"><a aria-hidden="true" href="#cb57-7" tabindex="-1"></a><span class="cf">for</span> (i <span class="cf">in</span> <span class="fu">seq_along</span>(n_values)) {</span>
<span id="cb57-8"><a aria-hidden="true" href="#cb57-8" tabindex="-1"></a>  n <span class="ot">&lt;-</span> n_values[i]</span>
<span id="cb57-9"><a aria-hidden="true" href="#cb57-9" tabindex="-1"></a>  x <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb57-10"><a aria-hidden="true" href="#cb57-10" tabindex="-1"></a>  u <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb57-11"><a aria-hidden="true" href="#cb57-11" tabindex="-1"></a>  y <span class="ot">&lt;-</span> beta_true <span class="sc">*</span> x <span class="sc">+</span> u</span>
<span id="cb57-12"><a aria-hidden="true" href="#cb57-12" tabindex="-1"></a></span>
<span id="cb57-13"><a aria-hidden="true" href="#cb57-13" tabindex="-1"></a>  <span class="co"># M-estimation (minimizing sum of 4th powers of errors)</span></span>
<span id="cb57-14"><a aria-hidden="true" href="#cb57-14" tabindex="-1"></a>  m_fit <span class="ot">&lt;-</span> <span class="fu">optim</span>(<span class="at">par=</span><span class="dv">0</span>, <span class="at">fn=</span>obj_fun, <span class="at">x=</span>x, <span class="at">y=</span>y, <span class="at">method=</span><span class="st">"Brent"</span>, <span class="at">lower=</span><span class="sc">-</span><span class="dv">10</span>, <span class="at">upper=</span><span class="dv">10</span>)</span>
<span id="cb57-15"><a aria-hidden="true" href="#cb57-15" tabindex="-1"></a>  results_m[i] <span class="ot">&lt;-</span> m_fit<span class="sc">$</span>par <span class="co"># Store the estimated slope coefficient</span></span>
<span id="cb57-16"><a aria-hidden="true" href="#cb57-16" tabindex="-1"></a>}</span>
<span id="cb57-17"><a aria-hidden="true" href="#cb57-17" tabindex="-1"></a></span>
<span id="cb57-18"><a aria-hidden="true" href="#cb57-18" tabindex="-1"></a>df_results_m <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">n =</span> n_values, <span class="at">beta_hat =</span> results_m)</span>
<span id="cb57-19"><a aria-hidden="true" href="#cb57-19" tabindex="-1"></a><span class="fu">ggplot</span>(df_results_m, <span class="fu">aes</span>(<span class="at">x =</span> n, <span class="at">y =</span> beta_hat)) <span class="sc">+</span></span>
<span id="cb57-20"><a aria-hidden="true" href="#cb57-20" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb57-21"><a aria-hidden="true" href="#cb57-21" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> beta_true, <span class="at">linetype =</span> <span class="st">"dashed"</span>, <span class="at">color =</span> <span class="st">"red"</span>) <span class="sc">+</span></span>
<span id="cb57-22"><a aria-hidden="true" href="#cb57-22" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Consistency of M-Estimator"</span>,</span>
<span id="cb57-23"><a aria-hidden="true" href="#cb57-23" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Sample Size (n)"</span>,</span>
<span id="cb57-24"><a aria-hidden="true" href="#cb57-24" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Estimated Slope Coefficient (beta_hat)"</span>) <span class="sc">+</span></span>
<span id="cb57-25"><a aria-hidden="true" href="#cb57-25" tabindex="-1"></a>    <span class="fu">scale_x_continuous</span>(<span class="at">breaks=</span>n_values) <span class="sc">+</span></span>
<span id="cb57-26"><a aria-hidden="true" href="#cb57-26" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap23_files/figure-html/unnamed-chunk-5-2.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Load Packages:</strong> We might not need additional packages beyond what’s been used before.</li>
<li><strong>Simulate Data:</strong> Simulate data from a simple linear model <span class="math inline">\(y = \beta x + u\)</span> for different sample sizes (<code>n_values</code>).</li>
<li><strong>OLS Estimation:</strong> For each sample size, estimate the slope coefficient <span class="math inline">\(\beta\)</span> using OLS (which is an extremum estimator, minimizing the sum of squared residuals).</li>
<li><strong>Store Results:</strong> Store the estimated slope coefficients in <code>results_ols</code>.</li>
<li><strong>Create Data Frame:</strong> Create a data frame to hold the sample sizes and corresponding estimated coefficients.</li>
<li><strong>Plot Results:</strong> Plot the estimated coefficients against the sample size. You should see that as the sample size increases, the estimated coefficients converge to the true value of <span class="math inline">\(\beta\)</span> (which is 2 in this simulation). This illustrates <strong>consistency</strong>.</li>
<li><strong>M-estimation:</strong> Repeat the simulation, but now instead of OLS, estimate the parameter by minimizing the sum of the 4th powers of the error. This shows consistency of a different extremum estimator.</li>
</ol>
<p><strong>Connection to Text:</strong></p>
<ul>
<li><strong>Extremum Estimators:</strong> This script uses OLS as an example of an extremum estimator.</li>
<li><strong>Consistency:</strong> The script demonstrates the concept of consistency (Theorem 23.1), which is a key property of extremum estimators. As the sample size grows, the estimator converges in probability to the true parameter value.</li>
<li><strong>Simulation:</strong> Simulation is a powerful tool for understanding asymptotic properties like consistency. We can visually see how the estimator behaves as the sample size increases.</li>
<li><strong>Non-linear objective function:</strong> The second example demonstrates the consistency when using a different objective function (sum of 4th power of errors).</li>
</ul>
</section>
</section>
<section class="level2" id="youtube-videos-on-gmm-and-extremum-estimators">
<h2 class="anchored" data-anchor-id="youtube-videos-on-gmm-and-extremum-estimators">YouTube Videos on GMM and Extremum Estimators</h2>
<p>Here are some YouTube videos that explain concepts related to the attached text, along with verification of their availability and explanations of their relevance:</p>
<section class="level3" id="generalized-method-of-moments-gmm---introduction">
<h3 class="anchored" data-anchor-id="generalized-method-of-moments-gmm---introduction">1. Generalized Method of Moments (GMM) - Introduction</h3>
<ul>
<li><strong>Title:</strong> “11.1 Generalized Method of Moments (GMM): Introduction”</li>
<li><strong>Channel:</strong> Ben Lambert</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=GMVh02P-Oug">https://www.youtube.com/watch?v=GMVh02P-Oug</a></li>
<li><strong>Availability:</strong> Verified (October 26, 2023)</li>
<li><strong>Relevance:</strong> This video provides a good introduction to GMM, explaining the basic idea of using moment conditions to estimate parameters. It connects directly to <strong>Section 23.1</strong> of the text. It introduces the concept in a clear, intuitive manner, starting with simpler examples before building up to the general framework. Covers the unidentified, exactly identified and overidentified cases.</li>
</ul>
</section>
<section class="level3" id="gmm---intuition">
<h3 class="anchored" data-anchor-id="gmm---intuition">2. GMM - Intuition</h3>
<ul>
<li><strong>Title:</strong> “11.2 Generalized Method of Moments (GMM): Intuition”</li>
<li><strong>Channel:</strong> Ben Lambert</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=3FzKZv4s8O8">https://www.youtube.com/watch?v=3FzKZv4s8O8</a></li>
<li><strong>Availability:</strong> Verified (October 26, 2023)</li>
<li><strong>Relevance:</strong> This video builds intuition for GMM, explaining why it works and how the weighting matrix is involved. It helps understand the underlying principles behind minimizing the sample moment conditions, as described in <strong>Section 23.1</strong>. The intuition behind identification is presented.</li>
</ul>
</section>
<section class="level3" id="gmm---example">
<h3 class="anchored" data-anchor-id="gmm---example">3. GMM - Example</h3>
<ul>
<li><strong>Title:</strong> “11.3 Generalized Method of Moments (GMM): Example”</li>
<li><strong>Channel:</strong> Ben Lambert</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=YS-ojO-t8aM">https://www.youtube.com/watch?v=YS-ojO-t8aM</a></li>
<li><strong>Availability:</strong> Verified (October 26, 2023)</li>
<li><strong>Relevance</strong>: This video goes through a specific example using GMM. It closely relates to <strong>Example 23.8</strong> and <strong>Example 23.9</strong>, showing a practical application of the GMM estimator in context that are very similar to linear regression and instrumental variables setting.</li>
</ul>
</section>
<section class="level3" id="gmm-derivation-of-asymptotic-distribution">
<h3 class="anchored" data-anchor-id="gmm-derivation-of-asymptotic-distribution">4. GMM Derivation of Asymptotic Distribution</h3>
<ul>
<li><strong>Title:</strong> “11.6 Generalized Method of Moments (GMM): Asymptotic properties”</li>
<li><strong>Channel:</strong> Ben Lambert</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=jPAwA-qiL4Y">https://www.youtube.com/watch?v=jPAwA-qiL4Y</a></li>
<li><strong>Availability:</strong> Verified (October 26, 2023)</li>
<li><strong>Relevance:</strong> This video covers the asymptotic properties of the GMM estimator, including consistency and asymptotic normality. It aligns with <strong>Section 23.2</strong>, particularly <strong>Theorem 23.4</strong>, although it focuses specifically on GMM rather than general extremum estimators.</li>
</ul>
</section>
<section class="level3" id="introduction-to-maximum-likelihood-estimation-mle">
<h3 class="anchored" data-anchor-id="introduction-to-maximum-likelihood-estimation-mle">5. Introduction to Maximum Likelihood Estimation (MLE)</h3>
<ul>
<li><strong>Title:</strong> “Maximum Likelihood, clearly explained!!!”</li>
<li><strong>Channel:</strong> StatQuest with Josh Starmer</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=XepXtl9YKwc">https://www.youtube.com/watch?v=XepXtl9YKwc</a></li>
<li><strong>Availability:</strong> Verified (October 26, 2023)</li>
<li><strong>Relevance:</strong> While the text focuses more on GMM, it also mentions Maximum Likelihood Estimation (MLE) as a special case of an extremum estimator. This video provides an excellent, intuitive introduction to MLE, explaining the core concepts in a visually clear way. This relates to the discussion of M-estimators and to the section on <strong>Correctly Specified Likelihood</strong> and <strong>Misspecified Likelihood</strong>, page 325.</li>
</ul>
</section>
<section class="level3" id="consistency-bias-and-efficiency-of-estimators">
<h3 class="anchored" data-anchor-id="consistency-bias-and-efficiency-of-estimators">6. Consistency, Bias and Efficiency of Estimators</h3>
<ul>
<li><strong>Title:</strong> “Estimators - Unbiasedness, Consistency, Efficiency (Intuition + Examples)”</li>
<li><strong>Channel:</strong> Shokoufeh Mirzaei</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=_SpL2HDA_Cc&amp;t=607s">https://www.youtube.com/watch?v=_SpL2HDA_Cc&amp;t=607s</a></li>
<li><strong>Availability:</strong> Verified (October 26, 2023)</li>
<li><strong>Relevance:</strong> This video helps build a solid foundation for <strong>Section 23.2</strong> by clearly explaining the concepts of <strong>consistency</strong>, bias, and efficiency of estimators. It is not specific to extremum estimators but introduces essential ideas for understanding their properties. It is a good primer before tackling Theorems 23.1, 23.3 and 23.4.</li>
</ul>
</section>
<section class="level3" id="quantile-regression-1">
<h3 class="anchored" data-anchor-id="quantile-regression-1">7. Quantile Regression</h3>
<ul>
<li><strong>Title:</strong> “Quantile Regression”</li>
<li><strong>Channel:</strong> Ben Jann</li>
<li><strong>Link:</strong> <a href="https://www.youtube.com/watch?v=gGz-T3qe5vg">https://www.youtube.com/watch?v=gGz-T3qe5vg</a></li>
<li><strong>Availability:</strong> Verified (October 26, 2023)</li>
<li><strong>Relevance:</strong> This video gives a comprehensive introduction to <strong>quantile regression</strong>, covering the basic idea, estimation, and interpretation. This is a good source for better understanding <strong>Section 23.3</strong> of the provided material. It complements the text by providing visual examples.</li>
</ul>
<p>These videos provide a combination of theoretical explanations, intuitive examples, and practical applications related to the core concepts in the provided text. They cover GMM, MLE, consistency, and quantile regression, and they use a mix of visual aids, derivations, and examples to enhance understanding.</p>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch23mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch23mcsolution1">MC Solution 1</a></p>
<p>In the Generalized Method of Moments (GMM), what does the equation <span class="math inline">\(\mathbb{E}[g(Z_i, \theta_0)] = 0\)</span> represent?</p>
<ol type="a">
<li>The objective function to be minimized.</li>
<li>The asymptotic variance of the estimator.</li>
<li>The moment conditions that identify the parameters.</li>
<li>The weighting matrix used in estimation.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch23mcsolution2">MC Solution 2</a></p>
<p>Which of the following cases describes an <strong>overidentified</strong> GMM model?</p>
<ol type="a">
<li>The number of parameters (<span class="math inline">\(p\)</span>) is greater than the number of moment conditions (<span class="math inline">\(q\)</span>).</li>
<li>The number of parameters (<span class="math inline">\(p\)</span>) is equal to the number of moment conditions (<span class="math inline">\(q\)</span>).</li>
<li>The number of parameters (<span class="math inline">\(p\)</span>) is less than the number of moment conditions (<span class="math inline">\(q\)</span>).</li>
<li>The number of parameters (<span class="math inline">\(p\)</span>) is unrelated to the number of moment conditions (<span class="math inline">\(q\)</span>).</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch23mcsolution3">MC Solution 3</a></p>
<p>In the simultaneous equations model <span class="math inline">\(B(\theta)y_i = C(\theta)x_i + u_i\)</span>, what is required for the reduced form to exist?</p>
<ol type="a">
<li><span class="math inline">\(C(\theta)\)</span> must be invertible.</li>
<li><span class="math inline">\(B(\theta)\)</span> must be invertible.</li>
<li><span class="math inline">\(u_i\)</span> must be normally distributed.</li>
<li><span class="math inline">\(x_i\)</span> and <span class="math inline">\(u_i\)</span> must be uncorrelated.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch23mcsolution4">MC Solution 4</a></p>
<p>In the Hansen and Singleton (1982) model (Example 23.2), what does <span class="math inline">\(\gamma\)</span> represent?</p>
<ol type="a">
<li>The discount factor.</li>
<li>The coefficient of relative risk aversion.</li>
<li>The gross return on an asset.</li>
<li>The utility function.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch23mcsolution5">MC Solution 5</a></p>
<p>What is the role of the weighting matrix <span class="math inline">\(W_n(\theta)\)</span> in GMM?</p>
<ol type="a">
<li>It determines the asymptotic variance of the estimator, regardless of the moment conditions.</li>
<li>It determines the relative importance given to each moment condition in estimation.</li>
<li>It ensures that the estimator is unbiased.</li>
<li>It is only relevant in the exactly identified case.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch23mcsolution6">MC Solution 6</a></p>
<p>Which of the following is NOT an example of an extremum estimator?</p>
<ol type="a">
<li>Ordinary Least Squares (OLS)</li>
<li>Maximum Likelihood Estimation (MLE)</li>
<li>Generalized Method of Moments (GMM)</li>
<li>The sample median</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch23mcsolution7">MC Solution 7</a></p>
<p>What does the <strong>identification condition</strong> for consistency of an extremum estimator require?</p>
<ol type="a">
<li>The objective function must be differentiable.</li>
<li>The limit of the objective function must have a unique global minimum at the true parameter value.</li>
<li>The sample size must be sufficiently large.</li>
<li>The data must be normally distributed.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch23mcsolution8">MC Solution 8</a></p>
<p>What does <strong>uniform convergence</strong> of <span class="math inline">\(Q_n(\theta)\)</span> to <span class="math inline">\(Q(\theta)\)</span> mean in the context of extremum estimators?</p>
<ol type="a">
<li><span class="math inline">\(Q_n(\theta)\)</span> converges to <span class="math inline">\(Q(\theta)\)</span> pointwise for each <span class="math inline">\(\theta\)</span>.</li>
<li><span class="math inline">\(Q_n(\theta)\)</span> converges to <span class="math inline">\(Q(\theta)\)</span> at the same rate for all <span class="math inline">\(\theta\)</span>.</li>
<li><span class="math inline">\(\sup_{\theta \in \Theta} |Q_n(\theta) - Q(\theta)|\)</span> converges to zero.</li>
<li><span class="math inline">\(Q_n(\theta)\)</span> and <span class="math inline">\(Q(\theta)\)</span> are both continuous functions.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch23mcsolution9">MC Solution 9</a></p>
<p>In the proof of Theorem 23.1 (Consistency), what is the role of term II, <span class="math inline">\(Q_n(\hat{\theta}) - Q_n(\theta_0)\)</span>?</p>
<ol type="a">
<li>It converges to zero in probability due to uniform convergence.</li>
<li>It is always non-positive because <span class="math inline">\(\hat{\theta}\)</span> minimizes <span class="math inline">\(Q_n(\theta)\)</span>.</li>
<li>It is equal to the asymptotic variance of the estimator.</li>
<li>It is used to establish the identification condition.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch23mcsolution10">MC Solution 10</a>(#sec-ch23mcsolution10}</p>
<p>Which condition is NOT required for the consistency of an extremum estimator (Theorem 23.1)?</p>
<ol type="a">
<li>The parameter space <span class="math inline">\(\Theta\)</span> is compact.</li>
<li><span class="math inline">\(Q_n(\theta)\)</span> is continuous in <span class="math inline">\(\theta\)</span>.</li>
<li><span class="math inline">\(Q_n(\theta)\)</span> converges uniformly to <span class="math inline">\(Q(\theta)\)</span>.</li>
<li><span class="math inline">\(Q_n(\theta)\)</span> is differentiable.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch23mcsolution11">MC Solution 11</a>(#sec-ch23mcsolution11}</p>
<p>In the context of Theorem 23.4 (Asymptotic Normality), what does the matrix <span class="math inline">\(A\)</span> represent?</p>
<ol type="a">
<li>The weighting matrix.</li>
<li>The asymptotic variance of the estimator.</li>
<li>The limit of the second derivative of the objective function.</li>
<li>The score function.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch23mcsolution12">MC Solution 12</a>(#sec-ch23mcsolution12}</p>
<p>Under correct specification of the likelihood, what is the relationship between the matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in Theorem 23.4?</p>
<ol type="a">
<li><span class="math inline">\(A = B^{-1}\)</span></li>
<li><span class="math inline">\(A = B\)</span></li>
<li><span class="math inline">\(A = 2B\)</span></li>
<li><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are unrelated.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch23mcsolution13">MC Solution 13</a>(#sec-ch23mcsolution13}</p>
<p>What is the typical form of the asymptotic variance of the GMM estimator in the overidentified case?</p>
<ol type="a">
<li><span class="math inline">\(\Gamma^{-1}\Omega\Gamma^{\top -1}\)</span></li>
<li><span class="math inline">\((\Gamma^\top W \Gamma)^{-1}\)</span></li>
<li><span class="math inline">\((\Gamma^\top W \Gamma)^{-1} \Gamma^\top W \Omega W \Gamma (\Gamma^\top W \Gamma)^{-1}\)</span></li>
<li><span class="math inline">\(W^{-1}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch23mcsolution14">MC Solution 14</a>(#sec-ch23mcsolution14}</p>
<p>In Example 23.9 (Instrumental Variables), what condition is necessary for the IV estimator to be well-defined?</p>
<ol type="a">
<li>The number of instruments (<span class="math inline">\(J\)</span>) must be less than the number of parameters (<span class="math inline">\(K\)</span>).</li>
<li>The number of instruments (<span class="math inline">\(J\)</span>) must be equal to the number of parameters (<span class="math inline">\(K\)</span>).</li>
<li>The number of instruments (<span class="math inline">\(J\)</span>) must be greater than or equal to the number of parameters (<span class="math inline">\(K\)</span>).</li>
<li>The instruments must be perfectly correlated with the endogenous regressors.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch23mcsolution15">MC Solution 15</a>(#sec-ch23mcsolution15}</p>
<p>What is the <strong>check function</strong>, <span class="math inline">\(\rho_\alpha(u)\)</span>, used for in quantile regression?</p>
<ol type="a">
<li>To define the weighting matrix.</li>
<li>To define the objective function that is minimized.</li>
<li>To define the asymptotic variance of the estimator.</li>
<li>To define the moment conditions.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch23mcsolution16">MC Solution 16</a>(#sec-ch23mcsolution16}</p>
<p>What is the <strong>monotone equivariance property</strong> of quantile regression?</p>
<ol type="a">
<li>Quantile regression is robust to outliers.</li>
<li>Quantile regression coefficients are always positive.</li>
<li>Applying a monotonic transformation to the response variable transforms the conditional quantiles in the same way.</li>
<li>The sum of the residuals in quantile regression is always zero.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch23mcsolution17">MC Solution 17</a>(#sec-ch23mcsolution17}</p>
<p>In the Glivenko-Cantelli theorem (Theorem 23.2), what does <span class="math inline">\(F_n(x)\)</span> represent? a) The probability density function. b) The true cumulative distribution function. c) The empirical cumulative distribution function. d) The quantile function</p>
</section>
<section class="level3" id="sec-ch23mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch23mcsolution18">MC Solution 18</a>(#sec-ch23mcsolution18}</p>
<p>In a <strong>misspecified likelihood</strong> setting, what can be said about the MLE of the parameters of interest?</p>
<ol type="a">
<li>It is always inconsistent.</li>
<li>It is always unbiased.</li>
<li>It may be consistent under certain conditions, such as correct specification of the conditional mean.</li>
<li>It is asymptotically normally distributed.</li>
</ol>
</section>
<section class="level3" id="sec-ch23mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch23mcsolution19%7D%20In%20Lemma%2023.1%20concerning%20identification%20of%20the%20MLE,%20what%20does%20Jensen's%20inequality%20imply?%20a">MC Solution 19</a> <span class="math inline">\(Q(\theta_0) - Q(\theta) &lt; 0\)</span> for all <span class="math inline">\(\theta \neq \theta_0\)</span>. b) <span class="math inline">\(Q(\theta_0) - Q(\theta) = 0\)</span> for all <span class="math inline">\(\theta \neq \theta_0\)</span>. c) <span class="math inline">\(Q(\theta_0) - Q(\theta) &gt; 0\)</span> for all <span class="math inline">\(\theta \neq \theta_0\)</span>. d) <span class="math inline">\(Q(\theta_0) - Q(\theta)\)</span> can be positive or negative.</p>
</section>
<section class="level3" id="sec-ch23mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch23mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch23mcsolution20%7D%20Which%20of%20the%20following%20best%20describes%20the%20concept%20of%20**shrinking%20neighborhoods**%20as%20used%20in%20Theorem%2023.3?%20a">MC Solution 20</a> Neighborhoods of <span class="math inline">\(\theta_0\)</span> of fixed size. b) Neighborhoods of <span class="math inline">\(\theta_0\)</span> whose size goes to infinity with <span class="math inline">\(n\)</span>. c) Neighborhoods of <span class="math inline">\(\theta_0\)</span> whose size decreases to zero as <span class="math inline">\(n\)</span> goes to infinity. d) The entire parameter space <span class="math inline">\(\Theta\)</span>.</p>
</section>
</section>
<section class="level2" id="multiple-choice-solutions">
<h2 class="anchored" data-anchor-id="multiple-choice-solutions">Multiple Choice Solutions</h2>
<section class="level3" id="sec-ch23mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch23mcexercise1">MC Exercise 1</a></p>
<p>The correct answer is (c).</p>
<p>The equation <span class="math inline">\(\mathbb{E}[g(Z_i, \theta_0)] = 0\)</span> represents the <strong>moment conditions</strong> that identify the parameters in GMM. These are the population moment conditions that the GMM estimator seeks to match using sample moments. This is the foundation of GMM, as explained in <strong>Section 23.1</strong>.</p>
</section>
<section class="level3" id="sec-ch23mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch23mcexercise2">MC Exercise 2</a></p>
<p>The correct answer is (c).</p>
<p>An <strong>overidentified</strong> GMM model has more moment conditions (<span class="math inline">\(q\)</span>) than parameters (<span class="math inline">\(p\)</span>). This means we have more information than is strictly necessary to identify the parameters, which allows for testing the validity of the moment conditions. The different cases are described on page 313.</p>
</section>
<section class="level3" id="sec-ch23mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch23mcexercise3">MC Exercise 3</a></p>
<p>The correct answer is (b).</p>
<p>The reduced form is obtained by solving for <span class="math inline">\(y_i\)</span> in terms of <span class="math inline">\(x_i\)</span> and <span class="math inline">\(u_i\)</span>. This requires multiplying both sides of the equation by <span class="math inline">\(B(\theta)^{-1}\)</span>, which is only possible if <span class="math inline">\(B(\theta)\)</span> is invertible. See <strong>Example 23.1</strong>.</p>
</section>
<section class="level3" id="sec-ch23mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch23mcexercise4">MC Exercise 4</a></p>
<p>The correct answer is (b).</p>
<p>In the Hansen and Singleton (1982) model, <span class="math inline">\(\gamma\)</span> represents the <strong>coefficient of relative risk aversion</strong>. This parameter determines the curvature of the utility function and reflects the agent’s aversion to risk. See <strong>Example 23.2</strong>.</p>
</section>
<section class="level3" id="sec-ch23mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch23mcexercise5">MC Exercise 5</a></p>
<p>The correct answer is (b).</p>
<p>The weighting matrix <span class="math inline">\(W_n(\theta)\)</span> determines the <strong>relative importance</strong> given to each moment condition in forming the GMM estimator. Different choices of <span class="math inline">\(W_n(\theta)\)</span> lead to different GMM estimators, with the optimal weighting matrix (inverse of the asymptotic variance of the sample moments) leading to the most efficient estimator. This is discussed on page 315.</p>
</section>
<section class="level3" id="sec-ch23mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch23mcexercise6">MC Exercise 6</a></p>
<p>The correct answer is (d).</p>
<p>The sample median is an estimator, but not necessarily an <em>extremum</em> estimator in the general sense described in the text. While the sample median <em>does</em> minimize the sum of absolute deviations, it is not generally presented within the framework of minimizing a smooth objective function. OLS, MLE, and GMM are all found by minimizing or maximizing a specific objective function.</p>
</section>
<section class="level3" id="sec-ch23mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch23mcexercise7">MC Exercise 7</a></p>
<p>The correct answer is (b).</p>
<p>The <strong>identification condition</strong> requires that the limit of the objective function, <span class="math inline">\(Q(\theta)\)</span>, has a <strong>unique global minimum</strong> at the true parameter value <span class="math inline">\(\theta_0\)</span>. This ensures that we can distinguish the true parameter value from other values based on the objective function. This concept is defined on page 316.</p>
</section>
<section class="level3" id="sec-ch23mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch23mcexercise8">MC Exercise 8</a></p>
<p>The correct answer is (c).</p>
<p><strong>Uniform convergence</strong> means that the <em>maximum</em> difference between <span class="math inline">\(Q_n(\theta)\)</span> and <span class="math inline">\(Q(\theta)\)</span> over the entire parameter space <span class="math inline">\(\Theta\)</span> converges to zero. This is a stronger condition than pointwise convergence (which only requires convergence for each fixed <span class="math inline">\(\theta\)</span>). This is defined on page 317.</p>
</section>
<section class="level3" id="sec-ch23mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch23mcexercise9">MC Exercise 9</a></p>
<p>The correct answer is (b).</p>
<p>Term II, <span class="math inline">\(Q_n(\hat{\theta}) - Q_n(\theta_0)\)</span>, is <strong>always non-positive</strong> because <span class="math inline">\(\hat{\theta}\)</span> is defined as the value that <em>minimizes</em> <span class="math inline">\(Q_n(\theta)\)</span>. Therefore, <span class="math inline">\(Q_n(\theta)\)</span> evaluated at <span class="math inline">\(\hat{\theta}\)</span> cannot be greater than <span class="math inline">\(Q_n(\theta)\)</span> evaluated at any other point, including <span class="math inline">\(\theta_0\)</span>. This is explained in the proof of Theorem 23.1 on page 317.</p>
</section>
<section class="level3" id="sec-ch23mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch23mcexercise10">MC Exercise 10</a></p>
<p>The correct answer is (d).</p>
<p>While differentiability is often assumed for convenience and for deriving asymptotic normality, it is <em>not</em> strictly required for consistency. The consistency theorem (Theorem 23.1) only requires continuity of <span class="math inline">\(Q_n(\theta)\)</span>, not differentiability.</p>
</section>
<section class="level3" id="sec-ch23mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch23mcexercise11">MC Exercise 11</a></p>
<p>The correct answer is (c).</p>
<p>The matrix <span class="math inline">\(A\)</span> represents the <strong>limit of the second derivative</strong> (Hessian) of the objective function <span class="math inline">\(Q_n(\theta)\)</span>. This is used in deriving the asymptotic normality of the estimator, as it relates to the curvature of the objective function around the true parameter value. This is defined in condition (D) of <strong>Theorem 23.4</strong>.</p>
</section>
<section class="level3" id="sec-ch23mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch23mcexercise12">MC Exercise 12</a></p>
<p>The correct answer is (b).</p>
<p>Under correct specification of the likelihood, the <strong>information matrix equality</strong> holds. This states that the negative expected value of the Hessian matrix (which corresponds to <span class="math inline">\(A\)</span>) is equal to the variance of the score function (which corresponds to <span class="math inline">\(B\)</span>). Therefore, <span class="math inline">\(A = B\)</span>. This is discussed on page 325.</p>
</section>
<section class="level3" id="sec-ch23mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch23mcexercise13">MC Exercise 13</a></p>
<p>The correct answer is (c).</p>
<p>The typical form of the asymptotic variance of the GMM estimator in the overidentified case is <span class="math inline">\((\Gamma^\top W \Gamma)^{-1} \Gamma^\top W \Omega W \Gamma (\Gamma^\top W \Gamma)^{-1}\)</span>, where <span class="math inline">\(\Gamma\)</span> is the expected derivative of the moment conditions, <span class="math inline">\(W\)</span> is the weighting matrix, and <span class="math inline">\(\Omega\)</span> is the variance of the moment conditions. This is the “sandwich” form, and is shown on page 326.</p>
</section>
<section class="level3" id="sec-ch23mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch23mcexercise14">MC Exercise 14</a></p>
<p>The correct answer is (c).</p>
<p>For the IV estimator to be well-defined, the number of instruments (<span class="math inline">\(J\)</span>) must be <strong>greater than or equal to</strong> the number of parameters (<span class="math inline">\(K\)</span>). If <span class="math inline">\(J &lt; K\)</span>, the model is underidentified. If <span class="math inline">\(J=K\)</span> (exact identification) we have a special case. If <span class="math inline">\(J&gt;K\)</span> we have overidentification, as discussed on page 326.</p>
</section>
<section class="level3" id="sec-ch23mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch23mcexercise15">MC Exercise 15</a></p>
<p>The correct answer is (b).</p>
<p>The <strong>check function</strong>, <span class="math inline">\(\rho_\alpha(u)\)</span>, is used to define the <strong>objective function</strong> that is minimized in quantile regression. The objective function is the sum of the check function applied to the residuals. This is defined on page 327.</p>
</section>
<section class="level3" id="sec-ch23mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch23mcexercise16">MC Exercise 16</a></p>
<p>The correct answer is (c).</p>
<p>The <strong>monotone equivariance property</strong> states that if we apply a strictly increasing (monotonic) transformation to the response variable, the conditional quantiles of the transformed variable are equal to the transformed conditional quantiles of the original variable. This is a key property of quantile regression and it is defined on page 329.</p>
</section>
<section class="level3" id="sec-ch23mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch23mcexercise17">MC Exercise 17</a></p>
<p>The correct answer is (c). In the Glivenko-Cantelli theorem, <span class="math inline">\(F_n(x)\)</span> represents the <strong>empirical cumulative distribution function</strong>. This theorem states the uniform convergence of <span class="math inline">\(F_n(x)\)</span> to the true cumulative distribution function <span class="math inline">\(F(x)\)</span>.</p>
</section>
<section class="level3" id="sec-ch23mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch23mcexercise18">MC Exercise 18</a></p>
<p>The correct answer is (c).</p>
<p>In a <strong>misspecified likelihood</strong> setting, the MLE may still be consistent under certain conditions. For example, in a regression model, if the conditional mean is correctly specified, the MLE (or OLS) estimator for the coefficients may still be consistent, even if the distributional assumption about the errors is incorrect. This is mentioned on page 325.</p>
</section>
<section class="level3" id="sec-ch23mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch23mcexercise19">MC Exercise 19</a></p>
<p>The correct answer is (c).</p>
<p>Lemma 23.1 states that <span class="math inline">\(Q(\theta_0) - Q(\theta) = \mathbb{E}\left[-\ln \frac{f(X|\theta)}{f(X|\theta_0)}\right]\)</span>. By Jensen’s inequality, <span class="math inline">\(\mathbb{E}\left[-\ln \frac{f(X|\theta)}{f(X|\theta_0)}\right] &gt; -\ln \mathbb{E}\left[\frac{f(X|\theta)}{f(X|\theta_0)}\right] = -\ln(1) = 0\)</span>, for all <span class="math inline">\(\theta \neq \theta_0\)</span>.</p>
</section>
<section class="level3" id="sec-ch23mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch23mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch23mcexercise20">MC Exercise 20</a></p>
<p>The correct answer is (c).</p>
<p><strong>Shrinking neighborhoods</strong> refer to neighborhoods of the true parameter value <span class="math inline">\(\theta_0\)</span> whose size <strong>decreases to zero</strong> as the sample size <span class="math inline">\(n\)</span> goes to infinity. This concept is used in the context of local analysis of extremum estimators, where we focus on the behavior of the objective function close to <span class="math inline">\(\theta_0\)</span> as <span class="math inline">\(n\)</span> increases. The discussion appears on page 323.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>