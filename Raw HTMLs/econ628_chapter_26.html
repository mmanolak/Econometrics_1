<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 26: Exercises and Complements – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap23.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 26: Exercises and Complements</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<section class="level3" id="exercise-1-covariance-matrix">
<h3 class="anchored" data-anchor-id="exercise-1-covariance-matrix">Exercise 1: Covariance Matrix</h3>
<p>If <span class="math inline">\(x\)</span> is an <span class="math inline">\(n\)</span>-dimensional column vector of random variables with mean vector <span class="math inline">\(\mu\)</span>, prove that the covariance matrix of <span class="math inline">\(x\)</span>, <span class="math inline">\(\sum = E[(x - \mu)(x-\mu)^T]\)</span> is positive semidefinite, by using the fact that the scalar <span class="math inline">\(c^Tx\)</span> has a non-negative variance. If <span class="math inline">\(\sum\)</span> is not positive definite, what does this imply about <span class="math inline">\(x\)</span>?</p>
</section>
<section class="level3" id="solution-1">
<h3 class="anchored" data-anchor-id="solution-1">Solution 1:</h3>
<p>Let <span class="math inline">\(c\)</span> be any non-zero <span class="math inline">\(n \times 1\)</span> vector. Define <span class="math inline">\(Y = c^T x\)</span>. The mean of <span class="math inline">\(Y\)</span> is:</p>
<p><span class="math inline">\(\qquad E[Y] = E[c^Tx] = c^T E[x] = c^T\mu\)</span>.</p>
<p>The variance of <span class="math inline">\(Y\)</span> is:</p>
<p><span class="math inline">\(\qquad \text{var}(Y) = E[(Y - E[Y])^2] = E[(c^Tx - c^T\mu)(c^Tx - c^T\mu)^T]\)</span>.</p>
<p>Since <span class="math inline">\(Y\)</span> is a scalar, <span class="math inline">\((Y - E[Y])^2 = (Y - E[Y])(Y-E[Y]) = (Y - E[Y])(Y-E[Y])^T\)</span>. Thus,</p>
<p><span class="math inline">\(\qquad \text{var}(Y) = E[(c^Tx - c^T\mu)(c^Tx - c^T\mu)^T]\)</span> <span class="math inline">\(\qquad = E[c^T(x-\mu)(x-\mu)^Tc]\)</span> <span class="math inline">\(\qquad = c^T E[(x-\mu)(x-\mu)^T] c\)</span> <span class="math inline">\(\qquad = c^T \sum c\)</span>.</p>
<p>Since the variance of any random variable is non-negative, <span class="math inline">\(\text{var}(Y) \geq 0\)</span>. Therefore, <span class="math inline">\(c^T \sum c \geq 0\)</span> for any non-zero vector <span class="math inline">\(c\)</span>. This means that <span class="math inline">\(\sum\)</span> is <strong>positive semidefinite</strong>.</p>
<p>If <span class="math inline">\(\sum\)</span> is not <strong>positive definite</strong>, it means that <span class="math inline">\(\sum\)</span> is <strong>positive semidefinite</strong> but not <strong>positive definite</strong>. This occurs when there exists some non-zero vector <span class="math inline">\(c\)</span> such that <span class="math inline">\(c^T\sum c = 0\)</span>. This implies that <span class="math inline">\(\text{var}(Y) = \text{var}(c^Tx) = 0\)</span>. If a random variable has zero variance it must be a constant. In this instance, <span class="math inline">\(c^T x = c^T \mu\)</span>, which is a constant. In turn, <span class="math inline">\(c^T x\)</span> is a linear combination of elements of x that is constant, indicating that there exists <strong>linear dependence</strong> among the elements of the random vector <span class="math inline">\(x\)</span>.</p>
</section>
<section class="level3" id="exercise-2-matrixvector-notation">
<h3 class="anchored" data-anchor-id="exercise-2-matrixvector-notation">Exercise 2: Matrix/vector notation</h3>
<p>Write the following expressions in matrix/vector notation:</p>
<ol type="a">
<li><p><span class="math inline">\(\sum_{i=1}^n a_i b_i\)</span>;</p></li>
<li><p><span class="math inline">\(a_ib_j, i = 1, \dots, n; j = 1, \dots, J\)</span></p></li>
<li><p><span class="math inline">\(\sum_{j=1}^J a_{ij}x_j, i = 1, \dots, n\)</span>;</p></li>
<li><p><span class="math inline">\(\sum_{j=1}^n a_{ij}b_{jk}, i = 1, \dots, n; k = 1, \dots, K\)</span>;</p></li>
<li><p><span class="math inline">\(\sum_{j=1}^K \sum_{k=1}^K a_{ij}b_{kj}c_{km}, i = 1, \dots, n; m = 1, \dots, M\)</span>;</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n \sum_{j=1}^n a_{ij}x_ix_j\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="solution-2">
<h3 class="anchored" data-anchor-id="solution-2">Solution 2:</h3>
<ol type="a">
<li><p>Let <span class="math inline">\(a = \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix}\)</span> and <span class="math inline">\(b = \begin{bmatrix} b_1 \\ \vdots \\ b_n \end{bmatrix}\)</span>. Then <span class="math inline">\(\sum_{i=1}^n a_i b_i = a^Tb\)</span>.</p></li>
<li><p>Let <span class="math inline">\(a = \begin{bmatrix} a_1 \\ \vdots \\ a_n \end{bmatrix}\)</span> and <span class="math inline">\(b = \begin{bmatrix} b_1 \\ \vdots \\ b_J \end{bmatrix}\)</span>. Then <span class="math inline">\(ab^T = \begin{bmatrix} a_1b_1 &amp; \dots &amp; a_1b_J \\ \vdots &amp; \ddots &amp; \vdots \\ a_nb_1 &amp; \dots &amp; a_nb_J\end{bmatrix}\)</span>, which gives <span class="math inline">\(a_ib_j\)</span> for <span class="math inline">\(i = 1, \dots, n\)</span> and <span class="math inline">\(j = 1, \dots, J\)</span>.</p></li>
<li><p>Let <span class="math inline">\(A = [a_{ij}]\)</span> be an <span class="math inline">\(n \times J\)</span> matrix and <span class="math inline">\(x = \begin{bmatrix} x_1 \\ \vdots \\ x_J \end{bmatrix}\)</span>. Then <span class="math inline">\(Ax\)</span> is an <span class="math inline">\(n \times 1\)</span> vector. The <span class="math inline">\(i\)</span>-th element of <span class="math inline">\(Ax\)</span> is <span class="math inline">\(\sum_{j=1}^J a_{ij}x_j\)</span>.</p></li>
<li><p>Let <span class="math inline">\(A = [a_{ij}]\)</span> be an <span class="math inline">\(n \times n\)</span> matrix and <span class="math inline">\(B = [b_{jk}]\)</span> be an <span class="math inline">\(n \times K\)</span> matrix. Then, the matrix product <span class="math inline">\(AB\)</span> is an <span class="math inline">\(n \times K\)</span> matrix whose <span class="math inline">\((i, k)\)</span>-th element is <span class="math inline">\(\sum_{j=1}^n a_{ij}b_{jk}\)</span>.</p></li>
<li><p>Let <span class="math inline">\(A = [a_{ij}]\)</span> be an <span class="math inline">\(n \times K\)</span> matrix, <span class="math inline">\(B=[b_{jk}]\)</span> be a <span class="math inline">\(K \times K\)</span> matrix, and <span class="math inline">\(C = [c_{km}]\)</span> be a <span class="math inline">\(K \times M\)</span> matrix. The product <span class="math inline">\(ABC\)</span> is an <span class="math inline">\(n \times M\)</span> matrix. The <span class="math inline">\((i, m)\)</span>-th element of <span class="math inline">\(ABC\)</span> is given by <span class="math inline">\(\sum_{j=1}^K \sum_{k=1}^K a_{ij}b_{jk}c_{km}\)</span>. Specifically, let us calculate the matrix multiplication step by step. Let <span class="math inline">\(D = AB\)</span>, <span class="math inline">\(D\)</span> will be an <span class="math inline">\(n \times K\)</span> matrix. <span class="math inline">\(d_{ik} = \sum_{j=1}^K a_{ij} b_{jk}\)</span>. Now let us calculate <span class="math inline">\(DC\)</span>, and call this matrix <span class="math inline">\(E\)</span>. <span class="math inline">\(E = DC\)</span> is an <span class="math inline">\(n \times M\)</span> matrix. <span class="math inline">\(e_{im} = \sum_{k=1}^K d_{ik} c_{km} = \sum_{k=1}^K (\sum_{j=1}^K a_{ij} b_{jk}) c_{km} = \sum_{j=1}^K \sum_{k=1}^K a_{ij} b_{jk} c_{km}\)</span>.</p></li>
<li><p>Let <span class="math inline">\(A = [a_{ij}]\)</span> be an <span class="math inline">\(n \times n\)</span> matrix and <span class="math inline">\(x = \begin{bmatrix} x_1 \\ \vdots \\ x_n \end{bmatrix}\)</span>. The given expression can be written as <span class="math inline">\(x^TAx\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="exercise-3-trace-of-matrix-products">
<h3 class="anchored" data-anchor-id="exercise-3-trace-of-matrix-products">Exercise 3: Trace of matrix products</h3>
<p>Prove that if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are matrices such that <span class="math inline">\(AB\)</span> and <span class="math inline">\(BA\)</span> both exist, then <span class="math inline">\(AB\)</span> and <span class="math inline">\(BA\)</span> have the same sum of diagonal elements, i.e. <span class="math inline">\(\text{tr}(AB) = \text{tr}(BA)\)</span>. Extend the result to show that</p>
<p><span class="math inline">\(\qquad \text{tr}(ABC) = \text{tr}(CAB) = \text{tr}(BCA)\)</span>,</p>
<p>provided the matrices are conformable. Show that, however, <span class="math inline">\(\text{tr}(BAC)\)</span> and <span class="math inline">\(\text{tr}(ACB)\)</span> may be different.</p>
</section>
<section class="level3" id="solution-3">
<h3 class="anchored" data-anchor-id="solution-3">Solution 3:</h3>
<p>Suppose <span class="math inline">\(A\)</span> is <span class="math inline">\(m \times n\)</span> and <span class="math inline">\(B\)</span> is <span class="math inline">\(n \times m\)</span>, so that <span class="math inline">\(AB\)</span> is <span class="math inline">\(m \times m\)</span> and <span class="math inline">\(BA\)</span> is <span class="math inline">\(n \times n\)</span>. Then,</p>
<p><span class="math inline">\(\qquad \text{tr}(AB) = \sum_{i=1}^m (AB)_{ii} = \sum_{i=1}^m \sum_{j=1}^n a_{ij}b_{ji}\)</span> <span class="math inline">\(\qquad \text{tr}(BA) = \sum_{j=1}^n (BA)_{jj} = \sum_{j=1}^n \sum_{i=1}^m b_{ji}a_{ij} = \sum_{i=1}^m \sum_{j=1}^n a_{ij}b_{ji}\)</span>.</p>
<p>Thus, <span class="math inline">\(\text{tr}(AB) = \text{tr}(BA)\)</span>.</p>
<p>Now suppose that <span class="math inline">\(A\)</span> is <span class="math inline">\(n \times p\)</span>, <span class="math inline">\(B\)</span> is <span class="math inline">\(p \times q\)</span>, and <span class="math inline">\(C\)</span> is <span class="math inline">\(q \times n\)</span>, so that <span class="math inline">\(ABC\)</span> is <span class="math inline">\(n \times n\)</span>. Then we have</p>
<p><span class="math inline">\(\qquad \text{tr}(ABC) = \text{tr}((AB)C) = \text{tr}(C(AB)) = \text{tr}((CAB))\)</span>,</p>
<p>and,</p>
<p><span class="math inline">\(\qquad \text{tr}(ABC) = \text{tr}(A(BC)) = \text{tr}((BC)A) = \text{tr}(BCA)\)</span>.</p>
<p>For the second part, let</p>
<p><span class="math inline">\(\qquad A = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}, \quad
B = \begin{bmatrix} 0 &amp; 1 \\ 0 &amp; 0 \end{bmatrix}, \quad
C = \begin{bmatrix} 0 &amp; 0 \\ 1 &amp; 0 \end{bmatrix}\)</span>.</p>
<p>Then</p>
<p><span class="math inline">\(\qquad BAC = \begin{bmatrix} 1 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}, \text{tr}(BAC) = 1\)</span> <span class="math inline">\(\qquad ACB = \begin{bmatrix} 0 &amp; 0 \\ 0 &amp; 0 \end{bmatrix}, \text{tr}(ACB) = 0\)</span>.</p>
</section>
<section class="level3" id="exercise-4-linear-system">
<h3 class="anchored" data-anchor-id="exercise-4-linear-system">Exercise 4: Linear System</h3>
<p>Suppose that <span class="math inline">\(A\)</span> is <span class="math inline">\(n \times n\)</span>, <span class="math inline">\(B\)</span> is <span class="math inline">\(K \times K\)</span>, and <span class="math inline">\(C\)</span> is <span class="math inline">\(n \times K\)</span> are given matrices. Consider the system of equations in <span class="math inline">\(n \times K\)</span> matrices <span class="math inline">\(X\)</span>:</p>
<p><span class="math inline">\(\qquad AXB = C\)</span>. (26.1)</p>
<p>Show that this system of equations is linear and solve for <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="solution-4">
<h3 class="anchored" data-anchor-id="solution-4">Solution 4:</h3>
<p>This equation is linear in <span class="math inline">\(X\)</span>, because for any <span class="math inline">\(X, X^*\)</span> of same dimensions</p>
<p><span class="math inline">\(\qquad AXB + AX^*B = A(X + X^*)B\)</span></p>
<p>If <span class="math inline">\(A, B\)</span> are invertible, then we have a unique solution</p>
<p><span class="math inline">\(\qquad X = A^{-1}CB^{-1}\)</span>.</p>
</section>
<section class="level3" id="example-special-case">
<h3 class="anchored" data-anchor-id="example-special-case">Example: Special Case</h3>
<p>Special case <span class="math inline">\(K = 1\)</span>. We have <span class="math inline">\(axb = c\)</span>, which implies that <span class="math inline">\(x = c/ab\)</span>. Question: Suppose that <span class="math inline">\(K=n\)</span>.</p>
<p><span class="math inline">\(\qquad AX^{-1}B = C\)</span></p>
<p>then solve for <span class="math inline">\(X\)</span>.</p>
</section>
<section class="level3" id="solution-special-case">
<h3 class="anchored" data-anchor-id="solution-special-case">Solution: Special Case</h3>
<p>We pre-multiply by <span class="math inline">\(A^{-1}\)</span> and post-multiply by <span class="math inline">\(B^{-1}\)</span> to get <span class="math inline">\(X^{-1} = A^{-1}CB^{-1}\)</span>. Taking the inverse again yields, <span class="math inline">\(X = (A^{-1}CB^{-1})^{-1} = BC^{-1}A\)</span> (assuming all matrices are square and invertible).</p>
</section>
<section class="level3" id="exercise-5-properties-of-xtx">
<h3 class="anchored" data-anchor-id="exercise-5-properties-of-xtx">Exercise 5: Properties of <span class="math inline">\(X^TX\)</span></h3>
<p>If <span class="math inline">\(X\)</span> is a non-zero matrix of order <span class="math inline">\(T \times K\)</span>, prove that <span class="math inline">\(X^TX\)</span> is:</p>
<ol type="a">
<li><p>symmetric and</p></li>
<li><p>positive semi-definite;</p></li>
<li><p>Under what conditions on <span class="math inline">\(X\)</span> is <span class="math inline">\(X^TX\)</span> positive definite.</p></li>
</ol>
</section>
<section class="level3" id="solution-5">
<h3 class="anchored" data-anchor-id="solution-5">Solution 5:</h3>
<ol type="a">
<li><p><span class="math inline">\((X^TX)^T = X^T(X^T)^T = X^TX\)</span>. Therefore, <span class="math inline">\(X^TX\)</span> is <strong>symmetric</strong>.</p></li>
<li><p>Let <span class="math inline">\(z\)</span> be any non-zero <span class="math inline">\(K \times 1\)</span> vector. Then <span class="math inline">\(z^TX^TXz = (Xz)^TXz = y^Ty\)</span>, where <span class="math inline">\(y = Xz\)</span>. Since <span class="math inline">\(y^Ty\)</span> is a sum of squares, <span class="math inline">\(y^Ty \geq 0\)</span>. Therefore, <span class="math inline">\(z^TX^TXz \geq 0\)</span> for any non-zero vector <span class="math inline">\(z\)</span>, implying that <span class="math inline">\(X^TX\)</span> is <strong>positive semi-definite</strong>.</p></li>
<li><p>For <span class="math inline">\(X^TX\)</span> to be <strong>positive definite</strong>, we need <span class="math inline">\(z^TX^TXz &gt; 0\)</span> for any non-zero <span class="math inline">\(K \times 1\)</span> vector <span class="math inline">\(z\)</span>. This condition is equivalent to <span class="math inline">\(Xz \neq 0\)</span> for any non-zero <span class="math inline">\(z\)</span>. In turn, this is true if and only if <span class="math inline">\(X\)</span> has <strong>full column rank</strong>, meaning the columns of <span class="math inline">\(X\)</span> are linearly independent. If the columns of <span class="math inline">\(X\)</span> are linearly independent, then <span class="math inline">\(Xz \neq 0\)</span>, so <span class="math inline">\(X^TX\)</span> is positive definite.</p></li>
</ol>
</section>
<section class="level3" id="exercise-6-properties-of-symmetric-matrix">
<h3 class="anchored" data-anchor-id="exercise-6-properties-of-symmetric-matrix">Exercise 6: Properties of Symmetric Matrix</h3>
<p>For a real symmetric <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span>, prove:</p>
<ol type="a">
<li><p><span class="math inline">\(\text{det}(A) = \prod_{j=1}^n \lambda_j\)</span>, where <span class="math inline">\(\lambda_j\)</span> are the eigenvalues of <span class="math inline">\(A\)</span>;</p></li>
<li><p><span class="math inline">\(\text{tr}(A) = \sum_{j=1}^n \lambda_j\)</span>, where <span class="math inline">\(\lambda_j\)</span> are the eigenvalues of <span class="math inline">\(A\)</span>;</p></li>
<li><p><span class="math inline">\(A\)</span> is positive definite, if and only if its eigenvalues are positive;</p></li>
<li><p><span class="math inline">\(A\)</span> is positive definite, if and only if <span class="math inline">\(A^{-1}\)</span> is positive definite;</p></li>
<li><p>If <span class="math inline">\(A\)</span> is positive definite, then there exists an <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(P\)</span> such that <span class="math inline">\(A = PP^T\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="solution-6">
<h3 class="anchored" data-anchor-id="solution-6">Solution 6:</h3>
<p>Since <span class="math inline">\(A\)</span> is a real symmetric matrix, it can be diagonalized as <span class="math inline">\(A = Q\Lambda Q^T\)</span>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix (<span class="math inline">\(Q^TQ = QQ^T = I\)</span>) containing the eigenvectors of <span class="math inline">\(A\)</span>, and <span class="math inline">\(\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)\)</span> is a diagonal matrix containing the corresponding eigenvalues.</p>
<ol type="a">
<li><p><span class="math inline">\(\text{det}(A) = \text{det}(Q\Lambda Q^T) = \text{det}(Q)\text{det}(\Lambda)\text{det}(Q^T) = \text{det}(QQ^T)\text{det}(\Lambda) = \text{det}(I)\text{det}(\Lambda) = 1 \cdot \prod_{j=1}^n \lambda_j = \prod_{j=1}^n \lambda_j\)</span>.</p></li>
<li><p><span class="math inline">\(\text{tr}(A) = \text{tr}(Q\Lambda Q^T) = \text{tr}(\Lambda Q^TQ) = \text{tr}(\Lambda I) = \text{tr}(\Lambda) = \sum_{j=1}^n \lambda_j\)</span>.</p></li>
<li><p>If <span class="math inline">\(A\)</span> is positive definite, then for any non-zero vector <span class="math inline">\(x\)</span>, <span class="math inline">\(x^TAx &gt; 0\)</span>. Let <span class="math inline">\(x\)</span> be an eigenvector of <span class="math inline">\(A\)</span>, say <span class="math inline">\(q_i\)</span>. Then <span class="math inline">\(Aq_i = \lambda_i q_i\)</span>, and <span class="math inline">\(q_i^TAq_i = q_i^T\lambda_i q_i = \lambda_i q_i^Tq_i = \lambda_i &gt; 0\)</span>, since <span class="math inline">\(q_i^Tq_i = 1\)</span>. Therefore, all eigenvalues are positive. Conversely, if all eigenvalues are positive, then for any non-zero <span class="math inline">\(x\)</span>, we can write <span class="math inline">\(x = Qz\)</span> for some <span class="math inline">\(z \ne 0\)</span>, since <span class="math inline">\(Q\)</span> is invertible, and so <span class="math inline">\(x^TAx = z^TQ^TQ\Lambda Q^TQz = z^T\Lambda z = \sum_i \lambda_i z_i^2 &gt; 0\)</span>. Hence <span class="math inline">\(A\)</span> is positive definite.</p></li>
<li><p>If <span class="math inline">\(A\)</span> is positive definite, then its eigenvalues are all positive. The eigenvalues of <span class="math inline">\(A^{-1}\)</span> are the reciprocals of the eigenvalues of <span class="math inline">\(A\)</span>, i.e. <span class="math inline">\(1/\lambda_i\)</span>. Thus, if <span class="math inline">\(\lambda_i &gt; 0\)</span>, then <span class="math inline">\(1/\lambda_i &gt; 0\)</span>, so all eigenvalues of <span class="math inline">\(A^{-1}\)</span> are positive, meaning <span class="math inline">\(A^{-1}\)</span> is positive definite. Conversely, if <span class="math inline">\(A^{-1}\)</span> is positive definite, all its eigenvalues are positive. The eigenvalues of <span class="math inline">\(A\)</span> are the reciprocals of the eigenvalues of <span class="math inline">\(A^{-1}\)</span>. Thus, the eigenvalues of <span class="math inline">\(A\)</span> are all positive, and <span class="math inline">\(A\)</span> is positive definite.</p></li>
<li><p>Since <span class="math inline">\(A\)</span> is positive definite, all of its eigenvalues are positive. We can define <span class="math inline">\(\Lambda^{1/2} = \text{diag}(\sqrt{\lambda_1}, \dots, \sqrt{\lambda_n})\)</span>. Then we can write <span class="math inline">\(A = Q\Lambda Q^T = Q\Lambda^{1/2}\Lambda^{1/2}Q^T = Q\Lambda^{1/2}(Q\Lambda^{1/2})^T\)</span>. Let <span class="math inline">\(P = Q\Lambda^{1/2}\)</span>. Then, <span class="math inline">\(A = PP^T\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="exercise-7-idempotent-matrix">
<h3 class="anchored" data-anchor-id="exercise-7-idempotent-matrix">Exercise 7: Idempotent Matrix</h3>
<p>A square matrix <span class="math inline">\(A\)</span> is <strong>idempotent</strong> if <span class="math inline">\(A = A^2\)</span>. Prove that:</p>
<ol type="a">
<li><p>The eigenvalues of <span class="math inline">\(A\)</span> are either zero or one;</p></li>
<li><p><span class="math inline">\(\text{rank}(A) = \text{tr}(A)\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="solution-7">
<h3 class="anchored" data-anchor-id="solution-7">Solution 7:</h3>
<ol type="a">
<li><p>Let <span class="math inline">\(\lambda\)</span> be an eigenvalue of <span class="math inline">\(A\)</span>, and <span class="math inline">\(x\)</span> be the corresponding eigenvector. Then, <span class="math inline">\(Ax = \lambda x\)</span>. Since <span class="math inline">\(A\)</span> is idempotent, <span class="math inline">\(A^2 = A\)</span>. Multiplying both sides by <span class="math inline">\(x\)</span>, <span class="math inline">\(A^2x = Ax\)</span>. Substituting <span class="math inline">\(Ax = \lambda x\)</span> gives, <span class="math inline">\(A(Ax) = Ax\)</span>, <span class="math inline">\(A(\lambda x) = \lambda x\)</span>, <span class="math inline">\(\lambda Ax = \lambda x\)</span>, <span class="math inline">\(\lambda(\lambda x) = \lambda x\)</span>, <span class="math inline">\(\lambda^2x = \lambda x\)</span>. Since <span class="math inline">\(x\)</span> is an eigenvector it cannot be the 0 vector, so we can divide both sides by <span class="math inline">\(x\)</span> to obtain, <span class="math inline">\((\lambda^2 - \lambda)x = 0 \implies \lambda^2 - \lambda = 0\)</span>. This means that <span class="math inline">\(\lambda(\lambda - 1) = 0\)</span>, so <span class="math inline">\(\lambda = 0\)</span> or <span class="math inline">\(\lambda = 1\)</span>.</p></li>
<li><p>Since <span class="math inline">\(A\)</span> is idempotent, and therefore symmetric, it can be diagonalized as <span class="math inline">\(A = Q\Lambda Q^T\)</span>, where <span class="math inline">\(Q\)</span> is an orthogonal matrix and <span class="math inline">\(\Lambda\)</span> is a diagonal matrix of eigenvalues. Each eigenvalue is either 0 or 1. Then <span class="math inline">\(\text{rank}(A) = \text{rank}(\Lambda)\)</span>, and the rank of <span class="math inline">\(\Lambda\)</span> is equal to the number of non-zero eigenvalues, which is the number of eigenvalues equal to 1. Also, <span class="math inline">\(\text{tr}(A) = \text{tr}(Q\Lambda Q^T) = \text{tr}(\Lambda Q^TQ) = \text{tr}(\Lambda)\)</span>. The trace of <span class="math inline">\(\Lambda\)</span> is equal to the sum of its diagonal elements, which are the eigenvalues. Since each eigenvalue is either 0 or 1, the trace is the number of eigenvalues equal to 1. Therefore, <span class="math inline">\(\text{rank}(A) = \text{tr}(A)\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="exercise-8-quadratic-equation-with-matrices">
<h3 class="anchored" data-anchor-id="exercise-8-quadratic-equation-with-matrices">Exercise 8: Quadratic Equation with Matrices</h3>
<p>Suppose that <span class="math inline">\(X\)</span> is a real symmetric matrix. Calculate the eigenvectors and eigenvalues of the matrix <span class="math inline">\(X^2\)</span> in terms of the eigenvalues and eigenvectors of the matrix <span class="math inline">\(X\)</span>. Now consider the matrix quadratic equation</p>
<p><span class="math inline">\(\qquad 2X^2 - 3X + I_n = 0_n\)</span>,</p>
<p>where <span class="math inline">\(X, I_n, 0_n\)</span> are <span class="math inline">\(n \times n\)</span> matrices. Find real valued matrix solutions to this equation, i.e., find the <span class="math inline">\(X\)</span> that solves this equation.</p>
</section>
<section class="level3" id="solution-8">
<h3 class="anchored" data-anchor-id="solution-8">Solution 8:</h3>
<p>If <span class="math inline">\(X\)</span> is symmetric, <span class="math inline">\(X = U\Lambda U^T\)</span>, where <span class="math inline">\(U\)</span> is an orthogonal matrix whose columns are eigenvectors of <span class="math inline">\(X\)</span>, and <span class="math inline">\(\Lambda = \text{diag}(\lambda_1, \dots, \lambda_n)\)</span> contains corresponding eigenvalues. Then <span class="math inline">\(X^2 = U\Lambda U^T U\Lambda U^T = U\Lambda^2 U^T\)</span>, where <span class="math inline">\(\Lambda^2 = \text{diag}(\lambda_1^2, \dots, \lambda_n^2)\)</span>. Thus, the eigenvalues of <span class="math inline">\(X^2\)</span> are the squares of the eigenvalues of <span class="math inline">\(X\)</span>, and the eigenvectors are the same.</p>
<p>If <span class="math inline">\(X\)</span> is symmetric</p>
<p><span class="math inline">\(\qquad X = U\Lambda U^T\)</span>,</p>
<p>where <span class="math inline">\(U U^T = I\)</span>. It follows that</p>
<p><span class="math inline">\(\qquad 2U\Lambda^2U^T - 3U\Lambda U^T + I_n = 0_n\)</span>.</p>
<p>Collecting terms we have</p>
<p><span class="math inline">\(\qquad U \begin{bmatrix} 2\lambda_1^2 - 3\lambda_1 + 1 &amp; &amp; 0 \\ &amp; \ddots &amp; \\ 0 &amp; &amp; 2\lambda_n^2 - 3\lambda_n + 1 \end{bmatrix} U^T = 0_n\)</span>.</p>
<p>Consider the quadratic equation</p>
<p><span class="math inline">\(\qquad 2x^2 - 3x + 1 = 0\)</span></p>
<p>This can be factorized as</p>
<p><span class="math inline">\(\qquad (2x - 1)(x - 1) = 0\)</span></p>
<p>so that the set of solutions includes all matrices of the form</p>
<p><span class="math inline">\(\qquad \{X = U\Lambda U^T : \lambda_i \in \{1, 1/2\}, i = 1, \dots, n, \text{where } U \text{ is any orthonormal matrix}\}\)</span>.</p>
</section>
<section class="level3" id="exercise-9-characteristic-polynomial">
<h3 class="anchored" data-anchor-id="exercise-9-characteristic-polynomial">Exercise 9: Characteristic Polynomial</h3>
<p>Suppose that the matrix <span class="math inline">\(A\)</span> has characteristic polynomial</p>
<p><span class="math inline">\(\qquad \lambda^2 - a\lambda - b = 0\)</span>.</p>
<p>Using the Cayley-Hamilton theorem, show that</p>
<p><span class="math inline">\(\qquad A^{-1} = \frac{1}{b}(A - aI)\)</span>.</p>
</section>
<section class="level3" id="solution-9">
<h3 class="anchored" data-anchor-id="solution-9">Solution 9:</h3>
<p>The Cayley-Hamilton theorem states that a matrix satisfies its own characteristic equation. Therefore, <span class="math inline">\(A^2 - aA - bI = 0\)</span>. Rearranging gives <span class="math inline">\(A^2 - aA = bI\)</span>, which can be factored as <span class="math inline">\(A(A - aI) = bI\)</span>. Dividing both sides by <span class="math inline">\(b\)</span> (assuming <span class="math inline">\(b \neq 0\)</span>), we get <span class="math inline">\(A(\frac{1}{b}(A - aI)) = I\)</span>. Thus, <span class="math inline">\(A^{-1} = \frac{1}{b}(A - aI)\)</span>. Note that for <span class="math inline">\(A^{-1}\)</span> to exist, the characteristic polynomial must have a non-zero constant term.</p>
</section>
<section class="level3" id="exercise-10-idempotent-matrix">
<h3 class="anchored" data-anchor-id="exercise-10-idempotent-matrix">Exercise 10: Idempotent Matrix</h3>
<p>Show that the matrix <span class="math inline">\(I - Z(Z^TZ)^{-1}Z^T\)</span>, where <span class="math inline">\(Z\)</span> is any <span class="math inline">\(n \times K\)</span> matrix of full rank <span class="math inline">\(K\)</span>, is idempotent and find its rank.</p>
</section>
<section class="level3" id="solution-10">
<h3 class="anchored" data-anchor-id="solution-10">Solution 10:</h3>
<p>Let <span class="math inline">\(M = I - Z(Z^TZ)^{-1}Z^T\)</span>. Then</p>
<p><span class="math inline">\(\qquad M^2 = (I - Z(Z^TZ)^{-1}Z^T)(I - Z(Z^TZ)^{-1}Z^T)\)</span> <span class="math inline">\(\qquad = I - 2Z(Z^TZ)^{-1}Z^T + Z(Z^TZ)^{-1}Z^TZ(Z^TZ)^{-1}Z^T\)</span> <span class="math inline">\(\qquad = I - 2Z(Z^TZ)^{-1}Z^T + Z(Z^TZ)^{-1}Z^T\)</span> <span class="math inline">\(\qquad = I - Z(Z^TZ)^{-1}Z^T\)</span> <span class="math inline">\(\qquad = M\)</span>.</p>
<p>Therefore, <span class="math inline">\(M\)</span> is idempotent. The rank of an idempotent matrix is equal to its trace. So</p>
<p><span class="math inline">\(\qquad \text{rank}(M) = \text{tr}(M) = \text{tr}(I_n - Z(Z^TZ)^{-1}Z^T)\)</span> <span class="math inline">\(\qquad = \text{tr}(I_n) - \text{tr}(Z(Z^TZ)^{-1}Z^T)\)</span> <span class="math inline">\(\qquad = \text{tr}(I_n) - \text{tr}((Z^TZ)^{-1}Z^TZ)\)</span> <span class="math inline">\(\qquad = \text{tr}(I_n) - \text{tr}(I_K)\)</span> <span class="math inline">\(\qquad = n - K\)</span>.</p>
</section>
<section class="level3" id="exercise-11-cross-product-matrix">
<h3 class="anchored" data-anchor-id="exercise-11-cross-product-matrix">Exercise 11: Cross-product Matrix</h3>
<p>An <span class="math inline">\(n \times K\)</span> matrix <span class="math inline">\(X\)</span> is partitioned column-wise</p>
<p><span class="math inline">\(\qquad X = (X_1, X_2)\)</span>,</p>
<p>where <span class="math inline">\(X_1\)</span> is <span class="math inline">\(n \times K_1\)</span> and <span class="math inline">\(X_2\)</span> is <span class="math inline">\(n \times K_2\)</span> where <span class="math inline">\(K_1 + K_2 = K\)</span>. Write the cross-product matrix <span class="math inline">\(X^TX\)</span> in terms of <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. An <span class="math inline">\(n \times K\)</span> matrix <span class="math inline">\(W\)</span> is partitioned column-wise</p>
<p><span class="math inline">\(\qquad W = \begin{pmatrix} W_1 \\ W_2 \end{pmatrix}\)</span>,</p>
<p>where <span class="math inline">\(W_1\)</span> is <span class="math inline">\(n_1 \times K\)</span> and <span class="math inline">\(W_2\)</span> is <span class="math inline">\(n_2 \times K\)</span> where <span class="math inline">\(n_1 + n_2 = n\)</span>. Write the cross-product matrix <span class="math inline">\(W^TW\)</span> in terms of <span class="math inline">\(W_1\)</span> and <span class="math inline">\(W_2\)</span>.</p>
</section>
<section class="level3" id="solution-11">
<h3 class="anchored" data-anchor-id="solution-11">Solution 11:</h3>
<p>The cross-product matrix <span class="math inline">\(X^TX\)</span> is</p>
<p><span class="math inline">\(\qquad X^TX = \begin{pmatrix} X_1^T \\ X_2^T \end{pmatrix} \begin{pmatrix} X_1 &amp; X_2 \end{pmatrix} = \begin{pmatrix} X_1^TX_1 &amp; X_1^TX_2 \\ X_2^TX_1 &amp; X_2^TX_2 \end{pmatrix}\)</span>.</p>
<p>The cross-product matrix <span class="math inline">\(W^TW\)</span> is</p>
<p><span class="math inline">\(\qquad W^TW = \begin{pmatrix} W_1^T &amp; W_2^T \end{pmatrix} \begin{pmatrix} W_1 \\ W_2 \end{pmatrix} = W_1^TW_1 + W_2^TW_2\)</span>.</p>
</section>
<section class="level3" id="exercise-12-distribution-of-quadratic-form">
<h3 class="anchored" data-anchor-id="exercise-12-distribution-of-quadratic-form">Exercise 12: Distribution of Quadratic Form</h3>
<p>If <span class="math inline">\(u\)</span> is an <span class="math inline">\(n\)</span>-dimensional vector of random variables distributed as <span class="math inline">\(N(0, \sigma^2I_n)\)</span>, and <span class="math inline">\(A\)</span> is a symmetric, idempotent matrix of order <span class="math inline">\(n\)</span> and rank <span class="math inline">\(p\)</span>, show that <span class="math inline">\(u^TAu/\sigma^2\)</span> is distributed as <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(p\)</span> degrees of freedom.</p>
</section>
<section class="level3" id="solution-12">
<h3 class="anchored" data-anchor-id="solution-12">Solution 12:</h3>
<p>Since <span class="math inline">\(A\)</span> is symmetric and idempotent, and <span class="math inline">\(u \sim N(0, \sigma^2I_n)\)</span>, the quadratic form <span class="math inline">\(u^TAu/\sigma^2\)</span> is distributed as a chi-squared distribution with degrees of freedom equal to the rank of <span class="math inline">\(A\)</span>. Given that <span class="math inline">\(A\)</span> has rank <span class="math inline">\(p\)</span>, we have <span class="math inline">\(u^TAu/\sigma^2 \sim \chi^2(p)\)</span>.</p>
<p>To see why, we know that <span class="math inline">\(A = A^T\)</span> and <span class="math inline">\(A = A^2\)</span> so we can diagonalize <span class="math inline">\(A = Q\Lambda Q^T\)</span>, where <span class="math inline">\(Q\)</span> is orthonormal and <span class="math inline">\(\Lambda\)</span> is diagonal of eigenvalues. Then,</p>
<p><span class="math inline">\(\qquad u^T A u = u^T Q \Lambda Q^T u = (Q^T u)^T \Lambda (Q^T u)\)</span></p>
<p>and since <span class="math inline">\(Q^T\)</span> is also orthonormal,</p>
<p><span class="math inline">\(\qquad Q^Tu \sim N(0, \sigma^2 I)\)</span>.</p>
<p>If we call <span class="math inline">\(v = Q^T u\)</span>, and knowing that the eigenvalues of <span class="math inline">\(A\)</span> are just <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, the quadratic form becomes</p>
<p><span class="math inline">\(\qquad u^T A u = v^T \Lambda v = \sum_{i=1}^n \lambda_i v_i^2 = \sum_{i=1}^p v_i^2\)</span></p>
<p>if <span class="math inline">\(A\)</span> has <span class="math inline">\(p\)</span> eigenvalues equal to one, and the rest equal to zero. If <span class="math inline">\(u\)</span> follows a normal distribution, so does <span class="math inline">\(v\)</span>. Hence each <span class="math inline">\(v_i\)</span> will be a standard normal. Hence we have the sum of squares of <span class="math inline">\(p\)</span> standard normal random variables. If we divide by <span class="math inline">\(\sigma^2\)</span> we obtain a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(p\)</span> degrees of freedom.</p>
</section>
<section class="level3" id="exercise-13-distribution-of-quadratic-form">
<h3 class="anchored" data-anchor-id="exercise-13-distribution-of-quadratic-form">Exercise 13: Distribution of Quadratic Form</h3>
<p>If <span class="math inline">\(u\)</span> is an <span class="math inline">\(n\)</span>-dimensional vector of random variables distributed as <span class="math inline">\(N(0, \sum)\)</span>, where <span class="math inline">\(\sum\)</span> is non-singular, show that <span class="math inline">\(u^T \sum^{-1}u\)</span> is distributed as <span class="math inline">\(\chi^2\)</span> with <span class="math inline">\(n\)</span> degrees of freedom.</p>
</section>
<section class="level3" id="solution-13">
<h3 class="anchored" data-anchor-id="solution-13">Solution 13:</h3>
<p>Since <span class="math inline">\(\sum\)</span> is a symmetric, positive definite matrix, we can write <span class="math inline">\(\sum = PP^T\)</span>, where <span class="math inline">\(P\)</span> is non-singular. Then <span class="math inline">\(\sum^{-1} = (P^{-1})^T P^{-1}\)</span>. Let <span class="math inline">\(v = P^{-1}u\)</span>. Then <span class="math inline">\(v \sim N(0, P^{-1}\sum (P^{-1})^T) = N(0, P^{-1}PP^T(P^{-1})^T) = N(0, I_n)\)</span>. Thus,</p>
<p><span class="math inline">\(\qquad u^T \sum^{-1} u = u^T(P^{-1})^TP^{-1}u = (P^{-1}u)^T(P^{-1}u) = v^Tv = \sum_{i=1}^n v_i^2\)</span>.</p>
<p>Since <span class="math inline">\(v_i\)</span> are i.i.d. standard normal random variables, <span class="math inline">\(v^Tv\)</span> follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(n\)</span> degrees of freedom.</p>
</section>
<section class="level3" id="exercise-14-inverse-of-matrix">
<h3 class="anchored" data-anchor-id="exercise-14-inverse-of-matrix">Exercise 14: Inverse of Matrix</h3>
<p>Consider the matrix</p>
<p><span class="math inline">\(\qquad A = \text{diag}(s) - ss^T\)</span>,</p>
<p>where <span class="math inline">\(s = (s_1, \dots, s_n)^T\)</span> is such that <span class="math inline">\(i^Ts \neq 0\)</span> and <span class="math inline">\(s_i \neq 0\)</span> for <span class="math inline">\(i=1, \dots, n\)</span>. Is this matrix symmetric? Show that its inverse is</p>
<p><span class="math inline">\(\qquad A^{-1} = \text{diag}(1/s_1, \dots, 1/s_n) + \frac{1}{1 - i^Ts}ii^T\)</span>.</p>
</section>
<section class="level3" id="solution-14">
<h3 class="anchored" data-anchor-id="solution-14">Solution 14:</h3>
<p>Yes, the matrix is symmetric because <span class="math inline">\(A^T = (\text{diag}(s) - ss^T)^T = \text{diag}(s)^T - (ss^T)^T = \text{diag}(s) - ss^T = A\)</span>. Let <span class="math inline">\(B = \text{diag}(1/s_1, \dots, 1/s_n) + \frac{1}{1-i^Ts}ii^T = \text{diag}(1/s) + \frac{1}{1 - i^Ts}ii^T\)</span>, <span class="math inline">\(A = \text{diag}(s) - ss^T\)</span>, and <span class="math inline">\(i = \begin{bmatrix} 1 \\ \vdots \\ 1 \end{bmatrix}\)</span>.</p>
<p>We want to show that <span class="math inline">\(AB = I\)</span>.</p>
<p><span class="math inline">\(\qquad AB = (\text{diag}(s) - ss^T)(\text{diag}(1/s) + \frac{1}{1 - i^Ts}ii^T)\)</span> <span class="math inline">\(\qquad = \text{diag}(s)\text{diag}(1/s) + \frac{1}{1 - i^Ts}\text{diag}(s)ii^T - ss^T\text{diag}(1/s) - \frac{1}{1 - i^Ts}ss^Tii^T\)</span> <span class="math inline">\(\qquad = I + \frac{1}{1 - i^Ts}si^T - si^T - \frac{i^Ts}{1-i^Ts}ss^T\)</span> <span class="math inline">\(\qquad = I + \frac{1}{1-i^Ts}(si^T - si^T(1-i^Ts) - (i^Ts)ss^T)\)</span> <span class="math inline">\(\qquad= I + \frac{1}{1-i^Ts}(si^T - si^T + s(i^Ts)i^T - (i^Ts)ss^T)\)</span> <span class="math inline">\(\qquad= I + \frac{1}{1 - i^Ts}(s(i^T s)i^T - (i^T s) ss^T)\)</span> <span class="math inline">\(\qquad = I + \frac{i^Ts}{1 - i^Ts}(si^T - ss^T) = I\)</span>.</p>
</section>
<section class="level3" id="exercise-15-orthonormal-matrix">
<h3 class="anchored" data-anchor-id="exercise-15-orthonormal-matrix">Exercise 15: Orthonormal Matrix</h3>
<p>Verify that the matrix</p>
<p><span class="math inline">\(\qquad U = \begin{pmatrix} \cos(\theta) &amp; -\sin(\theta) \\ \sin(\theta) &amp; \cos(\theta) \end{pmatrix}\)</span></p>
<p>is orthonormal for any <span class="math inline">\(\theta \in \mathbb{R}\)</span>.</p>
</section>
<section class="level3" id="solution-15">
<h3 class="anchored" data-anchor-id="solution-15">Solution 15:</h3>
<p><span class="math inline">\(U\)</span> is orthonormal if <span class="math inline">\(U^TU = UU^T = I\)</span>.</p>
<p><span class="math inline">\(\qquad U^TU = \begin{pmatrix} \cos(\theta) &amp; \sin(\theta) \\ -\sin(\theta) &amp; \cos(\theta) \end{pmatrix}\begin{pmatrix} \cos(\theta) &amp; -\sin(\theta) \\ \sin(\theta) &amp; \cos(\theta) \end{pmatrix} = \begin{pmatrix} \cos^2(\theta) + \sin^2(\theta) &amp; -\cos(\theta)\sin(\theta) + \sin(\theta)\cos(\theta) \\ -\sin(\theta)\cos(\theta) + \cos(\theta)\sin(\theta) &amp; \sin^2(\theta) + \cos^2(\theta) \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = I\)</span>.</p>
<p><span class="math inline">\(\qquad UU^T = \begin{pmatrix} \cos(\theta) &amp; -\sin(\theta) \\ \sin(\theta) &amp; \cos(\theta) \end{pmatrix}\begin{pmatrix} \cos(\theta) &amp; \sin(\theta) \\ -\sin(\theta) &amp; \cos(\theta) \end{pmatrix} = \begin{pmatrix} \cos^2(\theta) + \sin^2(\theta) &amp; \cos(\theta)\sin(\theta) - \sin(\theta)\cos(\theta) \\ \sin(\theta)\cos(\theta) - \cos(\theta)\sin(\theta) &amp; \sin^2(\theta) + \cos^2(\theta) \end{pmatrix} = \begin{pmatrix} 1 &amp; 0 \\ 0 &amp; 1 \end{pmatrix} = I\)</span>.</p>
</section>
<section class="level3" id="exercise-16-skew-symmetric-matrix">
<h3 class="anchored" data-anchor-id="exercise-16-skew-symmetric-matrix">Exercise 16: Skew Symmetric Matrix</h3>
<p>For any skew symmetric matrix <span class="math inline">\(A\)</span> (such that <span class="math inline">\(A^T = -A\)</span>), let</p>
<p><span class="math inline">\(\qquad U = (I+A)(I-A)^{-1}\)</span>.</p>
<p>Show that <span class="math inline">\(U\)</span> is orthonormal.</p>
</section>
<section class="level3" id="solution-16">
<h3 class="anchored" data-anchor-id="solution-16">Solution 16:</h3>
<p>We need to show that <span class="math inline">\(U^TU = I\)</span>. Since <span class="math inline">\(A^T = -A\)</span>,</p>
<p><span class="math inline">\(\qquad U^T = ((I-A)^{-1})^T(I+A)^T = ((I-A)^T)^{-1}(I+A)^T = (I-A^T)^{-1}(I+A^T) = (I+A)^{-1}(I-A)\)</span>. Then,</p>
<p><span class="math inline">\(\qquad U^TU = (I+A)^{-1}(I-A)(I+A)(I-A)^{-1}\)</span>.</p>
<p>Note that <span class="math inline">\((I-A)(I+A) = I - A^2\)</span> and <span class="math inline">\((I+A)(I-A) = I - A^2\)</span>, so <span class="math inline">\((I-A)(I+A) = (I+A)(I-A)\)</span>.</p>
<p>Therefore,</p>
<p><span class="math inline">\(\qquad U^TU = (I+A)^{-1}(I+A)(I-A)(I-A)^{-1} = I\)</span>.</p>
</section>
<section class="level3" id="exercise-17-conditional-distribution">
<h3 class="anchored" data-anchor-id="exercise-17-conditional-distribution">Exercise 17: Conditional Distribution</h3>
<p>Suppose that <span class="math inline">\(Y \in \mathbb{R}^{d_y}, X \in \mathbb{R}^{d_x}\)</span> with</p>
<p><span class="math inline">\(\qquad X \sim N(\mu_x, \sum_x)\)</span> <span class="math inline">\(\qquad Y|X = x \sim N(a+Bx, \sum_y)\)</span>,</p>
<p>where <span class="math inline">\(B\)</span> is <span class="math inline">\(d_y \times d_x\)</span>. Prove that</p>
<p><span class="math inline">\(\qquad X|Y = y \sim N(\mu_{x|y}, \sum_{x|y})\)</span> (26.2)</p>
<p><span class="math inline">\(\qquad \mu_{x|y} = \sum_{x|y}(B^T \sum_y^{-1}(y-a) + \sum_x^{-1}\mu_x)\)</span> <span class="math inline">\(\qquad \sum_{x|y} = (\sum_x^{-1} + B^T\sum_y^{-1}B)^{-1}\)</span>.</p>
</section>
<section class="level3" id="solution-17">
<h3 class="anchored" data-anchor-id="solution-17">Solution 17:</h3>
<p>This result can be derived from the properties of multivariate normal distributions. The joint distribution of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> can be written as:</p>
<p><span class="math inline">\(\qquad \begin{pmatrix} X \\ Y \end{pmatrix} \sim N \left( \begin{pmatrix} \mu_x \\ a + B\mu_x \end{pmatrix}, \begin{pmatrix} \sum_x &amp; \sum_{xy} \\ \sum_{yx} &amp; \sum_y + B\sum_x B^T \end{pmatrix} \right)\)</span>,</p>
<p>where <span class="math inline">\(\sum_{xy} = \text{Cov}(X, Y) = \sum_x B^T\)</span> and <span class="math inline">\(\sum_{yx} = \text{Cov}(Y, X) = B\sum_x\)</span>. From properties of the multivariate normal distribution, the conditional distribution of <span class="math inline">\(X|Y=y\)</span> is also normal, with</p>
<p><span class="math inline">\(\qquad \mu_{x|y} = \mu_x + \sum_{xy}\sum_{yy}^{-1}(y - \mu_y) = \mu_x + \sum_xB^T(\sum_y + B\sum_x B^T)^{-1}(y - a - B\mu_x)\)</span>,</p>
<p>and</p>
<p><span class="math inline">\(\qquad \sum_{x|y} = \sum_x - \sum_{xy}\sum_{yy}^{-1}\sum_{yx} = \sum_x - \sum_x B^T (\sum_y + B\sum_x B^T)^{-1} B \sum_x\)</span>.</p>
<p>We use the matrix identity <span class="math inline">\((A+UBV)^{-1} = A^{-1} - A^{-1}U(B^{-1} + VA^{-1}U)^{-1}VA^{-1}\)</span> with <span class="math inline">\(A = \sum_y, U = B, B = \sum_x, V = B^T\)</span>, which yields,</p>
<p><span class="math inline">\(\qquad (\sum_y + B\sum_x B^T)^{-1} = \sum_y^{-1} - \sum_y^{-1}B(\sum_x^{-1} + B^T\sum_y^{-1}B)^{-1}B^T\sum_y^{-1}\)</span>.</p>
<p>Substituting into the expression of <span class="math inline">\(\sum_{x|y}\)</span>,</p>
<p><span class="math inline">\(\qquad \sum_{x|y} = \sum_x - \sum_xB^T(\sum_y^{-1} - \sum_y^{-1}B(\sum_x^{-1} + B^T\sum_y^{-1}B)^{-1}B^T\sum_y^{-1})B\sum_x\)</span> <span class="math inline">\(\qquad = \sum_x - \sum_xB^T\sum_y^{-1}B\sum_x + \sum_x B^T \sum_y^{-1}B(\sum_x^{-1} + B^T\sum_y^{-1}B)^{-1}B^T\sum_y^{-1}B\sum_x\)</span>.</p>
<p>Let <span class="math inline">\(D = (\sum_x^{-1} + B^T\sum_y^{-1}B)\)</span>. Thus,</p>
<p><span class="math inline">\(\qquad \sum_{x|y} = \sum_x - \sum_x B^T\sum_y^{-1}B\sum_x + \sum_xB^T\sum_y^{-1}BD^{-1}B^T\sum_y^{-1}B\sum_x\)</span>. <span class="math inline">\(\qquad = \sum_x - \sum_x B^T\sum_y^{-1}B(\sum_x - D^{-1}B^T\sum_y^{-1}B\sum_x)\)</span> <span class="math inline">\(\qquad = \sum_x - \sum_x B^T \sum_y^{-1}B (\sum_x - (\sum_x^{-1} + B^T\sum_y^{-1}B)^{-1} B^T\sum_y^{-1} B\sum_x)\)</span> <span class="math inline">\(\qquad = \sum_x - \sum_x B^T\sum_y^{-1} B (\sum_x - (I + \sum_x B^T\sum_y^{-1} B)^{-1} \sum_x B^T\sum_y^{-1}B\sum_x)\)</span> After many manipulations one can establish that</p>
<p><span class="math inline">\(\qquad \sum_{x|y} = (\sum_x^{-1} + B^T\sum_y^{-1}B)^{-1}\)</span>.</p>
<p>Substituting in the expression of <span class="math inline">\(\mu_{x|y}\)</span>, and using the simplified version of <span class="math inline">\(\sum_{x|y}\)</span> leads to the stated result.</p>
</section>
<section class="level3" id="exercise-18-regression">
<h3 class="anchored" data-anchor-id="exercise-18-regression">Exercise 18: Regression</h3>
<p>Suppose that</p>
<p><span class="math inline">\(\qquad y_i = \beta x_i + \epsilon_i\)</span>,</p>
<p>where <span class="math inline">\((x_i, \epsilon_i)\)</span> are i.i.d. with <span class="math inline">\(\epsilon_i\)</span> independent of <span class="math inline">\(x_i\)</span> with mean zero but with unknown distribution. Suppose that <span class="math inline">\(x_i \sim N(0, 1)\)</span>. Then claim that</p>
<p><span class="math inline">\(\qquad t = \frac{\hat{\beta}}{s(\sum_{i=1}^n x_i^2)^{-1/2}} \sim t_{n-1}\)</span>,</p>
<p>where <span class="math inline">\(s\)</span> is the usual standard error. That is, the exact <span class="math inline">\(t\)</span>-test is valid as before even though the distribution of <span class="math inline">\(\epsilon\)</span> could be anything.</p>
</section>
<section class="level3" id="solution-18">
<h3 class="anchored" data-anchor-id="solution-18">Solution 18:</h3>
<p>We have</p>
<p><span class="math inline">\(\qquad \hat{\beta} - \beta_0 = \frac{\sum_{i=1}^n x_i \epsilon_i}{\sum_{i=1}^n x_i^2}\)</span> <span class="math inline">\(\qquad t = \frac{\sum_{i=1}^n x_i \epsilon_i}{\sqrt{\sum_{i=1}^n \epsilon_i^2}} \times \frac{\sqrt{\frac{\sum_{i=1}^n \epsilon_i^2}{\sum_{i=1}^n (x_i - \bar{x})^2}}}{\sqrt{\frac{1}{n-1}\sum_{i=1}^n x_i^2 \frac{\sum_{i=1}^n \epsilon_i^2}{\sum_{i=1}^n \hat{\epsilon}_i^2} }} = \frac{\sum_{i=1}^n x_i \epsilon_i}{\sqrt{\sum_{i=1}^n x_i^2}} / \sqrt{\frac{\sum_{i=1}^n \hat{\epsilon}_i^2}{n-1}}\)</span>.</p>
<p>Note that conditional on <span class="math inline">\(\epsilon_1, \dots, \epsilon_n\)</span></p>
<p><span class="math inline">\(\qquad \sum_{i=1}^n x_i \epsilon_i \sim N(0, \sum_{i=1}^n \epsilon_i^2)\)</span></p>
<p>so that conditionally and unconditionally</p>
<p><span class="math inline">\(\qquad \frac{\sum_{i=1}^n x_i \epsilon_i}{\sqrt{\sum_{i=1}^n \epsilon_i^2}} \sim N(0, 1)\)</span>.</p>
<p>Furthermore, we can write</p>
<p><span class="math inline">\(\qquad (n-1)s_*^2 \frac{(\hat{\epsilon}^T \epsilon)}{(\epsilon^T \epsilon)} = \frac{(\epsilon^T \epsilon)}{\epsilon^T \epsilon}\sum_{i=1}^n \epsilon_i^2\)</span> <span class="math inline">\(\qquad = \frac{(x^T x)}{(\epsilon^T \epsilon)}\epsilon^T \epsilon - \frac{(x^T x)}{(\epsilon^T \epsilon)} \epsilon^T x (x^T x)^{-1} x^T \epsilon\)</span> <span class="math inline">\(\qquad= \frac{(x^T x)}{(\epsilon^T \epsilon)} (\epsilon^T \epsilon) - \epsilon^T x (\epsilon^T \epsilon)^{-1} x^T \epsilon\)</span> <span class="math inline">\(\qquad = x^T M_{\epsilon} x\)</span>,</p>
<p>which is distributed as a chi-squared random variables with <span class="math inline">\(n-1\)</span> degrees of freedom conditional on <span class="math inline">\(\epsilon_1, \dots, \epsilon_n\)</span>. Moreover, they two terms are independent. Voila!</p>
</section>
<section class="level3" id="exercise-19-regression-statistics">
<h3 class="anchored" data-anchor-id="exercise-19-regression-statistics">Exercise 19: Regression Statistics</h3>
<p>Derive the equation of the regression line, the <span class="math inline">\(R^2\)</span> and the t-statistics for the slope coefficient for each of the following datasets <span class="math inline">\((X_i, Y_{ji}), i = 1, \dots, 11\)</span> and <span class="math inline">\(j = 1, 2, 3\)</span>. Graph the data and comment on your findings.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th><span class="math inline">\(X\)</span></th>
<th><span class="math inline">\(Y_1\)</span></th>
<th><span class="math inline">\(Y_2\)</span></th>
<th><span class="math inline">\(Y_3\)</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>10.0</td>
<td>8.04</td>
<td>9.14</td>
<td>7.46</td>
</tr>
<tr class="even">
<td>8.0</td>
<td>6.95</td>
<td>8.14</td>
<td>6.77</td>
</tr>
<tr class="odd">
<td>13.0</td>
<td>7.58</td>
<td>8.74</td>
<td>12.74</td>
</tr>
<tr class="even">
<td>9.0</td>
<td>8.81</td>
<td>8.77</td>
<td>7.11</td>
</tr>
<tr class="odd">
<td>11.0</td>
<td>8.33</td>
<td>9.26</td>
<td>7.81</td>
</tr>
<tr class="even">
<td>14.0</td>
<td>9.96</td>
<td>8.10</td>
<td>8.84</td>
</tr>
<tr class="odd">
<td>6.0</td>
<td>7.24</td>
<td>6.13</td>
<td>6.08</td>
</tr>
<tr class="even">
<td>4.0</td>
<td>4.26</td>
<td>3.10</td>
<td>5.39</td>
</tr>
<tr class="odd">
<td>12.0</td>
<td>10.84</td>
<td>9.13</td>
<td>8.15</td>
</tr>
<tr class="even">
<td>7.0</td>
<td>4.82</td>
<td>7.26</td>
<td>6.42</td>
</tr>
<tr class="odd">
<td>5.0</td>
<td>5.68</td>
<td>4.74</td>
<td>5.73</td>
</tr>
</tbody>
</table>
</section>
<section class="level3" id="solution-19">
<h3 class="anchored" data-anchor-id="solution-19">Solution 19:</h3>
<p>This exercise illustrates Anscombe’s quartet, a set of four datasets that have nearly identical simple descriptive statistics, yet appear very different when graphed. It highlights the importance of visualizing data before analyzing it. Let’s denote each dataset as <span class="math inline">\((X, Y_j)\)</span> for <span class="math inline">\(j = 1, 2, 3\)</span>. The linear regression model is <span class="math inline">\(Y_{ji} = \alpha_j + \beta_j X_i + \epsilon_{ji}\)</span>. The OLS estimators are:</p>
<p><span class="math inline">\(\qquad \hat{\beta_j} = \frac{\sum_{i=1}^{11} (X_i - \bar{X})(Y_{ji} - \bar{Y_j})}{\sum_{i=1}^{11} (X_i - \bar{X})^2}\)</span></p>
<p><span class="math inline">\(\qquad \hat{\alpha_j} = \bar{Y_j} - \hat{\beta_j}\bar{X}\)</span></p>
<p>The <span class="math inline">\(R^2\)</span> is:</p>
<p><span class="math inline">\(\qquad R^2_j = \frac{\sum_{i=1}^{11}(\hat{Y}_{ji} - \bar{Y_j})^2}{\sum_{i=1}^{11}(Y_{ji} - \bar{Y_j})^2} = \hat{\beta}_j^2 \frac{\sum_{i=1}^{11}(X_i - \bar{X})^2}{\sum_{i=1}^{11}(Y_{ji} - \bar{Y_j})^2}\)</span></p>
<p>The t-statistic for the slope coefficient <span class="math inline">\(\beta_j\)</span> is:</p>
<p><span class="math inline">\(\qquad t_j = \frac{\hat{\beta_j}}{SE(\hat{\beta_j})} = \frac{\hat{\beta_j}}{\sqrt{\frac{s_j^2}{\sum_{i=1}^{11} (X_i - \bar{X})^2}}}\)</span>, where <span class="math inline">\(s_j^2 = \frac{1}{11-2} \sum_{i=1}^{11} \hat{\epsilon}_{ji}^2\)</span></p>
<p>Using software (e.g., R, Python, or even a calculator) to compute these values:</p>
<p>For all three datasets, we have approximately:</p>
<ul>
<li><span class="math inline">\(\bar{X} = 9\)</span></li>
<li><span class="math inline">\(\bar{Y_j} = 7.5\)</span></li>
<li><span class="math inline">\(\hat{\beta_j} = 0.5\)</span></li>
<li><span class="math inline">\(\hat{\alpha_j} = 3\)</span></li>
<li>Regression Equation: <span class="math inline">\(Y_{ji} = 3 + 0.5 X_i\)</span></li>
<li><span class="math inline">\(R_j^2 = 0.667\)</span></li>
<li><span class="math inline">\(t_j \approx 4.24\)</span></li>
</ul>
<p>The findings highlight that despite having the same regression line, <span class="math inline">\(R^2\)</span>, and t-statistic, the datasets are visually very different. Dataset 1 is a standard linear relationship. Dataset 2 demonstrates a clear non-linear relationship. Dataset 3 is linear, but has an influential outlier. The summary statistics and regression results are identical, but the underlying data and appropriateness of the linear model vary drastically across the datasets. This emphasizes the importance of graphing data and not relying solely on summary statistics.</p>
</section>
<section class="level3" id="exercise-20-hypothesis-testing-in-regression-model">
<h3 class="anchored" data-anchor-id="exercise-20-hypothesis-testing-in-regression-model">Exercise 20: Hypothesis testing in Regression Model</h3>
<p>Consider the regression model</p>
<p><span class="math inline">\(\qquad Y_i = \alpha + \beta X_i + \epsilon_i\)</span></p>
<p>where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> and <span class="math inline">\(X_i\)</span> are fixed numbers. Let <span class="math inline">\(\theta = (\alpha, \beta, \sigma^2)\)</span>. Provide the Lagrange Multiplier, Wald, and Likelihood Ratio statistics [and give their asymptotic distribution] for testing the null hypothesis that <span class="math inline">\(\sigma^2 = 1\)</span> versus (a) the two sided alternative <span class="math inline">\(\sigma^2 \neq 1\)</span>; (b) the one-sided alternative <span class="math inline">\(\sigma^2 &lt; 1\)</span>.</p>
</section>
<section class="level3" id="solution-20">
<h3 class="anchored" data-anchor-id="solution-20">Solution 20:</h3>
<p>The t-test is straightforward. The log likelihood</p>
<p><span class="math inline">\(\qquad \log L(\alpha, \beta, \sigma^2) = -\frac{n}{2} \log 2\pi - \frac{n}{2}\log \sigma^2 - \frac{1}{2\sigma^2} \sum_{i=1}^n (Y_i - \alpha - \beta X_i)^2\)</span></p>
<p><span class="math inline">\(\qquad \log L(\alpha, \beta, 1) =  -\frac{n}{2} \log 2\pi - \frac{1}{2} \sum_{i=1}^n (Y_i - \alpha - \beta X_i)^2\)</span></p>
<p>In this case, the estimates of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> are unaffected by the restriction. Therefore, since</p>
<p><span class="math inline">\(\qquad \tilde{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n \hat{\epsilon}_i^2\)</span></p>
<p><span class="math inline">\(\qquad LR = 2 \{ \log L(\hat{\theta}) - \log L(\tilde{\theta}) \} = -n \log \tilde{\sigma}^2 + n (\tilde{\sigma}^2 - 1)\)</span>.</p>
<p>Now this looks complicated. But write <span class="math inline">\(\delta = \tilde{\sigma}^2 -1\)</span> and then</p>
<p><span class="math inline">\(\qquad LR = -n(\log(1+\delta) - \delta)\)</span>.</p>
<p>We now apply the fact that <span class="math inline">\(\ln(1+\epsilon) - \epsilon \sim \epsilon^2 / 2\)</span> for small <span class="math inline">\(\epsilon\)</span> so that</p>
<p><span class="math inline">\(\qquad LR \approx n (\tilde{\sigma}^2 - 1)^2\)</span>.</p>
<p>Formally, apply the delta method. Then further approximate by</p>
<p><span class="math inline">\(\qquad n \left(\frac{1}{n}\sum_{i=1}^n \hat{\epsilon}_i^2 - 1\right)^2 \overset{D}{\to} \chi^2(1)\)</span>.</p>
<p>In this case, we reject the null that <span class="math inline">\(\sigma^2 = 1\)</span> against the two-sided alternative if <span class="math inline">\(LR &gt; \chi^2_{\alpha}(1)\)</span>. To test the one sided alternative we consider the signed likelihood ratio test with</p>
<p><span class="math inline">\(\qquad LR_+ = -n (\log(1+\delta) - \delta) \times 1(\delta &gt; 0)\)</span>.</p>
</section>
<section class="level3" id="exercise-21-regression-with-fourth-power">
<h3 class="anchored" data-anchor-id="exercise-21-regression-with-fourth-power">Exercise 21: Regression with Fourth Power</h3>
<p>Consider the regression model <span class="math inline">\(Y_i = \beta X_i + \epsilon_i\)</span>, where <span class="math inline">\(X_i\)</span> is i.i.d., while <span class="math inline">\(\epsilon_i\)</span> are i.i.d. with mean zero and variance <span class="math inline">\(\sigma^2\)</span>. Discuss the consistency and asymptotic normality of <span class="math inline">\(\hat{\beta}\)</span>, where <span class="math inline">\(\hat{\beta}\)</span> minimizes</p>
<p><span class="math inline">\(\qquad \sum_{i=1}^n (Y_i - \beta X_i)^4\)</span>.</p>
<p>Discuss the case <span class="math inline">\(E(\epsilon_i^3) = 0\)</span> and the case <span class="math inline">\(E(\epsilon_i^3) \neq 0\)</span>.</p>
</section>
<section class="level3" id="solution-21">
<h3 class="anchored" data-anchor-id="solution-21">Solution 21:</h3>
<p>Let</p>
<p><span class="math inline">\(\qquad Q_n(\beta) = \frac{1}{n}\sum_{i=1}^n (Y_i - \beta X_i)^4\)</span>.</p>
<p>We can write</p>
<p><span class="math inline">\(\qquad Q_n(\beta) = \frac{1}{n}\sum_{i=1}^n (\epsilon_i + (\beta_0 - \beta)X_i)^4\)</span> <span class="math inline">\(\qquad = \frac{1}{n} \sum_{i=1}^n \epsilon_i^4 + 4(\beta_0 - \beta)\frac{1}{n}\sum_{i=1}^n \epsilon_i^3 X_i + 6(\beta_0 - \beta)^2 \frac{1}{n} \sum_{i=1}^n \epsilon_i^2 X_i^2 + 4(\beta_0 - \beta)^3 \frac{1}{n}\sum_{i=1}^n \epsilon_i X_i^3 + (\beta_0 - \beta)^4 \frac{1}{n}\sum_{i=1}^n X_i^4\)</span>.</p>
<p>Then, provided fourth moments exist, we have</p>
<p><span class="math inline">\(\qquad Q_n(\beta) \overset{P}{\to} Q(\beta) = a_0 + a_1(\beta_0 - \beta) + a_2(\beta_0 - \beta)^2 + a_3(\beta_0 - \beta)^3 + a_4(\beta_0-\beta)^4\)</span>.</p>
<p>This convergence is uniform in <span class="math inline">\(\beta\)</span> over a compact set because of the separation of the random variables from the parameters. When <span class="math inline">\(E(\epsilon_i^3) = 0\)</span>, we have</p>
<p><span class="math inline">\(\qquad Q(\beta) = a_0 + a_2 (\beta_0 - \beta)^2 + a_4(\beta_0 - \beta)^4\)</span>.</p>
<p>This function is uniquely minimized at <span class="math inline">\(\beta = \beta_0\)</span> because <span class="math inline">\(a_2, a_4 &gt; 0\)</span>. In general <span class="math inline">\(a_1, a_3\)</span> may not be zero, and the analysis is more complicated.</p>
</section>
<section class="level3" id="exercise-22-identification-in-gmm">
<h3 class="anchored" data-anchor-id="exercise-22-identification-in-gmm">Exercise 22: Identification in GMM</h3>
<p>Suppose that</p>
<p><span class="math inline">\(\qquad y_i = \sin \beta x_i + \epsilon_i\)</span>,</p>
<p>where <span class="math inline">\(E(\epsilon_i|x_i) = 0\)</span>. Suppose also that <span class="math inline">\(x_i\)</span> is uniformly distributed on <span class="math inline">\([a, b]\)</span> for some <span class="math inline">\(a,b\)</span>. The conditional moment restriction implies that <span class="math inline">\(E \epsilon_i h(x_i) = 0\)</span> for any measurable function <span class="math inline">\(h\)</span>. Consider two choices <span class="math inline">\(h(x) = 1\)</span> and <span class="math inline">\(h(x) = x \cos \beta x\)</span>. Check the identification issue for the GMM method with these two choices of instruments in the case that <span class="math inline">\(\beta = 0\)</span>.</p>
</section>
<section class="level3" id="solution-22">
<h3 class="anchored" data-anchor-id="solution-22">Solution 22:</h3>
<p>We have to check that</p>
<p><span class="math inline">\(\qquad E h(x_i) \sin \beta x_i = 0\)</span></p>
<p>if and only if <span class="math inline">\(\beta = 0\)</span>. The <em>if</em> part always works by assumption. We compute the above expectation for different <span class="math inline">\(h\)</span> and <span class="math inline">\(a, b\)</span>. First, suppose that <span class="math inline">\([a, b] = [0, \pi]\)</span>. Then</p>
<p><span class="math inline">\(\qquad \frac{1}{\pi} \int_0^\pi \sin \beta x dx = \frac{1-\cos \pi \beta}{\beta}\)</span></p>
<p><span class="math inline">\(\qquad \frac{1}{\pi} \int_0^\pi x \cos \beta x \sin \beta x dx = \frac{-2\pi \beta \cos^2 \pi \beta + \cos \pi \beta \sin \pi \beta + \pi \beta}{\beta^2}\)</span>.</p>
<p>These two functions are graphed below on the range <span class="math inline">\(-4 \leq \beta \leq 4\)</span>:</p>
<p>They both have very similar behavior, namely many isolated zeros. However, in the first graph, the zeros are at points where the derivative is zero, which suggests that these points do not correspond to a minimum (of whatever criterion function). In the second graph there are some zeros where the derivative is also positive, which suggests that these points correspond to local minima. It turns out that the zero at <span class="math inline">\(\beta=0\)</span> corresponds to the global minimum of the nonlinear least squares criterion function, which can be checked by looking at the quantity</p>
<p><span class="math inline">\(\qquad \frac{1}{\pi}\int_0^\pi (\sin(\beta x))^2 dx = \frac{- \cos \pi \beta \sin \pi \beta + \pi \beta}{2 \pi}\)</span>,</p>
<p>which is plotted below</p>
<p>Finally, we repeat the exercise for the case where <span class="math inline">\([a, b] = [-\pi/2, \pi/2]\)</span>. In this case,</p>
<p><span class="math inline">\(\qquad \frac{1}{\pi} \int_{-\pi/2}^{\pi/2} \sin(\beta x) dx = 0\)</span></p>
<p><span class="math inline">\(\qquad \frac{1}{\pi} \int_{-\pi/2}^{\pi/2} \sin(\beta x) x \cos(\beta x) dx = \frac{2\pi \beta \cos^2 \frac{1}{2}\pi \beta - 2\cos\frac{1}{2}\pi \beta \sin \frac{1}{2} \pi \beta - \pi \beta}{4\pi}\)</span></p>
<p><span class="math inline">\(\qquad \frac{1}{\pi} \int_{-\pi/2}^{\pi/2} (\sin(\beta x))^2 dx = -\frac{-2 \cos \frac{1}{2}\pi \beta \sin \frac{1}{2}\pi \beta + \pi \beta}{2\pi}\)</span>.</p>
<p>Now of course, the instrument <span class="math inline">\(h=1\)</span> is totally useless, while the nonlinear least squares instruments still work, as can be seen from the graph below of the first order condition and the least squares criterion</p>
</section>
<section class="level3" id="exercise-23-coastline-of-britain">
<h3 class="anchored" data-anchor-id="exercise-23-coastline-of-britain">Exercise 23: Coastline of Britain</h3>
<p>How long is the coastline of Britain? Lewis Fry Richardson conjectured that the length of the coastline <span class="math inline">\(L\)</span> was related to the length of the ruler <span class="math inline">\(G\)</span> by</p>
<p><span class="math inline">\(\qquad L = MG^{1-D}\)</span>,</p>
<p>where <span class="math inline">\(M, D\)</span> are constants with <span class="math inline">\(D \geq 1\)</span>. Suppose that you have a sample of observations <span class="math inline">\(\{L_i, G_i\}_{i=1}^n\)</span>, how would you test the hypothesis that the length is finite against the alternative that it is infinite (i.e., as <span class="math inline">\(G \to 0\)</span>)?</p>
</section>
<section class="level3" id="solution-23">
<h3 class="anchored" data-anchor-id="solution-23">Solution 23:</h3>
<p>Taking logs, we have</p>
<p><span class="math inline">\(\qquad \ln L_i = \ln M + (1-D) \ln G_i\)</span>.</p>
<p>Let <span class="math inline">\(y_i = \ln L_i\)</span>, <span class="math inline">\(x_i = \ln G_i\)</span>, <span class="math inline">\(\alpha = \ln M\)</span>, and <span class="math inline">\(\beta = 1-D\)</span>. Then we have a linear regression model</p>
<p><span class="math inline">\(\qquad y_i = \alpha + \beta x_i + \epsilon_i\)</span>.</p>
<p>We assume that <span class="math inline">\(\epsilon_i\)</span> are i.i.d. errors with mean zero. We can estimate <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> by OLS. The hypothesis that the length is finite corresponds to <span class="math inline">\(\beta = 0\)</span> (<span class="math inline">\(D=1\)</span>). The alternative hypothesis that the length is infinite is <span class="math inline">\(\beta &lt; 0\)</span> (<span class="math inline">\(D&gt;1\)</span>). Thus, we can conduct a one-sided t-test of <span class="math inline">\(H_0: \beta = 0\)</span> versus <span class="math inline">\(H_1: \beta &lt; 0\)</span>.</p>
</section>
<section class="level3" id="exercise-24-measuring-a-table">
<h3 class="anchored" data-anchor-id="exercise-24-measuring-a-table">Exercise 24: Measuring a Table</h3>
<p>Suppose that you want to measure the sides of a table, i.e., length (<span class="math inline">\(L\)</span>) and width (<span class="math inline">\(W\)</span>). However, your research assistant reports to you only the area (<span class="math inline">\(A\)</span>). Luckily, she was trained at Oxford and so makes an error in each measurement. Specifically,</p>
<p><span class="math inline">\(\qquad L_i = L + \epsilon_i ; \quad W_i = W + \eta_i\)</span>,</p>
<p>where <span class="math inline">\(\epsilon_i, \eta_i\)</span> are mutually independent standard normal random variables. The RA reports <span class="math inline">\(\{A_i\}_{i=1}^n\)</span>, where <span class="math inline">\(A_i = L_i W_i\)</span>. Suggest Method of Moments estimators of <span class="math inline">\(L, W\)</span> based on the sample information. Now suppose that <span class="math inline">\(\epsilon_i\)</span> and <span class="math inline">\(\eta_i\)</span> are both <span class="math inline">\(N(0, \sigma^2)\)</span> for some unknown <span class="math inline">\(\sigma^2\)</span>. Show how to estimate <span class="math inline">\(L, W\)</span>, and <span class="math inline">\(\sigma^2\)</span> from the sample data <span class="math inline">\(\{A_i\}_{i=1}^n\)</span>.</p>
</section>
<section class="level3" id="solution-24">
<h3 class="anchored" data-anchor-id="solution-24">Solution 24:</h3>
<p>We have</p>
<p><span class="math inline">\(\qquad E(A_i) = E[(L+\epsilon_i)(W+\eta_i)] = LW\)</span> <span class="math inline">\(\qquad E(A_i^2) = E[(L+\epsilon_i)^2(W+\eta_i)^2] = E[(L^2 + 2L\epsilon_i + \epsilon_i^2)(W^2 + 2W\eta_i + \eta_i^2)] = (L^2+1)(W^2+1) = L^2W^2 + L^2 + W^2 + 1\)</span>. <span class="math inline">\(\qquad \text{var}(A_i) = L^2 + W^2 + 1\)</span></p>
<p>These are two moment conditions in two unknowns. In fact by substitution <span class="math inline">\(L = E(A)/W\)</span> we get the quadratic equation</p>
<p><span class="math inline">\(\qquad \theta^2 + (1-E(A))\theta + E^2 A = 0\)</span></p>
<p>where <span class="math inline">\(\theta=W^2\)</span>. Note that the coefficient on <span class="math inline">\(\theta\)</span> is negative and the intercept is positive. We have</p>
<p><span class="math inline">\(\theta = \frac{E(A)-1 \pm \sqrt{(1-E(A))^2 - 4E^2A}}{2}\)</span></p>
<p>assuming this is well defined. We need</p>
<p><span class="math inline">\((1-E(A))^2 - 4E^2A \geq 0\)</span></p>
<p>which will be satisfied because we can expand out and find this quantity is <span class="math inline">\((L^2-W^2)^2\)</span>. We should take the positive root. Therefore, we estimate using sample mean and sample variance of the area measurements to obtain <span class="math inline">\(\hat{\theta}\)</span> and then estimate <span class="math inline">\(\hat{W} = \sqrt{\hat{\theta}}\)</span> and <span class="math inline">\(\hat{L} = E(A)/\sqrt{\hat{\theta}}\)</span>.</p>
</section>
<section class="level3" id="exercise-25-regression-with-error-in-variables">
<h3 class="anchored" data-anchor-id="exercise-25-regression-with-error-in-variables">Exercise 25: Regression with Error in Variables</h3>
<p>Consider the following regression model</p>
<p><span class="math inline">\(\qquad y = X_1 \beta_1 + X_2 \beta_2 + \epsilon\)</span>,</p>
<p>where <span class="math inline">\(X_j\)</span> is an <span class="math inline">\(n \times K_j\)</span> matrix of non-random regressors for <span class="math inline">\(j=1, 2\)</span>. The disturbance term <span class="math inline">\(\epsilon\)</span> satisfies</p>
<p><span class="math inline">\(\qquad E(\epsilon) = X_1 \gamma\)</span></p>
<p>for some non-zero vector <span class="math inline">\(\gamma\)</span>, and further suppose that</p>
<p><span class="math inline">\(\qquad E[(\epsilon - E(\epsilon))(\epsilon - E(\epsilon))^T] = \sigma^2 I_n\)</span>.</p>
<p>Calculate <span class="math inline">\(E(\hat{\beta_1})\)</span> and <span class="math inline">\(\text{var}(\hat{\beta_1})\)</span>.</p>
</section>
<section class="level3" id="solution-25">
<h3 class="anchored" data-anchor-id="solution-25">Solution 25:</h3>
<p>The OLS estimator of <span class="math inline">\(\beta_1\)</span> is obtained by regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Let <span class="math inline">\(X = \begin{pmatrix} X_1 &amp; X_2 \end{pmatrix}\)</span>. Then the OLS estimator is <span class="math inline">\(\hat{\beta} = \begin{pmatrix} \hat{\beta_1} \\ \hat{\beta_2} \end{pmatrix} = (X^TX)^{-1}X^Ty\)</span>. Let <span class="math inline">\(M_1 = I - X_1(X_1^TX_1)^{-1}X_1^T\)</span>. Then, the OLS estimator of <span class="math inline">\(\beta_1\)</span> is <span class="math inline">\(\hat{\beta_1} = (X_1^TX_1)^{-1}X_1^T y - (X_1^TX_1)^{-1}X_1^TX_2(X_2^TM_1X_2)^{-1}X_2^TM_1 y = (X_1^T M_2 X_1)^{-1} X_1^T M_2 y\)</span>, where <span class="math inline">\(M_2 = (I - X_2(X_2^TX_2)^{-1}X_2^T)\)</span>.</p>
<p><span class="math inline">\(\qquad E[\hat{\beta_1}] = E[(X_1^TM_2X_1)^{-1}X_1^TM_2(X_1\beta_1 + X_2\beta_2 + \epsilon)]\)</span> <span class="math inline">\(\qquad = \beta_1 + (X_1^TM_2X_1)^{-1}X_1^TM_2X_1\gamma\)</span> <span class="math inline">\(\qquad = \beta_1 + \gamma\)</span>. <span class="math inline">\(\qquad E(\epsilon|X) = X_1 \gamma\)</span></p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta_1}) = (X_1^T M_2 X_1)^{-1} X_1^T M_2 \text{Var}(\epsilon) M_2 X_1 (X_1^TM_2 X_1)^{-1}\)</span> <span class="math inline">\(\qquad \text{var}(\hat{\beta_1}) = \sigma^2 (X_1^TM_2X_1)^{-1}\)</span>.</p>
</section>
<section class="level3" id="exercise-26-true-or-false-a-b-c">
<h3 class="anchored" data-anchor-id="exercise-26-true-or-false-a-b-c">Exercise 26: True or False (a, b, c)</h3>
<p>True, False, or Indeterminate, and Explain. (a) Let <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> be two mean zero random variables. Then, <span class="math inline">\(E(X|Y)=0\)</span> and <span class="math inline">\(E(Y|X) = 0\)</span> if and only if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated.</p>
<ol start="2" type="a">
<li><p>Two events <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> with <span class="math inline">\(P(A) = 1\)</span> and <span class="math inline">\(P(B) = 1\)</span> must be mutually independent.</p></li>
<li><p>Whenever the cumulative distribution function is discontinuous, the median is not well-defined.</p></li>
</ol>
</section>
<section class="level3" id="solution-26">
<h3 class="anchored" data-anchor-id="solution-26">Solution 26:</h3>
<ol type="a">
<li><p><strong>False</strong>. If <span class="math inline">\(E(Y|X) = 0\)</span> or <span class="math inline">\(E(X|Y)\)</span>, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are uncorrelated, but not vice versa. Uncorrelatedness means <span class="math inline">\(E[XY] = E[X]E[Y]\)</span>, while <span class="math inline">\(E(Y|X) = 0\)</span> is a stronger condition, implying <span class="math inline">\(E[XY] = E[XE[Y|X]] = E[X \cdot 0] = 0\)</span>. However, two uncorrelated variables with means of zeros, may still have dependence between them that is missed by the measure of correlation.</p></li>
<li><p><strong>True</strong>. We have</p></li>
</ol>
<p><span class="math inline">\(\qquad P(A \cap B) = P(A) + P(B) - P(A \cup B) \geq P(A) + P(B) - 1 = 1\)</span></p>
<p>so <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span>.</p>
<ol start="3" type="a">
<li><strong>False</strong>. The cumulative distribution function could be discontinuous at some points but be continuous at the median.</li>
</ol>
</section>
<section class="level3" id="exercise-27-true-or-false-a-b-c">
<h3 class="anchored" data-anchor-id="exercise-27-true-or-false-a-b-c">Exercise 27: True or False (a, b, c)</h3>
<p>True, False, or Indeterminate, and Explain.</p>
<ol type="a">
<li><p>The Cramer-Rao Theorem says that with normally distributed errors the maximum likelihood estimator is Best Unbiased.</p></li>
<li><p>Suppose that for some estimator <span class="math inline">\(\hat{\theta}\)</span>, we have as <span class="math inline">\(n \to \infty\)</span></p></li>
</ol>
<p><span class="math inline">\(\qquad n(\hat{\theta} - 2\pi) \overset{D}{\to} N(0, 1)\)</span>,</p>
<p>where <span class="math inline">\(n\)</span> is the sample size. Then</p>
<p><span class="math inline">\(\qquad n \sin \hat{\theta} \overset{D}{\to} N(0, 1)\)</span>.</p>
<ol start="3" type="a">
<li>Suppose that <span class="math inline">\(A, B\)</span> are symmetric real matrices. The eigenvalues of <span class="math inline">\(A+B\)</span> are the same as the eigenvalues of <span class="math inline">\(B+A\)</span>.</li>
</ol>
</section>
<section class="level3" id="solution-27">
<h3 class="anchored" data-anchor-id="solution-27">Solution 27:</h3>
<ol type="a">
<li><p><strong>False</strong>. The CR theorem says that any unbiased estimator has variance greater than or equal to the inverse information. In the normal case, the MLE is unbiased and achieves the CR lower bound. The statement should say “… the MLE is Best Linear Unbiased”</p></li>
<li><p><strong>True</strong>. This is true from a simple application of the delta method with <span class="math inline">\(f(x) = \sin(x)\)</span> has derivative <span class="math inline">\(\cos(x)\)</span>, which is equal to 1 when <span class="math inline">\(x=2\pi\)</span>;</p></li>
<li><p><strong>True</strong> because <span class="math inline">\(A + B = B + A\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="exercise-28-reverse-regression">
<h3 class="anchored" data-anchor-id="exercise-28-reverse-regression">Exercise 28: Reverse Regression</h3>
<p>Consider the linear regression model</p>
<p><span class="math inline">\(\qquad Y = X\beta + \epsilon\)</span>,</p>
<p>where <span class="math inline">\(Y = (y_1, \dots, y_n)^T\)</span> and <span class="math inline">\(X\)</span> is the <span class="math inline">\(n \times K\)</span> matrix containing the regressors. Suppose that the classical assumptions hold, i.e., <span class="math inline">\(\epsilon_i\)</span> are i.i.d. with <span class="math inline">\(\epsilon_i \sim N(0, 1)\)</span> and independent of <span class="math inline">\(X\)</span>. Consider the reverse regression estimator <span class="math inline">\(\hat{\beta^R} = (\hat{\beta_1^R}, \dots, \hat{\beta_K^R})^T\)</span>, where</p>
<p><span class="math inline">\(\qquad \hat{\beta_j^R} = (Y^TY)^{-1}Y^TX_j\)</span>,</p>
<p>where <span class="math inline">\(X_j\)</span> are <span class="math inline">\(n \times 1\)</span> vectors containing the observations on the <span class="math inline">\(j\)</span>th regressor. Let <span class="math inline">\(x_i\)</span> be the <span class="math inline">\(K \times 1\)</span> vector containing the <span class="math inline">\(i\)</span>th observations on the regressors, and let <span class="math inline">\(\hat{x_i} = \hat{\beta^R} y_i\)</span> and <span class="math inline">\(\hat{u_i} = x_i - \hat{x_i}\)</span>. Which, if any, of the following hold and why:</p>
<ol type="i">
<li><p><span class="math inline">\(n^{-1}\sum_{i=1}^n x_i \hat{u_i} = 0\)</span>;</p></li>
<li><p><span class="math inline">\(n^{-1}\sum_{i=1}^n \hat{u_i} = 0\)</span>;</p></li>
<li><p><span class="math inline">\(n^{-1}\sum_{i=1}^n y_i \hat{u_i} = 0\)</span>. This is the only necessarily true answer, because the OLS procedure is constructed to make the right hand side variable, in this case <span class="math inline">\(y\)</span>, orthogonal to the residual <span class="math inline">\(\hat{u}\)</span>.</p></li>
<li><p><span class="math inline">\(X^Ty = X^T X \hat{\beta^R}\)</span></p></li>
</ol>
<p>Derive the probability limit of <span class="math inline">\(\hat{\beta^R}\)</span> as <span class="math inline">\(n\)</span> gets large. State clearly any additional assumptions you need to make. Consider the scalar case (<span class="math inline">\(K=1\)</span>). Then</p>
<p><span class="math inline">\(\qquad \hat{\beta^R} = \frac{Y^TX}{Y^TY} = \frac{\frac{1}{n}X^TY}{\frac{1}{n}Y^TY}\)</span>.</p>
</section>
<section class="level3" id="solution-28">
<h3 class="anchored" data-anchor-id="solution-28">Solution 28:</h3>
<p>We have</p>
<p><span class="math inline">\(\qquad \frac{1}{n}X^TY = \beta \frac{1}{n}X^TX + \frac{1}{n}X^T \epsilon\)</span> <span class="math inline">\(\qquad \frac{1}{n}Y^TY = \beta^2 \frac{1}{n} X^T X + 2\beta \frac{1}{n} X^T \epsilon + \frac{1}{n}\epsilon^T\epsilon\)</span>.</p>
<p>We apply law of large numbers to show that</p>
<p><span class="math inline">\(\qquad \frac{1}{n}X^TX \overset{P}{\to} E x_i^2\)</span> <span class="math inline">\(\qquad \frac{1}{n} X^T \epsilon \overset{P}{\to} 0\)</span> <span class="math inline">\(\qquad \frac{1}{n}\epsilon^T \epsilon \overset{P}{\to} 1\)</span>,</p>
<p>provided <span class="math inline">\(x_i\)</span> are i.i.d. with finite variance. We then apply Slutsky’s theorem to establish</p>
<p><span class="math inline">\(\qquad \hat{\beta^R} \overset{P}{\to} \frac{\beta E x_i^2}{\beta^2 E x_i^2 + 1}\)</span>.</p>
</section>
<section class="level3" id="exercise-29-hypothesis-testing">
<h3 class="anchored" data-anchor-id="exercise-29-hypothesis-testing">Exercise 29: Hypothesis Testing</h3>
<p>Consider the regression model</p>
<p><span class="math inline">\(\qquad Y_i = \alpha + \beta X_i + \epsilon_i\)</span>,</p>
<p>where <span class="math inline">\(\epsilon_i \sim N(0, \sigma^2)\)</span> and are independent of <span class="math inline">\(X_i\)</span>, which you may treat as fixed numbers. Let <span class="math inline">\(\{Y_i, X_i, i=1,\dots, n\}\)</span> be the observed sample, and let <span class="math inline">\(\theta = (\alpha, \beta, \sigma^2)\)</span> be unknown parameters. Provide the Likelihood Ratio statistic for testing the null hypothesis that <span class="math inline">\(\sigma^2=1\)</span> versus the two sided alternative <span class="math inline">\(\sigma^2 \ne 1\)</span>. Derive its large sample distribution under the null hypothesis. You may use the expansion <span class="math inline">\(\log(1+x) = x - x^2/2 + x^3/3 + \dots\)</span> for <span class="math inline">\(x \in (-1, 1)\)</span>.</p>
</section>
<section class="level3" id="solution-29">
<h3 class="anchored" data-anchor-id="solution-29">Solution 29:</h3>
<p>The t-test is straightforward. The log likelihood</p>
<p><span class="math inline">\(\qquad \log L(\alpha, \beta, \sigma^2) = -\frac{n}{2}\log 2\pi - \frac{n}{2} \log \sigma^2 - \frac{1}{2\sigma^2} \sum_{i=1}^n (Y_i - \alpha - \beta X_i)^2\)</span></p>
<p><span class="math inline">\(\qquad \log L(\alpha, \beta, 1) = -\frac{n}{2}\log 2\pi - \frac{1}{2} \sum_{i=1}^n (Y_i - \alpha - \beta X_i)^2\)</span>.</p>
<p>In this case, the estimates of <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span> are unaffected by the restriction. Therefore, since</p>
<p><span class="math inline">\(\qquad \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n \hat{\epsilon}_i^2\)</span></p>
<p><span class="math inline">\(LR = 2\{\log L(\hat{\theta}) - \log L(\tilde{\theta})\} = -n \log \hat{\sigma}^2 + n(\hat{\sigma}^2 - 1)\)</span></p>
<p>Now this looks nasty. But write <span class="math inline">\(\delta = \hat{\sigma}^2 -1\)</span> and then</p>
<p><span class="math inline">\(LR = -n (\log (1+\delta) - \delta)\)</span>. We now apply the fact that <span class="math inline">\(\ln(1+\epsilon) - \epsilon \approx \epsilon^2/2\)</span> for small <span class="math inline">\(\epsilon\)</span>, so that</p>
<p><span class="math inline">\(LR \approx n (\hat{\sigma}^2 -1)^2\)</span></p>
<p>and then approximate by</p>
<p><span class="math inline">\(n\left( \frac{1}{n} \sum_{i=1}^n \hat{\epsilon}_i^2 - 1 \right)^2 \overset{d}{\to} \chi^2(1)\)</span>.</p>
</section>
<section class="level3" id="exercise-30-rogoff-satchell-estimator">
<h3 class="anchored" data-anchor-id="exercise-30-rogoff-satchell-estimator">Exercise 30: Rogoff-Satchell Estimator</h3>
<p>Suppose that <span class="math inline">\(y_i = \beta x_i + \epsilon_i\)</span>, where <span class="math inline">\(\epsilon_i, x_i\)</span> are i.i.d. and mutually independent with <span class="math inline">\(\epsilon_i, x_i \sim N(0, 1)\)</span>. Consider the Rogoff-Satchell estimator. The idea is to remove observations in such a way as to change the sign of the estimator. One version of this is defined as follows:</p>
<p><span class="math inline">\(\qquad \hat{\beta}_{RS} = \frac{\sum_{i=1}^n x_i y_i 1(x_i y_i \hat{\beta} &lt; 0)}{\sum_{i=1}^n x_i^2 1(x_i y_i \hat{\beta} &lt; 0)}\)</span>.</p>
<p>Note that <span class="math inline">\(x_i y_i \hat{\beta} &lt; 0\)</span> if either <span class="math inline">\(\hat{\beta} &gt; 0\)</span> and <span class="math inline">\(x_i y_i &lt; 0\)</span> or <span class="math inline">\(\hat{\beta} &lt; 0\)</span> and <span class="math inline">\(x_i y_i &gt; 0\)</span>. We have <span class="math inline">\(x_i y_i &lt; 0\)</span> if either <span class="math inline">\(x_i &lt; 0\)</span> and <span class="math inline">\(y_i &gt; 0\)</span> or <span class="math inline">\(x_i &gt; 0\)</span> and <span class="math inline">\(y_i &lt; 0\)</span>. Essentially this keeps only the data in quadrants opposite to the least squares slope and then fits least squares to them. This estimator is nonlinear and it is biased. Derive the properties of this estimator in two cases:</p>
<ol type="a">
<li><p><span class="math inline">\(\beta \neq 0\)</span>;</p></li>
<li><p><span class="math inline">\(\beta = 0\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="solution-30">
<h3 class="anchored" data-anchor-id="solution-30">Solution 30:</h3>
<p>When <span class="math inline">\(\beta \neq 0\)</span> we have</p>
<p><span class="math inline">\(\qquad \hat{\beta}_{R-S}^* = \hat{\beta}_{R-S} + o_p(n^{-1/2})\)</span></p>
<p><span class="math inline">\(\qquad \hat{\beta}_{R-S}^* = \frac{\sum_{i=1}^n x_i y_i 1(x_i y_i \beta &lt; 0)}{\sum_{i=1}^n x_i^2 1(x_i y_i \beta &lt; 0)}\)</span></p>
<p>and this can be treated by standard arguments. When <span class="math inline">\(\beta = 0\)</span>, we argue as follows. Firstly, we may show that</p>
<p><span class="math inline">\(\qquad \sqrt{n}\hat{\beta} \overset{D}{\to} Z\)</span>,</p>
<p>where <span class="math inline">\(Z\)</span> is standard normal. Then we have</p>
<p><span class="math inline">\(\qquad \hat{\beta}_{R-S} = \hat{\beta}_{R-S}^* + o_p(n^{-1/2})\)</span> <span class="math inline">\(\qquad \hat{\beta}_{R-S}^* = \frac{\sum_{i=1}^n x_i y_i 1(x_i y_i Z &lt; 0)}{\sum_{i=1}^n x_i^2 1(x_i y_i Z &lt; 0)}\)</span>.</p>
<p>Since <span class="math inline">\(Z\)</span> is independent of the data we may condition on it. Furthermore, note that if <span class="math inline">\(Z &gt; 0\)</span> then <span class="math inline">\(x_i y_i &lt; 0\)</span> to ensure 1 and vice versa. We have</p>
<p><span class="math inline">\(\qquad \frac{1}{n} \sum_{i=1}^n x_i y_i 1(x_i y_i Z &lt; 0) \overset{P}{\to}_{Z&gt;0} E[x_i y_i 1(x_i y_i &lt; 0)]\)</span> <span class="math inline">\(\qquad \frac{1}{n} \sum_{i=1}^n x_i y_i 1(x_i y_i Z &lt; 0) \overset{P}{\to}_{Z&lt;0} E[x_i y_i 1(x_i y_i &gt; 0)]\)</span>.</p>
<p>Similarly</p>
<p><span class="math inline">\(\qquad \frac{1}{n} \sum_{i=1}^n x_i^2 1(x_i y_i Z &lt; 0) \overset{P}{\to}_{Z&gt;0} E[x_i^2 1(x_i y_i &lt; 0)]\)</span> <span class="math inline">\(\frac{1}{n} \sum_{i=1}^n x_i^2 1(x_i y_i Z &lt; 0) \overset{P}{\to}_{Z&lt;0} E[x_i^2 1(x_i y_i &gt; 0)]\)</span>.</p>
<p>Therefore, when <span class="math inline">\(Z&gt;0\)</span></p>
<p><span class="math inline">\(\qquad \hat{\beta}_{R-S} \overset{P}{\to}_{Z&gt;0} \frac{E[x_i y_i 1(x_i y_i &lt; 0)]}{E[x_i^2 1(x_i y_i &lt; 0)]}\)</span></p>
<p>but when <span class="math inline">\(Z&lt;0\)</span></p>
<p><span class="math inline">\(\qquad \hat{\beta}_{R-S} \overset{P}{\to}_{Z&lt;0} \frac{E[x_i y_i 1(x_i y_i &gt; 0)]}{E[x_i^2 1(x_i y_i &gt; 0)]}\)</span>.</p>
<p>This shows that</p>
<p><span class="math display">\[\hat{\beta}_{R-S} \overset{D}{\to} W = \begin{cases}
\frac{E[x_i y_i 1(x_i y_i &lt; 0)]}{E[x_i^2 1(x_i y_i &lt; 0)]} \text{  if } Z &gt; 0 \\
\frac{E[x_i y_i 1(x_i y_i &gt; 0)]}{E[x_i^2 1(x_i y_i &gt; 0)]} \text{  if } Z &lt; 0,
\end{cases}\]</span></p>
<p>which is a two point distribution.</p>
</section>
<section class="level3" id="exercise-31-true-or-false">
<h3 class="anchored" data-anchor-id="exercise-31-true-or-false">Exercise 31: True or False</h3>
<p>Establish whether the following statements are True, False, or Indeterminate. Explain your reasoning.</p>
<ol type="i">
<li><p>If <span class="math inline">\(X_n\)</span> is a discrete random variable for each <span class="math inline">\(n\)</span>, and if <span class="math inline">\(X_n \overset{P}{\to} 0\)</span>, then <span class="math inline">\(Pr(X_n = 0) \to 1\)</span>.</p></li>
<li><p>The Gauss-Markov theorem says that with normally distributed errors the ordinary least squares estimator is Best Linear Unbiased.</p></li>
<li><p>Suppose that <span class="math inline">\(A, B\)</span> are symmetric real matrices. Let <span class="math inline">\(\lambda\)</span> be a nonzero eigenvalue of <span class="math inline">\(AB\)</span>, then <span class="math inline">\(\lambda^2\)</span> is an eigenvalue of <span class="math inline">\((AB)^2\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="solution-31">
<h3 class="anchored" data-anchor-id="solution-31">Solution 31:</h3>
<ol type="i">
<li><p><strong>False</strong>. For example, <span class="math inline">\(X_n = \pm 1/n\)</span> with probability <span class="math inline">\(1/2\)</span>. However, if support of <span class="math inline">\(X_n\)</span> is fixed then this is true.</p></li>
<li><p><strong>True</strong>, but don’t need normality, unnecessary condition</p></li>
<li><p><strong>True</strong>. Suppose that</p></li>
</ol>
<p><span class="math inline">\(ABx = \lambda x\)</span>.</p>
<p>Then</p>
<p><span class="math inline">\(ABABx = \lambda ABx = \lambda^2 x\)</span>.</p>
</section>
<section class="level3" id="exercise-32-hypothesis-testing-in-regression">
<h3 class="anchored" data-anchor-id="exercise-32-hypothesis-testing-in-regression">Exercise 32: Hypothesis Testing in Regression</h3>
<p>Suppose that you observe data <span class="math inline">\(\{Y_i, X_i, i=1,\dots, n\}\)</span> with <span class="math inline">\(X_i \in \mathbb{R}^K\)</span>. You believe that the linear regression model holds, so that</p>
<p><span class="math inline">\(Y_i = \alpha + \beta^T X_i + \epsilon_i\)</span></p>
<p>where <span class="math inline">\(\beta \in \mathbb{R}^K\)</span> is an unknown parameter vector and <span class="math inline">\(\alpha \in \mathbb{R}\)</span> is an unknown scalar parameter. Explain how you would test the following hypotheses. In your answer, be clear about what additional assumptions you make and what role the assumptions have in making the test valid. Also give the test statistics and the critical values you would use (no table work).</p>
<ol type="i">
<li><p>The null hypothesis that <span class="math inline">\(\alpha=0\)</span> versus the alternative that <span class="math inline">\(\alpha &gt; 0\)</span>.</p></li>
<li><p>The null hypothesis that <span class="math inline">\(\beta = 0\)</span> versus the alternative that <span class="math inline">\(\beta \ne 0\)</span>.</p></li>
<li><p>The null hypothesis that <span class="math inline">\(\alpha = 0\)</span> and <span class="math inline">\(\beta = 0\)</span> versus the alternative that <span class="math inline">\(\alpha \ne 0\)</span> or <span class="math inline">\(\beta \ne 0\)</span>.</p></li>
</ol>
</section>
<section class="level3" id="solution-32">
<h3 class="anchored" data-anchor-id="solution-32">Solution 32:</h3>
<p>We assume i.i.d. observations, with <span class="math inline">\(E[\epsilon_i|X_i] = 0\)</span> and <span class="math inline">\(E[\epsilon_i^2|X_i] = \sigma^2\)</span>. We also need that second moments exist (<span class="math inline">\(E||X_i||^2 &lt; \infty\)</span>).</p>
<ol type="i">
<li>Under the null hypothesis <span class="math inline">\(H_0: \alpha = 0\)</span>, we can use a one-sided t-test. The test statistic is:</li>
</ol>
<p><span class="math inline">\(t = \frac{\hat{\alpha} - 0}{SE(\hat{\alpha})}\)</span></p>
<p>where <span class="math inline">\(\hat{\alpha}\)</span> is the OLS estimator of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(SE(\hat{\alpha})\)</span> is its standard error. Under the null hypothesis and the assumptions given, this statistic follows a <span class="math inline">\(t_{n-K-1}\)</span> distribution (or asymptotically a standard normal distribution). We reject the null if <span class="math inline">\(t &gt; t_{n-K-1, 1-\gamma}\)</span>, where <span class="math inline">\(\gamma\)</span> is the significance level.</p>
<ol start="2" type="i">
<li><p>Under the null hypothesis <span class="math inline">\(H_0: \beta = 0\)</span>, we use an F-test. The test statistic is based on comparing the restricted and unrestricted sum of squared residuals (RSS). Let <span class="math inline">\(RSS_R = \sum_{i=1}^n (Y_i - \hat{\alpha})^2\)</span> be the restricted RSS (under the null that <span class="math inline">\(\beta=0\)</span>) Let <span class="math inline">\(RSS_U = \sum_{i=1}^n (Y_i - \hat{\alpha} - \hat{\beta}^TX_i)^2\)</span> be the unrestricted RSS. The F-statistic is <span class="math inline">\(F = \frac{(RSS_R - RSS_U)/K}{RSS_U/(n-K-1)}\)</span>. Under the null hypothesis and our assumptions, this statistic follows an <span class="math inline">\(F_{K, n-K-1}\)</span> distribution. We reject the null if <span class="math inline">\(F &gt; F_{K, n-K-1, 1-\gamma}\)</span>.</p></li>
<li><p>Under the null hypothesis <span class="math inline">\(H_0: \alpha = 0\)</span> and <span class="math inline">\(\beta = 0\)</span>, we again use an F-test. This is a joint test. Let <span class="math inline">\(RSS_R = \sum_{i=1}^n Y_i^2\)</span> be the restricted RSS (under the null that <span class="math inline">\(\alpha = \beta = 0\)</span>). Let <span class="math inline">\(RSS_U = \sum_{i=1}^n (Y_i - \hat{\alpha} - \hat{\beta}^T X_i)^2\)</span> be the unrestricted RSS. The F-statistic is: <span class="math inline">\(F = \frac{(RSS_R - RSS_U)/(K+1)}{RSS_U/(n-K-1)}\)</span>. Under the null hypothesis, and assumptions given, <span class="math inline">\(F \sim F_{K+1, n-K-1}\)</span>. Reject the null if <span class="math inline">\(F &gt; F_{K+1, n-K-1, 1-\gamma}\)</span>.</p></li>
</ol>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>