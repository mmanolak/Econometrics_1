<!DOCTYPE html>

<html lang="en" xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><head>
<meta charset="utf-8"/>
<meta content="quarto-1.5.57" name="generator"/>
<meta content="width=device-width, initial-scale=1.0, user-scalable=yes" name="viewport"/>
<meta content="Peter Fuleky" name="author"/>
<title>Chapter 21: Omission of Relevant Variables, Inclusion of Irrelevant Variables, and Model Selection – Study notes for Econometrics I</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>
<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta content="../" name="quarto:offset"/>
<link href="../chapters/chap22.html" rel="next"/>
<link href="../chapters/chap20.html" rel="prev"/>
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet"/>
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" id="quarto-text-highlighting-styles" rel="stylesheet"/>
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet"/>
<link data-mode="light" href="../site_libs/bootstrap/bootstrap.min.css" id="quarto-bootstrap" rel="stylesheet"/>
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>
<link href="../style.css" rel="stylesheet"/>
</head>
<body class="nav-sidebar floating">
<div id="quarto-search-results"></div>
<header class="headroom fixed-top" id="quarto-header">

</header>
<!-- content -->
<div class="quarto-container page-columns page-rows-contents page-layout-article" id="quarto-content">
<!-- sidebar -->

<div class="quarto-sidebar-collapse-item" data-bs-target=".quarto-sidebar-collapse-item" data-bs-toggle="collapse" id="quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">

</div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">
<h1 class="title"><span class="chapter-title">Chapter 21: Omission of Relevant Variables, Inclusion of Irrelevant Variables, and Model Selection</span></h1>
</div>
<div class="quarto-title-meta">
</div>
</header>
<p>This chapter discusses the consequences of misspecifying a regression model by either omitting relevant variables or including irrelevant ones. It also covers model selection techniques.</p>
<section class="level2" id="omission-of-relevant-variables">
<h2 class="anchored" data-anchor-id="omission-of-relevant-variables">21.1 Omission of Relevant Variables</h2>
<p>Suppose the true model is:</p>
<p><span class="math inline">\(\qquad y = X_1\beta_1 + X_2\beta_2 + \epsilon\)</span>,</p>
<p>where Assumption A1 holds. However, we regress <span class="math inline">\(y\)</span> on <span class="math inline">\(X_1\)</span> only, perhaps because we did not observe <span class="math inline">\(X_2\)</span>.</p>
<p>The OLS estimator for <span class="math inline">\(\beta_1\)</span> in this misspecified model is:</p>
<p><span class="math inline">\(\qquad \tilde{\beta_1} = (X_1^T X_1)^{-1} X_1^T y\)</span>.</p>
<p>Substituting the true model for <span class="math inline">\(y\)</span>:</p>
<p><span class="math inline">\(\qquad \begin{aligned}
\tilde{\beta_1} &amp;= (X_1^T X_1)^{-1} X_1^T (X_1\beta_1 + X_2\beta_2 + \epsilon) \\
&amp;= (X_1^T X_1)^{-1} X_1^T X_1\beta_1 + (X_1^T X_1)^{-1} X_1^T X_2\beta_2 + (X_1^T X_1)^{-1} X_1^T \epsilon \\
&amp;= \beta_1 + (X_1^T X_1)^{-1} X_1^T X_2\beta_2 + (X_1^T X_1)^{-1} X_1^T \epsilon.
\end{aligned}\)</span></p>
<p>Taking expectations conditional on <span class="math inline">\(X\)</span>:</p>
<p><span class="math inline">\(\qquad \begin{aligned}
E(\tilde{\beta_1}|X) &amp;= \beta_1 + (X_1^T X_1)^{-1} X_1^T X_2\beta_2 \\
&amp;= \beta_1 + \beta_{12},
\end{aligned}\)</span></p>
<p>where <span class="math inline">\(\beta_{12} = (X_1^T X_1)^{-1} X_1^T X_2\beta_2\)</span>.</p>
<p><strong>Intuition:</strong> The OLS estimator <span class="math inline">\(\tilde{\beta_1}\)</span> is biased. The bias, <span class="math inline">\(\beta_{12}\)</span>, arises from the correlation between the included regressor <span class="math inline">\(X_1\)</span> and the omitted variable <span class="math inline">\(X_2\)</span>. If <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are uncorrelated, then <span class="math inline">\(X_1^T X_2 = 0\)</span>, and there is no bias. If they are correlated, the OLS estimator of the coefficient on <span class="math inline">\(X_1\)</span> picks up some of the effect of <span class="math inline">\(X_2\)</span> on <span class="math inline">\(y\)</span>.</p>
<p>In general, <span class="math inline">\(\tilde{\beta_1}\)</span> is biased and inconsistent. The direction and magnitude of the bias depend on <span class="math inline">\(\beta_2\)</span> and on <span class="math inline">\(X_1^T X_2\)</span>.</p>
<section class="level3" id="example-21.1">
<h3 class="anchored" data-anchor-id="example-21.1">Example 21.1</h3>
<p>Some common examples of omitted variables:</p>
<ol type="1">
<li><strong>Seasonality:</strong> In time series data, failing to include seasonal dummy variables can bias coefficients if other variables have seasonal patterns.</li>
<li><strong>Dynamics:</strong> Ignoring lagged variables in a dynamic model can lead to bias.</li>
<li><strong>Nonlinearity:</strong> If the true relationship is nonlinear but a linear model is fitted, the coefficients may be biased.</li>
<li><strong>Endogeneity:</strong> Omitting variables that are correlated with both the dependent variable and included independent variables leads to omitted variable bias, a specific case of which is endogeneity.</li>
</ol>
<p>For example: consider a wage equation. Wages depend on education and ability. If you omit ability, and ability is positively correlated with both wages and education, then omitting ability results in upward bias of the effect of education. Similarly, if studying the effect of race or gender on wages, you might omit experience or education which could bias the estimated effect of race or gender.</p>
<p><strong>Variance of the estimator:</strong></p>
<p>The conditional variance of the biased estimator <span class="math inline">\(\tilde{\beta_1}\)</span> given <span class="math inline">\(X\)</span> is:</p>
<p><span class="math inline">\(\qquad \text{var}(\tilde{\beta_1}|X) = (X_1^T X_1)^{-1} X_1^T \Sigma(X) X_1 (X_1^T X_1)^{-1}\)</span>,</p>
<p>where <span class="math inline">\(\Sigma(X)\)</span> is the variance-covariance matrix of the error term.</p>
<p>This should be compared with the variance of the estimator from the correct model that includes <span class="math inline">\(X_2\)</span>:</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta_1}|X) = (X_1^T M_2 X_1)^{-1} X_1^T M_2 \Sigma(X) M_2 X_1 (X_1^T M_2 X_1)^{-1}\)</span>,</p>
<p>where <span class="math inline">\(M_2 = I - X_2(X_2^T X_2)^{-1}X_2^T\)</span>.</p>
<p>In the homoskedastic case, <span class="math inline">\(\Sigma(X) = \sigma^2 I\)</span>, and the variances simplify to:</p>
<p><span class="math inline">\(\qquad \text{var}(\tilde{\beta_1}|X) = \sigma^2 (X_1^T X_1)^{-1}\)</span></p>
<p>and</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta_1}|X) = \sigma^2 (X_1^T M_2 X_1)^{-1}\)</span>.</p>
<p>Since <span class="math inline">\(X_1^T X_1\)</span> is “larger” than <span class="math inline">\(X_1^T M_2 X_1\)</span>, including the relevant variable (<span class="math inline">\(X_2\)</span>) reduces the variance of the estimator of the coefficients on <span class="math inline">\(X_1\)</span>. The comparison between the MSE of the two estimators is:</p>
<p><span class="math inline">\(\qquad (X_1^T X_1)^{-1} X_1^T X_2 \beta_2 \beta_2^T X_2^T X_1 (X_1^T X_1)^{-1} + \sigma^2(X_1^T X_1)^{-1} \text{ versus } \sigma^2(X_1^T M_2 X_1)^{-1}\)</span></p>
<p>There’s no ranking between these two performance measures in general. However, with large samples, the variance components go to zero. So you prefer the estimator that controls for <span class="math inline">\(X_2\)</span>.</p>
<p><strong>Effect on Inference:</strong></p>
<p>The standard estimated variance of <span class="math inline">\(\tilde{\beta_1}\)</span> is <span class="math inline">\(s_u^2 (X_1^T X_1)^{-1}\)</span>, where</p>
<p><span class="math inline">\(\qquad s_u^2 = \frac{y^T M_1 y}{n - K_1} = \frac{(X_2 \beta_2 + \epsilon)^T M_1 (X_2 \beta_2 + \epsilon)}{n - K_1}\)</span>.</p>
<p>Expanding the numerator and taking the expectation (under homoskedasticity): <span class="math inline">\(\qquad E[s_u^2] = \sigma^2 + \frac{\beta_2^T X_2^T M_1 X_2 \beta_2}{n - K_1} \geq \sigma^2\)</span>.</p>
<p>The inequality follows because <span class="math inline">\(M_1\)</span> is a positive semi-definite matrix. Therefore, the estimated variance of <span class="math inline">\(\tilde{\beta_1}\)</span> is upwardly biased. The t-statistic is not distributed as t anymore.</p>
<p>If <span class="math inline">\(X_1^T X_2 = 0\)</span>, then <span class="math inline">\(\tilde{\beta}\)</span> is unbiased, but the OLS standard errors are still biased. In this case, the t-ratio is downward biased, and so t-tests are less likely to reject the null hypothesis.</p>
<p>In general, the direction of the bias in the t-ratio is ambiguous, depending on the bias of <span class="math inline">\(\tilde{\beta_1}\)</span>.</p>
<p><strong>Nonlinear Example:</strong></p>
<p>Suppose the true regression function is nonlinear:</p>
<p><span class="math inline">\(\qquad y_i = \alpha + \beta x_i + \gamma x_i^2 + \epsilon_i\)</span>.</p>
<p>If we omit the quadratic term <span class="math inline">\(x_i^2\)</span>, the OLS estimators for the intercept and slope coefficients are:</p>
<p><span class="math inline">\(\qquad E[\tilde{\beta}] = \beta + \gamma \frac{\sum_{i=1}^n (x_i - \bar{x})x_i^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\)</span></p>
<p><span class="math inline">\(\qquad E[\tilde{\alpha}] = \alpha + \gamma \left[ \frac{1}{n}\sum_{i=1}^n x_i^2 - \bar{x} \frac{\sum_{i=1}^n (x_i - \bar{x})x_i^2}{\sum_{i=1}^n (x_i - \bar{x})^2} \right]\)</span>. If <span class="math inline">\(\bar{x} = 0\)</span>, the bias of the slope depends on the skewness of the regressors and <span class="math inline">\(\gamma\)</span>. The bias of the intercept depends on <span class="math inline">\(\gamma\)</span> and variance.</p>
</section>
</section>
<section class="level2" id="inclusion-of-irrelevant-variablesknowledge-of-parameters">
<h2 class="anchored" data-anchor-id="inclusion-of-irrelevant-variablesknowledge-of-parameters">21.2 Inclusion of Irrelevant Variables/Knowledge of Parameters</h2>
<p>Suppose the true model is:</p>
<p><span class="math inline">\(\qquad y = X_1\beta_1 + \epsilon\)</span>,</p>
<p>but we regress <span class="math inline">\(y\)</span> on both <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. The estimator of <span class="math inline">\(\beta_1\)</span> is:</p>
<p><span class="math inline">\(\qquad \tilde{\beta_1} = (X_1^T M_2 X_1)^{-1} X_1^T M_2 y = \beta_1 + (X_1^T M_2 X_1)^{-1} X_1^T M_2 \epsilon\)</span>.</p>
<p>The expected value and conditional variance are:</p>
<p><span class="math inline">\(\qquad E(\tilde{\beta_1}|X) = \beta_1\)</span></p>
<p><span class="math inline">\(\qquad \text{var}(\tilde{\beta_1}|X) = \sigma^2 (X_1^T M_2 X_1)^{-1}\)</span>.</p>
<p>Compare this to the correct model’s estimator:</p>
<p><span class="math inline">\(\qquad \text{var}(\hat{\beta_1}|X) = \sigma^2 (X_1^T X_1)^{-1}\)</span>.</p>
<p>Since <span class="math inline">\(X_1^T X_1 - X_1^T M_2 X_1 = X_1^T P_2 X_1 \geq 0\)</span>,</p>
<p><span class="math inline">\(\qquad (X_1^T X_1)^{-1} - (X_1^T M_2 X_1)^{-1} \leq 0\)</span>,</p>
<p>the variance is smaller when using only the relevant regressors (<span class="math inline">\(X_1\)</span>). Including irrelevant variables inflates the variance.</p>
<p><strong>Restricted least squares</strong> You have linear restrictions: <span class="math inline">\(R\beta=r\)</span>. Using restricted least squares gives smaller variances.</p>
<p><strong>Bias Variance Tradeoff:</strong> There is a trade-off between bias and variance. - Big Model: low bias, high variance. - Small Model: small variance, large bias.</p>
<p><strong>Known parameters:</strong> If we know <span class="math inline">\(\beta_2\)</span> and use <span class="math inline">\(y - X_2\beta_2\)</span> on <span class="math inline">\(X_1\)</span>, this gives better results than using <span class="math inline">\(y\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. Knowledge of parameters is valuable.</p>
</section>
<section class="level2" id="model-selection">
<h2 class="anchored" data-anchor-id="model-selection">21.3 Model Selection</h2>
<p>Let <span class="math inline">\(M\)</span> be a collection of linear regression models obtained from a given set of <span class="math inline">\(K\)</span> regressors <span class="math inline">\(X = (X_1, \dots, X_K)\)</span>. There are <span class="math inline">\(2^K - 1\)</span> different subsets of <span class="math inline">\(X\)</span>, and hence different regression models that can be counted as submodels. Suppose the true model lies in <span class="math inline">\(M\)</span>:</p>
<p><span class="math inline">\(\qquad y = X\beta + \epsilon\)</span>,</p>
<p>where <span class="math inline">\(\epsilon \sim N(0, \sigma^2 I_n)\)</span>. Some of the <span class="math inline">\(\beta_i\)</span> could be zero.</p>
<p>We want to select among different submodels when <span class="math inline">\(K &lt; n\)</span>, in which case all submodels can be estimated.</p>
<p>Let <span class="math inline">\(K_j\)</span> be the number of explanatory variables in a given regression (with some selection of <span class="math inline">\(K_j\)</span> regressors), and let <span class="math inline">\(\hat{\epsilon_j}\)</span> denote the vector of residuals. The unbiased estimator of the error variance is:</p>
<p><span class="math inline">\(\qquad s_{*j}^2 = \frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n - K_j}\)</span>.</p>
<p>This estimator makes a trade-off between goodness of fit and parsimony. Expanding around the average sum of squared residuals:</p>
<p><span class="math inline">\(\qquad s_{*j}^2 = \frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n - K_j} = \frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n} \cdot \frac{1}{1 - \frac{K_j}{n}} = \frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n} \left( 1 + \frac{K_j}{n} + ... \right)\)</span>.</p>
<p>The first term <span class="math inline">\(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}\)</span> measures goodness of fit. Minimizing <span class="math inline">\(s_{*j}^2\)</span> is equivalent to maximizing adjusted <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math inline">\(\qquad \bar{R_j}^2 = 1 - \frac{n-1}{n-K_j}(1 - R_j^2) = 1 - \frac{n-1}{n-K_j}\frac{\hat{\epsilon_j}^T\hat{\epsilon_j}}{\hat{e}^T\hat{e}}\)</span>.</p>
<p>Other model selection criteria: - <strong>Prediction Criterion:</strong> <span class="math inline">\(PC_j = \frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n-K_j}(1+\frac{K_j}{n})\)</span> - <strong>Akaike Information Criterion:</strong> <span class="math inline">\(AIC_j = \ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{2K_j}{n}\)</span> - <strong>Bayesian Information Criterion:</strong> <span class="math inline">\(BIC_j = \ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{K_j\ln(n)}{n}\)</span></p>
<p>These criteria allow model comparisons with different covariates. - All criteria have the property that the selected model is bigger or equal than true model with probability going to one. - Only <span class="math inline">\(BIC_j\)</span> correctly selects the true model with probability going to one (because it has the largest penalty).</p>
<section class="level3" id="theorem-21.1">
<h3 class="anchored" data-anchor-id="theorem-21.1">Theorem 21.1</h3>
<p><span class="math inline">\(\bar{R}^2\)</span> falls (rises) when the deleted variable has <span class="math inline">\(t &gt; (&lt;) 1\)</span>.</p>
</section>
<section class="level3" id="problems-and-issues">
<h3 class="anchored" data-anchor-id="problems-and-issues">21.3.1 Problems and Issues</h3>
<ol type="1">
<li><strong>Computational Issues:</strong> Searching over all <span class="math inline">\(2^K\)</span> regressions can be infeasible. You can start small and add variables, or start large and remove them.</li>
<li><strong>True model not in M:</strong> Even if the true model is not in M, the search is guaranteed to find the best in the model set M.</li>
<li><strong>Other criteria:</strong> Consistency with economic theory, consistency with the data (dependent variable in reasonable range, random residuals, out of sample performance)</li>
<li><strong>Data mining:</strong> White’s (2000) Reality Check Paper Consider regressions with 2 variables from a large set K. Total number of regressions is <span class="math inline">\(K(K-1)/2\)</span>. The best fitting regression will have high in-sample <span class="math inline">\(R^2\)</span> even if variables are all mutually independent.</li>
</ol>
</section>
</section>
<section class="level2" id="lasso">
<h2 class="anchored" data-anchor-id="lasso">21.4 LASSO</h2>
<p>The Lasso method (Tibshirani, 1996) is an alternative model selection procedure.</p>
<p>Suppose we have <span class="math inline">\(n\)</span> observations on candidate covariates <span class="math inline">\(X_1, X_2, \dots, X_K\)</span> and an outcome variable <span class="math inline">\(y\)</span>, where <span class="math inline">\(K &gt; n\)</span>.</p>
<p><strong>Ridge Regression Estimator:</strong></p>
<p><span class="math inline">\(\qquad \hat{\beta_R} = (X^T X + \lambda I_K)^{-1} X^T y\)</span>,</p>
<p>where <span class="math inline">\(\lambda &gt; 0\)</span>. This solution always exists because <span class="math inline">\(X^T X \geq 0\)</span> implies <span class="math inline">\(X^T X + \lambda I_K &gt; 0\)</span>.</p>
<p>The ridge regression is the solution to the penalized least squares criterion:</p>
<p><span class="math inline">\(\qquad \min_{b_1, b_2, \dots, b_K} \sum_{i=1}^n (y_i - b_1 - b_2 x_{2i} - b_3 x_{3i} - \dots - b_K x_{Ki})^2 + \lambda \sum_{j=1}^K b_j^2\)</span>.</p>
<p><strong>Sparsity Assumption:</strong></p>
<p>Assume that the true model is of much smaller dimension.</p>
<p><span class="math inline">\(\qquad E(Y|X_1, \dots, X_K) = \sum_{k=1}^{K_0} \beta_{jk} X_{jk}\)</span>,</p>
<p>for some <span class="math inline">\(K_0 \leq K\)</span>. Denote the active set by</p>
<p><span class="math inline">\(\qquad A = \{j : \beta_j \neq 0\}\)</span>,</p>
<p>and the inactive set by its complement in <span class="math inline">\(\{1, \dots, K\}\)</span>. The assumption that <span class="math inline">\(K_0\)</span> is much smaller than <span class="math inline">\(n\)</span> is called <strong>sparsity</strong>.</p>
<p>The Lasso method solves the following constrained minimization problem:</p>
<p><span class="math inline">\(\qquad \min_{b_1, b_2, \dots, b_K} \sum_{i=1}^n (y_i - b_1 - b_2 x_{2i} - \dots - b_K x_{Ki})^2 + \lambda \sum_{j=1}^K |b_j|\)</span>.</p>
<p>This is called <span class="math inline">\(L_1\)</span> penalty. The problem can be equivalently written as</p>
<p><span class="math inline">\(\qquad \min_{b_1, \dots, b_K} \sum_{i=1}^n (y_i - b_1 - b_2 x_{2i} - \dots - b_K x_{Ki})^2 \text{ s.t. } \sum_{j=1}^K |b_j| \leq s\)</span>.</p>
<ul>
<li><span class="math inline">\(s\)</span> is a turning parameter</li>
<li>If s is large enough the constraint does not matter, and it’s OLS.</li>
<li>If s is small, we get shrunken versions of the least squares estimates with several coefficients set to zero.</li>
</ul>
<p>Define the selected set: <span class="math inline">\(A =\{j : \beta_j \neq 0\}\)</span></p>
<p>It’s been shown under some conditions that: <span class="math inline">\(P(A = \hat{A}) \to 1\)</span> That is the Lasso selects the correct model with probability going to one.</p>
</section>
<section class="level2" id="exercises">
<h2 class="anchored" data-anchor-id="exercises">Exercises</h2>
<section class="level3" id="sec-ch21exercise1">
<h3 class="anchored" data-anchor-id="sec-ch21exercise1">Exercise 1</h3>
<p><a href="#sec-ch21solution1">Solution 1</a></p>
<p>Consider the model <span class="math inline">\(y_i = \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i\)</span>, where <span class="math inline">\(\epsilon_i\)</span> are i.i.d. with mean 0 and variance <span class="math inline">\(\sigma^2\)</span>. Suppose you estimate the model by regressing <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_{1i}\)</span> only. Derive the expression for the bias of the OLS estimator of <span class="math inline">\(\beta_1\)</span>. Under what condition is the estimator unbiased?</p>
</section>
<section class="level3" id="sec-ch21exercise2">
<h3 class="anchored" data-anchor-id="sec-ch21exercise2">Exercise 2</h3>
<p><a href="#sec-ch21solution2">Solution 2</a></p>
<p>Consider the model in Exercise 1. Assuming homoskedasticity, derive the variance of the OLS estimator of <span class="math inline">\(\beta_1\)</span> when you regress <span class="math inline">\(y_i\)</span> only on <span class="math inline">\(x_{1i}\)</span>. Compare this variance to the variance of the OLS estimator of <span class="math inline">\(\beta_1\)</span> when you regress <span class="math inline">\(y_i\)</span> on both <span class="math inline">\(x_{1i}\)</span> and <span class="math inline">\(x_{2i}\)</span>. Which variance is smaller? Explain the intuition.</p>
</section>
<section class="level3" id="sec-ch21exercise3">
<h3 class="anchored" data-anchor-id="sec-ch21exercise3">Exercise 3</h3>
<p><a href="#sec-ch21solution3">Solution 3</a></p>
<p>Consider the model in Exercise 1. Suppose you regress <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_{1i}\)</span> only. Show that the usual OLS estimator of the error variance is biased upwards.</p>
</section>
<section class="level3" id="sec-ch21exercise4">
<h3 class="anchored" data-anchor-id="sec-ch21exercise4">Exercise 4</h3>
<p><a href="#sec-ch21solution4">Solution 4</a></p>
<p>True or False: When relevant variables are omitted, the t-ratio for the included variables is always downward biased. Explain.</p>
</section>
<section class="level3" id="sec-ch21exercise5">
<h3 class="anchored" data-anchor-id="sec-ch21exercise5">Exercise 5</h3>
<p><a href="#sec-ch21solution5">Solution 5</a></p>
<p>Consider the true model <span class="math inline">\(y_i = \alpha + \beta x_i + \gamma x_i^2 + \epsilon_i\)</span>. Suppose you estimate the model by regressing <span class="math inline">\(y_i\)</span> on a constant and <span class="math inline">\(x_i\)</span>, ignoring the quadratic term. Show that if <span class="math inline">\(\bar{x} = 0\)</span>, the bias of the OLS estimator of the slope coefficient depends on the skewness of <span class="math inline">\(x\)</span>.</p>
</section>
<section class="level3" id="sec-ch21exercise6">
<h3 class="anchored" data-anchor-id="sec-ch21exercise6">Exercise 6</h3>
<p><a href="#sec-ch21solution6">Solution 6</a></p>
<p>Consider the true model <span class="math inline">\(y = X_1 \beta_1 + \epsilon\)</span>. You estimate the model by regressing <span class="math inline">\(y\)</span> on <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>, where <span class="math inline">\(X_2\)</span> are irrelevant variables. Show that the OLS estimator of <span class="math inline">\(\beta_1\)</span> is unbiased.</p>
</section>
<section class="level3" id="sec-ch21exercise7">
<h3 class="anchored" data-anchor-id="sec-ch21exercise7">Exercise 7</h3>
<p><a href="#sec-ch21solution7">Solution 7</a></p>
<p>Consider the scenario in Exercise 6. Show that the variance of the OLS estimator of <span class="math inline">\(\beta_1\)</span> is larger when <span class="math inline">\(X_2\)</span> is included compared to when it is excluded.</p>
</section>
<section class="level3" id="sec-ch21exercise8">
<h3 class="anchored" data-anchor-id="sec-ch21exercise8">Exercise 8</h3>
<p><a href="#sec-ch21solution8">Solution 8</a></p>
<p>Explain the bias-variance trade-off in the context of model selection.</p>
</section>
<section class="level3" id="sec-ch21exercise9">
<h3 class="anchored" data-anchor-id="sec-ch21exercise9">Exercise 9</h3>
<p><a href="#sec-ch21solution9">Solution 9</a></p>
<p>Explain how the adjusted <span class="math inline">\(R^2\)</span> is related to the unbiased estimator of the error variance in a regression model.</p>
</section>
<section class="level3" id="sec-ch21exercise10">
<h3 class="anchored" data-anchor-id="sec-ch21exercise10">Exercise 10</h3>
<p><a href="#sec-ch21solution10">Solution 10</a></p>
<p>What is the key difference between the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC) in terms of model selection?</p>
</section>
<section class="level3" id="sec-ch21exercise11">
<h3 class="anchored" data-anchor-id="sec-ch21exercise11">Exercise 11</h3>
<p><a href="#sec-ch21solution11">Solution 11</a></p>
<p>Among the model selection criteria discussed in the text (adjusted <span class="math inline">\(R^2\)</span>, PC, AIC, BIC), which one is guaranteed to select the true model with probability tending to one as the sample size grows?</p>
</section>
<section class="level3" id="sec-ch21exercise12">
<h3 class="anchored" data-anchor-id="sec-ch21exercise12">Exercise 12</h3>
<p><a href="#sec-ch21solution12">Solution 12</a></p>
<p>What does Theorem 21.1 in the text state about the relationship between the adjusted <span class="math inline">\(R^2\)</span> and the t-statistic of a deleted variable?</p>
</section>
<section class="level3" id="sec-ch21exercise13">
<h3 class="anchored" data-anchor-id="sec-ch21exercise13">Exercise 13</h3>
<p><a href="#sec-ch21solution13">Solution 13</a></p>
<p>What does “sparsity” mean in the context of the LASSO method?</p>
</section>
<section class="level3" id="sec-ch21exercise14">
<h3 class="anchored" data-anchor-id="sec-ch21exercise14">Exercise 14</h3>
<p><a href="#sec-ch21solution14">Solution 14</a></p>
<p>Write down the objective function that the LASSO method minimizes. Explain the role of the tuning parameter <span class="math inline">\(\lambda\)</span>.</p>
</section>
<section class="level3" id="sec-ch21exercise15">
<h3 class="anchored" data-anchor-id="sec-ch21exercise15">Exercise 15</h3>
<p><a href="#sec-ch21solution15">Solution 15</a></p>
<p>How does the LASSO method achieve variable selection?</p>
</section>
<section class="level3" id="sec-ch21exercise16">
<h3 class="anchored" data-anchor-id="sec-ch21exercise16">Exercise 16</h3>
<p><a href="#sec-ch21solution16">Solution 16</a></p>
<p>What is the main difference between the penalty terms in Ridge regression and LASSO?</p>
</section>
<section class="level3" id="sec-ch21exercise17">
<h3 class="anchored" data-anchor-id="sec-ch21exercise17">Exercise 17</h3>
<p><a href="#sec-ch21solution17">Solution 17</a></p>
<p>Explain the concept of “active set” in the context of the LASSO method.</p>
</section>
<section class="level3" id="sec-ch21exercise18">
<h3 class="anchored" data-anchor-id="sec-ch21exercise18">Exercise 18</h3>
<p><a href="#sec-ch21solution18">Solution 18</a></p>
<p>Under what condition does the LASSO select the correct model with probability tending to one?</p>
</section>
<section class="level3" id="sec-ch21exercise19">
<h3 class="anchored" data-anchor-id="sec-ch21exercise19">Exercise 19</h3>
<p><a href="#sec-ch21solution19">Solution 19</a></p>
<p>If the bound <em>s</em> in the alternative formulation of the LASSO problem is very large, what does the LASSO solution converge to?</p>
</section>
<section class="level3" id="sec-ch21exercise20">
<h3 class="anchored" data-anchor-id="sec-ch21exercise20">Exercise 20</h3>
<p><a href="#sec-ch21solution20">Solution 20</a> How is cross-validation used in the context of the LASSO?</p>
</section>
</section>
<section class="level2" id="solutions">
<h2 class="anchored" data-anchor-id="solutions">Solutions</h2>
<section class="level3" id="sec-ch21solution1">
<h3 class="anchored" data-anchor-id="sec-ch21solution1">Solution 1</h3>
<p><a href="#sec-ch21exercise1">Exercise 1</a></p>
<p>The true model is:</p>
<p><span class="math inline">\(\qquad y_i = \beta_1 x_{1i} + \beta_2 x_{2i} + \epsilon_i\)</span>.</p>
<p>The misspecified model regresses <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_{1i}\)</span> only:</p>
<p><span class="math inline">\(\qquad y_i = \beta_1 x_{1i} + v_i\)</span>,</p>
<p>where <span class="math inline">\(v_i = \beta_2 x_{2i} + \epsilon_i\)</span>.</p>
<p>The OLS estimator of <span class="math inline">\(\beta_1\)</span> in the misspecified model is:</p>
<p><span class="math inline">\(\qquad \tilde{\beta_1} = (X_1^T X_1)^{-1} X_1^T y\)</span>.</p>
<p>Substituting the true model for <span class="math inline">\(y\)</span>:</p>
<p><span class="math inline">\(\qquad \begin{aligned}
\tilde{\beta_1} &amp;= (X_1^T X_1)^{-1} X_1^T (\beta_1 X_1 + \beta_2 X_2 + \epsilon) \\
&amp;= (X_1^T X_1)^{-1} X_1^T X_1 \beta_1 + (X_1^T X_1)^{-1} X_1^T X_2 \beta_2 + (X_1^T X_1)^{-1} X_1^T \epsilon \\
&amp;= \beta_1 + (X_1^T X_1)^{-1} X_1^T X_2 \beta_2 + (X_1^T X_1)^{-1} X_1^T \epsilon.
\end{aligned}\)</span></p>
<p>Taking expectations:</p>
<p><span class="math inline">\(\qquad E(\tilde{\beta_1}) = \beta_1 + (X_1^T X_1)^{-1} X_1^T X_2 \beta_2\)</span>.</p>
<p>The <strong>bias</strong> is <span class="math inline">\(E(\tilde{\beta_1}) - \beta_1 = (X_1^T X_1)^{-1} X_1^T X_2 \beta_2\)</span>.</p>
<p>The estimator is unbiased if <span class="math inline">\((X_1^T X_1)^{-1} X_1^T X_2 \beta_2 = 0\)</span>. This occurs if <span class="math inline">\(X_1^T X_2 = 0\)</span> (i.e., <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are orthogonal) or if <span class="math inline">\(\beta_2 = 0\)</span> (i.e., <span class="math inline">\(X_2\)</span> does not belong in the true model).</p>
</section>
<section class="level3" id="sec-ch21solution2">
<h3 class="anchored" data-anchor-id="sec-ch21solution2">Solution 2</h3>
<p><a href="#sec-ch21exercise2">Exercise 2</a></p>
<p><strong>Misspecified model (regressing <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_{1i}\)</span> only):</strong></p>
<p><span class="math inline">\(\qquad \tilde{\beta_1} = (X_1^T X_1)^{-1} X_1^T y\)</span>.</p>
<p>Under homoskedasticity, <span class="math inline">\(Var(\epsilon|X) = \sigma^2 I\)</span>. Since <span class="math inline">\(v_i = \beta_2 x_{2i} + \epsilon_i\)</span>, <span class="math inline">\(Var(v|X) = \sigma^2 I\)</span> assuming <span class="math inline">\(X_2\)</span> is non-stochastic or conditioning on <span class="math inline">\(X_2\)</span></p>
<p><span class="math inline">\(\qquad \begin{aligned}
Var(\tilde{\beta_1}|X) &amp;= Var((X_1^T X_1)^{-1} X_1^T y | X) \\
&amp;= (X_1^T X_1)^{-1} X_1^T Var(y|X) X_1 (X_1^T X_1)^{-1} \\
&amp;= (X_1^T X_1)^{-1} X_1^T Var(\beta_2 X_2 + \epsilon|X) X_1 (X_1^T X_1)^{-1} \\
&amp;= \sigma^2 (X_1^T X_1)^{-1}.
\end{aligned}\)</span></p>
<p><strong>Correct model (regressing <span class="math inline">\(y_i\)</span> on <span class="math inline">\(x_{1i}\)</span> and <span class="math inline">\(x_{2i}\)</span>):</strong></p>
<p><span class="math inline">\(\qquad \hat{\beta} = (X^T X)^{-1} X^T y\)</span>, where <span class="math inline">\(X = [X_1, X_2]\)</span>.</p>
<p><span class="math inline">\(\qquad Var(\hat{\beta}|X) = \sigma^2 (X^T X)^{-1}\)</span>. Partitioning: <span class="math inline">\(Var(\hat{\beta_1}|X) = \sigma^2(X_1^TM_2X_1)^{-1}\)</span></p>
<p>where <span class="math inline">\(M_2 = I - X_2(X_2^TX_2)^{-1}X_2^T\)</span></p>
<p><strong>Comparison:</strong></p>
<p>We need to compare <span class="math inline">\(\sigma^2 (X_1^T X_1)^{-1}\)</span> and <span class="math inline">\(\sigma^2 (X_1^T M_2 X_1)^{-1}\)</span>.</p>
<p>Since <span class="math inline">\(M_2\)</span> is idempotent and symmetric, we can write: <span class="math inline">\(X_1^T X_1 = X_1^T M_2 X_1 + X_1^T P_2 X_1\)</span>, where <span class="math inline">\(P_2=X_2(X_2^TX_2)X_2^T\)</span></p>
<p>Because <span class="math inline">\(X_1^T P_2 X_1\)</span> is positive semi-definite, <span class="math inline">\((X_1^T X_1)^{-1} - (X_1^T M_2 X_1)^{-1} \leq 0\)</span> by properties of positive definite matrices.</p>
<p>Therefore, <span class="math inline">\(Var(\tilde{\beta_1}|X) \geq Var(\hat{\beta_1}|X)\)</span>. The variance is smaller when the correct model (including <span class="math inline">\(X_2\)</span>) is used.</p>
<p><strong>Intuition:</strong> Including the relevant variable <span class="math inline">\(X_2\)</span> reduces the noise in the estimation of <span class="math inline">\(\beta_1\)</span>, leading to a more precise (lower variance) estimator.</p>
</section>
<section class="level3" id="sec-ch21solution3">
<h3 class="anchored" data-anchor-id="sec-ch21solution3">Solution 3</h3>
<p><a href="#sec-ch21exercise3">Exercise 3</a></p>
<p>The misspecified model is <span class="math inline">\(y_i = \beta_1 x_{1i} + v_i\)</span>, where <span class="math inline">\(v_i = \beta_2 x_{2i} + \epsilon_i\)</span>. The OLS estimator of the error variance is:</p>
<p><span class="math inline">\(\qquad s_u^2 = \frac{\tilde{\epsilon}^T \tilde{\epsilon}}{n - 1} = \frac{y^T M_1 y}{n-1}\)</span>, where <span class="math inline">\(M_1 = I - X_1(X_1^T X_1)^{-1}X_1^T\)</span> Substituting the true model for <span class="math inline">\(y\)</span>: <span class="math inline">\(s_u^2= \frac{(\beta_2X_2 + \epsilon)^T M_1(\beta_2X_2 + \epsilon)}{n-1}\)</span></p>
<p>Expanding the numerator and using <span class="math inline">\(M_1X_1=0\)</span>:</p>
<p><span class="math inline">\(\qquad \begin{aligned}
s_u^2 &amp;= \frac{\beta_2^T X_2^T M_1 X_2 \beta_2 + 2 \beta_2^T X_2^T M_1 \epsilon + \epsilon^T M_1 \epsilon}{n - 1}.
\end{aligned}\)</span> Taking expectations, and noting that <span class="math inline">\(E[\epsilon|X]=0\)</span> and <span class="math inline">\(E[\epsilon^T M_1 \epsilon|X]=\sigma^2 tr(M_1) = \sigma^2(n-1)\)</span></p>
<p><span class="math inline">\(\qquad E[s_u^2|X] = \frac{\beta_2^T X_2^T M_1 X_2 \beta_2 + \sigma^2(n - 1)}{n - 1} = \sigma^2 + \frac{\beta_2^T X_2^T M_1 X_2 \beta_2}{n - 1}\)</span>.</p>
<p>Since <span class="math inline">\(M_1\)</span> is positive semi-definite, <span class="math inline">\(\beta_2^T X_2^T M_1 X_2 \beta_2 \ge 0\)</span>. Thus, <span class="math inline">\(E[s_u^2|X] \ge \sigma^2\)</span>.</p>
<p>The usual OLS estimator of the error variance is biased upwards.</p>
</section>
<section class="level3" id="sec-ch21solution4">
<h3 class="anchored" data-anchor-id="sec-ch21solution4">Solution 4</h3>
<p><a href="#sec-ch21exercise4">Exercise 4</a></p>
<p>False. The t-ratio can be either upward or downward biased. The direction of the bias in the t-ratio depends on the direction of the bias in the coefficient estimate and the bias in the estimated standard error. As shown in the text and in Solution 3, the estimated variance of the coefficient is upward biased. However, the coefficient estimate itself can be either upward or downward biased, depending on the correlation between the included and omitted variables, and the sign of the coefficient on the omitted variable.</p>
</section>
<section class="level3" id="sec-ch21solution5">
<h3 class="anchored" data-anchor-id="sec-ch21solution5">Solution 5</h3>
<p><a href="#sec-ch21exercise5">Exercise 5</a></p>
<p>The true model is:</p>
<p><span class="math inline">\(\qquad y_i = \alpha + \beta x_i + \gamma x_i^2 + \epsilon_i\)</span>.</p>
<p>The misspecified model is:</p>
<p><span class="math inline">\(\qquad y_i = \alpha + \beta x_i + v_i\)</span>, where <span class="math inline">\(v_i = \gamma x_i^2 + \epsilon_i\)</span>.</p>
<p>The OLS estimator for <span class="math inline">\(\beta\)</span> is:</p>
<p><span class="math inline">\(\qquad \tilde{\beta} = \frac{\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})}{\sum_{i=1}^n (x_i - \bar{x})^2}\)</span>.</p>
<p>Substituting the true model for <span class="math inline">\(y_i\)</span> and simplifying:</p>
<p><span class="math inline">\(\qquad \tilde{\beta} = \beta + \gamma \frac{\sum_{i=1}^n (x_i - \bar{x})x_i^2}{\sum_{i=1}^n (x_i - \bar{x})^2} +  \frac{\sum_{i=1}^n (x_i-\bar{x})\epsilon_i}{\sum_{i=1}^n (x_i-\bar{x})^2}\)</span>.</p>
<p>Taking expectations:</p>
<p><span class="math inline">\(\qquad E[\tilde{\beta}] = \beta + \gamma \frac{\sum_{i=1}^n (x_i - \bar{x})x_i^2}{\sum_{i=1}^n (x_i - \bar{x})^2}\)</span>.</p>
<p>If <span class="math inline">\(\bar{x} = 0\)</span>, then</p>
<p><span class="math inline">\(\qquad E[\tilde{\beta}] = \beta + \gamma \frac{\sum_{i=1}^n x_i^3}{\sum_{i=1}^n x_i^2}\)</span>.</p>
<p>The term <span class="math inline">\(\frac{\sum_{i=1}^n x_i^3}{n}\)</span> is a measure of the skewness of <span class="math inline">\(x\)</span>. Thus, the bias depends on the skewness of <span class="math inline">\(x\)</span> and the value of <span class="math inline">\(\gamma\)</span>.</p>
</section>
<section class="level3" id="sec-ch21solution6">
<h3 class="anchored" data-anchor-id="sec-ch21solution6">Solution 6</h3>
<p><a href="#sec-ch21exercise6">Exercise 6</a></p>
<p>True model: <span class="math inline">\(y = X_1 \beta_1 + \epsilon\)</span>.</p>
<p>Misspecified model: <span class="math inline">\(y = X_1 \beta_1 + X_2 \beta_2 + \epsilon\)</span>. Since <span class="math inline">\(X_2\)</span> are irrelevant, <span class="math inline">\(\beta_2=0\)</span> in population.</p>
<p>The OLS estimator of <span class="math inline">\(\beta = [\beta_1, \beta_2]\)</span> is: <span class="math inline">\(\tilde{\beta} = (X^TX)^{-1}X^Ty\)</span> with <span class="math inline">\(X = [X_1 \vdots X_2]\)</span></p>
<p>We can partition the estimator as follows:</p>
<p><span class="math inline">\(\qquad \begin{bmatrix} \tilde{\beta_1} \\ \tilde{\beta_2} \end{bmatrix} = \begin{bmatrix} X_1^TX_1 &amp; X_1^TX_2 \\ X_2^TX_1 &amp; X_2^TX_2 \end{bmatrix} ^{-1}\begin{bmatrix} X_1^T \\ X_2^T \end{bmatrix}y\)</span></p>
<p>Using the formula for the inverse of partitioned matrix and simplifying:</p>
<p><span class="math inline">\(\tilde{\beta_1}=(X_1^TM_2X_1)^{-1}X_1^TM_2y\)</span> where <span class="math inline">\(M_2 = I - X_2(X_2^TX_2)^{-1}X_2^T\)</span> Substitute for <span class="math inline">\(y=X_1\beta_1 + \epsilon\)</span> <span class="math inline">\(\tilde{\beta_1}=(X_1^TM_2X_1)^{-1}X_1^TM_2(X_1\beta_1 + \epsilon) = \beta_1 + (X_1^TM_2X_1)^{-1}X_1^TM_2\epsilon\)</span></p>
<p>Taking expectations:</p>
<p><span class="math inline">\(\qquad E[\tilde{\beta_1}|X] = \beta_1\)</span>.</p>
<p>The OLS estimator of <span class="math inline">\(\beta_1\)</span> is unbiased, even when irrelevant variables are included.</p>
</section>
<section class="level3" id="sec-ch21solution7">
<h3 class="anchored" data-anchor-id="sec-ch21solution7">Solution 7</h3>
<p><a href="#sec-ch21exercise7">Exercise 7</a></p>
<p>From Solution 6, the estimator of <span class="math inline">\(\beta_1\)</span> when <span class="math inline">\(X_2\)</span> is included is:</p>
<p><span class="math inline">\(\qquad \tilde{\beta_1} = (X_1^T M_2 X_1)^{-1} X_1^T M_2 y\)</span>,</p>
<p>where <span class="math inline">\(M_2 = I - X_2(X_2^T X_2)^{-1} X_2^T\)</span>.</p>
<p>The variance is:</p>
<p><span class="math inline">\(\qquad \begin{aligned}
Var(\tilde{\beta_1}|X) &amp;= (X_1^T M_2 X_1)^{-1} X_1^T M_2 Var(y|X) M_2 X_1 (X_1^T M_2 X_1)^{-1} \\
&amp;= \sigma^2 (X_1^T M_2 X_1)^{-1}.
\end{aligned}\)</span></p>
<p>When <span class="math inline">\(X_2\)</span> is excluded, the estimator of <span class="math inline">\(\beta_1\)</span> is:</p>
<p><span class="math inline">\(\qquad \hat{\beta_1} = (X_1^T X_1)^{-1} X_1^T y\)</span>.</p>
<p>The variance is:</p>
<p><span class="math inline">\(\qquad Var(\hat{\beta_1}|X) = \sigma^2 (X_1^T X_1)^{-1}\)</span>.</p>
<p>Since <span class="math inline">\(X_1^T X_1 - X_1^T M_2 X_1 = X_1^T P_2 X_1 \ge 0\)</span>,</p>
<p><span class="math inline">\(\qquad (X_1^T X_1)^{-1} \le (X_1^T M_2 X_1)^{-1}\)</span>.</p>
<p>Therefore, <span class="math inline">\(Var(\hat{\beta_1}|X) \le Var(\tilde{\beta_1}|X)\)</span>. The variance is larger when irrelevant variables are included.</p>
</section>
<section class="level3" id="sec-ch21solution8">
<h3 class="anchored" data-anchor-id="sec-ch21solution8">Solution 8</h3>
<p><a href="#sec-ch21exercise8">Exercise 8</a></p>
<p>The <strong>bias-variance trade-off</strong> refers to the relationship between the bias and variance of an estimator in different model specifications.</p>
<ul>
<li><strong>Large Model (more variables):</strong> Tends to have <em>low bias</em> because it is more likely to include all relevant variables. However, it has <em>high variance</em> because it includes irrelevant variables, adding noise to the estimation.</li>
<li><strong>Small Model (fewer variables):</strong> Tends to have <em>low variance</em> because it excludes irrelevant variables. However, it has <em>high bias</em> because it may omit relevant variables.</li>
</ul>
<p>Model selection involves finding a balance between these two extremes.</p>
</section>
<section class="level3" id="sec-ch21solution9">
<h3 class="anchored" data-anchor-id="sec-ch21solution9">Solution 9</h3>
<p><a href="#sec-ch21exercise9">Exercise 9</a></p>
<p>The unbiased estimator of the error variance is:</p>
<p><span class="math inline">\(\qquad s_{*j}^2 = \frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n - K_j}\)</span>.</p>
<p>The adjusted <span class="math inline">\(R^2\)</span> is:</p>
<p><span class="math inline">\(\qquad \bar{R_j}^2 = 1 - \frac{n-1}{n-K_j}(1 - R_j^2) = 1 - \frac{n-1}{n-K_j} \frac{ESS}{TSS} = 1 - \frac{\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n - K_j}}{\frac{\sum(y_i-\bar{y})^2}{n-1}}\)</span>.</p>
<p>Where ESS is the error sum of squares, TSS is the total sum of squares, and <span class="math inline">\(K_j\)</span> is the number of regressors in model <span class="math inline">\(j\)</span>. The adjusted <span class="math inline">\(R^2\)</span> can be written in terms of the unbiased error of variance (<span class="math inline">\(s_{*j}^2\)</span>):</p>
<p><span class="math inline">\(\qquad \bar{R_j}^2 = 1 - \frac{s_{*j}^2}{\frac{\sum(y_i-\bar{y})^2}{n-1}}\)</span>.</p>
<p>Maximizing <span class="math inline">\(\bar{R_j}^2\)</span> is equivalent to minimizing <span class="math inline">\(s_{*j}^2\)</span>. The adjusted <span class="math inline">\(R^2\)</span> penalizes the inclusion of additional variables by dividing the sum of squared residuals by <span class="math inline">\((n - K_j)\)</span> instead of <span class="math inline">\(n\)</span>. This penalty reflects the loss of degrees of freedom.</p>
</section>
<section class="level3" id="sec-ch21solution10">
<h3 class="anchored" data-anchor-id="sec-ch21solution10">Solution 10</h3>
<p><a href="#sec-ch21exercise10">Exercise 10</a></p>
<p>AIC and BIC both penalize model complexity, but BIC imposes a larger penalty for additional parameters.</p>
<ul>
<li><strong>AIC:</strong> <span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{2K_j}{n}\)</span>. Penalty term: <span class="math inline">\(\frac{2K_j}{n}\)</span>.</li>
<li><strong>BIC:</strong> <span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{K_j \ln(n)}{n}\)</span>. Penalty term: <span class="math inline">\(\frac{K_j \ln(n)}{n}\)</span>.</li>
</ul>
<p>Since <span class="math inline">\(\ln(n) &gt; 2\)</span> for <span class="math inline">\(n &gt; 7\)</span>, BIC penalizes additional parameters more heavily than AIC. This means that BIC tends to favor simpler models compared to AIC.</p>
</section>
<section class="level3" id="sec-ch21solution11">
<h3 class="anchored" data-anchor-id="sec-ch21solution11">Solution 11</h3>
<p><a href="#sec-ch21exercise11">Exercise 11</a></p>
<p>The <strong>Bayesian Information Criterion (BIC)</strong> is guaranteed to select the true model with probability tending to one as the sample size grows. This is because it has the largest penalty term among the mentioned criteria.</p>
</section>
<section class="level3" id="sec-ch21solution12">
<h3 class="anchored" data-anchor-id="sec-ch21solution12">Solution 12</h3>
<p><a href="#sec-ch21exercise12">Exercise 12</a></p>
<p>Theorem 21.1 states that the adjusted <span class="math inline">\(R^2\)</span> (<span class="math inline">\(\bar{R}^2\)</span>) <em>falls</em> when the deleted variable has a t-statistic with absolute value <em>greater</em> than 1, and <span class="math inline">\(\bar{R}^2\)</span> <em>rises</em> when the deleted variable has a t-statistic with absolute value <em>less</em> than 1. In other words, <span class="math inline">\(\bar{R}^2\)</span> increases if and only if the t-statistic of the added (or deleted) variable is less than 1 in absolute value.</p>
</section>
<section class="level3" id="sec-ch21solution13">
<h3 class="anchored" data-anchor-id="sec-ch21solution13">Solution 13</h3>
<p><a href="#sec-ch21exercise13">Exercise 13</a></p>
<p><strong>Sparsity</strong> means that the true model has a relatively small number of non-zero coefficients compared to the total number of potential predictors. In other words, only a few of the available covariates actually affect the dependent variable; most have coefficients of zero.</p>
</section>
<section class="level3" id="sec-ch21solution14">
<h3 class="anchored" data-anchor-id="sec-ch21solution14">Solution 14</h3>
<p><a href="#sec-ch21exercise14">Exercise 14</a></p>
<p>The LASSO minimizes the following objective function:</p>
<p><span class="math inline">\(\qquad \min_{b_1, b_2, \dots, b_K} \sum_{i=1}^n (y_i - b_1 - b_2 x_{2i} - \dots - b_K x_{Ki})^2 + \lambda \sum_{j=1}^K |b_j|\)</span>.</p>
<p>The first part is the residual sum of squares, measuring the goodness of fit. The second part is the <span class="math inline">\(L_1\)</span> penalty, which is the sum of the absolute values of the coefficients, multiplied by the tuning parameter <span class="math inline">\(\lambda\)</span>.</p>
<ul>
<li><span class="math inline">\(\lambda\)</span> controls the strength of the penalty.
<ul>
<li>If <span class="math inline">\(\lambda = 0\)</span>, there is no penalty, and LASSO reduces to OLS.</li>
<li>As <span class="math inline">\(\lambda\)</span> increases, the penalty becomes stronger, forcing more coefficients to be exactly zero. This leads to variable selection.</li>
</ul></li>
</ul>
</section>
<section class="level3" id="sec-ch21solution15">
<h3 class="anchored" data-anchor-id="sec-ch21solution15">Solution 15</h3>
<p><a href="#sec-ch21exercise15">Exercise 15</a></p>
<p>The LASSO achieves variable selection through the <span class="math inline">\(L_1\)</span> penalty, <span class="math inline">\(\lambda \sum_{j=1}^K |b_j|\)</span>. Because of the absolute value in the penalty, the optimization problem has a non-differentiable point at zero for each coefficient. This encourages some coefficients to be <em>exactly</em> zero, effectively removing the corresponding variables from the model. This is in contrast to Ridge regression, which uses an <span class="math inline">\(L_2\)</span> penalty and shrinks coefficients towards zero but rarely sets them exactly to zero.</p>
</section>
<section class="level3" id="sec-ch21solution16">
<h3 class="anchored" data-anchor-id="sec-ch21solution16">Solution 16</h3>
<p><a href="#sec-ch21exercise16">Exercise 16</a></p>
<ul>
<li><strong>Ridge Regression:</strong> Uses an <span class="math inline">\(L_2\)</span> penalty: <span class="math inline">\(\lambda \sum_{j=1}^K b_j^2\)</span>. This penalty shrinks coefficients towards zero but generally does not set them exactly to zero.</li>
<li><strong>LASSO:</strong> Uses an <span class="math inline">\(L_1\)</span> penalty: <span class="math inline">\(\lambda \sum_{j=1}^K |b_j|\)</span>. This penalty encourages sparsity, setting some coefficients to exactly zero.</li>
</ul>
<p>The key difference is the use of the absolute value (<span class="math inline">\(L_1\)</span>) versus the square (<span class="math inline">\(L_2\)</span>) of the coefficients in the penalty term.</p>
</section>
<section class="level3" id="sec-ch21solution17">
<h3 class="anchored" data-anchor-id="sec-ch21solution17">Solution 17</h3>
<p><a href="#sec-ch21exercise17">Exercise 17</a></p>
<p>The <strong>active set</strong> in the context of the LASSO is the set of variables with non-zero coefficients in the LASSO solution. It represents the variables that the LASSO has selected as being important for predicting the outcome variable. Formally, <span class="math inline">\(A = \{j : \beta_j \neq 0\}\)</span>.</p>
</section>
<section class="level3" id="sec-ch21solution18">
<h3 class="anchored" data-anchor-id="sec-ch21solution18">Solution 18</h3>
<p><a href="#sec-ch21exercise18">Exercise 18</a></p>
<p>Under certain conditions (which are beyond the scope of the provided text but generally involve restrictions on the correlation between predictors and the signal strength of the true coefficients), the LASSO selects the correct model with probability tending to one as the sample size grows. That is, <span class="math inline">\(\Pr(\hat{A} = A) \to 1\)</span>, where <span class="math inline">\(\hat{A}\)</span> is the estimated active set, and <span class="math inline">\(A\)</span> is the true active set.</p>
</section>
<section class="level3" id="sec-ch21solution19">
<h3 class="anchored" data-anchor-id="sec-ch21solution19">Solution 19</h3>
<p><a href="#sec-ch21exercise19">Exercise 19</a></p>
<p>If the bound <span class="math inline">\(s\)</span> in the alternative formulation of the LASSO problem is very large, the constraint <span class="math inline">\(\sum_{j=1}^K |b_j| \leq s\)</span> becomes non-binding. In this case, the LASSO solution converges to the ordinary least squares (OLS) solution. This is because the constraint is no longer restricting the magnitude of the coefficients.</p>
</section>
<section class="level3" id="sec-ch21solution20">
<h3 class="anchored" data-anchor-id="sec-ch21solution20">Solution 20</h3>
<p><a href="#sec-ch21exercise20">Exercise 20</a></p>
<p><strong>Cross-validation</strong> is used in LASSO to select the optimal value of the tuning parameter <span class="math inline">\(\lambda\)</span> (or equivalently, the bound <span class="math inline">\(s\)</span>). The data is split into multiple folds. For each value of <span class="math inline">\(\lambda\)</span> in a grid, the LASSO is trained on a subset of the folds and validated on the remaining fold. The value of <span class="math inline">\(\lambda\)</span> that minimizes the average cross-validation error (e.g., mean squared error) is chosen as the optimal value. This helps prevent overfitting and provides a more reliable estimate of the model’s performance on unseen data.</p>
</section>
</section>
<section class="level2" id="r-scripts">
<h2 class="anchored" data-anchor-id="r-scripts">R Scripts</h2>
<section class="level3" id="r-script-1-omitted-variable-bias">
<h3 class="anchored" data-anchor-id="r-script-1-omitted-variable-bias">R Script 1: Omitted Variable Bias</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors</code></pre>
</div>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="fu">library</span>(MASS) <span class="co"># For multivariate normal simulation</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'MASS'

The following object is masked from 'package:dplyr':

    select</code></pre>
</div>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb5-1"><a aria-hidden="true" href="#cb5-1" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb5-2"><a aria-hidden="true" href="#cb5-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb5-3"><a aria-hidden="true" href="#cb5-3" tabindex="-1"></a></span>
<span id="cb5-4"><a aria-hidden="true" href="#cb5-4" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb5-5"><a aria-hidden="true" href="#cb5-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span>  <span class="co"># Sample size</span></span>
<span id="cb5-6"><a aria-hidden="true" href="#cb5-6" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">2</span> <span class="co"># True coefficient for x1</span></span>
<span id="cb5-7"><a aria-hidden="true" href="#cb5-7" tabindex="-1"></a>beta2 <span class="ot">&lt;-</span> <span class="dv">3</span> <span class="co"># True coefficient for x2</span></span>
<span id="cb5-8"><a aria-hidden="true" href="#cb5-8" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="co"># Error standard deviation</span></span>
<span id="cb5-9"><a aria-hidden="true" href="#cb5-9" tabindex="-1"></a></span>
<span id="cb5-10"><a aria-hidden="true" href="#cb5-10" tabindex="-1"></a><span class="co"># Generate correlated x1 and x2</span></span>
<span id="cb5-11"><a aria-hidden="true" href="#cb5-11" tabindex="-1"></a>mu <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">0</span>)</span>
<span id="cb5-12"><a aria-hidden="true" href="#cb5-12" tabindex="-1"></a>Sigma <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">c</span>(<span class="dv">1</span>, <span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="dv">1</span>), <span class="at">nrow =</span> <span class="dv">2</span>) <span class="co"># Covariance matrix with correlation 0.5</span></span>
<span id="cb5-13"><a aria-hidden="true" href="#cb5-13" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">mvrnorm</span>(n, mu, Sigma)</span>
<span id="cb5-14"><a aria-hidden="true" href="#cb5-14" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> X[, <span class="dv">1</span>]</span>
<span id="cb5-15"><a aria-hidden="true" href="#cb5-15" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> X[, <span class="dv">2</span>]</span>
<span id="cb5-16"><a aria-hidden="true" href="#cb5-16" tabindex="-1"></a></span>
<span id="cb5-17"><a aria-hidden="true" href="#cb5-17" tabindex="-1"></a><span class="co"># Generate the dependent variable y</span></span>
<span id="cb5-18"><a aria-hidden="true" href="#cb5-18" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb5-19"><a aria-hidden="true" href="#cb5-19" tabindex="-1"></a>y <span class="ot">&lt;-</span> beta1 <span class="sc">*</span> x1 <span class="sc">+</span> beta2 <span class="sc">*</span> x2 <span class="sc">+</span> epsilon</span>
<span id="cb5-20"><a aria-hidden="true" href="#cb5-20" tabindex="-1"></a></span>
<span id="cb5-21"><a aria-hidden="true" href="#cb5-21" tabindex="-1"></a><span class="co"># --- Model 1: Correctly specified model (includes x1 and x2) ---</span></span>
<span id="cb5-22"><a aria-hidden="true" href="#cb5-22" tabindex="-1"></a>df_correct <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x1, x2)</span>
<span id="cb5-23"><a aria-hidden="true" href="#cb5-23" tabindex="-1"></a>model_correct <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> df_correct)</span>
<span id="cb5-24"><a aria-hidden="true" href="#cb5-24" tabindex="-1"></a><span class="fu">summary</span>(model_correct) <span class="co"># Observe estimates close to beta1 and beta2</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x1 + x2, data = df_correct)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.8730 -0.6607 -0.1245  0.6214  2.0798 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.13507    0.09614   1.405    0.163    
x1           1.89930    0.11346  16.741   &lt;2e-16 ***
x2           2.94692    0.11857  24.853   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9513 on 97 degrees of freedom
Multiple R-squared:  0.9433,    Adjusted R-squared:  0.9421 
F-statistic: 806.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb7-1"><a aria-hidden="true" href="#cb7-1" tabindex="-1"></a><span class="co"># --- Model 2: Misspecified model (omits x2) ---</span></span>
<span id="cb7-2"><a aria-hidden="true" href="#cb7-2" tabindex="-1"></a>df_misspecified <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x1)</span>
<span id="cb7-3"><a aria-hidden="true" href="#cb7-3" tabindex="-1"></a>model_misspecified <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1, <span class="at">data =</span> df_misspecified)</span>
<span id="cb7-4"><a aria-hidden="true" href="#cb7-4" tabindex="-1"></a><span class="fu">summary</span>(model_misspecified) <span class="co"># Observe biased estimate of beta1</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x1, data = df_misspecified)

Residuals:
    Min      1Q  Median      3Q     Max 
-6.1108 -1.6878 -0.2221  1.6733  6.5244 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.03752    0.25941   0.145    0.885    
x1           3.18502    0.27268  11.680   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 2.569 on 98 degrees of freedom
Multiple R-squared:  0.582, Adjusted R-squared:  0.5777 
F-statistic: 136.4 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb9-1"><a aria-hidden="true" href="#cb9-1" tabindex="-1"></a><span class="co"># --- Visualization ---</span></span>
<span id="cb9-2"><a aria-hidden="true" href="#cb9-2" tabindex="-1"></a><span class="co"># Plot y vs x1, coloring by x2 to illustrate the omitted variable effect</span></span>
<span id="cb9-3"><a aria-hidden="true" href="#cb9-3" tabindex="-1"></a><span class="fu">ggplot</span>(df_correct, <span class="fu">aes</span>(<span class="at">x =</span> x1, <span class="at">y =</span> y, <span class="at">color =</span> x2)) <span class="sc">+</span></span>
<span id="cb9-4"><a aria-hidden="true" href="#cb9-4" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb9-5"><a aria-hidden="true" href="#cb9-5" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="at">method =</span> <span class="st">"lm"</span>, <span class="at">se =</span> <span class="cn">FALSE</span>, <span class="at">linetype =</span> <span class="st">"dashed"</span>) <span class="sc">+</span></span>
<span id="cb9-6"><a aria-hidden="true" href="#cb9-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Omitted Variable Bias Illustration"</span>,</span>
<span id="cb9-7"><a aria-hidden="true" href="#cb9-7" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"x1"</span>,</span>
<span id="cb9-8"><a aria-hidden="true" href="#cb9-8" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"y"</span>,</span>
<span id="cb9-9"><a aria-hidden="true" href="#cb9-9" tabindex="-1"></a>       <span class="at">color =</span> <span class="st">"x2"</span>) <span class="sc">+</span></span>
<span id="cb9-10"><a aria-hidden="true" href="#cb9-10" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>`geom_smooth()` using formula = 'y ~ x'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Warning: The following aesthetics were dropped during statistical transformation:
colour.
ℹ This can happen when ggplot fails to infer the correct grouping structure in
  the data.
ℹ Did you forget to specify a `group` aesthetic or to convert a numerical
  variable into a factor?</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap21_files/figure-html/unnamed-chunk-1-1.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb12-1"><a aria-hidden="true" href="#cb12-1" tabindex="-1"></a><span class="co"># --- Calculate the theoretical bias ---</span></span>
<span id="cb12-2"><a aria-hidden="true" href="#cb12-2" tabindex="-1"></a><span class="co"># bias = (X1'X1)^(-1) X1'X2 * beta2</span></span>
<span id="cb12-3"><a aria-hidden="true" href="#cb12-3" tabindex="-1"></a>X1 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(x1, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb12-4"><a aria-hidden="true" href="#cb12-4" tabindex="-1"></a>X2 <span class="ot">&lt;-</span> <span class="fu">matrix</span>(x2, <span class="at">ncol =</span> <span class="dv">1</span>)</span>
<span id="cb12-5"><a aria-hidden="true" href="#cb12-5" tabindex="-1"></a>theoretical_bias <span class="ot">&lt;-</span> <span class="fu">solve</span>(<span class="fu">t</span>(X1) <span class="sc">%*%</span> X1) <span class="sc">%*%</span> <span class="fu">t</span>(X1) <span class="sc">%*%</span> X2 <span class="sc">*</span> beta2</span>
<span id="cb12-6"><a aria-hidden="true" href="#cb12-6" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Theoretical Bias:"</span>, theoretical_bias))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Theoretical Bias: 1.29438426797482"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb14-1"><a aria-hidden="true" href="#cb14-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Estimated Coefficient of x1 (misspecified model):"</span>, <span class="fu">coef</span>(model_misspecified)[<span class="dv">2</span>]))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Estimated Coefficient of x1 (misspecified model): 3.18502020668086"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb16-1"><a aria-hidden="true" href="#cb16-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Estimated Coefficient of x1 (correct model) + Theoretical Bias:"</span> ,<span class="fu">coef</span>(model_correct)[<span class="dv">2</span>] <span class="sc">+</span> theoretical_bias ))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Estimated Coefficient of x1 (correct model) + Theoretical Bias: 3.19368636237153"</code></pre>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><strong>Setup:</strong>
<ul>
<li>Loads the <code>tidyverse</code> for data manipulation and visualization and <code>MASS</code> for generating multivariate normal data.</li>
<li>Sets a seed for reproducibility.</li>
<li>Defines simulation parameters: sample size (<code>n</code>), true coefficients (<code>beta1</code>, <code>beta2</code>), and error standard deviation (<code>sigma</code>).</li>
</ul></li>
<li><strong>Data Generation:</strong>
<ul>
<li>Creates correlated regressors <code>x1</code> and <code>x2</code> using <code>mvrnorm</code> from the <code>MASS</code> package. A correlation of 0.5 is introduced between <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>.</li>
<li>Generates the dependent variable <code>y</code> according to the true model: <span class="math inline">\(y = \beta_1 x_1 + \beta_2 x_2 + \epsilon\)</span>.</li>
</ul></li>
<li><strong>Model Estimation:</strong>
<ul>
<li><strong>Correct Model:</strong> Creates a data frame <code>df_correct</code> and estimates the correct model using <code>lm(y ~ x1 + x2, data = df_correct)</code>. The <code>summary()</code> function provides estimates of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span>, which should be close to their true values.</li>
<li><strong>Misspecified Model:</strong> Creates a data frame <code>df_misspecified</code> (including only <span class="math inline">\(x_1\)</span>) and estimates the misspecified model using <code>lm(y ~ x1, data = df_misspecified)</code>. The <code>summary()</code> function now provides a <em>biased</em> estimate of <span class="math inline">\(\beta_1\)</span>. This illustrates <strong>omitted variable bias</strong>, as discussed in Section 21.1 of the text.</li>
</ul></li>
<li><strong>Visualization:</strong>
<ul>
<li>Creates a scatter plot of <code>y</code> versus <code>x1</code>, with points colored by the value of <code>x2</code>. This visually demonstrates how the omitted variable <code>x2</code> affects the relationship between <code>y</code> and <code>x1</code>. The dashed line shows the fitted relationship from the misspecified model.</li>
</ul></li>
<li><strong>Theoretical Bias Calculation:</strong>
<ul>
<li>Calculates the theoretical bias using the formula: <span class="math inline">\(\text{bias} = (X_1^T X_1)^{-1} X_1^T X_2 \beta_2\)</span>.</li>
<li>Prints the theoretical bias.</li>
<li>Prints the estimated coefficient of x1 in misspecified model.</li>
<li>Prints the sum of the estimated coefficient for <span class="math inline">\(x_1\)</span> in the correct model and the theoretical bias, it should match closely the coefficient from the misspecified model. This concretely demonstrates the formula for omitted variable bias presented in the text.</li>
</ul></li>
</ol>
</section>
<section class="level3" id="r-script-2-inclusion-of-irrelevant-variables">
<h3 class="anchored" data-anchor-id="r-script-2-inclusion-of-irrelevant-variables">R Script 2: Inclusion of Irrelevant Variables</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb18-1"><a aria-hidden="true" href="#cb18-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb18-2"><a aria-hidden="true" href="#cb18-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb18-3"><a aria-hidden="true" href="#cb18-3" tabindex="-1"></a></span>
<span id="cb18-4"><a aria-hidden="true" href="#cb18-4" tabindex="-1"></a><span class="co"># Set seed for reproducibility</span></span>
<span id="cb18-5"><a aria-hidden="true" href="#cb18-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">456</span>)</span>
<span id="cb18-6"><a aria-hidden="true" href="#cb18-6" tabindex="-1"></a></span>
<span id="cb18-7"><a aria-hidden="true" href="#cb18-7" tabindex="-1"></a><span class="co"># Simulation parameters</span></span>
<span id="cb18-8"><a aria-hidden="true" href="#cb18-8" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb18-9"><a aria-hidden="true" href="#cb18-9" tabindex="-1"></a>beta1 <span class="ot">&lt;-</span> <span class="dv">2</span></span>
<span id="cb18-10"><a aria-hidden="true" href="#cb18-10" tabindex="-1"></a>sigma <span class="ot">&lt;-</span> <span class="dv">1</span></span>
<span id="cb18-11"><a aria-hidden="true" href="#cb18-11" tabindex="-1"></a></span>
<span id="cb18-12"><a aria-hidden="true" href="#cb18-12" tabindex="-1"></a><span class="co"># Generate x1 (relevant) and x2 (irrelevant)</span></span>
<span id="cb18-13"><a aria-hidden="true" href="#cb18-13" tabindex="-1"></a>x1 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb18-14"><a aria-hidden="true" href="#cb18-14" tabindex="-1"></a>x2 <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)  <span class="co"># x2 is independent of y</span></span>
<span id="cb18-15"><a aria-hidden="true" href="#cb18-15" tabindex="-1"></a></span>
<span id="cb18-16"><a aria-hidden="true" href="#cb18-16" tabindex="-1"></a><span class="co"># Generate y</span></span>
<span id="cb18-17"><a aria-hidden="true" href="#cb18-17" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n, <span class="dv">0</span>, sigma)</span>
<span id="cb18-18"><a aria-hidden="true" href="#cb18-18" tabindex="-1"></a>y <span class="ot">&lt;-</span> beta1 <span class="sc">*</span> x1 <span class="sc">+</span> epsilon</span>
<span id="cb18-19"><a aria-hidden="true" href="#cb18-19" tabindex="-1"></a></span>
<span id="cb18-20"><a aria-hidden="true" href="#cb18-20" tabindex="-1"></a><span class="co"># --- Model 1: Correct model (includes only x1) ---</span></span>
<span id="cb18-21"><a aria-hidden="true" href="#cb18-21" tabindex="-1"></a>df_correct <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x1)</span>
<span id="cb18-22"><a aria-hidden="true" href="#cb18-22" tabindex="-1"></a>model_correct <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1, <span class="at">data =</span> df_correct)</span>
<span id="cb18-23"><a aria-hidden="true" href="#cb18-23" tabindex="-1"></a><span class="fu">summary</span>(model_correct)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x1, data = df_correct)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.44544 -0.70333 -0.01888  0.59612  2.53030 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.08417    0.09601   0.877    0.383    
x1           2.11989    0.09565  22.163   &lt;2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9532 on 98 degrees of freedom
Multiple R-squared:  0.8337,    Adjusted R-squared:  0.832 
F-statistic: 491.2 on 1 and 98 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb20-1"><a aria-hidden="true" href="#cb20-1" tabindex="-1"></a><span class="co"># --- Model 2: Includes irrelevant variable x2 ---</span></span>
<span id="cb20-2"><a aria-hidden="true" href="#cb20-2" tabindex="-1"></a>df_irrelevant <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, x1, x2)</span>
<span id="cb20-3"><a aria-hidden="true" href="#cb20-3" tabindex="-1"></a>model_irrelevant <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> df_irrelevant)</span>
<span id="cb20-4"><a aria-hidden="true" href="#cb20-4" tabindex="-1"></a><span class="fu">summary</span>(model_irrelevant)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
lm(formula = y ~ x1 + x2, data = df_irrelevant)

Residuals:
     Min       1Q   Median       3Q      Max 
-2.43186 -0.70864 -0.00242  0.59762  2.50212 

Coefficients:
            Estimate Std. Error t value Pr(&gt;|t|)    
(Intercept)  0.08686    0.09715   0.894    0.373    
x1           2.12028    0.09613  22.057   &lt;2e-16 ***
x2           0.02346    0.09910   0.237    0.813    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 0.9578 on 97 degrees of freedom
Multiple R-squared:  0.8338,    Adjusted R-squared:  0.8303 
F-statistic: 243.3 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb22-1"><a aria-hidden="true" href="#cb22-1" tabindex="-1"></a><span class="co"># --- Compare variances of beta1 estimates ---</span></span>
<span id="cb22-2"><a aria-hidden="true" href="#cb22-2" tabindex="-1"></a>var_beta1_correct <span class="ot">&lt;-</span> <span class="fu">vcov</span>(model_correct)[<span class="dv">2</span>,<span class="dv">2</span>]  <span class="co"># Variance of beta1 in correct model</span></span>
<span id="cb22-3"><a aria-hidden="true" href="#cb22-3" tabindex="-1"></a>var_beta1_irrelevant <span class="ot">&lt;-</span> <span class="fu">vcov</span>(model_irrelevant)[<span class="dv">2</span>,<span class="dv">2</span>] <span class="co"># Variance of beta1 with irrelevant variable</span></span>
<span id="cb22-4"><a aria-hidden="true" href="#cb22-4" tabindex="-1"></a></span>
<span id="cb22-5"><a aria-hidden="true" href="#cb22-5" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Variance of beta1 (correct model):"</span>, var_beta1_correct))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Variance of beta1 (correct model): 0.00914900135959905"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb24-1"><a aria-hidden="true" href="#cb24-1" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Variance of beta1 (irrelevant variable included):"</span>, var_beta1_irrelevant))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Variance of beta1 (irrelevant variable included): 0.00924066938296475"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb26-1"><a aria-hidden="true" href="#cb26-1" tabindex="-1"></a><span class="co"># --- Visualization: Coefficient estimates and confidence intervals ---</span></span>
<span id="cb26-2"><a aria-hidden="true" href="#cb26-2" tabindex="-1"></a>coef_df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(</span>
<span id="cb26-3"><a aria-hidden="true" href="#cb26-3" tabindex="-1"></a>  <span class="at">Model =</span> <span class="fu">c</span>(<span class="st">"Correct"</span>, <span class="st">"Irrelevant"</span>),</span>
<span id="cb26-4"><a aria-hidden="true" href="#cb26-4" tabindex="-1"></a>  <span class="at">Estimate =</span> <span class="fu">c</span>(<span class="fu">coef</span>(model_correct)[<span class="dv">2</span>], <span class="fu">coef</span>(model_irrelevant)[<span class="dv">2</span>]),</span>
<span id="cb26-5"><a aria-hidden="true" href="#cb26-5" tabindex="-1"></a>  <span class="at">Lower =</span> <span class="fu">c</span>(<span class="fu">confint</span>(model_correct)[<span class="dv">2</span>,<span class="dv">1</span>], <span class="fu">confint</span>(model_irrelevant)[<span class="dv">2</span>,<span class="dv">1</span>]),</span>
<span id="cb26-6"><a aria-hidden="true" href="#cb26-6" tabindex="-1"></a>  <span class="at">Upper =</span> <span class="fu">c</span>(<span class="fu">confint</span>(model_correct)[<span class="dv">2</span>,<span class="dv">2</span>], <span class="fu">confint</span>(model_irrelevant)[<span class="dv">2</span>,<span class="dv">2</span>])</span>
<span id="cb26-7"><a aria-hidden="true" href="#cb26-7" tabindex="-1"></a>)</span>
<span id="cb26-8"><a aria-hidden="true" href="#cb26-8" tabindex="-1"></a></span>
<span id="cb26-9"><a aria-hidden="true" href="#cb26-9" tabindex="-1"></a><span class="fu">ggplot</span>(coef_df, <span class="fu">aes</span>(<span class="at">x =</span> Model, <span class="at">y =</span> Estimate, <span class="at">ymin =</span> Lower, <span class="at">ymax =</span> Upper)) <span class="sc">+</span></span>
<span id="cb26-10"><a aria-hidden="true" href="#cb26-10" tabindex="-1"></a>  <span class="fu">geom_pointrange</span>() <span class="sc">+</span></span>
<span id="cb26-11"><a aria-hidden="true" href="#cb26-11" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Comparison of Coefficient Estimates and Confidence Intervals"</span>,</span>
<span id="cb26-12"><a aria-hidden="true" href="#cb26-12" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Coefficient Estimate (beta1)"</span>) <span class="sc">+</span></span>
<span id="cb26-13"><a aria-hidden="true" href="#cb26-13" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap21_files/figure-html/unnamed-chunk-2-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><p><strong>Setup:</strong> Loads the <code>tidyverse</code> package, sets a seed, and defines simulation parameters.</p></li>
<li><p><strong>Data Generation:</strong> Generates a relevant variable <code>x1</code> and an <em>irrelevant</em> variable <code>x2</code> (independent of <code>y</code>). <code>y</code> is generated based only on <code>x1</code> and the error term.</p></li>
<li><p><strong>Model Estimation:</strong></p>
<ul>
<li><strong>Correct Model:</strong> Estimates the model with only the relevant variable <code>x1</code>.</li>
<li><strong>Model with Irrelevant Variable:</strong> Estimates the model including both <code>x1</code> and the irrelevant variable <code>x2</code>.</li>
</ul></li>
<li><p><strong>Variance Comparison:</strong></p>
<ul>
<li>Extracts the variance of the <span class="math inline">\(\hat{\beta_1}\)</span> estimates from both models using <code>vcov()</code>.</li>
<li>Prints the variances. The variance of <span class="math inline">\(\hat{\beta_1}\)</span> will be <em>larger</em> when the irrelevant variable <code>x2</code> is included, demonstrating the concept from Section 21.2 of the text that including irrelevant variables inflates the variance of the estimators.</li>
</ul></li>
<li><p><strong>Visualization:</strong> Creates a plot showing the point estimates and confidence intervals for <span class="math inline">\(\beta_1\)</span> from both models. The confidence interval will be wider for the model including the irrelevant variable, visually confirming the increased variance.</p></li>
</ol>
</section>
<section class="level3" id="r-script-3-model-selection-with-adjusted-r-squared">
<h3 class="anchored" data-anchor-id="r-script-3-model-selection-with-adjusted-r-squared">R Script 3: Model Selection with Adjusted R-squared</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb27-1"><a aria-hidden="true" href="#cb27-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb27-2"><a aria-hidden="true" href="#cb27-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb27-3"><a aria-hidden="true" href="#cb27-3" tabindex="-1"></a></span>
<span id="cb27-4"><a aria-hidden="true" href="#cb27-4" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb27-5"><a aria-hidden="true" href="#cb27-5" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb27-6"><a aria-hidden="true" href="#cb27-6" tabindex="-1"></a></span>
<span id="cb27-7"><a aria-hidden="true" href="#cb27-7" tabindex="-1"></a><span class="co"># Generate data with multiple potential predictors</span></span>
<span id="cb27-8"><a aria-hidden="true" href="#cb27-8" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb27-9"><a aria-hidden="true" href="#cb27-9" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span>  <span class="co"># Number of potential predictors</span></span>
<span id="cb27-10"><a aria-hidden="true" href="#cb27-10" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> k), <span class="at">ncol =</span> k)</span>
<span id="cb27-11"><a aria-hidden="true" href="#cb27-11" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">0</span>, <span class="fl">1.5</span>, <span class="dv">0</span>, <span class="dv">0</span>)  <span class="co"># True coefficients: x1 and x3 are relevant, others are not</span></span>
<span id="cb27-12"><a aria-hidden="true" href="#cb27-12" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb27-13"><a aria-hidden="true" href="#cb27-13" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> epsilon</span>
<span id="cb27-14"><a aria-hidden="true" href="#cb27-14" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, X)</span>
<span id="cb27-15"><a aria-hidden="true" href="#cb27-15" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"y"</span>, <span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">1</span><span class="sc">:</span>k))</span>
<span id="cb27-16"><a aria-hidden="true" href="#cb27-16" tabindex="-1"></a></span>
<span id="cb27-17"><a aria-hidden="true" href="#cb27-17" tabindex="-1"></a><span class="co"># --- Function to calculate adjusted R-squared ---</span></span>
<span id="cb27-18"><a aria-hidden="true" href="#cb27-18" tabindex="-1"></a>calculate_adj_r_squared <span class="ot">&lt;-</span> <span class="cf">function</span>(model, data) {</span>
<span id="cb27-19"><a aria-hidden="true" href="#cb27-19" tabindex="-1"></a>  n <span class="ot">&lt;-</span> <span class="fu">nrow</span>(data)</span>
<span id="cb27-20"><a aria-hidden="true" href="#cb27-20" tabindex="-1"></a>  k <span class="ot">&lt;-</span> <span class="fu">length</span>(<span class="fu">coef</span>(model)) <span class="sc">-</span> <span class="dv">1</span> <span class="co"># Number of predictors (excluding intercept)</span></span>
<span id="cb27-21"><a aria-hidden="true" href="#cb27-21" tabindex="-1"></a>  r_squared <span class="ot">&lt;-</span> <span class="fu">summary</span>(model)<span class="sc">$</span>r.squared</span>
<span id="cb27-22"><a aria-hidden="true" href="#cb27-22" tabindex="-1"></a>  adj_r_squared <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">-</span> (<span class="dv">1</span> <span class="sc">-</span> r_squared) <span class="sc">*</span> (n <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">/</span> (n <span class="sc">-</span> k <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb27-23"><a aria-hidden="true" href="#cb27-23" tabindex="-1"></a>  <span class="fu">return</span>(adj_r_squared)</span>
<span id="cb27-24"><a aria-hidden="true" href="#cb27-24" tabindex="-1"></a>}</span>
<span id="cb27-25"><a aria-hidden="true" href="#cb27-25" tabindex="-1"></a></span>
<span id="cb27-26"><a aria-hidden="true" href="#cb27-26" tabindex="-1"></a><span class="co"># --- Model selection using adjusted R-squared ---</span></span>
<span id="cb27-27"><a aria-hidden="true" href="#cb27-27" tabindex="-1"></a><span class="co"># Consider all possible combinations of predictors</span></span>
<span id="cb27-28"><a aria-hidden="true" href="#cb27-28" tabindex="-1"></a>all_combinations <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>k, <span class="cf">function</span>(i) <span class="fu">combn</span>(<span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">1</span><span class="sc">:</span>k), i, <span class="at">simplify =</span> <span class="cn">FALSE</span>)), <span class="at">recursive =</span> <span class="cn">FALSE</span>)</span>
<span id="cb27-29"><a aria-hidden="true" href="#cb27-29" tabindex="-1"></a></span>
<span id="cb27-30"><a aria-hidden="true" href="#cb27-30" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Model =</span> <span class="fu">character</span>(), <span class="at">Adj_R_squared =</span> <span class="fu">numeric</span>())</span>
<span id="cb27-31"><a aria-hidden="true" href="#cb27-31" tabindex="-1"></a></span>
<span id="cb27-32"><a aria-hidden="true" href="#cb27-32" tabindex="-1"></a><span class="cf">for</span> (combination <span class="cf">in</span> all_combinations) {</span>
<span id="cb27-33"><a aria-hidden="true" href="#cb27-33" tabindex="-1"></a>  formula_str <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"y ~"</span>, <span class="fu">paste</span>(combination, <span class="at">collapse =</span> <span class="st">" + "</span>))</span>
<span id="cb27-34"><a aria-hidden="true" href="#cb27-34" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">as.formula</span>(formula_str), <span class="at">data =</span> df)</span>
<span id="cb27-35"><a aria-hidden="true" href="#cb27-35" tabindex="-1"></a>  adj_r2 <span class="ot">&lt;-</span> <span class="fu">calculate_adj_r_squared</span>(model, df)</span>
<span id="cb27-36"><a aria-hidden="true" href="#cb27-36" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">data.frame</span>(<span class="at">Model =</span> formula_str, <span class="at">Adj_R_squared =</span> adj_r2))</span>
<span id="cb27-37"><a aria-hidden="true" href="#cb27-37" tabindex="-1"></a>}</span>
<span id="cb27-38"><a aria-hidden="true" href="#cb27-38" tabindex="-1"></a></span>
<span id="cb27-39"><a aria-hidden="true" href="#cb27-39" tabindex="-1"></a><span class="co"># Find the model with the highest adjusted R-squared</span></span>
<span id="cb27-40"><a aria-hidden="true" href="#cb27-40" tabindex="-1"></a>best_model <span class="ot">&lt;-</span> results[<span class="fu">which.max</span>(results<span class="sc">$</span>Adj_R_squared), ]</span>
<span id="cb27-41"><a aria-hidden="true" href="#cb27-41" tabindex="-1"></a><span class="fu">print</span>(best_model)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Model Adj_R_squared
20 y ~ x1 + x3 + x5     0.8980251</code></pre>
</div>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb29-1"><a aria-hidden="true" href="#cb29-1" tabindex="-1"></a><span class="co"># --- Visualization: Adjusted R-squared for different models ---</span></span>
<span id="cb29-2"><a aria-hidden="true" href="#cb29-2" tabindex="-1"></a><span class="fu">ggplot</span>(results, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Model, Adj_R_squared), <span class="at">y =</span> Adj_R_squared)) <span class="sc">+</span></span>
<span id="cb29-3"><a aria-hidden="true" href="#cb29-3" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>) <span class="sc">+</span></span>
<span id="cb29-4"><a aria-hidden="true" href="#cb29-4" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb29-5"><a aria-hidden="true" href="#cb29-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"Adjusted R-squared for Different Models"</span>,</span>
<span id="cb29-6"><a aria-hidden="true" href="#cb29-6" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Model"</span>,</span>
<span id="cb29-7"><a aria-hidden="true" href="#cb29-7" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Adjusted R-squared"</span>) <span class="sc">+</span></span>
<span id="cb29-8"><a aria-hidden="true" href="#cb29-8" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap21_files/figure-html/unnamed-chunk-3-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><p><strong>Setup:</strong> Loads <code>tidyverse</code>, sets a seed, and generates data with multiple potential predictors (<code>x1</code> through <code>x5</code>). The true coefficients are set so that only <code>x1</code> and <code>x3</code> are truly relevant.</p></li>
<li><p><strong><code>calculate_adj_r_squared</code> Function:</strong> Defines a function to calculate the adjusted <span class="math inline">\(R^2\)</span> given a model and data. This implements the formula from the text (Equation 21.1).</p></li>
<li><p><strong>Model Selection:</strong></p>
<ul>
<li><code>all_combinations</code>: Generates all possible combinations of predictors (from single predictors up to all five predictors). <code>combn</code> function helps to get combinations, <code>lapply</code> applies this for each possible number of variables and <code>unlist(,recursive = FALSE)</code> makes a flat list from a list of lists.</li>
<li>Iterates through each combination:
<ul>
<li>Constructs the model formula string (e.g., “y ~ x1 + x3”).</li>
<li>Fits the linear model using <code>lm()</code>.</li>
<li>Calculates the adjusted <span class="math inline">\(R^2\)</span> using the defined function.</li>
<li>Stores the model formula and adjusted <span class="math inline">\(R^2\)</span> in the <code>results</code> data frame.</li>
</ul></li>
<li>Finds the model with the maximum adjusted <span class="math inline">\(R^2\)</span> using <code>which.max()</code>.</li>
<li>Prints the best model. This demonstrates the model selection process based on adjusted <span class="math inline">\(R^2\)</span>, as described in Section 21.3.</li>
</ul></li>
<li><p><strong>Visualization:</strong> Creates a bar plot showing the adjusted <span class="math inline">\(R^2\)</span> for each model, sorted in ascending order. This makes it easy to visually identify the best model.</p></li>
</ol>
</section>
<section class="level3" id="r-script-4-model-selection-with-aic-and-bic">
<h3 class="anchored" data-anchor-id="r-script-4-model-selection-with-aic-and-bic">R Script 4: Model Selection with AIC and BIC</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb30-1"><a aria-hidden="true" href="#cb30-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb30-2"><a aria-hidden="true" href="#cb30-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb30-3"><a aria-hidden="true" href="#cb30-3" tabindex="-1"></a></span>
<span id="cb30-4"><a aria-hidden="true" href="#cb30-4" tabindex="-1"></a><span class="co"># Use the same data generation as in Script 3</span></span>
<span id="cb30-5"><a aria-hidden="true" href="#cb30-5" tabindex="-1"></a><span class="co"># (Re-run the data generation part of Script 3 here if not already in your environment)</span></span>
<span id="cb30-6"><a aria-hidden="true" href="#cb30-6" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb30-7"><a aria-hidden="true" href="#cb30-7" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">789</span>)</span>
<span id="cb30-8"><a aria-hidden="true" href="#cb30-8" tabindex="-1"></a></span>
<span id="cb30-9"><a aria-hidden="true" href="#cb30-9" tabindex="-1"></a><span class="co"># Generate data with multiple potential predictors</span></span>
<span id="cb30-10"><a aria-hidden="true" href="#cb30-10" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb30-11"><a aria-hidden="true" href="#cb30-11" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">5</span>  <span class="co"># Number of potential predictors</span></span>
<span id="cb30-12"><a aria-hidden="true" href="#cb30-12" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> k), <span class="at">ncol =</span> k)</span>
<span id="cb30-13"><a aria-hidden="true" href="#cb30-13" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">0</span>, <span class="fl">1.5</span>, <span class="dv">0</span>, <span class="dv">0</span>)  <span class="co"># True coefficients: x1 and x3 are relevant, others are not</span></span>
<span id="cb30-14"><a aria-hidden="true" href="#cb30-14" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb30-15"><a aria-hidden="true" href="#cb30-15" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> epsilon</span>
<span id="cb30-16"><a aria-hidden="true" href="#cb30-16" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y, X)</span>
<span id="cb30-17"><a aria-hidden="true" href="#cb30-17" tabindex="-1"></a><span class="fu">colnames</span>(df) <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">"y"</span>, <span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">1</span><span class="sc">:</span>k))</span>
<span id="cb30-18"><a aria-hidden="true" href="#cb30-18" tabindex="-1"></a></span>
<span id="cb30-19"><a aria-hidden="true" href="#cb30-19" tabindex="-1"></a><span class="co"># --- Model selection using AIC and BIC ---</span></span>
<span id="cb30-20"><a aria-hidden="true" href="#cb30-20" tabindex="-1"></a>all_combinations <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">lapply</span>(<span class="dv">1</span><span class="sc">:</span>k, <span class="cf">function</span>(i) <span class="fu">combn</span>(<span class="fu">paste0</span>(<span class="st">"x"</span>, <span class="dv">1</span><span class="sc">:</span>k), i, <span class="at">simplify =</span> <span class="cn">FALSE</span>)), <span class="at">recursive =</span> <span class="cn">FALSE</span>)</span>
<span id="cb30-21"><a aria-hidden="true" href="#cb30-21" tabindex="-1"></a></span>
<span id="cb30-22"><a aria-hidden="true" href="#cb30-22" tabindex="-1"></a>results <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">Model =</span> <span class="fu">character</span>(), <span class="at">AIC =</span> <span class="fu">numeric</span>(), <span class="at">BIC =</span> <span class="fu">numeric</span>())</span>
<span id="cb30-23"><a aria-hidden="true" href="#cb30-23" tabindex="-1"></a></span>
<span id="cb30-24"><a aria-hidden="true" href="#cb30-24" tabindex="-1"></a><span class="cf">for</span> (combination <span class="cf">in</span> all_combinations) {</span>
<span id="cb30-25"><a aria-hidden="true" href="#cb30-25" tabindex="-1"></a>  formula_str <span class="ot">&lt;-</span> <span class="fu">paste</span>(<span class="st">"y ~"</span>, <span class="fu">paste</span>(combination, <span class="at">collapse =</span> <span class="st">" + "</span>))</span>
<span id="cb30-26"><a aria-hidden="true" href="#cb30-26" tabindex="-1"></a>  model <span class="ot">&lt;-</span> <span class="fu">lm</span>(<span class="fu">as.formula</span>(formula_str), <span class="at">data =</span> df)</span>
<span id="cb30-27"><a aria-hidden="true" href="#cb30-27" tabindex="-1"></a>  aic_val <span class="ot">&lt;-</span> <span class="fu">AIC</span>(model)</span>
<span id="cb30-28"><a aria-hidden="true" href="#cb30-28" tabindex="-1"></a>  bic_val <span class="ot">&lt;-</span> <span class="fu">BIC</span>(model)</span>
<span id="cb30-29"><a aria-hidden="true" href="#cb30-29" tabindex="-1"></a>  results <span class="ot">&lt;-</span> <span class="fu">rbind</span>(results, <span class="fu">data.frame</span>(<span class="at">Model =</span> formula_str, <span class="at">AIC =</span> aic_val, <span class="at">BIC =</span> bic_val))</span>
<span id="cb30-30"><a aria-hidden="true" href="#cb30-30" tabindex="-1"></a>}</span>
<span id="cb30-31"><a aria-hidden="true" href="#cb30-31" tabindex="-1"></a></span>
<span id="cb30-32"><a aria-hidden="true" href="#cb30-32" tabindex="-1"></a><span class="co"># Find the best model according to AIC</span></span>
<span id="cb30-33"><a aria-hidden="true" href="#cb30-33" tabindex="-1"></a>best_model_aic <span class="ot">&lt;-</span> results[<span class="fu">which.min</span>(results<span class="sc">$</span>AIC), ]</span>
<span id="cb30-34"><a aria-hidden="true" href="#cb30-34" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Best model according to AIC:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Best model according to AIC:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb32-1"><a aria-hidden="true" href="#cb32-1" tabindex="-1"></a><span class="fu">print</span>(best_model_aic)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>              Model      AIC      BIC
20 y ~ x1 + x3 + x5 258.1546 271.1805</code></pre>
</div>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb34-1"><a aria-hidden="true" href="#cb34-1" tabindex="-1"></a><span class="co"># Find the best model according to BIC</span></span>
<span id="cb34-2"><a aria-hidden="true" href="#cb34-2" tabindex="-1"></a>best_model_bic <span class="ot">&lt;-</span> results[<span class="fu">which.min</span>(results<span class="sc">$</span>BIC), ]</span>
<span id="cb34-3"><a aria-hidden="true" href="#cb34-3" tabindex="-1"></a><span class="fu">print</span>(<span class="st">"Best model according to BIC:"</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Best model according to BIC:"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb36-1"><a aria-hidden="true" href="#cb36-1" tabindex="-1"></a><span class="fu">print</span>(best_model_bic)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>        Model      AIC      BIC
7 y ~ x1 + x3 258.4832 268.9039</code></pre>
</div>
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb38-1"><a aria-hidden="true" href="#cb38-1" tabindex="-1"></a><span class="co"># --- Visualization: AIC and BIC for different models ---</span></span>
<span id="cb38-2"><a aria-hidden="true" href="#cb38-2" tabindex="-1"></a>results_long <span class="ot">&lt;-</span> results <span class="sc">%&gt;%</span></span>
<span id="cb38-3"><a aria-hidden="true" href="#cb38-3" tabindex="-1"></a>  <span class="fu">pivot_longer</span>(<span class="at">cols =</span> <span class="fu">c</span>(AIC, BIC), <span class="at">names_to =</span> <span class="st">"Criterion"</span>, <span class="at">values_to =</span> <span class="st">"Value"</span>)</span>
<span id="cb38-4"><a aria-hidden="true" href="#cb38-4" tabindex="-1"></a></span>
<span id="cb38-5"><a aria-hidden="true" href="#cb38-5" tabindex="-1"></a><span class="fu">ggplot</span>(results_long, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Model, Value), <span class="at">y =</span> Value, <span class="at">fill =</span> Criterion)) <span class="sc">+</span></span>
<span id="cb38-6"><a aria-hidden="true" href="#cb38-6" tabindex="-1"></a>  <span class="fu">geom_bar</span>(<span class="at">stat =</span> <span class="st">"identity"</span>, <span class="at">position =</span> <span class="st">"dodge"</span>) <span class="sc">+</span></span>
<span id="cb38-7"><a aria-hidden="true" href="#cb38-7" tabindex="-1"></a>  <span class="fu">coord_flip</span>() <span class="sc">+</span></span>
<span id="cb38-8"><a aria-hidden="true" href="#cb38-8" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title =</span> <span class="st">"AIC and BIC for Different Models"</span>,</span>
<span id="cb38-9"><a aria-hidden="true" href="#cb38-9" tabindex="-1"></a>       <span class="at">x =</span> <span class="st">"Model"</span>,</span>
<span id="cb38-10"><a aria-hidden="true" href="#cb38-10" tabindex="-1"></a>       <span class="at">y =</span> <span class="st">"Criterion Value"</span>) <span class="sc">+</span></span>
<span id="cb38-11"><a aria-hidden="true" href="#cb38-11" tabindex="-1"></a>  <span class="fu">theme_bw</span>()</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap21_files/figure-html/unnamed-chunk-4-1.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><p><strong>Data:</strong> Uses the same data generation process as R Script 3.</p></li>
<li><p><strong>Model Selection:</strong></p>
<ul>
<li>Similar to Script 3, generates all possible combinations of predictors.</li>
<li>Iterates through each combination:
<ul>
<li>Constructs the model formula.</li>
<li>Fits the linear model.</li>
<li>Calculates AIC and BIC using the built-in <code>AIC()</code> and <code>BIC()</code> functions. These functions implement the formulas from the text (Equations 21.3 and 21.4).</li>
<li>Stores the model formula, AIC, and BIC in the <code>results</code> data frame.</li>
</ul></li>
<li>Finds the best model according to AIC (minimum AIC).</li>
<li>Finds the best model according to BIC (minimum BIC).</li>
<li>Prints the best models. Note that AIC and BIC may select different models. BIC will tend to favor simpler models.</li>
</ul></li>
<li><p><strong>Visualization:</strong> Uses <code>pivot_longer</code> from <code>tidyr</code> to reshape the data for easier plotting. Creates a bar plot showing both AIC and BIC for each model. This allows for a visual comparison of the models based on both criteria.</p></li>
</ol>
</section>
<section class="level3" id="r-script-5-lasso-regression">
<h3 class="anchored" data-anchor-id="r-script-5-lasso-regression">R Script 5: LASSO Regression</h3>
<div class="cell">
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb39-1"><a aria-hidden="true" href="#cb39-1" tabindex="-1"></a><span class="co"># Load necessary libraries</span></span>
<span id="cb39-2"><a aria-hidden="true" href="#cb39-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb39-3"><a aria-hidden="true" href="#cb39-3" tabindex="-1"></a><span class="fu">library</span>(glmnet)  <span class="co"># For LASSO regression</span></span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>Loading required package: Matrix</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'Matrix'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:tidyr':

    expand, pack, unpack</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>Loaded glmnet 4.1-8</code></pre>
</div>
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb44-1"><a aria-hidden="true" href="#cb44-1" tabindex="-1"></a><span class="co"># Set seed</span></span>
<span id="cb44-2"><a aria-hidden="true" href="#cb44-2" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1011</span>)</span>
<span id="cb44-3"><a aria-hidden="true" href="#cb44-3" tabindex="-1"></a></span>
<span id="cb44-4"><a aria-hidden="true" href="#cb44-4" tabindex="-1"></a><span class="co"># Generate data with many predictors, but only a few are relevant</span></span>
<span id="cb44-5"><a aria-hidden="true" href="#cb44-5" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">100</span></span>
<span id="cb44-6"><a aria-hidden="true" href="#cb44-6" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="dv">20</span> <span class="co"># Number of predictors</span></span>
<span id="cb44-7"><a aria-hidden="true" href="#cb44-7" tabindex="-1"></a>X <span class="ot">&lt;-</span> <span class="fu">matrix</span>(<span class="fu">rnorm</span>(n <span class="sc">*</span> k), <span class="at">ncol =</span> k)</span>
<span id="cb44-8"><a aria-hidden="true" href="#cb44-8" tabindex="-1"></a>beta <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="fu">rep</span>(<span class="dv">2</span>, <span class="dv">5</span>), <span class="fu">rep</span>(<span class="dv">0</span>, k <span class="sc">-</span> <span class="dv">5</span>))  <span class="co"># First 5 predictors are relevant, rest are irrelevant</span></span>
<span id="cb44-9"><a aria-hidden="true" href="#cb44-9" tabindex="-1"></a>epsilon <span class="ot">&lt;-</span> <span class="fu">rnorm</span>(n)</span>
<span id="cb44-10"><a aria-hidden="true" href="#cb44-10" tabindex="-1"></a>y <span class="ot">&lt;-</span> X <span class="sc">%*%</span> beta <span class="sc">+</span> epsilon</span>
<span id="cb44-11"><a aria-hidden="true" href="#cb44-11" tabindex="-1"></a></span>
<span id="cb44-12"><a aria-hidden="true" href="#cb44-12" tabindex="-1"></a><span class="co"># --- LASSO Regression with Cross-Validation ---</span></span>
<span id="cb44-13"><a aria-hidden="true" href="#cb44-13" tabindex="-1"></a></span>
<span id="cb44-14"><a aria-hidden="true" href="#cb44-14" tabindex="-1"></a><span class="co"># Create a data frame for glmnet</span></span>
<span id="cb44-15"><a aria-hidden="true" href="#cb44-15" tabindex="-1"></a>df <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(y,X)</span>
<span id="cb44-16"><a aria-hidden="true" href="#cb44-16" tabindex="-1"></a></span>
<span id="cb44-17"><a aria-hidden="true" href="#cb44-17" tabindex="-1"></a><span class="co"># glmnet requires a matrix for X and a vector for y</span></span>
<span id="cb44-18"><a aria-hidden="true" href="#cb44-18" tabindex="-1"></a>x_matrix <span class="ot">&lt;-</span> <span class="fu">model.matrix</span>(y <span class="sc">~</span> ., df)[,<span class="sc">-</span><span class="dv">1</span>] <span class="co"># Remove intercept column</span></span>
<span id="cb44-19"><a aria-hidden="true" href="#cb44-19" tabindex="-1"></a>y_vector <span class="ot">&lt;-</span> df<span class="sc">$</span>y</span>
<span id="cb44-20"><a aria-hidden="true" href="#cb44-20" tabindex="-1"></a><span class="co"># Perform cross-validation to find the optimal lambda</span></span>
<span id="cb44-21"><a aria-hidden="true" href="#cb44-21" tabindex="-1"></a>cv_model <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x_matrix, y_vector, <span class="at">alpha =</span> <span class="dv">1</span>)  <span class="co"># alpha = 1 for LASSO</span></span>
<span id="cb44-22"><a aria-hidden="true" href="#cb44-22" tabindex="-1"></a></span>
<span id="cb44-23"><a aria-hidden="true" href="#cb44-23" tabindex="-1"></a><span class="co"># Find the best lambda</span></span>
<span id="cb44-24"><a aria-hidden="true" href="#cb44-24" tabindex="-1"></a>best_lambda <span class="ot">&lt;-</span> cv_model<span class="sc">$</span>lambda.min</span>
<span id="cb44-25"><a aria-hidden="true" href="#cb44-25" tabindex="-1"></a><span class="fu">print</span>(<span class="fu">paste</span>(<span class="st">"Best lambda:"</span>, best_lambda))</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "Best lambda: 0.103203794548873"</code></pre>
</div>
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb46-1"><a aria-hidden="true" href="#cb46-1" tabindex="-1"></a><span class="co"># Fit the LASSO model with the best lambda</span></span>
<span id="cb46-2"><a aria-hidden="true" href="#cb46-2" tabindex="-1"></a>lasso_model <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x_matrix, y_vector, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> best_lambda)</span>
<span id="cb46-3"><a aria-hidden="true" href="#cb46-3" tabindex="-1"></a></span>
<span id="cb46-4"><a aria-hidden="true" href="#cb46-4" tabindex="-1"></a><span class="co"># Extract the coefficients</span></span>
<span id="cb46-5"><a aria-hidden="true" href="#cb46-5" tabindex="-1"></a>coefficients <span class="ot">&lt;-</span> <span class="fu">coef</span>(lasso_model)</span>
<span id="cb46-6"><a aria-hidden="true" href="#cb46-6" tabindex="-1"></a><span class="fu">print</span>(coefficients)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>21 x 1 sparse Matrix of class "dgCMatrix"
                     s0
(Intercept)  0.05261701
X1           2.12012238
X2           1.94425413
X3           2.00064168
X4           1.91416014
X5           1.82219096
X6           0.08016068
X7           0.01767611
X8          -0.12686034
X9           .         
X10          .         
X11          .         
X12          .         
X13          .         
X14          .         
X15          .         
X16          .         
X17          0.01608481
X18          .         
X19          .         
X20          .         </code></pre>
</div>
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb48-1"><a aria-hidden="true" href="#cb48-1" tabindex="-1"></a><span class="co"># --- Visualization: Coefficient Path ---</span></span>
<span id="cb48-2"><a aria-hidden="true" href="#cb48-2" tabindex="-1"></a></span>
<span id="cb48-3"><a aria-hidden="true" href="#cb48-3" tabindex="-1"></a><span class="co"># Fit LASSO for a range of lambda values (without cross-validation, for visualization)</span></span>
<span id="cb48-4"><a aria-hidden="true" href="#cb48-4" tabindex="-1"></a>lasso_path <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x_matrix, y_vector, <span class="at">alpha =</span> <span class="dv">1</span>)</span>
<span id="cb48-5"><a aria-hidden="true" href="#cb48-5" tabindex="-1"></a></span>
<span id="cb48-6"><a aria-hidden="true" href="#cb48-6" tabindex="-1"></a><span class="co"># Plot the coefficient path</span></span>
<span id="cb48-7"><a aria-hidden="true" href="#cb48-7" tabindex="-1"></a><span class="fu">plot</span>(lasso_path, <span class="at">xvar =</span> <span class="st">"lambda"</span>, <span class="at">label =</span> <span class="cn">TRUE</span>)</span>
<span id="cb48-8"><a aria-hidden="true" href="#cb48-8" tabindex="-1"></a><span class="fu">title</span>(<span class="st">"LASSO Coefficient Path"</span>, <span class="at">line =</span> <span class="fl">2.5</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap21_files/figure-html/unnamed-chunk-5-1.png" width="672"/></p>
</figure>
</div>
</div>
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb49-1"><a aria-hidden="true" href="#cb49-1" tabindex="-1"></a><span class="co"># --- Visualization: Cross-validation Error ---</span></span>
<span id="cb49-2"><a aria-hidden="true" href="#cb49-2" tabindex="-1"></a><span class="fu">plot</span>(cv_model)</span>
<span id="cb49-3"><a aria-hidden="true" href="#cb49-3" tabindex="-1"></a><span class="fu">title</span>(<span class="st">"Cross-Validation Error for LASSO"</span>, <span class="at">line=</span><span class="fl">2.5</span>)</span></code><button class="code-copy-button" title="Copy to Clipboard"><i class="bi"></i></button></pre></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img class="img-fluid figure-img" src="chap21_files/figure-html/unnamed-chunk-5-2.png" width="672"/></p>
</figure>
</div>
</div>
</div>
<p><strong>Explanation:</strong></p>
<ol type="1">
<li><p><strong>Setup:</strong> Loads <code>tidyverse</code> and <code>glmnet</code>, sets a seed, and generates data with many predictors (<code>k = 20</code>), but only the first five have non-zero coefficients (illustrating <strong>sparsity</strong>).</p></li>
<li><p><strong>Data Preparation for <code>glmnet</code>:</strong></p>
<ul>
<li><code>glmnet</code> requires the input data to be in a specific format: a matrix for the predictors (<code>x_matrix</code>) and a vector for the response (<code>y_vector</code>). <code>model.matrix</code> is used to create dummy variable and remove intercept.</li>
</ul></li>
<li><p><strong>LASSO with Cross-Validation:</strong></p>
<ul>
<li><code>cv.glmnet(x_matrix, y_vector, alpha = 1)</code> performs cross-validation to find the optimal value of the regularization parameter <span class="math inline">\(\lambda\)</span>. <code>alpha = 1</code> specifies LASSO regression (as opposed to Ridge regression or elastic net).</li>
<li><code>cv_model$lambda.min</code> extracts the value of <span class="math inline">\(\lambda\)</span> that minimizes the cross-validation error.</li>
<li><code>glmnet(x_matrix, y_vector, alpha = 1, lambda = best_lambda)</code> fits the final LASSO model using the best <span class="math inline">\(\lambda\)</span>.</li>
</ul></li>
<li><p><strong>Coefficient Extraction:</strong> <code>coef(lasso_model)</code> extracts the estimated coefficients from the fitted LASSO model. Many of these coefficients will be exactly zero, demonstrating LASSO’s <strong>variable selection</strong> property.</p></li>
<li><p><strong>Visualization:</strong></p>
<ul>
<li><strong>Coefficient Path:</strong> <code>glmnet(x_matrix, y_vector, alpha = 1)</code> (without specifying <code>lambda</code>) fits the LASSO model for a <em>range</em> of <span class="math inline">\(\lambda\)</span> values. <code>plot(lasso_path, xvar = "lambda", label = TRUE)</code> creates a “coefficient path” plot. This plot shows how the estimated coefficients change as <span class="math inline">\(\lambda\)</span> changes. As <span class="math inline">\(\lambda\)</span> increases (moving from right to left on the plot), more coefficients are shrunk to zero.</li>
<li><strong>Cross-Validation Error:</strong> <code>plot(cv_model)</code> plots the cross-validation error as a function of <span class="math inline">\(\lambda\)</span>. This helps to visualize how the choice of <span class="math inline">\(\lambda\)</span> affects the model’s performance. The minimum point on this curve corresponds to <code>cv_model$lambda.min</code>.</li>
</ul></li>
</ol>
<p>These scripts illustrate the key concepts from Chapter 21 of the text, using simulations, model fitting, and visualizations to provide a practical understanding of omitted variable bias, inclusion of irrelevant variables, model selection techniques, and LASSO regression.</p>
</section>
</section>
<section class="level2" id="youtube-videos-on-econometrics-concepts">
<h2 class="anchored" data-anchor-id="youtube-videos-on-econometrics-concepts">YouTube Videos on Econometrics Concepts</h2>
<p>Here are some YouTube videos that explain the concepts discussed in Chapter 21, along with explanations of how they relate to the text:</p>
<section class="level3" id="omitted-variable-bias">
<h3 class="anchored" data-anchor-id="omitted-variable-bias">Omitted Variable Bias</h3>
<ol type="1">
<li><strong>Title:</strong> “Omitted Variable Bias Explained” <strong>Channel:</strong> Ben Lambert <strong>Link:</strong> <a href="https://www.youtube.com/watch?v=FW-v5HXYv_M">https://www.youtube.com/watch?v=FW-v5HXYv_M</a> <strong>Relevance:</strong> This video provides a clear and intuitive explanation of <strong>omitted variable bias (OVB)</strong>. It covers the core concepts presented in Section 21.1 of the text, including:
<ul>
<li>The conditions under which OVB occurs (correlation between the omitted variable and both the dependent and included independent variables).</li>
<li>The direction and magnitude of the bias.</li>
<li>The mathematical derivation of the bias, similar to the derivation presented in the text (<span class="math inline">\(\beta_{12} = (X_1^T X_1)^{-1} X_1^T X_2\beta_2\)</span>).</li>
<li>The use of directed acyclic graphs (DAGs) to visualize the relationships between variables, which is helpful, although not directly in the text.</li>
</ul></li>
<li><strong>Title:</strong> “Omitted Variable Bias: An Introduction” <strong>Channel:</strong> A Crash Course in Causality <strong>Link:</strong> <a href="https://www.youtube.com/watch?v=oPXeWfIuJs0">https://www.youtube.com/watch?v=oPXeWfIuJs0</a> <strong>Relevance:</strong> This video provides another excellent explanation of OVB, covering similar ground to the Ben Lambert video but with slightly different examples. It reinforces the core concepts of Section 21.1. It does a good job in connecting the bias in the coefficient with a bias in estimating causal effects.</li>
</ol>
</section>
<section class="level3" id="inclusion-of-irrelevant-variables">
<h3 class="anchored" data-anchor-id="inclusion-of-irrelevant-variables">Inclusion of Irrelevant Variables</h3>
<ol type="1">
<li><strong>Title:</strong> “Irrelevant variables in OLS” <strong>Channel:</strong> Economics and Guitars <strong>Link:</strong> <a href="https://www.youtube.com/watch?v=G3YD7-b5jO8">https://www.youtube.com/watch?v=G3YD7-b5jO8</a> <strong>Relevance:</strong> This short video directly addresses the consequences of including irrelevant variables in a regression model. It aligns with Section 21.2 of the text, particularly the points that:
<ul>
<li>The OLS estimator remains <em>unbiased</em>.</li>
<li>The <em>variance</em> of the estimator <em>increases</em> when irrelevant variables are included.</li>
<li>It does the demonstration of the matrix algebra proof in the case where a single extra variable is added.</li>
</ul></li>
</ol>
</section>
<section class="level3" id="model-selection-aic-bic">
<h3 class="anchored" data-anchor-id="model-selection-aic-bic">Model Selection (AIC, BIC)</h3>
<ol type="1">
<li><strong>Title:</strong> “AIC, BIC, and Model Selection in R” <strong>Channel:</strong> Quant Psych <strong>Link:</strong> <a href="https://www.youtube.com/watch?v=i-e6g_SzJ90">https://www.youtube.com/watch?v=i-e6g_SzJ90</a> <strong>Relevance:</strong> This video explains and demonstrates the use of the <strong>Akaike Information Criterion (AIC)</strong> and the <strong>Bayesian Information Criterion (BIC)</strong> for model selection, specifically in the context of R. This directly relates to Section 21.3 of the text, where AIC and BIC are introduced as model selection criteria (Equations 21.3 and 21.4). The video covers:
<ul>
<li>The formulas for AIC and BIC.</li>
<li>The interpretation of AIC and BIC values (lower is better).</li>
<li>How to use AIC and BIC to compare different models.</li>
<li>Practical implementation in R.</li>
</ul></li>
<li><strong>Title</strong>: StatQuest: AIC and BIC <strong>Channel</strong>: StatQuest with Josh Starmer <strong>Link</strong>: <a href="https://www.youtube.com/watch?v=5-OM5yFAGaA">https://www.youtube.com/watch?v=5-OM5yFAGaA</a> <strong>Relevance</strong>: Very clear and conceptual explanation of the use of AIC and BIC for model selection. Discusses the underlying likelihood considerations.</li>
</ol>
</section>
<section class="level3" id="lasso-regression">
<h3 class="anchored" data-anchor-id="lasso-regression">LASSO Regression</h3>
<ol type="1">
<li><strong>Title:</strong> “StatQuest: Lasso Regression, Clearly Explained!!!” <strong>Channel:</strong> StatQuest with Josh Starmer <strong>Link:</strong> <a href="https://www.youtube.com/watch?v=NGf0voTMlcs">https://www.youtube.com/watch?v=NGf0voTMlcs</a> <strong>Relevance:</strong> This video provides a very clear, intuitive explanation of <strong>LASSO regression</strong>. It aligns perfectly with Section 21.4 of the text, covering:
<ul>
<li>The motivation for LASSO (dealing with a large number of predictors).</li>
<li>The <span class="math inline">\(L_1\)</span> penalty and how it leads to variable selection (setting coefficients to exactly zero).</li>
<li>The concept of the tuning parameter <span class="math inline">\(\lambda\)</span>.</li>
<li>A visual explanation of how LASSO works geometrically.</li>
<li>The connection between LASSO and other methods (like best subset selection).</li>
</ul></li>
<li><strong>Title:</strong> “Lasso Regression - a Lasso in a Haystack | SciPy 2019 Tutorial | Gaël Varoquaux” <strong>Channel:</strong> Enthought <strong>Link:</strong> <a href="https://www.youtube.com/watch?v=74-SNkbp_6c">https://www.youtube.com/watch?v=74-SNkbp_6c</a> <strong>Relevance</strong>: Although this video uses Python instead of R, it is very valuable to understand LASSO. It covers:
<ul>
<li>The motivation of the Lasso method.</li>
<li>The concept of sparsity.</li>
<li>The meaning of the parameter s that appears in the alternative formulation of the LASSO method.</li>
</ul></li>
<li><strong>Title</strong>: 17.1 Lasso method - an introduction <strong>Channel</strong>: mathematicalmonk <strong>Link</strong>: <a href="https://www.youtube.com/watch?v=2P-e-AGv4DU">https://www.youtube.com/watch?v=2P-e-AGv4DU</a> <strong>Relevance</strong>: Introduction to the lasso method and motivation, using mathematical notation, similar to the text.</li>
</ol>
<p>These videos provide a mix of theoretical explanations, visual aids, and practical demonstrations (using R for AIC/BIC) that complement the material presented in Chapter 21. They are all currently available on YouTube at the provided links (verified on October 26, 2023).</p>
</section>
</section>
<section class="level2" id="multiple-choice-exercises">
<h2 class="anchored" data-anchor-id="multiple-choice-exercises">Multiple Choice Exercises</h2>
<section class="level3" id="sec-ch21mcexercise1">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise1">MC Exercise 1</h3>
<p><a href="#sec-ch21mcsolution1">MC Solution 1</a></p>
<p>Omitted variable bias occurs when:</p>
<ol type="a">
<li>An irrelevant variable is included in the regression.</li>
<li>A relevant variable is excluded from the regression, and it is correlated with an included regressor.</li>
<li>The error term is heteroskedastic.</li>
<li>The sample size is too small.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise2">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise2">MC Exercise 2</h3>
<p><a href="#sec-ch21mcsolution2">MC Solution 2</a></p>
<p>The direction of omitted variable bias depends on:</p>
<ol type="a">
<li>The sample size.</li>
<li>The variance of the error term.</li>
<li>The sign of the coefficient of the omitted variable and the correlation between the omitted and included variables.</li>
<li>Whether the model includes an intercept.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise3">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise3">MC Exercise 3</h3>
<p><a href="#sec-ch21mcsolution3">MC Solution 3</a></p>
<p>If you omit a relevant variable from a regression, the OLS estimator of the coefficient of an included variable is generally:</p>
<ol type="a">
<li>Unbiased.</li>
<li>Biased and inconsistent.</li>
<li>Biased but consistent.</li>
<li>Efficient.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise4">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise4">MC Exercise 4</h3>
<p><a href="#sec-ch21mcsolution4">MC Solution 4</a></p>
<p>Including irrelevant variables in a regression model will:</p>
<ol type="a">
<li>Decrease the variance of the OLS estimators.</li>
<li>Increase the variance of the OLS estimators.</li>
<li>Bias the OLS estimators.</li>
<li>Improve the adjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise5">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise5">MC Exercise 5</h3>
<p><a href="#sec-ch21mcsolution5">MC Solution 5</a></p>
<p>When irrelevant variables are included in a regression, the OLS estimators of the coefficients of the <em>relevant</em> variables are:</p>
<ol type="a">
<li>Biased.</li>
<li>Unbiased.</li>
<li>Inconsistent.</li>
<li>Biased and Inconsistent.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise6">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise6">MC Exercise 6</h3>
<p><a href="#sec-ch21mcsolution6">MC Solution 6</a></p>
<p>The adjusted <span class="math inline">\(R^2\)</span>:</p>
<ol type="a">
<li>Always increases when a new variable is added to the model.</li>
<li>Can be negative.</li>
<li>Penalizes the inclusion of additional variables.</li>
<li>Is always larger than the unadjusted <span class="math inline">\(R^2\)</span>.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise7">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise7">MC Exercise 7</h3>
<p><a href="#sec-ch21mcsolution7">MC Solution 7</a></p>
<p>Which of the following model selection criteria imposes the largest penalty for adding more variables?</p>
<ol type="a">
<li><span class="math inline">\(R^2\)</span></li>
<li>Adjusted <span class="math inline">\(R^2\)</span></li>
<li>AIC</li>
<li>BIC</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise8">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise8">MC Exercise 8</h3>
<p><a href="#sec-ch21mcsolution8">MC Solution 8</a></p>
<p>The Akaike Information Criterion (AIC) is defined as:</p>
<ol type="a">
<li><span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{K_j}{n}\)</span></li>
<li><span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{2K_j}{n}\)</span></li>
<li><span class="math inline">\(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n - K_j}\)</span></li>
<li><span class="math inline">\(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise9">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise9">MC Exercise 9</h3>
<p><a href="#sec-ch21mcsolution9">MC Solution 9</a></p>
<p>The Bayesian Information Criterion (BIC) is defined as:</p>
<ol type="a">
<li><span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{K_j}{n}\)</span></li>
<li><span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{2K_j}{n}\)</span></li>
<li><span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{K_j \ln(n)}{n}\)</span></li>
<li><span class="math inline">\(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}\)</span></li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise10">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise10">MC Exercise 10</h3>
<p><a href="#sec-ch21mcsolution10">MC Solution 10</a></p>
<p>When using AIC or BIC for model selection, you should choose the model with the:</p>
<ol type="a">
<li>Highest AIC and highest BIC.</li>
<li>Lowest AIC and lowest BIC.</li>
<li>Highest AIC and lowest BIC.</li>
<li>Lowest AIC and highest BIC.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise11">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise11">MC Exercise 11</h3>
<p><a href="#sec-ch21mcsolution11">MC Solution 11</a></p>
<p>The LASSO method is used for:</p>
<ol type="a">
<li>Estimating coefficients in a linear regression.</li>
<li>Variable selection.</li>
<li>Dealing with multicollinearity.</li>
<li>All of the above.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise12">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise12">MC Exercise 12</h3>
<p><a href="#sec-ch21mcsolution12">MC Solution 12</a></p>
<p>The LASSO uses which type of penalty?</p>
<ol type="a">
<li><span class="math inline">\(L_0\)</span> penalty</li>
<li><span class="math inline">\(L_1\)</span> penalty</li>
<li><span class="math inline">\(L_2\)</span> penalty</li>
<li>No penalty</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise13">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise13">MC Exercise 13</h3>
<p><a href="#sec-ch21mcsolution13">MC Solution 13</a></p>
<p>Ridge regression uses which type of penalty?</p>
<ol type="a">
<li><span class="math inline">\(L_0\)</span> penalty</li>
<li><span class="math inline">\(L_1\)</span> penalty</li>
<li><span class="math inline">\(L_2\)</span> penalty</li>
<li>No penalty</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise14">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise14">MC Exercise 14</h3>
<p><a href="#sec-ch21mcexercise14">MC Solution 14</a></p>
<p>As the tuning parameter <span class="math inline">\(\lambda\)</span> in LASSO increases, the number of non-zero coefficients in the model generally:</p>
<ol type="a">
<li>Increases.</li>
<li>Decreases.</li>
<li>Stays the same.</li>
<li>Becomes equal to the sample size.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise15">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise15">MC Exercise 15</h3>
<p><a href="#sec-ch21mcsolution15">MC Solution 15</a></p>
<p>The term “sparsity” in the context of LASSO refers to:</p>
<ol type="a">
<li>A large number of non-zero coefficients.</li>
<li>A small number of non-zero coefficients.</li>
<li>A large sample size.</li>
<li>A small sample size.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise16">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise16">MC Exercise 16</h3>
<p><a href="#sec-ch21mcexercise16">MC Solution 16</a></p>
<p>In the alternative formulation of the LASSO problem, what happens as the bound ‘s’ increases?</p>
<ol type="a">
<li>More variables enter the model.</li>
<li>Fewer variables enter the model.</li>
<li>The solution approaches the ridge regression solution.</li>
<li>The solution approaches the ordinary least squares solution.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise17">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise17">MC Exercise 17</h3>
<p><a href="#sec-ch21mcexercise17">MC Solution 17</a></p>
<p>Cross-validation in the context of LASSO is used to:</p>
<ol type="a">
<li>Estimate the coefficients.</li>
<li>Choose the optimal value of the tuning parameter.</li>
<li>Calculate the standard errors.</li>
<li>Test for heteroskedasticity.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise18">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise18">MC Exercise 18</h3>
<p><a href="#sec-ch21mcexercise18">MC Solution 18</a></p>
<p>Theorem 21.1 states that the adjusted R-squared falls when a variable is deleted if its t-statistic is:</p>
<ol type="a">
<li>Greater than 1 in absolute value</li>
<li>Less than 1 in absolute value</li>
<li>Greater than 2 in absolute value</li>
<li>Less than 2 in absolute value.</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise19">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise19">MC Exercise 19</h3>
<p><a href="#sec-ch21mcexercise19">MC Solution 19</a></p>
<p>When omitting a relevant variable, the usual OLS estimator of the error variance is:</p>
<ol type="a">
<li>Unbiased</li>
<li>Biased downwards</li>
<li>Biased upwards</li>
<li>Consistent</li>
</ol>
</section>
<section class="level3" id="sec-ch21mcexercise20">
<h3 class="anchored" data-anchor-id="sec-ch21mcexercise20">MC Exercise 20</h3>
<p><a href="#sec-ch21mcsolution20">MC Solution 20</a></p>
<p>The active set in the LASSO method is defined as:</p>
<ol type="a">
<li>The set of all variables in the model.</li>
<li>The set of variables with non-zero coefficients.</li>
<li>The set of variables with zero coefficients.</li>
<li>The set of variables used for cross-validation.</li>
</ol>
</section>
</section>
<section class="level2" id="multiple-choice-solutions">
<h2 class="anchored" data-anchor-id="multiple-choice-solutions">Multiple Choice Solutions</h2>
<section class="level3" id="sec-ch21mcsolution1">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution1">MC Solution 1</h3>
<p><a href="#sec-ch21mcexercise1">MC Exercise 1</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> Omitted variable bias arises when a variable that is both <em>relevant</em> (affects the dependent variable) and <em>correlated with an included regressor</em> is left out of the regression. This is the core concept of Section 21.1.</p>
</section>
<section class="level3" id="sec-ch21mcsolution2">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution2">MC Solution 2</h3>
<p><a href="#sec-ch21mcexercise2">MC Exercise 2</a></p>
<p><strong>Correct Answer: c)</strong></p>
<p><strong>Explanation:</strong> The direction (positive or negative) of the omitted variable bias depends on two factors: (1) the sign of the coefficient (<span class="math inline">\(\beta_2\)</span>) of the omitted variable in the true model, and (2) the sign of the correlation between the omitted variable and the included regressor(s). This is discussed in Section 21.1.</p>
</section>
<section class="level3" id="sec-ch21mcsolution3">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution3">MC Solution 3</h3>
<p><a href="#sec-ch21mcexercise3">MC Exercise 3</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> Omitting a relevant variable generally makes the OLS estimators of the included variables both biased (not centered on the true value) and inconsistent (does not converge to the true value as the sample size increases). This is a key result from Section 21.1.</p>
</section>
<section class="level3" id="sec-ch21mcsolution4">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution4">MC Solution 4</h3>
<p><a href="#sec-ch21mcexercise4">MC Exercise 4</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> Including irrelevant variables <em>increases</em> the variance of the OLS estimators. While the estimators remain unbiased, they become less precise. This is discussed in Section 21.2.</p>
</section>
<section class="level3" id="sec-ch21mcsolution5">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution5">MC Solution 5</h3>
<p><a href="#sec-ch21mcexercise5">MC Exercise 5</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> A crucial result from Section 21.2 is that including irrelevant variables does <em>not</em> bias the OLS estimators of the coefficients of the <em>relevant</em> variables. The estimators remain unbiased.</p>
</section>
<section class="level3" id="sec-ch21mcsolution6">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution6">MC Solution 6</h3>
<p><a href="#sec-ch21mcexercise6">MC Exercise 6</a></p>
<p><strong>Correct Answer: c)</strong></p>
<p><strong>Explanation:</strong> The adjusted <span class="math inline">\(R^2\)</span> is a modified version of <span class="math inline">\(R^2\)</span> that penalizes the inclusion of additional variables. It accounts for the degrees of freedom used by the model. This is explained in Section 21.3.</p>
</section>
<section class="level3" id="sec-ch21mcsolution7">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution7">MC Solution 7</h3>
<p><a href="#sec-ch21mcexercise7">MC Exercise 7</a></p>
<p><strong>Correct Answer: d)</strong></p>
<p><strong>Explanation:</strong> The Bayesian Information Criterion (BIC) imposes a larger penalty for additional variables than AIC or adjusted <span class="math inline">\(R^2\)</span> (especially for larger sample sizes). This is because the BIC’s penalty term includes <span class="math inline">\(\ln(n)\)</span>, where <span class="math inline">\(n\)</span> is the sample size. This is discussed in Section 21.3.</p>
</section>
<section class="level3" id="sec-ch21mcsolution8">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution8">MC Solution 8</h3>
<p><a href="#sec-ch21mcexercise8">MC Exercise 8</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> The AIC is defined as <span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{2K_j}{n}\)</span>, where <span class="math inline">\(K_j\)</span> is the number of parameters in model <span class="math inline">\(j\)</span>. This is Equation 21.3 in the text.</p>
</section>
<section class="level3" id="sec-ch21mcsolution9">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution9">MC Solution 9</h3>
<p><a href="#sec-ch21mcexercise9">MC Exercise 9</a></p>
<p><strong>Correct Answer: c)</strong></p>
<p><strong>Explanation:</strong> The BIC is defined as <span class="math inline">\(\ln(\frac{\hat{\epsilon_j}^T \hat{\epsilon_j}}{n}) + \frac{K_j \ln(n)}{n}\)</span>. This is Equation 21.4 in the text.</p>
</section>
<section class="level3" id="sec-ch21mcsolution10">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution10">MC Solution 10</h3>
<p><a href="#sec-ch21mcexercise10">MC Exercise 10</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> For both AIC and BIC, lower values indicate a better model fit, taking into account model complexity. So you want the model with the lowest AIC and the lowest BIC.</p>
</section>
<section class="level3" id="sec-ch21mcsolution11">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution11">MC Solution 11</h3>
<p><a href="#sec-ch21mcexercise11">MC Exercise 11</a></p>
<p><strong>Correct Answer: d)</strong></p>
<p><strong>Explanation:</strong> The LASSO (Least Absolute Shrinkage and Selection Operator) is a method that does all of these: estimates coefficients, performs variable selection by shrinking some coefficients to exactly zero, and helps with multicollinearity by reducing the impact of highly correlated predictors. This corresponds to Section 21.4.</p>
</section>
<section class="level3" id="sec-ch21mcsolution12">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution12">MC Solution 12</h3>
<p><a href="#sec-ch21mcexercise12">MC Exercise 12</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> LASSO uses an <span class="math inline">\(L_1\)</span> penalty, which is the sum of the absolute values of the coefficients: <span class="math inline">\(\lambda \sum_{j=1}^K |b_j|\)</span>. This is the defining characteristic of LASSO, as explained in Section 21.4.</p>
</section>
<section class="level3" id="sec-ch21mcsolution13">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution13">MC Solution 13</h3>
<p><a href="#sec-ch21mcexercise13">MC Exercise 13</a></p>
<p><strong>Correct Answer: c)</strong></p>
<p><strong>Explanation:</strong> Ridge regression, mentioned as a precursor to LASSO, uses an <span class="math inline">\(L_2\)</span> penalty which involves the sum of <em>squared</em> coefficients: <span class="math inline">\(\lambda \sum_{j=1}^K b_j^2\)</span>.</p>
</section>
<section class="level3" id="sec-ch21mcsolution14">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution14">MC Solution 14</h3>
<p><a href="#sec-ch21mcexercise14">MC Exercise 14</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> As the tuning parameter <span class="math inline">\(\lambda\)</span> in LASSO increases, the penalty on the magnitude of the coefficients becomes stronger. This forces more coefficients to be shrunk to zero, thus <em>decreasing</em> the number of non-zero coefficients. This is how LASSO performs variable selection.</p>
</section>
<section class="level3" id="sec-ch21mcsolution15">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution15">MC Solution 15</h3>
<p><a href="#sec-ch21mcexercise15">MC Exercise 15</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> Sparsity, in the context of LASSO and other regularization methods, means that only a <em>small number</em> of the potential predictors have non-zero coefficients in the true model (or in the estimated model).</p>
</section>
<section class="level3" id="sec-ch21mcsolution16">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution16">MC Solution 16</h3>
<p><a href="#sec-ch21mcexercise16">MC Exercise 16</a></p>
<p><strong>Correct Answer: d)</strong></p>
<p><strong>Explanation</strong>: The alternative formulation of the LASSO problem minimizes the residual sum of squares subject to the constraint <span class="math inline">\(\sum_{j=1}^K |b_j| \leq s\)</span>. As ‘s’ increases, the constraint becomes less restrictive. When ‘s’ is large enough, the constraint is no longer binding, and the LASSO solution becomes equivalent to the ordinary least squares (OLS) solution.</p>
</section>
<section class="level3" id="sec-ch21mcsolution17">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution17">MC Solution 17</h3>
<p><a href="#sec-ch21mcexercise17">MC Exercise 17</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> Cross-validation is a technique used to estimate the prediction error of a model and to choose the optimal value of the tuning parameter (<span class="math inline">\(\lambda\)</span> in LASSO) that minimizes this error.</p>
</section>
<section class="level3" id="sec-ch21mcsolution18">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution18">MC Solution 18</h3>
<p><a href="#sec-ch21mcexercise18">MC Exercise 18</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> Theorem 21.1 states that the adjusted <span class="math inline">\(R^2\)</span> will <em>fall</em> if a variable is deleted and its t-statistic is <em>greater</em> than 1 in absolute value. Conversely, the adjusted <span class="math inline">\(R^2\)</span> will rise if a variable is added and its t-statistic is less than one in absolute value.</p>
</section>
<section class="level3" id="sec-ch21mcsolution19">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution19">MC Solution 19</h3>
<p><a href="#sec-ch21mcexercise19">MC Exercise 19</a></p>
<p><strong>Correct Answer: c)</strong></p>
<p><strong>Explanation:</strong> When a relevant variable is omitted, the usual OLS estimator of the error variance (<span class="math inline">\(s_u^2\)</span>) is biased <em>upwards</em>. This is because the variation explained by the omitted variable is incorrectly attributed to the error term. This is shown mathematically in Section 21.1.</p>
</section>
<section class="level3" id="sec-ch21mcsolution20">
<h3 class="anchored" data-anchor-id="sec-ch21mcsolution20">MC Solution 20</h3>
<p><a href="#sec-ch21mcexercise20">MC Exercise 20</a></p>
<p><strong>Correct Answer: b)</strong></p>
<p><strong>Explanation:</strong> The active set in LASSO is the set of variables that have <em>non-zero</em> coefficients in the LASSO solution. These are the variables that LASSO has deemed important for predicting the outcome.</p>
</section>
</section>
</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>

</div> <!-- /content -->
<footer class="footer">
<div class="nav-footer">
<div class="nav-footer-left">
<p>Author: Peter Fuleky</p>
</div>
<div class="nav-footer-center">
       
    </div>
<div class="nav-footer-right">
<p>This book was built with <a href="https://quarto.org/">Quarto</a></p>
</div>
</div>
</footer>
</body></html>